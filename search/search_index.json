{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#neuro-py","title":"neuro-py","text":"<p>Analysis of neuroelectrophysiology data in Python.</p> CI/CD Package Repository Metadata"},{"location":"#overview","title":"Overview","text":"<p><code>neuro_py</code> is a Python package for analysis of neuroelectrophysiology data. It is built on top of the nelpy package, which provides core data objects. <code>neuro_py</code> provides a set of functions for analysis of freely moving electrophysiology, including behavior tracking utilities, neural ensemble detection, peri-event analyses, robust batch analysis tools, and more. </p> <p>Tutorials are here and more will be added. </p>"},{"location":"#installation","title":"Installation","text":"<pre><code>git clone\ncd neuro_py\npip install -e .\n</code></pre> <p>To sync the <code>nelpy</code> dependency to latest version, use following instead,</p> <pre><code>pip install -e . --force-reinstall --no-cache-dir\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>import neuro_py as npy\n</code></pre>"},{"location":"#dependencies","title":"Dependencies","text":"<p>For ease of use, this package uses <code>nelpy</code> core data objects. See nelpy </p>"},{"location":"#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.</p> <p>Please make sure to update tests as appropriate.</p>"},{"location":"#authors","title":"Authors","text":"<ul> <li>@ryanharvey1</li> <li>@lolaBerkowitz</li> <li>@kushaangupta</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> neuro_py<ul> <li> behavior</li> <li> detectors</li> <li> ensemble</li> <li> io</li> <li> lfp</li> <li> plotting</li> <li> process</li> <li> raw</li> <li> session</li> <li> spikes</li> <li> stats</li> <li> tuning</li> <li> util</li> </ul> </li> <li>behavior<ul> <li> cheeseboard</li> <li> get_trials</li> <li> kinematics</li> <li> linear_positions</li> <li> linearization_pipeline</li> <li> preprocessing</li> <li> well_traversal_classification</li> </ul> </li> <li>detectors<ul> <li> dentate_spike</li> <li> up_down_state</li> </ul> </li> <li>ensemble<ul> <li> assembly</li> <li> assembly_reactivation</li> <li> decoding</li> <li> dynamics</li> <li> explained_variance</li> <li> geometry</li> <li> pairwise_bias_correlation</li> <li> replay</li> <li> similarity_index</li> <li> similaritymat</li> </ul> </li> <li>decoding<ul> <li> bayesian</li> <li> lstm</li> <li> m2mlstm</li> <li> mlp</li> <li> pipeline</li> <li> preprocess</li> <li> transformer</li> </ul> </li> <li>io<ul> <li> loading</li> <li> saving</li> </ul> </li> <li>lfp<ul> <li> CSD</li> <li> preprocessing</li> <li> spectral</li> <li> theta_cycles</li> </ul> </li> <li>plotting<ul> <li> decorators</li> <li> events</li> <li> figure_helpers</li> </ul> </li> <li>process<ul> <li> batch_analysis</li> <li> correlations</li> <li> intervals</li> <li> peri_event</li> <li> precession_utils</li> <li> pychronux</li> <li> utils</li> </ul> </li> <li>raw<ul> <li> preprocessing</li> <li> spike_sorting</li> </ul> </li> <li>session<ul> <li> locate_epochs</li> </ul> </li> <li>spikes<ul> <li> spike_tools</li> </ul> </li> <li>stats<ul> <li> circ_stats</li> <li> regression</li> <li> stats</li> <li> system_identifier</li> </ul> </li> <li>tuning<ul> <li> fields</li> <li> maps</li> </ul> </li> <li>util<ul> <li> array</li> </ul> </li> </ul>"},{"location":"reference/neuro_py/","title":"neuro_py","text":""},{"location":"reference/neuro_py/behavior/","title":"neuro_py.behavior","text":""},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker","title":"<code>NodePicker</code>","text":"<p>Interactive creation of track graph by looking at video frames.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The matplotlib axes to draw on, by default None.</p> <code>None</code> <code>basepath</code> <code>str</code> <p>The base path for saving data, by default None.</p> <code>None</code> <code>node_color</code> <code>str</code> <p>The color of the nodes, by default \"#177ee6\".</p> <code>'#177ee6'</code> <code>node_size</code> <code>int</code> <p>The size of the nodes, by default 100.</p> <code>100</code> <code>epoch</code> <code>int</code> <p>The epoch number, by default None.</p> <code>None</code> <code>interval</code> <code>Tuple[float, float]</code> <code>None</code> <p>Attributes:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The matplotlib axes to draw on.</p> <code>canvas</code> <code>FigureCanvas</code> <p>The matplotlib figure canvas.</p> <code>cid</code> <code>int</code> <p>The connection id for the event handler.</p> <code>_nodes</code> <code>list</code> <p>The list of node positions.</p> <code>node_color</code> <code>str</code> <p>The color of the nodes.</p> <code>_nodes_plot</code> <code>scatter</code> <p>The scatter plot of the nodes.</p> <code>edges</code> <code>list</code> <p>The list of edges.</p> <code>basepath</code> <code>str</code> <p>The base path for saving data.</p> <code>epoch</code> <code>int</code> <p>The epoch number.</p> <code>use_HMM</code> <code>bool</code> <p>Whether to use the hidden markov model.</p> <p>Methods:</p> Name Description <code>node_positions</code> <p>Get the positions of the nodes.</p> <code>connect</code> <p>Connect the event handlers.</p> <code>disconnect</code> <p>Disconnect the event handlers.</p> <code>process_key</code> <p>Process key press events.</p> <code>click_event</code> <p>Process mouse click events.</p> <code>redraw</code> <p>Redraw the nodes and edges.</p> <code>remove_point</code> <p>Remove a point from the nodes.</p> <code>clear</code> <p>Clear all nodes and edges.</p> <code>format_and_save</code> <p>Format the data and save it to disk.</p> <code>save_nodes_edges</code> <p>Save the nodes and edges to a pickle file.</p> <code>save_nodes_edges_to_behavior</code> <p>Store nodes and edges into behavior file.</p> <p>Examples:</p>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker--in-command-line","title":"in command line","text":"<pre><code>&gt;&gt;&gt; python linearization_pipeline.py path/to/session\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker--for-a-specific-epoch","title":"for a specific epoch","text":"<pre><code>&gt;&gt;&gt; python linearization_pipeline.py path/to/session 1\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker--for-a-specific-interval","title":"for a specific interval","text":"<pre><code>&gt;&gt;&gt; python linearization_pipeline.py path/to/session 0 100\n</code></pre> References <p>https://github.com/LorenFrankLab/track_linearization</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>class NodePicker:\n    \"\"\"\n    Interactive creation of track graph by looking at video frames.\n\n    Parameters\n    ----------\n    ax : plt.Axes, optional\n        The matplotlib axes to draw on, by default None.\n    basepath : str, optional\n        The base path for saving data, by default None.\n    node_color : str, optional\n        The color of the nodes, by default \"#177ee6\".\n    node_size : int, optional\n        The size of the nodes, by default 100.\n    epoch : int, optional\n        The epoch number, by default None.\n    interval : Tuple[float, float], optional\n\n    Attributes\n    ----------\n    ax : plt.Axes\n        The matplotlib axes to draw on.\n    canvas : plt.FigureCanvas\n        The matplotlib figure canvas.\n    cid : int\n        The connection id for the event handler.\n    _nodes : list\n        The list of node positions.\n    node_color : str\n        The color of the nodes.\n    _nodes_plot : plt.scatter\n        The scatter plot of the nodes.\n    edges : list\n        The list of edges.\n    basepath : str\n        The base path for saving data.\n    epoch : int\n        The epoch number.\n    use_HMM : bool\n        Whether to use the hidden markov model.\n\n    Methods\n    -------\n    node_positions\n        Get the positions of the nodes.\n    connect\n        Connect the event handlers.\n    disconnect\n        Disconnect the event handlers.\n    process_key\n        Process key press events.\n    click_event\n        Process mouse click events.\n    redraw\n        Redraw the nodes and edges.\n    remove_point\n        Remove a point from the nodes.\n    clear\n        Clear all nodes and edges.\n    format_and_save\n        Format the data and save it to disk.\n    save_nodes_edges\n        Save the nodes and edges to a pickle file.\n    save_nodes_edges_to_behavior\n        Store nodes and edges into behavior file.\n\n    Examples\n    --------\n    # in command line\n    &gt;&gt;&gt; python linearization_pipeline.py path/to/session\n\n    # for a specific epoch\n    &gt;&gt;&gt; python linearization_pipeline.py path/to/session 1\n\n    # for a specific interval\n    &gt;&gt;&gt; python linearization_pipeline.py path/to/session 0 100\n\n    References\n    ----------\n    https://github.com/LorenFrankLab/track_linearization\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ax: Optional[plt.Axes] = None,\n        basepath: Optional[str] = None,\n        node_color: str = \"#177ee6\",\n        node_size: int = 100,\n        epoch: Optional[int] = None,\n        interval: Optional[Tuple[float, float]] = None,\n    ):\n        \"\"\"\n        Initialize the NodePicker.\n\n        Parameters\n        ----------\n        ax : plt.Axes, optional\n            The matplotlib axes to draw on, by default None.\n        basepath : str, optional\n            The base path for saving data, by default None.\n        node_color : str, optional\n            The color of the nodes, by default \"#177ee6\".\n        node_size : int, optional\n            The size of the nodes, by default 100.\n        epoch : int, optional\n            The epoch number, by default None.\n        \"\"\"\n        if ax is None:\n            ax = plt.gca()\n        self.ax = ax\n        self.canvas = ax.get_figure().canvas\n        self.cid = None\n        self._nodes = []\n        self.node_color = node_color\n        self._nodes_plot = ax.scatter([], [], zorder=5, s=node_size, color=node_color)\n        self.edges = [[]]\n        self.basepath = basepath\n        self.epoch = epoch\n        self.interval = interval\n        self.use_HMM = True\n\n        if self.epoch is not None:\n            self.epoch = int(self.epoch)\n\n        ax.set_title(\n            \"Left click to place node.\\nRight click to remove node.\"\n            \"\\nShift+Left click to clear nodes.\\nCntrl+Left click two nodes to place an edge\"\n            \"\\nEnter to save and exit.\",\n            fontsize=8,\n        )\n\n        self.canvas.draw()\n\n        self.connect()\n\n    @property\n    def node_positions(self) -&gt; np.ndarray:\n        \"\"\"\n        Get the positions of the nodes.\n\n        Returns\n        -------\n        np.ndarray\n            An array of node positions.\n        \"\"\"\n        return np.asarray(self._nodes)\n\n    def connect(self) -&gt; None:\n        \"\"\"\n        Connect the event handlers.\n        \"\"\"\n        print(\"Connecting to events\")\n        if self.cid is None:\n            self.cid = self.canvas.mpl_connect(\"button_press_event\", self.click_event)\n            self.canvas.mpl_connect(\"key_press_event\", self.process_key)\n            print(\"Mouse click event connected!\")  # Debugging\n\n    def disconnect(self) -&gt; None:\n        \"\"\"\n        Disconnect the event handlers.\n        \"\"\"\n        if self.cid is not None:\n            self.canvas.mpl_disconnect(self.cid)\n            self.cid = None\n\n    def process_key(self, event: Any) -&gt; None:\n        \"\"\"\n        Process key press events.\n\n        Parameters\n        ----------\n        event : Any\n            The key press event.\n        \"\"\"\n        if event.key == \"enter\":\n            self.format_and_save()\n\n    def click_event(self, event: Any) -&gt; None:\n        \"\"\"\n        Process mouse click events.\n\n        Parameters\n        ----------\n        event : Any\n            The mouse click event.\n        \"\"\"\n\n        print(\n            f\"Mouse clicked at: {event.xdata}, {event.ydata}, button: {event.button}, key: {event.key}\"\n        )  # Debugging\n        if not event.inaxes:\n            return\n\n        if event.key is None:  # Regular mouse clicks\n            if event.button == 1:  # Left click\n                self._nodes.append((event.xdata, event.ydata))\n            elif event.button == 3:  # Right click\n                self.remove_point((event.xdata, event.ydata))\n\n        elif event.key == \"shift\" and event.button == 1:  # Shift + Left click\n            self.clear()\n\n        elif (\n            event.key == \"control\" and event.button == 1\n        ):  # Ctrl + Left click (Edge creation)\n            if len(self._nodes) == 0:\n                return\n            point = (event.xdata, event.ydata)\n            distance_to_nodes = np.linalg.norm(self.node_positions - point, axis=1)\n            closest_node_ind = np.argmin(distance_to_nodes)\n            if len(self.edges[-1]) &lt; 2:\n                self.edges[-1].append(closest_node_ind)\n            else:\n                self.edges.append([closest_node_ind])\n\n        elif event.key == \"enter\":  # Pressing Enter\n            self.format_and_save()\n\n        self.redraw()\n\n    def redraw(self) -&gt; None:\n        \"\"\"\n        Redraw the nodes and edges.\n        \"\"\"\n        # Draw Node Circles\n        if len(self.node_positions) &gt; 0:\n            self._nodes_plot.set_offsets(self.node_positions)\n        else:\n            self._nodes_plot.set_offsets([])\n\n        # Draw Node Numbers\n        for ind, (x, y) in enumerate(self.node_positions):\n            self.ax.text(\n                x,\n                y,\n                ind,\n                zorder=6,\n                fontsize=10,\n                horizontalalignment=\"center\",\n                verticalalignment=\"center\",\n                clip_on=True,\n                bbox=None,\n                transform=self.ax.transData,\n            )\n        # Draw Edges\n        for edge in self.edges:\n            if len(edge) &gt; 1:\n                x1, y1 = self.node_positions[edge[0]]\n                x2, y2 = self.node_positions[edge[1]]\n                self.ax.plot(\n                    [x1, x2], [y1, y2], color=\"#1f8e4f\", linewidth=3, zorder=1000\n                )\n        self.canvas.draw()\n\n    def remove_point(self, point: Tuple[float, float]) -&gt; None:\n        \"\"\"\n        Remove a point from the nodes.\n\n        Parameters\n        ----------\n        point : Tuple[float, float]\n            The point to remove.\n        \"\"\"\n        if len(self._nodes) &gt; 0:\n            distance_to_nodes = np.linalg.norm(self.node_positions - point, axis=1)\n            closest_node_ind = np.argmin(distance_to_nodes)\n            self._nodes.pop(closest_node_ind)\n\n    def clear(self) -&gt; None:\n        \"\"\"\n        Clear all nodes and edges.\n        \"\"\"\n        self._nodes = []\n        self.edges = [[]]\n        self.redraw()\n\n    def format_and_save(self) -&gt; None:\n        \"\"\"\n        Format the data and save it to disk.\n        \"\"\"\n        behave_df = load_animal_behavior(self.basepath)\n\n        if self.epoch is not None:\n            epochs = load_epoch(self.basepath)\n\n            cur_epoch = (\n                ~np.isnan(behave_df.x)\n                &amp; (behave_df.time &gt;= epochs.iloc[self.epoch].startTime)\n                &amp; (behave_df.time &lt;= epochs.iloc[self.epoch].stopTime)\n            )\n        elif self.interval is not None:\n            cur_epoch = (\n                ~np.isnan(behave_df.x)\n                &amp; (behave_df.time &gt;= self.interval[0])\n                &amp; (behave_df.time &lt;= self.interval[1])\n            )\n        else:\n            cur_epoch = ~np.isnan(behave_df.x)\n\n        print(\"running hmm...\")\n        track_graph = make_track_graph(self.node_positions, self.edges)\n\n        position = np.vstack(\n            [behave_df[cur_epoch].x.values, behave_df[cur_epoch].y.values]\n        ).T\n\n        position_df = get_linearized_position(\n            position=position,\n            track_graph=track_graph,\n            edge_order=self.edges,\n            use_HMM=self.use_HMM,\n        )\n\n        print(\"saving to disk...\")\n        behave_df.loc[cur_epoch, \"linearized\"] = position_df.linear_position.values\n        behave_df.loc[cur_epoch, \"states\"] = position_df.track_segment_id.values\n        behave_df.loc[cur_epoch, \"projected_x_position\"] = (\n            position_df.projected_x_position.values\n        )\n        behave_df.loc[cur_epoch, \"projected_y_position\"] = (\n            position_df.projected_y_position.values\n        )\n\n        filename = glob.glob(os.path.join(self.basepath, \"*.animal.behavior.mat\"))[0]\n        data = loadmat(filename, simplify_cells=True)\n\n        data[\"behavior\"][\"position\"][\"linearized\"] = behave_df.linearized.values\n        data[\"behavior\"][\"states\"] = behave_df.states.values\n        data[\"behavior\"][\"position\"][\"projected_x\"] = (\n            behave_df.projected_x_position.values\n        )\n        data[\"behavior\"][\"position\"][\"projected_y\"] = (\n            behave_df.projected_y_position.values\n        )\n\n        # store nodes and edges within behavior file\n        data = self.save_nodes_edges_to_behavior(data, behave_df)\n\n        savemat(filename, data, long_field_names=True)\n\n        self.save_nodes_edges()\n        self.disconnect()\n        plt.close()\n\n    def save_nodes_edges(self) -&gt; None:\n        \"\"\"\n        Save the nodes and edges to a pickle file.\n        \"\"\"\n        results = {\"node_positions\": self.node_positions, \"edges\": self.edges}\n        save_file = os.path.join(self.basepath, \"linearization_nodes_edges.pkl\")\n        with open(save_file, \"wb\") as f:\n            pickle.dump(results, f)\n\n    def save_nodes_edges_to_behavior(self, data: dict, behave_df: pd.DataFrame) -&gt; dict:\n        \"\"\"\n        Store nodes and edges into behavior file.\n        Searches to find epochs with valid linearized coords.\n        Nodes and edges are stored within behavior.epochs{n}.{node_positions and edges}\n\n        Parameters\n        ----------\n        data : dict\n            The behavior data dictionary.\n        behave_df : pd.DataFrame\n            The DataFrame containing behavior data.\n\n        Returns\n        -------\n        dict\n            The updated behavior data dictionary.\n        \"\"\"\n        if self.epoch is None and self.interval is None:\n            # load epochs\n            epochs = load_epoch(self.basepath)\n            # iter over each epoch\n            for epoch_i, ep in enumerate(epochs.itertuples()):\n                # locate index for given epoch\n                idx = behave_df.time.between(ep.startTime, ep.stopTime)\n                # if linearized is not all nan, add nodes and edges\n                if not all(np.isnan(behave_df[idx].linearized)) &amp; (\n                    behave_df[idx].shape[0] != 0\n                ):\n                    # adding nodes and edges\n                    data[\"behavior\"][\"epochs\"][epoch_i][\"node_positions\"] = (\n                        self.node_positions\n                    )\n                    data[\"behavior\"][\"epochs\"][epoch_i][\"edges\"] = self.edges\n        elif self.interval is not None:\n            # if interval was used, add nodes and edges just the epochs within that interval\n            epochs = load_epoch(self.basepath)\n            for epoch_i, ep in enumerate(epochs.itertuples()):\n                # amount of overlap between interval and epoch\n                start_overlap = max(self.interval[0], ep.startTime)\n                end_overlap = min(self.interval[1], ep.stopTime)\n                overlap = max(0, end_overlap - start_overlap)\n\n                # if overlap is greater than 1 second, add nodes and edges\n                if overlap &gt; 1:\n                    data[\"behavior\"][\"epochs\"][epoch_i][\"node_positions\"] = (\n                        self.node_positions\n                    )\n                    data[\"behavior\"][\"epochs\"][epoch_i][\"edges\"] = self.edges\n        else:\n            # if epoch was used, add nodes and edges just that that epoch\n            data[\"behavior\"][\"epochs\"][self.epoch][\"node_positions\"] = (\n                self.node_positions\n            )\n            data[\"behavior\"][\"epochs\"][self.epoch][\"edges\"] = self.edges\n\n        return data\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker.node_positions","title":"<code>node_positions</code>  <code>property</code>","text":"<p>Get the positions of the nodes.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of node positions.</p>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker.clear","title":"<code>clear()</code>","text":"<p>Clear all nodes and edges.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clear all nodes and edges.\n    \"\"\"\n    self._nodes = []\n    self.edges = [[]]\n    self.redraw()\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker.click_event","title":"<code>click_event(event)</code>","text":"<p>Process mouse click events.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Any</code> <p>The mouse click event.</p> required Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def click_event(self, event: Any) -&gt; None:\n    \"\"\"\n    Process mouse click events.\n\n    Parameters\n    ----------\n    event : Any\n        The mouse click event.\n    \"\"\"\n\n    print(\n        f\"Mouse clicked at: {event.xdata}, {event.ydata}, button: {event.button}, key: {event.key}\"\n    )  # Debugging\n    if not event.inaxes:\n        return\n\n    if event.key is None:  # Regular mouse clicks\n        if event.button == 1:  # Left click\n            self._nodes.append((event.xdata, event.ydata))\n        elif event.button == 3:  # Right click\n            self.remove_point((event.xdata, event.ydata))\n\n    elif event.key == \"shift\" and event.button == 1:  # Shift + Left click\n        self.clear()\n\n    elif (\n        event.key == \"control\" and event.button == 1\n    ):  # Ctrl + Left click (Edge creation)\n        if len(self._nodes) == 0:\n            return\n        point = (event.xdata, event.ydata)\n        distance_to_nodes = np.linalg.norm(self.node_positions - point, axis=1)\n        closest_node_ind = np.argmin(distance_to_nodes)\n        if len(self.edges[-1]) &lt; 2:\n            self.edges[-1].append(closest_node_ind)\n        else:\n            self.edges.append([closest_node_ind])\n\n    elif event.key == \"enter\":  # Pressing Enter\n        self.format_and_save()\n\n    self.redraw()\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker.connect","title":"<code>connect()</code>","text":"<p>Connect the event handlers.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"\n    Connect the event handlers.\n    \"\"\"\n    print(\"Connecting to events\")\n    if self.cid is None:\n        self.cid = self.canvas.mpl_connect(\"button_press_event\", self.click_event)\n        self.canvas.mpl_connect(\"key_press_event\", self.process_key)\n        print(\"Mouse click event connected!\")  # Debugging\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker.disconnect","title":"<code>disconnect()</code>","text":"<p>Disconnect the event handlers.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def disconnect(self) -&gt; None:\n    \"\"\"\n    Disconnect the event handlers.\n    \"\"\"\n    if self.cid is not None:\n        self.canvas.mpl_disconnect(self.cid)\n        self.cid = None\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker.format_and_save","title":"<code>format_and_save()</code>","text":"<p>Format the data and save it to disk.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def format_and_save(self) -&gt; None:\n    \"\"\"\n    Format the data and save it to disk.\n    \"\"\"\n    behave_df = load_animal_behavior(self.basepath)\n\n    if self.epoch is not None:\n        epochs = load_epoch(self.basepath)\n\n        cur_epoch = (\n            ~np.isnan(behave_df.x)\n            &amp; (behave_df.time &gt;= epochs.iloc[self.epoch].startTime)\n            &amp; (behave_df.time &lt;= epochs.iloc[self.epoch].stopTime)\n        )\n    elif self.interval is not None:\n        cur_epoch = (\n            ~np.isnan(behave_df.x)\n            &amp; (behave_df.time &gt;= self.interval[0])\n            &amp; (behave_df.time &lt;= self.interval[1])\n        )\n    else:\n        cur_epoch = ~np.isnan(behave_df.x)\n\n    print(\"running hmm...\")\n    track_graph = make_track_graph(self.node_positions, self.edges)\n\n    position = np.vstack(\n        [behave_df[cur_epoch].x.values, behave_df[cur_epoch].y.values]\n    ).T\n\n    position_df = get_linearized_position(\n        position=position,\n        track_graph=track_graph,\n        edge_order=self.edges,\n        use_HMM=self.use_HMM,\n    )\n\n    print(\"saving to disk...\")\n    behave_df.loc[cur_epoch, \"linearized\"] = position_df.linear_position.values\n    behave_df.loc[cur_epoch, \"states\"] = position_df.track_segment_id.values\n    behave_df.loc[cur_epoch, \"projected_x_position\"] = (\n        position_df.projected_x_position.values\n    )\n    behave_df.loc[cur_epoch, \"projected_y_position\"] = (\n        position_df.projected_y_position.values\n    )\n\n    filename = glob.glob(os.path.join(self.basepath, \"*.animal.behavior.mat\"))[0]\n    data = loadmat(filename, simplify_cells=True)\n\n    data[\"behavior\"][\"position\"][\"linearized\"] = behave_df.linearized.values\n    data[\"behavior\"][\"states\"] = behave_df.states.values\n    data[\"behavior\"][\"position\"][\"projected_x\"] = (\n        behave_df.projected_x_position.values\n    )\n    data[\"behavior\"][\"position\"][\"projected_y\"] = (\n        behave_df.projected_y_position.values\n    )\n\n    # store nodes and edges within behavior file\n    data = self.save_nodes_edges_to_behavior(data, behave_df)\n\n    savemat(filename, data, long_field_names=True)\n\n    self.save_nodes_edges()\n    self.disconnect()\n    plt.close()\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker.process_key","title":"<code>process_key(event)</code>","text":"<p>Process key press events.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Any</code> <p>The key press event.</p> required Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def process_key(self, event: Any) -&gt; None:\n    \"\"\"\n    Process key press events.\n\n    Parameters\n    ----------\n    event : Any\n        The key press event.\n    \"\"\"\n    if event.key == \"enter\":\n        self.format_and_save()\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker.redraw","title":"<code>redraw()</code>","text":"<p>Redraw the nodes and edges.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def redraw(self) -&gt; None:\n    \"\"\"\n    Redraw the nodes and edges.\n    \"\"\"\n    # Draw Node Circles\n    if len(self.node_positions) &gt; 0:\n        self._nodes_plot.set_offsets(self.node_positions)\n    else:\n        self._nodes_plot.set_offsets([])\n\n    # Draw Node Numbers\n    for ind, (x, y) in enumerate(self.node_positions):\n        self.ax.text(\n            x,\n            y,\n            ind,\n            zorder=6,\n            fontsize=10,\n            horizontalalignment=\"center\",\n            verticalalignment=\"center\",\n            clip_on=True,\n            bbox=None,\n            transform=self.ax.transData,\n        )\n    # Draw Edges\n    for edge in self.edges:\n        if len(edge) &gt; 1:\n            x1, y1 = self.node_positions[edge[0]]\n            x2, y2 = self.node_positions[edge[1]]\n            self.ax.plot(\n                [x1, x2], [y1, y2], color=\"#1f8e4f\", linewidth=3, zorder=1000\n            )\n    self.canvas.draw()\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker.remove_point","title":"<code>remove_point(point)</code>","text":"<p>Remove a point from the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>Tuple[float, float]</code> <p>The point to remove.</p> required Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def remove_point(self, point: Tuple[float, float]) -&gt; None:\n    \"\"\"\n    Remove a point from the nodes.\n\n    Parameters\n    ----------\n    point : Tuple[float, float]\n        The point to remove.\n    \"\"\"\n    if len(self._nodes) &gt; 0:\n        distance_to_nodes = np.linalg.norm(self.node_positions - point, axis=1)\n        closest_node_ind = np.argmin(distance_to_nodes)\n        self._nodes.pop(closest_node_ind)\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker.save_nodes_edges","title":"<code>save_nodes_edges()</code>","text":"<p>Save the nodes and edges to a pickle file.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def save_nodes_edges(self) -&gt; None:\n    \"\"\"\n    Save the nodes and edges to a pickle file.\n    \"\"\"\n    results = {\"node_positions\": self.node_positions, \"edges\": self.edges}\n    save_file = os.path.join(self.basepath, \"linearization_nodes_edges.pkl\")\n    with open(save_file, \"wb\") as f:\n        pickle.dump(results, f)\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.NodePicker.save_nodes_edges_to_behavior","title":"<code>save_nodes_edges_to_behavior(data, behave_df)</code>","text":"<p>Store nodes and edges into behavior file. Searches to find epochs with valid linearized coords. Nodes and edges are stored within behavior.epochs{n}.{node_positions and edges}</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The behavior data dictionary.</p> required <code>behave_df</code> <code>DataFrame</code> <p>The DataFrame containing behavior data.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The updated behavior data dictionary.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def save_nodes_edges_to_behavior(self, data: dict, behave_df: pd.DataFrame) -&gt; dict:\n    \"\"\"\n    Store nodes and edges into behavior file.\n    Searches to find epochs with valid linearized coords.\n    Nodes and edges are stored within behavior.epochs{n}.{node_positions and edges}\n\n    Parameters\n    ----------\n    data : dict\n        The behavior data dictionary.\n    behave_df : pd.DataFrame\n        The DataFrame containing behavior data.\n\n    Returns\n    -------\n    dict\n        The updated behavior data dictionary.\n    \"\"\"\n    if self.epoch is None and self.interval is None:\n        # load epochs\n        epochs = load_epoch(self.basepath)\n        # iter over each epoch\n        for epoch_i, ep in enumerate(epochs.itertuples()):\n            # locate index for given epoch\n            idx = behave_df.time.between(ep.startTime, ep.stopTime)\n            # if linearized is not all nan, add nodes and edges\n            if not all(np.isnan(behave_df[idx].linearized)) &amp; (\n                behave_df[idx].shape[0] != 0\n            ):\n                # adding nodes and edges\n                data[\"behavior\"][\"epochs\"][epoch_i][\"node_positions\"] = (\n                    self.node_positions\n                )\n                data[\"behavior\"][\"epochs\"][epoch_i][\"edges\"] = self.edges\n    elif self.interval is not None:\n        # if interval was used, add nodes and edges just the epochs within that interval\n        epochs = load_epoch(self.basepath)\n        for epoch_i, ep in enumerate(epochs.itertuples()):\n            # amount of overlap between interval and epoch\n            start_overlap = max(self.interval[0], ep.startTime)\n            end_overlap = min(self.interval[1], ep.stopTime)\n            overlap = max(0, end_overlap - start_overlap)\n\n            # if overlap is greater than 1 second, add nodes and edges\n            if overlap &gt; 1:\n                data[\"behavior\"][\"epochs\"][epoch_i][\"node_positions\"] = (\n                    self.node_positions\n                )\n                data[\"behavior\"][\"epochs\"][epoch_i][\"edges\"] = self.edges\n    else:\n        # if epoch was used, add nodes and edges just that that epoch\n        data[\"behavior\"][\"epochs\"][self.epoch][\"node_positions\"] = (\n            self.node_positions\n        )\n        data[\"behavior\"][\"epochs\"][self.epoch][\"edges\"] = self.edges\n\n    return data\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.enter_exit_target","title":"<code>enter_exit_target(position, target, max_distance=1.0)</code>","text":"<p>Marks when a position has reached a target (\"enter\") and when it has left a target (\"exit\").</p> <p>The position is considered to have reached a target when it is less than the <code>max_distance</code> from the target.</p> <p>Enter and exit times are marked as follows:  1: entered the target radius  0: neither -1: exited the target radius</p> <p>Works for 1D position and 2D position.</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>Union[ndarray, list]</code> <p>Array or list of shape (n_time, n_space).</p> required <code>target</code> <code>Union[ndarray, list]</code> <p>Array or list of shape (1, n_space).</p> required <code>max_distance</code> <code>float</code> <p>How close the position is to the target to be considered at the target, by default 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple of two arrays: - The first array contains the enter/exit times. - The second array contains the times when the position is at the target.</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def enter_exit_target(\n    position: Union[np.ndarray, list],\n    target: Union[np.ndarray, list],\n    max_distance: float = 1.0,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Marks when a position has reached a target (\"enter\") and when it has left a target (\"exit\").\n\n    The position is considered to have reached a target when it is less than\n    the `max_distance` from the target.\n\n    Enter and exit times are marked as follows:\n     1: entered the target radius\n     0: neither\n    -1: exited the target radius\n\n    Works for 1D position and 2D position.\n\n    Parameters\n    ----------\n    position : Union[np.ndarray, list]\n        Array or list of shape (n_time, n_space).\n    target : Union[np.ndarray, list]\n        Array or list of shape (1, n_space).\n    max_distance : float, optional\n        How close the position is to the target to be considered at the target, by default 1.0.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        Tuple of two arrays:\n        - The first array contains the enter/exit times.\n        - The second array contains the times when the position is at the target.\n    \"\"\"\n    distance_from_target = paired_distances(position, target)\n    at_target = distance_from_target &lt; max_distance\n    enter_exit = np.r_[0, np.diff(at_target.astype(float))]\n    return enter_exit, at_target\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.enter_exit_target_dio","title":"<code>enter_exit_target_dio(dio_indicator)</code>","text":"<p>Marks when a digital input/output (DIO) indicator has entered or exited a target state.</p> <p>Parameters:</p> Name Type Description Default <code>dio_indicator</code> <code>ndarray</code> <p>Array of DIO indicator values.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple containing: - enter_exit: np.ndarray     Array indicating enter (1) and exit (-1) events. - at_target: np.ndarray     Array indicating whether the target is active (1) or not (0).</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def enter_exit_target_dio(dio_indicator: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Marks when a digital input/output (DIO) indicator has entered or exited a target state.\n\n    Parameters\n    ----------\n    dio_indicator : np.ndarray\n        Array of DIO indicator values.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing:\n        - enter_exit: np.ndarray\n            Array indicating enter (1) and exit (-1) events.\n        - at_target: np.ndarray\n            Array indicating whether the target is active (1) or not (0).\n    \"\"\"\n    at_target = (dio_indicator &gt; 0).astype(np.float16)\n    enter_exit = np.r_[0, np.diff(at_target)]\n    return enter_exit, at_target\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.filter_tracker_jumps","title":"<code>filter_tracker_jumps(beh_df, max_speed=100)</code>","text":"<p>Filter out tracker jumps (to NaN) in the behavior data.</p> <p>Parameters:</p> Name Type Description Default <code>beh_df</code> <code>DataFrame</code> <p>Behavior data with columns x, y, and ts.</p> required <code>max_speed</code> <code>Union[int, float]</code> <p>Maximum allowed speed in cm per second.</p> <code>100</code> <p>Returns:</p> Type Description <code>DataFrame</code> Notes <p>Will force dtypes of x and y to float64</p> Source code in <code>neuro_py/behavior/preprocessing.py</code> <pre><code>def filter_tracker_jumps(\n    beh_df: pd.DataFrame, max_speed: Union[int, float] = 100\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter out tracker jumps (to NaN) in the behavior data.\n\n    Parameters\n    ----------\n    beh_df : pd.DataFrame\n        Behavior data with columns x, y, and ts.\n    max_speed : Union[int,float], optional\n        Maximum allowed speed in cm per second.\n\n    Returns\n    -------\n    pd.DataFrame\n\n    Notes\n    -----\n    Will force dtypes of x and y to float64\n    \"\"\"\n\n    # Calculate the Euclidean distance between consecutive points\n    beh_df[\"dx\"] = beh_df[\"x\"].diff()\n    beh_df[\"dy\"] = beh_df[\"y\"].diff()\n    beh_df[\"distance\"] = np.sqrt(beh_df[\"dx\"] ** 2 + beh_df[\"dy\"] ** 2)\n\n    # Calculate the time difference between consecutive timestamps\n    beh_df[\"dt\"] = beh_df[\"ts\"].diff()\n\n    # Calculate the speed between consecutive points (distance / time)\n    beh_df[\"speed\"] = beh_df[\"distance\"] / beh_df[\"dt\"]\n\n    # Identify the start of each jump\n    # A jump starts when the speed exceeds the threshold, and the previous speed did not\n    jump_starts = (beh_df[\"speed\"] &gt; max_speed) &amp; (\n        beh_df[\"speed\"].shift(1) &lt;= max_speed\n    )\n\n    # Mark x and y as NaN only for the first frame of each jump\n    beh_df.loc[jump_starts, [\"x\", \"y\"]] = np.nan\n\n    beh_df = beh_df.drop(columns=[\"dx\", \"dy\", \"distance\", \"dt\", \"speed\"])\n\n    return beh_df\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.filter_tracker_jumps_in_file","title":"<code>filter_tracker_jumps_in_file(basepath, epoch_number=None, epoch_interval=None)</code>","text":"<p>Filter out tracker jumps in the behavior data (to NaN) and save the filtered data back to the file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Basepath to the behavior file.</p> required <code>epoch_number</code> <code>int</code> <p>Epoch number to filter the behavior data to.</p> <code>None</code> <code>epoch_interval</code> <code>tuple</code> <p>Epoch interval to filter the behavior data to.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; basepath = \"path/to/behavior/file\"\n&gt;&gt;&gt; filter_tracker_jumps_in_file(basepath, epoch_number=1)\n</code></pre> Source code in <code>neuro_py/behavior/preprocessing.py</code> <pre><code>def filter_tracker_jumps_in_file(\n    basepath: str, epoch_number=None, epoch_interval=None\n) -&gt; None:\n    \"\"\"\n    Filter out tracker jumps in the behavior data (to NaN) and save the filtered data back to the file.\n\n    Parameters\n    ----------\n    basepath : str\n        Basepath to the behavior file.\n    epoch_number : int, optional\n        Epoch number to filter the behavior data to.\n    epoch_interval : tuple, optional\n        Epoch interval to filter the behavior data to.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; basepath = \"path/to/behavior/file\"\n    &gt;&gt;&gt; filter_tracker_jumps_in_file(basepath, epoch_number=1)\n    \"\"\"\n\n    # Load the behavior data\n    file = os.path.join(basepath, os.path.basename(basepath) + \"animal.behavior.mat\")\n\n    behavior = loadmat(file, simplify_cells=True)\n\n    # Filter the behavior data to remove tracker jumps\n    if epoch_number is not None:\n        epoch_df = npy.io.load_epoch(basepath)\n        idx = (\n            behavior[\"behavior\"][\"timestamps\"] &gt; epoch_df.loc[epoch_number].startTime\n        ) &amp; (behavior[\"behavior\"][\"timestamps\"] &lt; epoch_df.loc[epoch_number].stopTime)\n    elif epoch_interval is not None:\n        idx = (behavior[\"behavior\"][\"timestamps\"] &gt; epoch_interval[0]) &amp; (\n            behavior[\"behavior\"][\"timestamps\"] &lt; epoch_interval[1]\n        )\n    else:\n        # bool length of the same length as the number of timestamps\n        idx = np.ones(len(behavior[\"behavior\"][\"timestamps\"]), dtype=bool)\n\n    # Filter the behavior data and add to dataframe\n    x = behavior[\"behavior\"][\"position\"][\"x\"][idx]\n    y = behavior[\"behavior\"][\"position\"][\"y\"][idx]\n    ts = behavior[\"behavior\"][\"timestamps\"][idx]\n    beh_df = pd.DataFrame({\"x\": x, \"y\": y, \"ts\": ts})\n\n    # Filter out tracker jumps\n    beh_df = filter_tracker_jumps(beh_df)\n\n    # Save the filtered behavior data back to the file\n    behavior[\"behavior\"][\"position\"][\"x\"][idx] = beh_df.x.values\n    behavior[\"behavior\"][\"position\"][\"y\"][idx] = beh_df.y.values\n\n    savemat(file, behavior, long_field_names=True)\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.find_good_lap_epochs","title":"<code>find_good_lap_epochs(pos, dir_epoch, thres=0.5, binsize=6, min_laps=10)</code>","text":"<p>Find good laps in behavior data</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>AnalogSignalArray</code> <p>A nelpy AnalogSignalArray containing the position data with a single dimension.</p> required <code>dir_epoch</code> <code>EpochArray</code> <p>EpochArray defining the laps to analyze for good laps.</p> required <code>thres</code> <code>float</code> <p>Occupancy threshold to determine good laps, by default 0.5.</p> <code>0.5</code> <code>binsize</code> <code>int</code> <p>Size of the bins for calculating occupancy, by default 6.</p> <code>6</code> <code>min_laps</code> <code>int</code> <p>Minimum number of laps required to consider laps as 'good', by default 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>EpochArray</code> <p>An EpochArray containing the good laps based on the occupancy threshold. Returns an empty EpochArray if no good laps are found or if the number of laps is less than <code>min_laps</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; good_laps = find_good_lap_epochs(pos, dir_epoch)\n</code></pre> Notes <p>The function calculates the percent occupancy over position bins per lap, and identifies laps that meet the occupancy threshold criteria. The laps that meet this condition are returned as an EpochArray.</p> Source code in <code>neuro_py/behavior/linear_positions.py</code> <pre><code>def find_good_lap_epochs(\n    pos: nel.AnalogSignalArray,\n    dir_epoch: nel.EpochArray,\n    thres: float = 0.5,\n    binsize: int = 6,\n    min_laps: int = 10,\n) -&gt; nel.EpochArray:\n    \"\"\"\n    Find good laps in behavior data\n\n    Parameters\n    ----------\n    pos : nelpy.AnalogSignalArray\n        A nelpy AnalogSignalArray containing the position data with a single dimension.\n    dir_epoch : nelpy.EpochArray\n        EpochArray defining the laps to analyze for good laps.\n    thres : float, optional\n        Occupancy threshold to determine good laps, by default 0.5.\n    binsize : int, optional\n        Size of the bins for calculating occupancy, by default 6.\n    min_laps : int, optional\n        Minimum number of laps required to consider laps as 'good', by default 10.\n\n    Returns\n    -------\n    nelpy.EpochArray\n        An EpochArray containing the good laps based on the occupancy threshold.\n        Returns an empty EpochArray if no good laps are found or if the number\n        of laps is less than `min_laps`.\n\n    Examples\n    -------\n    &gt;&gt;&gt; good_laps = find_good_lap_epochs(pos, dir_epoch)\n\n    Notes\n    -----\n    The function calculates the percent occupancy over position bins per lap,\n    and identifies laps that meet the occupancy threshold criteria. The laps\n    that meet this condition are returned as an EpochArray.\n    \"\"\"\n    # Ensure the input data is valid\n    if pos.isempty or dir_epoch.isempty:\n        return nel.EpochArray()\n\n    # make bin edges to calc occupancy\n    x_edges = np.arange(np.nanmin(pos.data[0]), np.nanmax(pos.data[0]), binsize)\n    # initialize occupancy matrix (position x time)\n    occ = np.zeros([len(x_edges) - 1, dir_epoch.n_intervals])\n\n    # much faster to not use nelpy objects here, so pull out needed data\n    x_coord = pos.data[0]\n    time = pos.abscissa_vals\n    epochs = dir_epoch.data\n\n    # iterate through laps\n    for i, ep in enumerate(epochs):\n        # bin position per lap\n        occ[:, i], _ = np.histogram(\n            x_coord[(time &gt;= ep[0]) &amp; (time &lt;= ep[1])], bins=x_edges\n        )\n\n    # calc percent occupancy over position bins per lap and find good laps\n    good_laps = np.where(~((np.sum(occ == 0, axis=0) / occ.shape[0]) &gt; thres))[0]\n    # if no good laps, return empty epoch\n    if (len(good_laps) == 0) | (len(good_laps) &lt; min_laps):\n        dir_epoch = nel.EpochArray()\n    else:\n        dir_epoch = dir_epoch[good_laps]\n    return dir_epoch\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.find_last_non_center_well","title":"<code>find_last_non_center_well(segments_df, segment_ind)</code>","text":"<p>Find the last non-center well before the given segment index.</p> <p>Parameters:</p> Name Type Description Default <code>segments_df</code> <code>DataFrame</code> <p>DataFrame containing segment information.</p> required <code>segment_ind</code> <code>int</code> <p>The segment index to search up to.</p> required <p>Returns:</p> Type Description <code>Union[str, int]</code> <p>The last non-center well before the given segment index. If no non-center wells are found, returns an empty string.</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def find_last_non_center_well(\n    segments_df: pd.DataFrame, segment_ind: int\n) -&gt; Union[str, int]:\n    \"\"\"\n    Find the last non-center well before the given segment index.\n\n    Parameters\n    ----------\n    segments_df : pd.DataFrame\n        DataFrame containing segment information.\n    segment_ind : int\n        The segment index to search up to.\n\n    Returns\n    -------\n    Union[str, int]\n        The last non-center well before the given segment index. If no non-center wells are found,\n        returns an empty string.\n    \"\"\"\n    last_wells = segments_df.iloc[:segment_ind].to_well\n    try:\n        return last_wells[last_wells != \"Center\"].iloc[-1]\n    except IndexError:\n        # There are no non-center wells. Just return current well.\n        return \"\"\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.get_cheeseboard_trials","title":"<code>get_cheeseboard_trials(basepath, min_distance_from_home=15, max_trial_time=600, min_trial_time=5, kernel_size=2, min_std_away_from_home=6)</code>","text":"<p>Get epochs of cheeseboard trials.</p> <p>This function retrieves epochs for cheeseboard trials based on specified  distance and time criteria.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the session data.</p> required <code>min_distance_from_home</code> <code>int</code> <p>The minimum distance from home to be considered a trial. Default is 15.</p> <code>15</code> <code>max_trial_time</code> <code>int</code> <p>The maximum duration of a trial in seconds. Default is 600 (10 minutes).</p> <code>600</code> <code>min_trial_time</code> <code>int</code> <p>The minimum duration of a trial in seconds. Default is 5.</p> <code>5</code> <code>kernel_size</code> <code>int</code> <p>The size of the kernel to use for smoothing. Default is 2.</p> <code>2</code> <code>min_std_away_from_home</code> <code>int</code> <p>The minimum standard deviation away from home to be considered a trial. Default is 6.</p> <code>6</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>PositionArray</code> <p>The position data for the cheeseboard trials.</p> <code>trials</code> <code>EpochArray</code> <p>The epochs of the trials.</p> Notes <p>This function requires the following metadata dependencies:</p> <ul> <li><code>animal.behavior.mat</code>: contains homebox_x and homebox_y coordinates within epochs.</li> </ul> <p>You can label these with <code>label_key_locations_cheeseboard.m</code> or manually.</p> Source code in <code>neuro_py/behavior/get_trials.py</code> <pre><code>def get_cheeseboard_trials(\n    basepath: str,\n    min_distance_from_home: int = 15,\n    max_trial_time: int = 600,  # Default is 60 * 10\n    min_trial_time: int = 5,\n    kernel_size: int = 2,\n    min_std_away_from_home: int = 6,\n) -&gt; Tuple[nel.PositionArray, nel.EpochArray]:\n    \"\"\"\n    Get epochs of cheeseboard trials.\n\n    This function retrieves epochs for cheeseboard trials based on specified \n    distance and time criteria.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the session data.\n    min_distance_from_home : int, optional\n        The minimum distance from home to be considered a trial. Default is 15.\n    max_trial_time : int, optional\n        The maximum duration of a trial in seconds. Default is 600 (10 minutes).\n    min_trial_time : int, optional\n        The minimum duration of a trial in seconds. Default is 5.\n    kernel_size : int, optional\n        The size of the kernel to use for smoothing. Default is 2.\n    min_std_away_from_home : int, optional\n        The minimum standard deviation away from home to be considered a trial.\n        Default is 6.\n\n    Returns\n    -------\n    pos : PositionArray\n        The position data for the cheeseboard trials.\n    trials : EpochArray\n        The epochs of the trials.\n\n    Notes\n    -----\n    This function requires the following metadata dependencies:\n\n    - `animal.behavior.mat`: contains homebox_x and homebox_y coordinates within epochs.\n\n    You can label these with `label_key_locations_cheeseboard.m` or manually.\n    \"\"\"\n\n    # load position and key location metadata\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".animal.behavior.mat\"\n    )\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    # load epochs and place in array\n    epoch_df = loading.load_epoch(basepath)\n    epoch = nel.EpochArray(\n        [np.array([epoch_df.startTime, epoch_df.stopTime]).T], label=\"session_epochs\"\n    )\n\n    # load position and place in array\n    position_df = loading.load_animal_behavior(basepath)\n    position_df_no_nan = position_df.query(\"not x.isnull() &amp; not y.isnull()\")\n    pos = nel.PositionArray(\n        data=position_df_no_nan[[\"x\", \"y\"]].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n    # calculate kernel samples size based on sampling rate for x seconds\n    kernel_size = int(pos.fs * kernel_size)\n    # check if even number\n    if kernel_size % 2 == 0:\n        kernel_size += 1\n\n    cheeseboard_idx = np.where(epoch_df.environment == \"cheeseboard\")[0]\n    trials_temp = []\n    stddev = []\n    for idx in cheeseboard_idx:\n        # get homebox location\n        homebox_x = data[\"behavior\"][\"epochs\"][idx][\"homebox_x\"]\n        homebox_y = data[\"behavior\"][\"epochs\"][idx][\"homebox_y\"]\n\n        # get position during epoch\n        current_pos = pos[epoch[int(idx)]]\n        x, y = current_pos.data\n\n        # calculate distance from homebox\n        distance = np.sqrt((x - homebox_x) ** 2 + (y - homebox_y) ** 2)\n\n        # median filter distance to remove noise (jumps in position)\n        distance = medfilt(distance, kernel_size=kernel_size)\n\n        # find intervals where distance is greater than min_distance_from_home\n        dist_intervals = np.array(find_interval((distance &gt; min_distance_from_home)))\n\n        close_distances = distance[distance &lt; min_distance_from_home]\n        for trial in dist_intervals:\n            far_distances = distance[trial[0] : trial[1]].mean()\n\n            stddev.append(\n                (np.abs(far_distances) - np.nanmean(np.abs(close_distances), axis=0))\n                / np.nanstd(np.abs(close_distances), axis=0)\n            )\n\n        # get start and stop times of intervals\n        if len(dist_intervals) &gt; 0:\n            trials_temp.append(current_pos.time[dist_intervals])\n\n    # concatenate trials and place in EpochArray\n    trials = nel.EpochArray(np.vstack(trials_temp))\n\n    # remove trials that are too long or too short\n    trials._data = trials.data[\n        (trials.durations &lt; max_trial_time)\n        &amp; (trials.durations &gt; min_trial_time)\n        &amp; (np.array(stddev) &gt; min_std_away_from_home)\n    ]\n\n    return pos, trials\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.get_correct_inbound_outbound","title":"<code>get_correct_inbound_outbound(segments_df)</code>","text":"<p>Determine the task type (inbound or outbound), correctness, and turn direction for each segment.</p> <p>Parameters:</p> Name Type Description Default <code>segments_df</code> <code>DataFrame</code> <p>DataFrame containing segment information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Updated DataFrame with additional columns for task type, correctness, and turn direction.</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def get_correct_inbound_outbound(segments_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Determine the task type (inbound or outbound), correctness, and turn direction for each segment.\n\n    Parameters\n    ----------\n    segments_df : pd.DataFrame\n        DataFrame containing segment information.\n\n    Returns\n    -------\n    pd.DataFrame\n        Updated DataFrame with additional columns for task type, correctness, and turn direction.\n    \"\"\"\n    n_segments = segments_df.shape[0]\n    task = np.empty((n_segments,), dtype=object)\n    turn = np.empty((n_segments,), dtype=object)\n    is_correct = np.zeros((n_segments,), dtype=bool)\n\n    for segment_ind in np.arange(n_segments):\n        cur_segment = segments_df.iloc[segment_ind]\n        if cur_segment.from_well == \"Center\":\n            task[segment_ind] = \"Outbound\"\n            last_non_center_well = find_last_non_center_well(segments_df, segment_ind)\n            is_correct[segment_ind] = (cur_segment.to_well != last_non_center_well) &amp; (\n                cur_segment.to_well != \"Center\"\n            )\n            if (last_non_center_well != \"\") | ~is_correct[segment_ind]:\n                turn[segment_ind] = last_non_center_well\n            else:\n                is_left_turn = (\n                    (cur_segment.from_well == \"Left\")\n                    &amp; (cur_segment.to_well == \"Center\")\n                ) | (\n                    (cur_segment.from_well == \"Center\")\n                    &amp; (cur_segment.to_well == \"Right\")\n                )\n\n                turn[segment_ind] = \"Left\" if is_left_turn else \"Right\"\n        else:\n            task[segment_ind] = \"Inbound\"\n            is_correct[segment_ind] = segments_df.iloc[segment_ind].to_well == \"Center\"\n            turn[segment_ind] = cur_segment.from_well\n\n    segments_df[\"task\"] = task\n    segments_df[\"is_correct\"] = is_correct\n    segments_df[\"turn\"] = turn\n\n    return segments_df\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.get_linear_maze_trials","title":"<code>get_linear_maze_trials(basepath, epoch_input=None)</code>","text":"<p>Get trials for linear maze.</p> <p>Locates inbound and outbound laps for each linear track in the session.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The path to the base directory of the session data.</p> required <code>epoch_input</code> <code>None</code> <p>Deprecated parameter. This is no longer supported.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>PositionArray or None</code> <p>The position data for the linear maze trials.</p> <code>inbound_laps</code> <code>EpochArray or None</code> <p>The epochs corresponding to inbound laps.</p> <code>outbound_laps</code> <code>EpochArray or None</code> <p>The epochs corresponding to outbound laps.</p> Notes <p>If no valid position data is found, None values are returned for all outputs.</p> Source code in <code>neuro_py/behavior/get_trials.py</code> <pre><code>def get_linear_maze_trials(basepath: str, epoch_input: None = None) -&gt; Tuple[\n    Union[nel.PositionArray, None],\n    Union[nel.EpochArray, None],\n    Union[nel.EpochArray, None],\n]:\n    \"\"\"Get trials for linear maze.\n\n    Locates inbound and outbound laps for each linear track in the session.\n\n    Parameters\n    ----------\n    basepath : str\n        The path to the base directory of the session data.\n    epoch_input : None, optional\n        Deprecated parameter. This is no longer supported.\n\n    Returns\n    -------\n    pos : PositionArray or None\n        The position data for the linear maze trials.\n    inbound_laps : EpochArray or None\n        The epochs corresponding to inbound laps.\n    outbound_laps : EpochArray or None\n        The epochs corresponding to outbound laps.\n\n    Notes\n    -----\n    If no valid position data is found, None values are returned for all\n    outputs.\n    \"\"\"\n    if epoch_input is not None:\n        logging.warning(\"epoch_input is no longer supported\")\n\n    position_df = loading.load_animal_behavior(basepath)\n    position_df_no_nan = position_df.query(\"not x.isnull() &amp; not y.isnull()\")\n\n    if position_df_no_nan.shape[0] == 0:\n        return None, None, None\n\n    if \"linearized\" not in position_df_no_nan.columns:\n        return None, None, None\n\n    pos = nel.PositionArray(\n        data=position_df_no_nan[\"linearized\"].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n\n    epoch_df = loading.load_epoch(basepath)\n    epoch = nel.EpochArray([np.array([epoch_df.startTime, epoch_df.stopTime]).T])\n\n    domain = nel.EpochArray(\n        [np.array([epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]]).T]\n    )\n\n    inbound_laps_temp = []\n    outbound_laps_temp = []\n    maze_idx = np.where(epoch_df.environment == \"linear\")[0]\n    for idx in maze_idx:\n        current_position = pos[epoch[int(idx)]]\n\n        # get outbound and inbound epochs\n        outbound_laps, inbound_laps = linear_positions.get_linear_track_lap_epochs(\n            current_position.abscissa_vals, current_position.data[0], newLapThreshold=20\n        )\n        if not inbound_laps.isempty:\n            inbound_laps = linear_positions.find_good_lap_epochs(\n                current_position, inbound_laps, min_laps=5\n            )\n\n        if not outbound_laps.isempty:\n            outbound_laps = linear_positions.find_good_lap_epochs(\n                current_position, outbound_laps, min_laps=5\n            )\n\n        if not inbound_laps.isempty:\n            inbound_laps_temp.append(inbound_laps.data)\n        if not outbound_laps.isempty:\n            outbound_laps_temp.append(outbound_laps.data)\n\n    inbound_laps = nel.EpochArray(np.vstack(inbound_laps_temp), domain=domain)\n    outbound_laps = nel.EpochArray(np.vstack(outbound_laps_temp), domain=domain)\n\n    return pos, inbound_laps, outbound_laps\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.get_linear_track_lap_epochs","title":"<code>get_linear_track_lap_epochs(ts, x, newLapThreshold=15, good_laps=False, edgethresh=0.1, completeprop=0.2, posbins=50)</code>","text":"<p>Identifies lap epochs on a linear track and classifies them into outbound and inbound directions.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray</code> <p>Array of timestamps corresponding to position data.</p> required <code>x</code> <code>ndarray</code> <p>Array of position data along the linear track.</p> required <code>newLapThreshold</code> <code>float</code> <p>Minimum distance between laps to define a new lap, by default 15.</p> <code>15</code> <code>good_laps</code> <code>bool</code> <p>If True, filter out laps that do not meet certain quality criteria, by default False.</p> <code>False</code> <code>edgethresh</code> <code>float</code> <p>Threshold proportion of the track edge to identify potential boundary errors, by default 0.1.</p> <code>0.1</code> <code>completeprop</code> <code>float</code> <p>Minimum proportion of the track that must be traversed for a lap to be considered complete, by default 0.2.</p> <code>0.2</code> <code>posbins</code> <code>int</code> <p>Number of bins to divide the track into for analysis, by default 50.</p> <code>50</code> <p>Returns:</p> Type Description <code>Tuple[EpochArray, EpochArray]</code> <p>A tuple containing two nelpy EpochArray objects: - outbound_epochs: Epochs representing outbound runs (towards the far end of the track). - inbound_epochs: Epochs representing inbound runs (back towards the start).</p> Notes <ul> <li>This function calls <code>find_laps</code> to determine the lap structure, then segregates epochs into outbound and inbound directions.</li> <li>The EpochArray objects represent the start and stop timestamps for each identified lap.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; outbound_epochs, inbound_epochs = get_linear_track_lap_epochs(ts, x)\n</code></pre> Source code in <code>neuro_py/behavior/linear_positions.py</code> <pre><code>def get_linear_track_lap_epochs(\n    ts: np.ndarray,\n    x: np.ndarray,\n    newLapThreshold: float = 15,\n    good_laps: bool = False,\n    edgethresh: float = 0.1,\n    completeprop: float = 0.2,\n    posbins: int = 50,\n) -&gt; Tuple[nel.EpochArray, nel.EpochArray]:\n    \"\"\"\n    Identifies lap epochs on a linear track and classifies them into outbound and inbound directions.\n\n    Parameters\n    ----------\n    ts : np.ndarray\n        Array of timestamps corresponding to position data.\n    x : np.ndarray\n        Array of position data along the linear track.\n    newLapThreshold : float, optional\n        Minimum distance between laps to define a new lap, by default 15.\n    good_laps : bool, optional\n        If True, filter out laps that do not meet certain quality criteria, by default False.\n    edgethresh : float, optional\n        Threshold proportion of the track edge to identify potential boundary errors, by default 0.1.\n    completeprop : float, optional\n        Minimum proportion of the track that must be traversed for a lap to be considered complete, by default 0.2.\n    posbins : int, optional\n        Number of bins to divide the track into for analysis, by default 50.\n\n    Returns\n    -------\n    Tuple[nel.EpochArray, nel.EpochArray]\n        A tuple containing two nelpy EpochArray objects:\n        - outbound_epochs: Epochs representing outbound runs (towards the far end of the track).\n        - inbound_epochs: Epochs representing inbound runs (back towards the start).\n\n    Notes\n    ------\n    - This function calls `find_laps` to determine the lap structure, then segregates epochs into outbound and inbound directions.\n    - The EpochArray objects represent the start and stop timestamps for each identified lap.\n\n    Examples\n    -------\n    &gt;&gt;&gt; outbound_epochs, inbound_epochs = get_linear_track_lap_epochs(ts, x)\n\n    \"\"\"\n    laps = __find_laps(\n        np.array(ts),\n        np.array(x),\n        newLapThreshold=newLapThreshold,\n        good_laps=good_laps,\n        edgethresh=edgethresh,\n        completeprop=completeprop,\n        posbins=posbins,\n    )\n\n    # Handle no laps\n    if len(laps) == 0:\n        return nel.EpochArray(), nel.EpochArray()\n\n    outbound_start = []\n    outbound_stop = []\n    inbound_start = []\n    inbound_stop = []\n\n    for i in range(len(laps) - 1):\n        if laps.iloc[i].direction == 1:\n            outbound_start.append(laps.iloc[i].start_ts)\n            outbound_stop.append(laps.iloc[i + 1].start_ts)\n\n        if laps.iloc[i].direction == -1:\n            inbound_start.append(laps.iloc[i].start_ts)\n            inbound_stop.append(laps.iloc[i + 1].start_ts)\n\n    outbound_epochs = nel.EpochArray([np.array([outbound_start, outbound_stop]).T])\n    inbound_epochs = nel.EpochArray([np.array([inbound_start, inbound_stop]).T])\n\n    return outbound_epochs, inbound_epochs\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.get_openfield_trials","title":"<code>get_openfield_trials(basepath, epoch_type='epochs', spatial_binsize=3, n_time_bins=1, bin_method='dynamic', trial_time_bin_size=60, prop_trial_sampled=0.5, environments=['box', 'bigSquare', 'midSquare', 'bigSquarePlus', 'plus'], minimum_correlation=0.6, method='correlation')</code>","text":"<p>Get epochs of openfield trials.</p> <p>This function identifies trials in an open field environment that meet specific criteria for spatial sampling to assess spatial stability and population correlations.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the session data.</p> required <code>epoch_type</code> <code>str</code> <p>The type of epoch to use ('trials' or 'epochs'). Default is 'epochs'.</p> <code>'epochs'</code> <code>spatial_binsize</code> <code>int</code> <p>The size of spatial bins to use for occupancy. Default is 3.</p> <code>3</code> <code>n_time_bins</code> <code>int</code> <p>The number of time bins to use for occupancy for fixed bin method.  Default is 1.</p> <code>1</code> <code>bin_method</code> <code>str</code> <p>The method to use for binning time ('dynamic' or 'fixed').  Default is 'dynamic'.</p> <code>'dynamic'</code> <code>trial_time_bin_size</code> <code>Union[int, float]</code> <p>The size of time bins to use for occupancy for dynamic bin method  (in seconds). Default is 60.</p> <code>60</code> <code>prop_trial_sampled</code> <code>float</code> <p>The proportion of trials to sample. Default is 0.5.</p> <code>0.5</code> <code>environments</code> <code>List[str]</code> <p>A list of environments to include as open field. Default includes  several environments such as 'box' and 'plus'.</p> <code>['box', 'bigSquare', 'midSquare', 'bigSquarePlus', 'plus']</code> <code>minimum_correlation</code> <code>float</code> <p>The minimum correlation between trials to be considered a trial.  Default is 0.6.</p> <code>0.6</code> <code>method</code> <code>str</code> <p>The method to use ('correlation' or 'proportion'). Default is  'correlation'. <code>correlation</code> - use correlation between the trial map and the overall map to determine if it is a trial. <code>proportion</code> - use the proportion of the trial map that is sampled to determine if it is a trial</p> <code>'correlation'</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>PositionArray</code> <p>The position data for the open field trials.</p> <code>trials</code> <code>EpochArray</code> <p>The epochs of the identified trials.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the method is not 'correlation' or 'proportion'.</p> Notes <p>This function requires the loading of animal behavior and epoch data from the specified base path.</p> Source code in <code>neuro_py/behavior/get_trials.py</code> <pre><code>def get_openfield_trials(\n    basepath: str,\n    epoch_type: str = \"epochs\",\n    spatial_binsize: int = 3,\n    n_time_bins: int = 1,  # for bin_method = \"fixed\", not used for bin_method = \"dynamic\"\n    bin_method: str = \"dynamic\",\n    trial_time_bin_size: Union[int, float] = 60,  # in seconds for bin_method = \"dynamic\", not used for bin_method = \"fixed\"\n    prop_trial_sampled: float = 0.5,\n    environments: List[str] = [\n        \"box\",\n        \"bigSquare\",\n        \"midSquare\",\n        \"bigSquarePlus\",\n        \"plus\",\n    ],\n    minimum_correlation: float = 0.6,\n    method: str = \"correlation\",\n) -&gt; Tuple[nel.PositionArray, nel.EpochArray]:\n    \"\"\"\n    Get epochs of openfield trials.\n\n    This function identifies trials in an open field environment that meet\n    specific criteria for spatial sampling to assess spatial stability and\n    population correlations.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the session data.\n    epoch_type : str, optional\n        The type of epoch to use ('trials' or 'epochs'). Default is 'epochs'.\n    spatial_binsize : int, optional\n        The size of spatial bins to use for occupancy. Default is 3.\n    n_time_bins : int, optional\n        The number of time bins to use for occupancy for fixed bin method. \n        Default is 1.\n    bin_method : str, optional\n        The method to use for binning time ('dynamic' or 'fixed'). \n        Default is 'dynamic'.\n    trial_time_bin_size : Union[int, float], optional\n        The size of time bins to use for occupancy for dynamic bin method \n        (in seconds). Default is 60.\n    prop_trial_sampled : float, optional\n        The proportion of trials to sample. Default is 0.5.\n    environments : List[str], optional\n        A list of environments to include as open field. Default includes \n        several environments such as 'box' and 'plus'.\n    minimum_correlation : float, optional\n        The minimum correlation between trials to be considered a trial. \n        Default is 0.6.\n    method : str, optional\n        The method to use ('correlation' or 'proportion'). Default is \n        'correlation'. `correlation` - use correlation between the trial map and\n        the overall map to determine if it is a trial. `proportion` - use the\n        proportion of the trial map that is sampled to determine if it is a\n        trial\n\n    Returns\n    -------\n    pos : PositionArray\n        The position data for the open field trials.\n    trials : EpochArray\n        The epochs of the identified trials.\n\n    Raises\n    ------\n    ValueError\n        If the method is not 'correlation' or 'proportion'.\n\n    Notes\n    -----\n    This function requires the loading of animal behavior and epoch data\n    from the specified base path.\n    \"\"\"\n\n    def compute_occupancy_2d(\n        pos_run: object, x_edges: list, y_edges: list\n    ) -&gt; np.ndarray:\n        \"\"\"Compute occupancy of 2D position\n\n        Parameters\n        ----------\n        pos_run : object\n            Position data for the run\n        x_edges : list\n            Bin edges of x position\n        y_edges : list\n            Bin edges of y position\n\n        Returns\n        -------\n        np.ndarray\n            Occupancy map of the position\n        \"\"\"\n        occupancy, _, _ = np.histogram2d(\n            pos_run.data[0, :], pos_run.data[1, :], bins=(x_edges, y_edges)\n        )\n        return occupancy / pos_run.fs\n\n    # load position and place in array\n    position_df = loading.load_animal_behavior(basepath)\n    position_df_no_nan = position_df.query(\"not x.isnull() &amp; not y.isnull()\")\n    pos = nel.PositionArray(\n        data=position_df_no_nan[[\"x\", \"y\"]].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n\n    if pos.isempty:\n        return pos, nel.EpochArray([], label=\"session_epochs\")\n\n    # load epochs and place in array\n    if epoch_type == \"trials\":\n        epoch_df = loading.load_trials(basepath)\n        openfield_idx = np.arange(\n            0, len(epoch_df)\n        )  # assume trials make up all epochs associated with position\n        trialsID = epoch_df.trialsID.values\n    elif epoch_type == \"epochs\":\n        epoch_df = loading.load_epoch(basepath)\n        # find epochs that are these environments\n        openfield_idx = np.where(np.isin(epoch_df.environment, environments))[0]\n\n    epoch = nel.EpochArray([np.array([epoch_df.startTime, epoch_df.stopTime]).T])\n\n    # find epochs that are these environments\n    trials = []\n    if epoch_type == \"trials\":\n        trial_ID = []\n\n    # loop through epochs\n    for idx in openfield_idx:\n        # get position during epoch\n        current_position = pos[epoch[int(idx)]]\n\n        if current_position.isempty:\n            continue\n\n        # get the edges of the position\n        ext_xmin, ext_xmax = (\n            np.floor(np.nanmin(current_position.data[0, :])),\n            np.ceil(np.nanmax(current_position.data[0, :])),\n        )\n        ext_ymin, ext_ymax = (\n            np.floor(np.nanmin(current_position.data[1, :])),\n            np.ceil(np.nanmax(current_position.data[1, :])),\n        )\n        # create bin edges for occupancy map at spatial_binsize\n        x_edges = np.arange(ext_xmin, ext_xmax + spatial_binsize, spatial_binsize)\n        y_edges = np.arange(ext_ymin, ext_ymax + spatial_binsize, spatial_binsize)\n\n        # compute occupancy map and get proportion of environment sampled\n        occupancy = compute_occupancy_2d(current_position, x_edges, y_edges)\n        overall_prop_sampled = sum(occupancy.flatten() &gt; 0) / (\n            (len(x_edges) - 1) * (len(y_edges) - 1)\n        )\n        # create possible trials based on trial_time_bin_size\n        # these will be iterated over to find trials that are sampled enough\n        duration = epoch_df.iloc[idx].stopTime - epoch_df.iloc[idx].startTime\n\n        if bin_method == \"dynamic\":\n            bins = np.linspace(\n                epoch_df.iloc[idx].startTime,\n                epoch_df.iloc[idx].stopTime,\n                int(np.ceil(duration / (trial_time_bin_size))),\n            )\n        elif bin_method == \"fixed\":\n            bins = np.arange(\n                epoch_df.iloc[idx].startTime,\n                epoch_df.iloc[idx].stopTime,\n                int(np.floor(epoch[int(idx)].duration / n_time_bins)),\n            )\n        trials_temp = nel.EpochArray(np.array([bins[:-1], bins[1:]]).T)\n        if epoch_type == \"trials\":\n            temp_ID = trialsID[idx]\n\n        trial_i = 0\n        # loop through possible trials and find when sampled enough\n        for i_interval in range(trials_temp.n_intervals):\n            # compute occupancy map and get proportion of environment sampled for trial\n            trial_occupancy = compute_occupancy_2d(\n                current_position[trials_temp[trial_i : i_interval + 1]],\n                x_edges,\n                y_edges,\n            )\n\n            if method == \"correlation\":\n                # correlate trial_occupancy with overall occupancy\n                r = np.corrcoef(\n                    occupancy.flatten() &gt; 0,\n                    trial_occupancy.flatten() &gt; 0,\n                )[0, 1]\n\n                # if sampled enough, add to trials\n                if r &gt; minimum_correlation:\n                    trials.append(\n                        [\n                            trials_temp[trial_i : i_interval + 1].start,\n                            trials_temp[trial_i : i_interval + 1].stop,\n                        ]\n                    )\n                    if epoch_type == \"trials\":\n                        trial_ID.append(temp_ID + \"_\" + str(idx))\n                    # update trial_i to next interval to start from\n                    trial_i = i_interval + 1\n\n            elif method == \"proportion\":\n                trial_prop_sampled = sum(trial_occupancy.flatten() &gt; 0) / (\n                    (len(x_edges) - 1) * (len(y_edges) - 1)\n                )\n                if trial_prop_sampled &gt; prop_trial_sampled * overall_prop_sampled:\n                    trials.append(\n                        [\n                            trials_temp[trial_i : i_interval + 1].start,\n                            trials_temp[trial_i : i_interval + 1].stop,\n                        ]\n                    )\n                    if epoch_type == \"trials\":\n                        trial_ID.append(temp_ID + \"_\" + str(idx))\n                    # update trial_i to next interval to start from\n                    trial_i = i_interval + 1\n            else:\n                raise ValueError(\"method must be correlation or proportion\")\n\n    # concatenate trials and place in EpochArray\n    if epoch_type == \"trials\":\n        trials = nel.EpochArray(np.vstack(trials), label=np.vstack(trial_ID))\n    else:\n        trials = nel.EpochArray(np.vstack(trials), label=\"session_epochs\")\n\n    return pos, trials\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.get_speed","title":"<code>get_speed(position, time=None)</code>","text":"<p>Computes the speed from position data.</p> <p>Speed is the magnitude of the velocity vector at each time point. If time is not provided, it assumes a constant time step between position samples.</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>ndarray</code> <p>An array of position data. This can be 1D (for single-dimensional positions) or 2D (for multi-dimensional positions, e.g., x and y coordinates over time).</p> required <code>time</code> <code>Union[ndarray, None]</code> <p>An array of time values corresponding to the position data. If None, the function assumes a constant time step. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of speed values, where each speed is the magnitude of the velocity at the corresponding time point.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; position = np.array([0, 1, 4, 9, 16])\n&gt;&gt;&gt; get_speed(position)\narray([1.41421356, 2.82842712, 5.65685425, 8.48528137, 9.89949494])\n</code></pre> <pre><code>&gt;&gt;&gt; position = np.array([[0, 0], [1, 1], [4, 4], [9, 9], [16, 16]])\n&gt;&gt;&gt; time = np.array([0, 1, 2, 3, 4])\n&gt;&gt;&gt; get_speed(position, time)\narray([1.41421356, 2.82842712, 5.65685425, 8.48528137, 9.89949494])\n</code></pre> Source code in <code>neuro_py/behavior/kinematics.py</code> <pre><code>def get_speed(position: np.ndarray, time: Union[np.ndarray, None] = None) -&gt; np.ndarray:\n    \"\"\"\n    Computes the speed from position data.\n\n    Speed is the magnitude of the velocity vector at each time point. If time is\n    not provided, it assumes a constant time step between position samples.\n\n    Parameters\n    ----------\n    position : np.ndarray\n        An array of position data. This can be 1D (for single-dimensional positions)\n        or 2D (for multi-dimensional positions, e.g., x and y coordinates over time).\n    time : Union[np.ndarray, None], optional\n        An array of time values corresponding to the position data. If None,\n        the function assumes a constant time step. Default is None.\n\n    Returns\n    -------\n    np.ndarray\n        An array of speed values, where each speed is the magnitude of the velocity\n        at the corresponding time point.\n\n    Examples\n    --------\n    &gt;&gt;&gt; position = np.array([0, 1, 4, 9, 16])\n    &gt;&gt;&gt; get_speed(position)\n    array([1.41421356, 2.82842712, 5.65685425, 8.48528137, 9.89949494])\n\n    &gt;&gt;&gt; position = np.array([[0, 0], [1, 1], [4, 4], [9, 9], [16, 16]])\n    &gt;&gt;&gt; time = np.array([0, 1, 2, 3, 4])\n    &gt;&gt;&gt; get_speed(position, time)\n    array([1.41421356, 2.82842712, 5.65685425, 8.48528137, 9.89949494])\n    \"\"\"\n    velocity = get_velocity(position, time=time)\n    return np.sqrt(np.sum(velocity**2, axis=1))\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.get_t_maze_trials","title":"<code>get_t_maze_trials(basepath, epoch, bypass_standard_behavior=False)</code>","text":"<p>Get trials for T maze.</p> <p>This function retrieves position data and epochs for right and left trials based on the specified epoch. It checks if the number of outbound laps exceeds the number of inbound laps unless bypassed.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the session data.</p> required <code>epoch</code> <code>EpochArray</code> <p>The epoch to get trials for.</p> required <code>bypass_standard_behavior</code> <code>bool</code> <p>If True, allows for more outbound than inbound trials. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>PositionArray or None</code> <p>The position data for the T maze trials.</p> <code>right_epochs</code> <code>EpochArray or None</code> <p>The epochs corresponding to right trials.</p> <code>left_epochs</code> <code>EpochArray or None</code> <p>The epochs corresponding to left trials.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If inbound laps exceed outbound laps and bypass_standard_behavior is False.</p> Notes <p>If there are no valid positions or states in the session data, None is returned for all outputs.</p> Source code in <code>neuro_py/behavior/get_trials.py</code> <pre><code>def get_t_maze_trials(\n    basepath: str, epoch: nel.EpochArray, bypass_standard_behavior: bool = False\n) -&gt; Tuple[\n    Union[nel.PositionArray, None],\n    Union[nel.EpochArray, None],\n    Union[nel.EpochArray, None],\n]:\n    \"\"\"\n    Get trials for T maze.\n\n    This function retrieves position data and epochs for right and left trials\n    based on the specified epoch. It checks if the number of outbound laps exceeds\n    the number of inbound laps unless bypassed.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the session data.\n    epoch : nel.EpochArray\n        The epoch to get trials for.\n    bypass_standard_behavior : bool, optional\n        If True, allows for more outbound than inbound trials. Default is False.\n\n    Returns\n    -------\n    pos : PositionArray or None\n        The position data for the T maze trials.\n    right_epochs : EpochArray or None\n        The epochs corresponding to right trials.\n    left_epochs : EpochArray or None\n        The epochs corresponding to left trials.\n\n    Raises\n    ------\n    TypeError\n        If inbound laps exceed outbound laps and bypass_standard_behavior is False.\n\n    Notes\n    -----\n    If there are no valid positions or states in the session data, None is returned\n    for all outputs.\n    \"\"\"\n\n    def dissociate_laps_by_states(states, dir_epoch, states_of_interest=[1, 2]):\n        # unique_states = np.unique(states.data[~np.isnan(states.data)])\n        lap_id = []\n        for ep in dir_epoch:\n            state_count = []\n            for us in states_of_interest:\n                state_count.append(np.nansum(states[ep].data == us))\n            lap_id.append(states_of_interest[np.argmax(state_count)])\n        return np.array(lap_id).astype(int)\n\n    position_df = loading.load_animal_behavior(basepath)\n    position_df_no_nan = position_df.query(\"not x.isnull() &amp; not y.isnull()\")\n\n    if position_df_no_nan.shape[0] == 0:\n        return None, None, None\n\n    if \"linearized\" not in position_df_no_nan.columns:\n        return None, None, None\n\n    if \"states\" not in position_df_no_nan.columns:\n        return None, None, None\n\n    pos = nel.PositionArray(\n        data=position_df_no_nan[\"linearized\"].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n\n    pos = pos[epoch]\n    if pos.isempty:\n        return None, None, None\n\n    states = nel.AnalogSignalArray(\n        data=position_df_no_nan[\"states\"].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n    states = states[epoch]\n\n    # get outbound and inbound epochs\n    outbound_laps, inbound_laps = linear_positions.get_linear_track_lap_epochs(\n        pos.abscissa_vals, pos.data[0], newLapThreshold=20\n    )\n\n    inbound_laps = linear_positions.find_good_lap_epochs(pos, inbound_laps, min_laps=5)\n    outbound_laps = linear_positions.find_good_lap_epochs(\n        pos, outbound_laps, min_laps=5\n    )\n\n    if outbound_laps.isempty:\n        return None, None, None\n\n    if not inbound_laps.isempty:\n        logging.warning(\"inbound_laps should be empty for tmaze\")\n\n    if (\n        inbound_laps.n_intervals &gt; outbound_laps.n_intervals\n    ) and not bypass_standard_behavior:\n        raise TypeError(\"inbound_laps should be less than outbound_laps for tmaze\")\n\n    # locate laps with the majority in state 1 or 2\n    lap_id = dissociate_laps_by_states(states, outbound_laps, states_of_interest=[1, 2])\n\n    right_epochs = nel.EpochArray(data=outbound_laps.data[lap_id == 1, :])\n    left_epochs = nel.EpochArray(data=outbound_laps.data[lap_id == 2, :])\n\n    position_df_no_nan = position_df_no_nan[\n        position_df_no_nan[\"time\"].between(epoch.start, epoch.stop)\n    ]\n    return pos, right_epochs, left_epochs\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.get_velocity","title":"<code>get_velocity(position, time=None)</code>","text":"<p>Computes the velocity from position data.</p> <p>If time is not provided, it assumes a constant time step between position samples. The velocity is calculated as the gradient of the position with respect to time along the first axis.</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>ndarray</code> <p>An array of position data. This can be 1D (for single-dimensional positions) or 2D (for multi-dimensional positions, e.g., x and y coordinates over time).</p> required <code>time</code> <code>Union[ndarray, None]</code> <p>An array of time values corresponding to the position data. If None, the function assumes a constant time step. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of velocity values, where each velocity is the rate of change of position with respect to time.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; position = np.array([0, 1, 4, 9, 16])\n&gt;&gt;&gt; get_velocity(position)\narray([1., 2., 4., 6., 7.])\n</code></pre> <pre><code>&gt;&gt;&gt; position = np.array([[0, 0], [1, 1], [4, 4], [9, 9], [16, 16]])\n&gt;&gt;&gt; time = np.array([0, 1, 2, 3, 4])\n&gt;&gt;&gt; get_velocity(position, time)\narray([[1., 1.],\n    [2., 2.],\n    [4., 4.],\n    [6., 6.],\n    [7., 7.]])\n</code></pre> Source code in <code>neuro_py/behavior/kinematics.py</code> <pre><code>def get_velocity(\n    position: np.ndarray, time: Union[np.ndarray, None] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Computes the velocity from position data.\n\n    If time is not provided, it assumes a constant time step between position\n    samples. The velocity is calculated as the gradient of the position with\n    respect to time along the first axis.\n\n    Parameters\n    ----------\n    position : np.ndarray\n        An array of position data. This can be 1D (for single-dimensional positions)\n        or 2D (for multi-dimensional positions, e.g., x and y coordinates over time).\n    time : Union[np.ndarray, None], optional\n        An array of time values corresponding to the position data. If None,\n        the function assumes a constant time step. Default is None.\n\n    Returns\n    -------\n    np.ndarray\n        An array of velocity values, where each velocity is the rate of change of\n        position with respect to time.\n\n    Examples\n    --------\n    &gt;&gt;&gt; position = np.array([0, 1, 4, 9, 16])\n    &gt;&gt;&gt; get_velocity(position)\n    array([1., 2., 4., 6., 7.])\n\n    &gt;&gt;&gt; position = np.array([[0, 0], [1, 1], [4, 4], [9, 9], [16, 16]])\n    &gt;&gt;&gt; time = np.array([0, 1, 2, 3, 4])\n    &gt;&gt;&gt; get_velocity(position, time)\n    array([[1., 1.],\n        [2., 2.],\n        [4., 4.],\n        [6., 6.],\n        [7., 7.]])\n    \"\"\"\n    if time is None:\n        time = np.arange(position.shape[0])\n    return np.gradient(position, time, axis=0)\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.get_w_maze_trials","title":"<code>get_w_maze_trials(basepath, max_distance_from_well=20, min_distance_traveled=50)</code>","text":"<p>Get trials for W maze.</p> <p>This function retrieves position data and identifies trials for the W maze based on specified distance criteria.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the session data.</p> required <code>max_distance_from_well</code> <code>int</code> <p>The maximum distance from the well to be considered a trial. Default is 20.</p> <code>20</code> <code>min_distance_traveled</code> <code>int</code> <p>The minimum distance traveled to be considered a trial. Default is 50.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>PositionArray or None</code> <p>The position data for the W maze trials.</p> <code>trials</code> <code>ndarray or None</code> <p>The indices of the trials.</p> <code>right_trials</code> <code>ndarray or None</code> <p>The indices of the right trials.</p> <code>left_trials</code> <code>ndarray or None</code> <p>The indices of the left trials.</p> Notes <p>This function requires the following metadata dependencies:</p> <ul> <li><code>animal.behavior.mat</code>: contains center, left, and right x y coordinates.</li> </ul> <p>You can label these with <code>label_key_locations_wmaze.m</code> or manually.</p> Source code in <code>neuro_py/behavior/get_trials.py</code> <pre><code>def get_w_maze_trials(\n    basepath: str, max_distance_from_well: int = 20, min_distance_traveled: int = 50\n) -&gt; Tuple[\n    Union[nel.PositionArray, None],\n    Union[np.ndarray, None],\n    Union[np.ndarray, None],\n    Union[np.ndarray, None],\n]:\n    \"\"\"\n    Get trials for W maze.\n\n    This function retrieves position data and identifies trials for the W maze\n    based on specified distance criteria.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the session data.\n    max_distance_from_well : int, optional\n        The maximum distance from the well to be considered a trial. Default is 20.\n    min_distance_traveled : int, optional\n        The minimum distance traveled to be considered a trial. Default is 50.\n\n    Returns\n    -------\n    pos : PositionArray or None\n        The position data for the W maze trials.\n    trials : ndarray or None\n        The indices of the trials.\n    right_trials : ndarray or None\n        The indices of the right trials.\n    left_trials : ndarray or None\n        The indices of the left trials.\n\n    Notes\n    -----\n    This function requires the following metadata dependencies:\n\n    - `animal.behavior.mat`: contains center, left, and right x y coordinates.\n\n    You can label these with `label_key_locations_wmaze.m` or manually.\n    \"\"\"\n\n    # load position and key location metadata\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".animal.behavior.mat\"\n    )\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    # load epochs and place in array\n    epoch_df = loading.load_epoch(basepath)\n\n    # load position and place in array\n    position_df = loading.load_animal_behavior(basepath)\n    position_df_no_nan = position_df.query(\"not x.isnull() &amp; not y.isnull()\")\n\n    pos = nel.PositionArray(\n        data=position_df_no_nan[\"linearized\"].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n    wmaze_idx = np.where(epoch_df.environment == \"wmaze\")[0]\n    for idx in wmaze_idx:\n        # get key locations\n        right_x = data[\"behavior\"][\"epochs\"][idx][\"right_x\"]\n        right_y = data[\"behavior\"][\"epochs\"][idx][\"right_y\"]\n\n        center_x = data[\"behavior\"][\"epochs\"][idx][\"center_x\"]\n        center_y = data[\"behavior\"][\"epochs\"][idx][\"center_y\"]\n\n        left_x = data[\"behavior\"][\"epochs\"][idx][\"left_x\"]\n        left_y = data[\"behavior\"][\"epochs\"][idx][\"left_y\"]\n\n        well_locations = np.array(\n            [[center_x, center_y], [left_x, left_y], [right_x, right_y]]\n        )\n\n        current_ts_idx = position_df_no_nan[\"timestamps\"].between(\n            epoch_df.iloc[idx].startTime, epoch_df.iloc[idx].stopTime\n        )\n\n        # temp_df = position_df[~np.isnan(position_df.x)]\n        segments_df, _ = well_traversal_classification.segment_path(\n            position_df_no_nan[\"timestamps\"].values[current_ts_idx],\n            position_df_no_nan[[\"x\", \"y\"]].values[current_ts_idx],\n            well_locations,\n            max_distance_from_well=max_distance_from_well,\n        )\n\n        segments_df = well_traversal_classification.score_inbound_outbound(\n            segments_df, min_distance_traveled=min_distance_traveled\n        )\n        conditions = [\n            \"from_well == 'Center' &amp; to_well == 'Left'\",\n            \"from_well == 'Left' &amp; to_well == 'Center'\",\n            \"from_well == 'Center' &amp; to_well == 'Right'\",\n            \"from_well == 'Right' &amp; to_well == 'Center'\",\n        ]\n        condition_labels = [\n            \"center_left\",\n            \"left_center\",\n            \"center_right\",\n            \"right_center\",\n        ]\n        trajectories = {}\n        for con, con_label in zip(conditions, condition_labels):\n            trajectories[con_label] = nel.EpochArray(\n                np.array(\n                    [segments_df.query(con).start_time, segments_df.query(con).end_time]\n                ).T\n            )\n\n    return pos, trajectories\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.linearize_position","title":"<code>linearize_position(x, y)</code>","text":"<p>Use PCA (a dimensionality reduction technique) to find the direction of maximal variance in our position data, and use this as the new 1D linear track axis.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>x-coordinates of shape (n, 1)</p> required <code>y</code> <code>ndarray</code> <p>y-coordinates of shape (n, 1)</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Linearized x and y coordinates, both of shape (n, 1).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; linear_x, linear_y = npy.behavior.linearize_position(x, y)\n&gt;&gt;&gt; linear_x\narray([0.        , 1.41421356, 2.82842712, 4.24264069, 5.65685425])\n&gt;&gt;&gt; linear_y\narray([3.92192151e-16, 0.00000000e+00, 9.80480378e-17, 1.96096076e-16, 2.94144113e-16])\n</code></pre> Source code in <code>neuro_py/behavior/linear_positions.py</code> <pre><code>def linearize_position(x: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Use PCA (a dimensionality reduction technique) to find the direction of maximal variance\n    in our position data, and use this as the new 1D linear track axis.\n\n    Parameters\n    ----------\n    x : numpy.ndarray\n        x-coordinates of shape (n, 1)\n    y : numpy.ndarray\n        y-coordinates of shape (n, 1)\n\n    Returns\n    -------\n    tuple[numpy.ndarray, numpy.ndarray]\n        Linearized x and y coordinates, both of shape (n, 1).\n\n    Examples\n    --------\n    &gt;&gt;&gt; x = np.array([1, 2, 3, 4, 5])\n    &gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5])\n    &gt;&gt;&gt; linear_x, linear_y = npy.behavior.linearize_position(x, y)\n    &gt;&gt;&gt; linear_x\n    array([0.        , 1.41421356, 2.82842712, 4.24264069, 5.65685425])\n    &gt;&gt;&gt; linear_y\n    array([3.92192151e-16, 0.00000000e+00, 9.80480378e-17, 1.96096076e-16, 2.94144113e-16])\n    \"\"\"\n    # locate and remove nans (sklearn pca does not like nans)\n    badidx = (np.isnan(x)) | (np.isnan(y))\n    badidx_pos = np.where(badidx)\n    goodidx_pos = np.where(~badidx)\n    n = len(x)\n\n    x = x[~badidx]\n    y = y[~badidx]\n\n    # perform pca and return the first 2 components\n    pca = PCA(n_components=2)\n    # transform our coords\n    linear = pca.fit_transform(np.array([x, y]).T)\n\n    # add back nans\n    x = np.zeros([n])\n    x[badidx_pos] = np.nan\n    x[goodidx_pos] = linear[:, 0]\n\n    y = np.zeros([n])\n    y[badidx_pos] = np.nan\n    y[goodidx_pos] = linear[:, 1]\n\n    # pca will center data at 0,0... adjust for this here\n    x = x + np.abs(np.nanmin(x))\n    y = y + np.abs(np.nanmin(y))\n\n    return x, y\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.paired_distances","title":"<code>paired_distances(x, y)</code>","text":"<p>Euclidean distance between x and y at each time point.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, list]</code> <p>Array or list of shape (n_time, n_space).</p> required <code>y</code> <code>Union[ndarray, list]</code> <p>Array or list of shape (n_time, n_space).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (n_time,) containing the distances.</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def paired_distances(\n    x: Union[np.ndarray, list], y: Union[np.ndarray, list]\n) -&gt; np.ndarray:\n    \"\"\"\n    Euclidean distance between x and y at each time point.\n\n    Parameters\n    ----------\n    x : Union[np.ndarray, list]\n        Array or list of shape (n_time, n_space).\n    y : Union[np.ndarray, list]\n        Array or list of shape (n_time, n_space).\n\n    Returns\n    -------\n    np.ndarray\n        Array of shape (n_time,) containing the distances.\n    \"\"\"\n    x, y = np.array(x), np.array(y)\n    x = np.atleast_2d(x).T if x.ndim &lt; 2 else x\n    y = np.atleast_2d(y).T if y.ndim &lt; 2 else y\n    return np.linalg.norm(x - y, axis=1)\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.plot_grid_with_circle_and_random_dots","title":"<code>plot_grid_with_circle_and_random_dots()</code>","text":"<p>Plots a 15x15 grid of dots within a circle, highlights 3 randomly chosen dots  within the circle, and draws a grey box at the bottom.</p> <p>The function generates a grid of points within a circle of a specified radius  and randomly selects three points from within the circle. These points are  colored red and slightly enlarged. Additionally, a grey box is drawn at the  bottom of the plot.</p> Notes <ul> <li>The grid is plotted on a 15x15 layout, with points that fall within the    circle of radius 6.8 being displayed.</li> <li>The randomly selected points must be at least 4 grid units apart.</li> <li>A grey rectangular box is drawn near the bottom of the plot for aesthetic    purposes.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plot_grid_with_circle_and_random_dots()\n# This will display a plot of a circle containing a grid of dots with \n# 3 randomly chosen dots highlighted in red.\n</code></pre> Source code in <code>neuro_py/behavior/cheeseboard.py</code> <pre><code>def plot_grid_with_circle_and_random_dots():\n    \"\"\"\n    Plots a 15x15 grid of dots within a circle, highlights 3 randomly chosen dots \n    within the circle, and draws a grey box at the bottom.\n\n    The function generates a grid of points within a circle of a specified radius \n    and randomly selects three points from within the circle. These points are \n    colored red and slightly enlarged. Additionally, a grey box is drawn at the \n    bottom of the plot.\n\n    Notes\n    -----\n    - The grid is plotted on a 15x15 layout, with points that fall within the \n      circle of radius 6.8 being displayed.\n    - The randomly selected points must be at least 4 grid units apart.\n    - A grey rectangular box is drawn near the bottom of the plot for aesthetic \n      purposes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; plot_grid_with_circle_and_random_dots()\n    # This will display a plot of a circle containing a grid of dots with \n    # 3 randomly chosen dots highlighted in red.\n    \"\"\"\n    # Create a 15x15 grid of dots within the circle\n    x = np.linspace(-7, 7, 17)\n    y = np.linspace(-7, 7, 17)\n    X, Y = np.meshgrid(x, y)\n\n    # Calculate the circle parameters with an offset\n    radius = 6.8  # Radius of the circle\n    circle_center = (0, 0)  # Center of the circle\n\n    # Create a mask to display only the dots within the circle\n    circle_mask = (X**2 + Y**2) &lt;= radius**2\n\n    # Plot the grid of dots within the circle\n    plt.figure(figsize=(8, 8))\n    plt.plot(X[circle_mask], Y[circle_mask], \"o\", color=\"k\", markersize=6)\n\n    # Plot the circle\n    circle = plt.Circle(circle_center, radius, color=\"black\", fill=False)\n    plt.gca().add_patch(circle)\n\n    # Randomly pick 3 dots within the circle\n    num_dots = 3\n    chosen_indices = np.random.choice(np.sum(circle_mask), size=num_dots, replace=False)\n    chosen_dots = np.argwhere(circle_mask)\n    chosen_dots = chosen_dots[chosen_indices]\n\n    # Ensure minimum separation of 4 dots between the randomly chosen dots\n    min_separation = 4\n    for i in range(num_dots):\n        for j in range(i + 1, num_dots):\n            while np.linalg.norm(chosen_dots[i] - chosen_dots[j]) &lt; min_separation:\n                chosen_indices[j] = np.random.choice(np.sum(circle_mask), size=1)[0]\n                chosen_dots[j] = np.argwhere(circle_mask)[chosen_indices[j]]\n\n    # Color the randomly chosen dots red and make them slightly larger\n    for dot in chosen_dots:\n        plt.plot(X[dot[0], dot[1]], Y[dot[0], dot[1]], \"o\", color=\"red\", markersize=9)\n\n    # Draw a grey box at the bottom\n    plt.fill_between([-1.5, 1.5], -8.5, -6.5, color=\"darkgray\", alpha=1)\n\n    plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n    plt.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.score_inbound_outbound","title":"<code>score_inbound_outbound(segments_df, min_distance_traveled=50, well_names={1: 'Center', 2: 'Left', 3: 'Right'})</code>","text":"<p>In the alternating arm task, determines whether the trial should be inbound (running to the center arm) or outbound (running to the opposite outer arm as before) and if the trial was performed correctly.</p> <p>Parameters:</p> Name Type Description Default <code>segments_df</code> <code>DataFrame</code> <p>Output of <code>segment_path</code> function.</p> required <code>min_distance_traveled</code> <code>float</code> <p>Minimum path length (in cm) while outside of the well radius for a segment to be considered as a trial, by default 50.</p> <code>50</code> <code>well_names</code> <code>Dict[int, str]</code> <p>Dictionary mapping well indices to well names, by default {1: \"Center\", 2: \"Left\", 3: \"Right\"}.</p> <code>{1: 'Center', 2: 'Left', 3: 'Right'}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Same as the input dataframe but with the wells labeled (left, right, center) and columns for <code>task</code> (inbound/outbound) and <code>is_correct</code> (True/False).</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def score_inbound_outbound(\n    segments_df: pd.DataFrame,\n    min_distance_traveled: float = 50,\n    well_names: Dict[int, str] = {1: \"Center\", 2: \"Left\", 3: \"Right\"},\n) -&gt; pd.DataFrame:\n    \"\"\"\n    In the alternating arm task, determines whether the trial should be\n    inbound (running to the center arm) or outbound (running to the opposite\n    outer arm as before) and if the trial was performed correctly.\n\n    Parameters\n    ----------\n    segments_df : pd.DataFrame\n        Output of `segment_path` function.\n    min_distance_traveled : float, optional\n        Minimum path length (in cm) while outside of the well radius for\n        a segment to be considered as a trial, by default 50.\n    well_names : Dict[int, str], optional\n        Dictionary mapping well indices to well names, by default {1: \"Center\", 2: \"Left\", 3: \"Right\"}.\n\n    Returns\n    -------\n    pd.DataFrame\n        Same as the input dataframe but with the wells labeled\n        (left, right, center) and columns for `task` (inbound/outbound) and\n        `is_correct` (True/False).\n    \"\"\"\n    segments_df = (\n        segments_df.copy()\n        .loc[segments_df.distance_traveled &gt; min_distance_traveled]\n        .dropna()\n    )\n    segments_df = segments_df.assign(\n        to_well=lambda df: df.to_well.map(well_names),\n        from_well=lambda df: df.from_well.map(well_names),\n    )\n    return get_correct_inbound_outbound(segments_df)\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.segment_path","title":"<code>segment_path(time, position, well_locations, max_distance_from_well=10)</code>","text":"<p>Label traversals between each well location.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>(ndarray, shape(n_time))</code> <p>Array of time points.</p> required <code>position</code> <code>(ndarray, shape(n_time, n_space))</code> <p>Array of positions at each time point.</p> required <code>well_locations</code> <code>(ndarray, shape(n_wells, n_space))</code> <p>Array of well locations.</p> required <code>max_distance_from_well</code> <code>float</code> <p>The animal is considered at a well location if its position is closer than this value, by default 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>segments_df: DataFrame of shape (n_segments, 6) containing segment information.</li> <li>labeled_segments: DataFrame of shape (n_time,) containing labeled segments.</li> </ul> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def segment_path(\n    time: np.ndarray,\n    position: np.ndarray,\n    well_locations: np.ndarray,\n    max_distance_from_well: float = 10,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Label traversals between each well location.\n\n    Parameters\n    ----------\n    time : np.ndarray, shape (n_time,)\n        Array of time points.\n    position : np.ndarray, shape (n_time, n_space)\n        Array of positions at each time point.\n    well_locations : np.ndarray, shape (n_wells, n_space)\n        Array of well locations.\n    max_distance_from_well : float, optional\n        The animal is considered at a well location if its position is closer\n        than this value, by default 10.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        - segments_df: DataFrame of shape (n_segments, 6) containing segment information.\n        - labeled_segments: DataFrame of shape (n_time,) containing labeled segments.\n    \"\"\"\n\n    well_enter_exit, at_target = np.stack(\n        [\n            enter_exit_target(position, np.atleast_2d(well), max_distance_from_well)\n            for well in well_locations\n        ],\n        axis=1,\n    )\n    n_wells = len(well_locations)\n    well_labels = np.arange(n_wells) + 1\n    well_enter_exit = np.sum(well_enter_exit.T * well_labels, axis=1)\n    shifted_well_enter_exit = shift_well_enters(well_enter_exit)\n    is_segment = ~(np.sum(at_target, axis=0) &gt; 0)\n    labeled_segments, n_segment_labels = label(is_segment)\n    segment_labels = np.arange(n_segment_labels) + 1\n\n    start_time, end_time, duration = [], [], []\n    distance_traveled, from_well, to_well = [], [], []\n\n    for segment_label in segment_labels:\n        is_seg = np.in1d(labeled_segments, segment_label)\n        segment_time = time[is_seg]\n        start_time.append(segment_time.min())\n        end_time.append(segment_time.max())\n        duration.append(segment_time.max() - segment_time.min())\n        try:\n            start, _, end = np.unique(shifted_well_enter_exit[is_seg])\n        except ValueError:\n            start, end = np.nan, np.nan\n\n        from_well.append(np.abs(start))\n        to_well.append(np.abs(end))\n        p = position[is_seg]\n        distance_traveled.append(np.sum(paired_distances(p[1:], p[:-1])))\n\n    data = [\n        (\"start_time\", start_time),\n        (\"end_time\", end_time),\n        (\"duration\", duration),\n        (\"from_well\", from_well),\n        (\"to_well\", to_well),\n        (\"distance_traveled\", distance_traveled),\n    ]\n    index = pd.Index(segment_labels, name=\"segment\")\n    return (\n        pd.DataFrame.from_dict(dict(data)).set_index(index),\n        pd.DataFrame(dict(labeled_segments=labeled_segments), index=time),\n    )\n</code></pre>"},{"location":"reference/neuro_py/behavior/#neuro_py.behavior.shift_well_enters","title":"<code>shift_well_enters(enter_exit)</code>","text":"<p>Shifts the enter times back one time point.</p> <p>Parameters:</p> Name Type Description Default <code>enter_exit</code> <code>ndarray</code> <p>Array indicating enter (positive values) and exit (negative values) events.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array with enter times shifted back by one time point.</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def shift_well_enters(enter_exit: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Shifts the enter times back one time point.\n\n    Parameters\n    ----------\n    enter_exit : np.ndarray\n        Array indicating enter (positive values) and exit (negative values) events.\n\n    Returns\n    -------\n    np.ndarray\n        Array with enter times shifted back by one time point.\n    \"\"\"\n    shifted_enter_exit = enter_exit.copy()\n    old_ind = np.where(enter_exit &gt; 0)[0]  # positive entries are well-entries\n    new_ind = old_ind - 1\n    shifted_enter_exit[new_ind] = enter_exit[old_ind]\n    shifted_enter_exit[old_ind] = 0\n    return shifted_enter_exit\n</code></pre>"},{"location":"reference/neuro_py/behavior/cheeseboard/","title":"neuro_py.behavior.cheeseboard","text":""},{"location":"reference/neuro_py/behavior/cheeseboard/#neuro_py.behavior.cheeseboard.plot_grid_with_circle_and_random_dots","title":"<code>plot_grid_with_circle_and_random_dots()</code>","text":"<p>Plots a 15x15 grid of dots within a circle, highlights 3 randomly chosen dots  within the circle, and draws a grey box at the bottom.</p> <p>The function generates a grid of points within a circle of a specified radius  and randomly selects three points from within the circle. These points are  colored red and slightly enlarged. Additionally, a grey box is drawn at the  bottom of the plot.</p> Notes <ul> <li>The grid is plotted on a 15x15 layout, with points that fall within the    circle of radius 6.8 being displayed.</li> <li>The randomly selected points must be at least 4 grid units apart.</li> <li>A grey rectangular box is drawn near the bottom of the plot for aesthetic    purposes.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; plot_grid_with_circle_and_random_dots()\n# This will display a plot of a circle containing a grid of dots with \n# 3 randomly chosen dots highlighted in red.\n</code></pre> Source code in <code>neuro_py/behavior/cheeseboard.py</code> <pre><code>def plot_grid_with_circle_and_random_dots():\n    \"\"\"\n    Plots a 15x15 grid of dots within a circle, highlights 3 randomly chosen dots \n    within the circle, and draws a grey box at the bottom.\n\n    The function generates a grid of points within a circle of a specified radius \n    and randomly selects three points from within the circle. These points are \n    colored red and slightly enlarged. Additionally, a grey box is drawn at the \n    bottom of the plot.\n\n    Notes\n    -----\n    - The grid is plotted on a 15x15 layout, with points that fall within the \n      circle of radius 6.8 being displayed.\n    - The randomly selected points must be at least 4 grid units apart.\n    - A grey rectangular box is drawn near the bottom of the plot for aesthetic \n      purposes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; plot_grid_with_circle_and_random_dots()\n    # This will display a plot of a circle containing a grid of dots with \n    # 3 randomly chosen dots highlighted in red.\n    \"\"\"\n    # Create a 15x15 grid of dots within the circle\n    x = np.linspace(-7, 7, 17)\n    y = np.linspace(-7, 7, 17)\n    X, Y = np.meshgrid(x, y)\n\n    # Calculate the circle parameters with an offset\n    radius = 6.8  # Radius of the circle\n    circle_center = (0, 0)  # Center of the circle\n\n    # Create a mask to display only the dots within the circle\n    circle_mask = (X**2 + Y**2) &lt;= radius**2\n\n    # Plot the grid of dots within the circle\n    plt.figure(figsize=(8, 8))\n    plt.plot(X[circle_mask], Y[circle_mask], \"o\", color=\"k\", markersize=6)\n\n    # Plot the circle\n    circle = plt.Circle(circle_center, radius, color=\"black\", fill=False)\n    plt.gca().add_patch(circle)\n\n    # Randomly pick 3 dots within the circle\n    num_dots = 3\n    chosen_indices = np.random.choice(np.sum(circle_mask), size=num_dots, replace=False)\n    chosen_dots = np.argwhere(circle_mask)\n    chosen_dots = chosen_dots[chosen_indices]\n\n    # Ensure minimum separation of 4 dots between the randomly chosen dots\n    min_separation = 4\n    for i in range(num_dots):\n        for j in range(i + 1, num_dots):\n            while np.linalg.norm(chosen_dots[i] - chosen_dots[j]) &lt; min_separation:\n                chosen_indices[j] = np.random.choice(np.sum(circle_mask), size=1)[0]\n                chosen_dots[j] = np.argwhere(circle_mask)[chosen_indices[j]]\n\n    # Color the randomly chosen dots red and make them slightly larger\n    for dot in chosen_dots:\n        plt.plot(X[dot[0], dot[1]], Y[dot[0], dot[1]], \"o\", color=\"red\", markersize=9)\n\n    # Draw a grey box at the bottom\n    plt.fill_between([-1.5, 1.5], -8.5, -6.5, color=\"darkgray\", alpha=1)\n\n    plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n    plt.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"reference/neuro_py/behavior/get_trials/","title":"neuro_py.behavior.get_trials","text":""},{"location":"reference/neuro_py/behavior/get_trials/#neuro_py.behavior.get_trials.get_cheeseboard_trials","title":"<code>get_cheeseboard_trials(basepath, min_distance_from_home=15, max_trial_time=600, min_trial_time=5, kernel_size=2, min_std_away_from_home=6)</code>","text":"<p>Get epochs of cheeseboard trials.</p> <p>This function retrieves epochs for cheeseboard trials based on specified  distance and time criteria.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the session data.</p> required <code>min_distance_from_home</code> <code>int</code> <p>The minimum distance from home to be considered a trial. Default is 15.</p> <code>15</code> <code>max_trial_time</code> <code>int</code> <p>The maximum duration of a trial in seconds. Default is 600 (10 minutes).</p> <code>600</code> <code>min_trial_time</code> <code>int</code> <p>The minimum duration of a trial in seconds. Default is 5.</p> <code>5</code> <code>kernel_size</code> <code>int</code> <p>The size of the kernel to use for smoothing. Default is 2.</p> <code>2</code> <code>min_std_away_from_home</code> <code>int</code> <p>The minimum standard deviation away from home to be considered a trial. Default is 6.</p> <code>6</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>PositionArray</code> <p>The position data for the cheeseboard trials.</p> <code>trials</code> <code>EpochArray</code> <p>The epochs of the trials.</p> Notes <p>This function requires the following metadata dependencies:</p> <ul> <li><code>animal.behavior.mat</code>: contains homebox_x and homebox_y coordinates within epochs.</li> </ul> <p>You can label these with <code>label_key_locations_cheeseboard.m</code> or manually.</p> Source code in <code>neuro_py/behavior/get_trials.py</code> <pre><code>def get_cheeseboard_trials(\n    basepath: str,\n    min_distance_from_home: int = 15,\n    max_trial_time: int = 600,  # Default is 60 * 10\n    min_trial_time: int = 5,\n    kernel_size: int = 2,\n    min_std_away_from_home: int = 6,\n) -&gt; Tuple[nel.PositionArray, nel.EpochArray]:\n    \"\"\"\n    Get epochs of cheeseboard trials.\n\n    This function retrieves epochs for cheeseboard trials based on specified \n    distance and time criteria.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the session data.\n    min_distance_from_home : int, optional\n        The minimum distance from home to be considered a trial. Default is 15.\n    max_trial_time : int, optional\n        The maximum duration of a trial in seconds. Default is 600 (10 minutes).\n    min_trial_time : int, optional\n        The minimum duration of a trial in seconds. Default is 5.\n    kernel_size : int, optional\n        The size of the kernel to use for smoothing. Default is 2.\n    min_std_away_from_home : int, optional\n        The minimum standard deviation away from home to be considered a trial.\n        Default is 6.\n\n    Returns\n    -------\n    pos : PositionArray\n        The position data for the cheeseboard trials.\n    trials : EpochArray\n        The epochs of the trials.\n\n    Notes\n    -----\n    This function requires the following metadata dependencies:\n\n    - `animal.behavior.mat`: contains homebox_x and homebox_y coordinates within epochs.\n\n    You can label these with `label_key_locations_cheeseboard.m` or manually.\n    \"\"\"\n\n    # load position and key location metadata\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".animal.behavior.mat\"\n    )\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    # load epochs and place in array\n    epoch_df = loading.load_epoch(basepath)\n    epoch = nel.EpochArray(\n        [np.array([epoch_df.startTime, epoch_df.stopTime]).T], label=\"session_epochs\"\n    )\n\n    # load position and place in array\n    position_df = loading.load_animal_behavior(basepath)\n    position_df_no_nan = position_df.query(\"not x.isnull() &amp; not y.isnull()\")\n    pos = nel.PositionArray(\n        data=position_df_no_nan[[\"x\", \"y\"]].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n    # calculate kernel samples size based on sampling rate for x seconds\n    kernel_size = int(pos.fs * kernel_size)\n    # check if even number\n    if kernel_size % 2 == 0:\n        kernel_size += 1\n\n    cheeseboard_idx = np.where(epoch_df.environment == \"cheeseboard\")[0]\n    trials_temp = []\n    stddev = []\n    for idx in cheeseboard_idx:\n        # get homebox location\n        homebox_x = data[\"behavior\"][\"epochs\"][idx][\"homebox_x\"]\n        homebox_y = data[\"behavior\"][\"epochs\"][idx][\"homebox_y\"]\n\n        # get position during epoch\n        current_pos = pos[epoch[int(idx)]]\n        x, y = current_pos.data\n\n        # calculate distance from homebox\n        distance = np.sqrt((x - homebox_x) ** 2 + (y - homebox_y) ** 2)\n\n        # median filter distance to remove noise (jumps in position)\n        distance = medfilt(distance, kernel_size=kernel_size)\n\n        # find intervals where distance is greater than min_distance_from_home\n        dist_intervals = np.array(find_interval((distance &gt; min_distance_from_home)))\n\n        close_distances = distance[distance &lt; min_distance_from_home]\n        for trial in dist_intervals:\n            far_distances = distance[trial[0] : trial[1]].mean()\n\n            stddev.append(\n                (np.abs(far_distances) - np.nanmean(np.abs(close_distances), axis=0))\n                / np.nanstd(np.abs(close_distances), axis=0)\n            )\n\n        # get start and stop times of intervals\n        if len(dist_intervals) &gt; 0:\n            trials_temp.append(current_pos.time[dist_intervals])\n\n    # concatenate trials and place in EpochArray\n    trials = nel.EpochArray(np.vstack(trials_temp))\n\n    # remove trials that are too long or too short\n    trials._data = trials.data[\n        (trials.durations &lt; max_trial_time)\n        &amp; (trials.durations &gt; min_trial_time)\n        &amp; (np.array(stddev) &gt; min_std_away_from_home)\n    ]\n\n    return pos, trials\n</code></pre>"},{"location":"reference/neuro_py/behavior/get_trials/#neuro_py.behavior.get_trials.get_linear_maze_trials","title":"<code>get_linear_maze_trials(basepath, epoch_input=None)</code>","text":"<p>Get trials for linear maze.</p> <p>Locates inbound and outbound laps for each linear track in the session.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The path to the base directory of the session data.</p> required <code>epoch_input</code> <code>None</code> <p>Deprecated parameter. This is no longer supported.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>PositionArray or None</code> <p>The position data for the linear maze trials.</p> <code>inbound_laps</code> <code>EpochArray or None</code> <p>The epochs corresponding to inbound laps.</p> <code>outbound_laps</code> <code>EpochArray or None</code> <p>The epochs corresponding to outbound laps.</p> Notes <p>If no valid position data is found, None values are returned for all outputs.</p> Source code in <code>neuro_py/behavior/get_trials.py</code> <pre><code>def get_linear_maze_trials(basepath: str, epoch_input: None = None) -&gt; Tuple[\n    Union[nel.PositionArray, None],\n    Union[nel.EpochArray, None],\n    Union[nel.EpochArray, None],\n]:\n    \"\"\"Get trials for linear maze.\n\n    Locates inbound and outbound laps for each linear track in the session.\n\n    Parameters\n    ----------\n    basepath : str\n        The path to the base directory of the session data.\n    epoch_input : None, optional\n        Deprecated parameter. This is no longer supported.\n\n    Returns\n    -------\n    pos : PositionArray or None\n        The position data for the linear maze trials.\n    inbound_laps : EpochArray or None\n        The epochs corresponding to inbound laps.\n    outbound_laps : EpochArray or None\n        The epochs corresponding to outbound laps.\n\n    Notes\n    -----\n    If no valid position data is found, None values are returned for all\n    outputs.\n    \"\"\"\n    if epoch_input is not None:\n        logging.warning(\"epoch_input is no longer supported\")\n\n    position_df = loading.load_animal_behavior(basepath)\n    position_df_no_nan = position_df.query(\"not x.isnull() &amp; not y.isnull()\")\n\n    if position_df_no_nan.shape[0] == 0:\n        return None, None, None\n\n    if \"linearized\" not in position_df_no_nan.columns:\n        return None, None, None\n\n    pos = nel.PositionArray(\n        data=position_df_no_nan[\"linearized\"].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n\n    epoch_df = loading.load_epoch(basepath)\n    epoch = nel.EpochArray([np.array([epoch_df.startTime, epoch_df.stopTime]).T])\n\n    domain = nel.EpochArray(\n        [np.array([epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]]).T]\n    )\n\n    inbound_laps_temp = []\n    outbound_laps_temp = []\n    maze_idx = np.where(epoch_df.environment == \"linear\")[0]\n    for idx in maze_idx:\n        current_position = pos[epoch[int(idx)]]\n\n        # get outbound and inbound epochs\n        outbound_laps, inbound_laps = linear_positions.get_linear_track_lap_epochs(\n            current_position.abscissa_vals, current_position.data[0], newLapThreshold=20\n        )\n        if not inbound_laps.isempty:\n            inbound_laps = linear_positions.find_good_lap_epochs(\n                current_position, inbound_laps, min_laps=5\n            )\n\n        if not outbound_laps.isempty:\n            outbound_laps = linear_positions.find_good_lap_epochs(\n                current_position, outbound_laps, min_laps=5\n            )\n\n        if not inbound_laps.isempty:\n            inbound_laps_temp.append(inbound_laps.data)\n        if not outbound_laps.isempty:\n            outbound_laps_temp.append(outbound_laps.data)\n\n    inbound_laps = nel.EpochArray(np.vstack(inbound_laps_temp), domain=domain)\n    outbound_laps = nel.EpochArray(np.vstack(outbound_laps_temp), domain=domain)\n\n    return pos, inbound_laps, outbound_laps\n</code></pre>"},{"location":"reference/neuro_py/behavior/get_trials/#neuro_py.behavior.get_trials.get_openfield_trials","title":"<code>get_openfield_trials(basepath, epoch_type='epochs', spatial_binsize=3, n_time_bins=1, bin_method='dynamic', trial_time_bin_size=60, prop_trial_sampled=0.5, environments=['box', 'bigSquare', 'midSquare', 'bigSquarePlus', 'plus'], minimum_correlation=0.6, method='correlation')</code>","text":"<p>Get epochs of openfield trials.</p> <p>This function identifies trials in an open field environment that meet specific criteria for spatial sampling to assess spatial stability and population correlations.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the session data.</p> required <code>epoch_type</code> <code>str</code> <p>The type of epoch to use ('trials' or 'epochs'). Default is 'epochs'.</p> <code>'epochs'</code> <code>spatial_binsize</code> <code>int</code> <p>The size of spatial bins to use for occupancy. Default is 3.</p> <code>3</code> <code>n_time_bins</code> <code>int</code> <p>The number of time bins to use for occupancy for fixed bin method.  Default is 1.</p> <code>1</code> <code>bin_method</code> <code>str</code> <p>The method to use for binning time ('dynamic' or 'fixed').  Default is 'dynamic'.</p> <code>'dynamic'</code> <code>trial_time_bin_size</code> <code>Union[int, float]</code> <p>The size of time bins to use for occupancy for dynamic bin method  (in seconds). Default is 60.</p> <code>60</code> <code>prop_trial_sampled</code> <code>float</code> <p>The proportion of trials to sample. Default is 0.5.</p> <code>0.5</code> <code>environments</code> <code>List[str]</code> <p>A list of environments to include as open field. Default includes  several environments such as 'box' and 'plus'.</p> <code>['box', 'bigSquare', 'midSquare', 'bigSquarePlus', 'plus']</code> <code>minimum_correlation</code> <code>float</code> <p>The minimum correlation between trials to be considered a trial.  Default is 0.6.</p> <code>0.6</code> <code>method</code> <code>str</code> <p>The method to use ('correlation' or 'proportion'). Default is  'correlation'. <code>correlation</code> - use correlation between the trial map and the overall map to determine if it is a trial. <code>proportion</code> - use the proportion of the trial map that is sampled to determine if it is a trial</p> <code>'correlation'</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>PositionArray</code> <p>The position data for the open field trials.</p> <code>trials</code> <code>EpochArray</code> <p>The epochs of the identified trials.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the method is not 'correlation' or 'proportion'.</p> Notes <p>This function requires the loading of animal behavior and epoch data from the specified base path.</p> Source code in <code>neuro_py/behavior/get_trials.py</code> <pre><code>def get_openfield_trials(\n    basepath: str,\n    epoch_type: str = \"epochs\",\n    spatial_binsize: int = 3,\n    n_time_bins: int = 1,  # for bin_method = \"fixed\", not used for bin_method = \"dynamic\"\n    bin_method: str = \"dynamic\",\n    trial_time_bin_size: Union[int, float] = 60,  # in seconds for bin_method = \"dynamic\", not used for bin_method = \"fixed\"\n    prop_trial_sampled: float = 0.5,\n    environments: List[str] = [\n        \"box\",\n        \"bigSquare\",\n        \"midSquare\",\n        \"bigSquarePlus\",\n        \"plus\",\n    ],\n    minimum_correlation: float = 0.6,\n    method: str = \"correlation\",\n) -&gt; Tuple[nel.PositionArray, nel.EpochArray]:\n    \"\"\"\n    Get epochs of openfield trials.\n\n    This function identifies trials in an open field environment that meet\n    specific criteria for spatial sampling to assess spatial stability and\n    population correlations.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the session data.\n    epoch_type : str, optional\n        The type of epoch to use ('trials' or 'epochs'). Default is 'epochs'.\n    spatial_binsize : int, optional\n        The size of spatial bins to use for occupancy. Default is 3.\n    n_time_bins : int, optional\n        The number of time bins to use for occupancy for fixed bin method. \n        Default is 1.\n    bin_method : str, optional\n        The method to use for binning time ('dynamic' or 'fixed'). \n        Default is 'dynamic'.\n    trial_time_bin_size : Union[int, float], optional\n        The size of time bins to use for occupancy for dynamic bin method \n        (in seconds). Default is 60.\n    prop_trial_sampled : float, optional\n        The proportion of trials to sample. Default is 0.5.\n    environments : List[str], optional\n        A list of environments to include as open field. Default includes \n        several environments such as 'box' and 'plus'.\n    minimum_correlation : float, optional\n        The minimum correlation between trials to be considered a trial. \n        Default is 0.6.\n    method : str, optional\n        The method to use ('correlation' or 'proportion'). Default is \n        'correlation'. `correlation` - use correlation between the trial map and\n        the overall map to determine if it is a trial. `proportion` - use the\n        proportion of the trial map that is sampled to determine if it is a\n        trial\n\n    Returns\n    -------\n    pos : PositionArray\n        The position data for the open field trials.\n    trials : EpochArray\n        The epochs of the identified trials.\n\n    Raises\n    ------\n    ValueError\n        If the method is not 'correlation' or 'proportion'.\n\n    Notes\n    -----\n    This function requires the loading of animal behavior and epoch data\n    from the specified base path.\n    \"\"\"\n\n    def compute_occupancy_2d(\n        pos_run: object, x_edges: list, y_edges: list\n    ) -&gt; np.ndarray:\n        \"\"\"Compute occupancy of 2D position\n\n        Parameters\n        ----------\n        pos_run : object\n            Position data for the run\n        x_edges : list\n            Bin edges of x position\n        y_edges : list\n            Bin edges of y position\n\n        Returns\n        -------\n        np.ndarray\n            Occupancy map of the position\n        \"\"\"\n        occupancy, _, _ = np.histogram2d(\n            pos_run.data[0, :], pos_run.data[1, :], bins=(x_edges, y_edges)\n        )\n        return occupancy / pos_run.fs\n\n    # load position and place in array\n    position_df = loading.load_animal_behavior(basepath)\n    position_df_no_nan = position_df.query(\"not x.isnull() &amp; not y.isnull()\")\n    pos = nel.PositionArray(\n        data=position_df_no_nan[[\"x\", \"y\"]].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n\n    if pos.isempty:\n        return pos, nel.EpochArray([], label=\"session_epochs\")\n\n    # load epochs and place in array\n    if epoch_type == \"trials\":\n        epoch_df = loading.load_trials(basepath)\n        openfield_idx = np.arange(\n            0, len(epoch_df)\n        )  # assume trials make up all epochs associated with position\n        trialsID = epoch_df.trialsID.values\n    elif epoch_type == \"epochs\":\n        epoch_df = loading.load_epoch(basepath)\n        # find epochs that are these environments\n        openfield_idx = np.where(np.isin(epoch_df.environment, environments))[0]\n\n    epoch = nel.EpochArray([np.array([epoch_df.startTime, epoch_df.stopTime]).T])\n\n    # find epochs that are these environments\n    trials = []\n    if epoch_type == \"trials\":\n        trial_ID = []\n\n    # loop through epochs\n    for idx in openfield_idx:\n        # get position during epoch\n        current_position = pos[epoch[int(idx)]]\n\n        if current_position.isempty:\n            continue\n\n        # get the edges of the position\n        ext_xmin, ext_xmax = (\n            np.floor(np.nanmin(current_position.data[0, :])),\n            np.ceil(np.nanmax(current_position.data[0, :])),\n        )\n        ext_ymin, ext_ymax = (\n            np.floor(np.nanmin(current_position.data[1, :])),\n            np.ceil(np.nanmax(current_position.data[1, :])),\n        )\n        # create bin edges for occupancy map at spatial_binsize\n        x_edges = np.arange(ext_xmin, ext_xmax + spatial_binsize, spatial_binsize)\n        y_edges = np.arange(ext_ymin, ext_ymax + spatial_binsize, spatial_binsize)\n\n        # compute occupancy map and get proportion of environment sampled\n        occupancy = compute_occupancy_2d(current_position, x_edges, y_edges)\n        overall_prop_sampled = sum(occupancy.flatten() &gt; 0) / (\n            (len(x_edges) - 1) * (len(y_edges) - 1)\n        )\n        # create possible trials based on trial_time_bin_size\n        # these will be iterated over to find trials that are sampled enough\n        duration = epoch_df.iloc[idx].stopTime - epoch_df.iloc[idx].startTime\n\n        if bin_method == \"dynamic\":\n            bins = np.linspace(\n                epoch_df.iloc[idx].startTime,\n                epoch_df.iloc[idx].stopTime,\n                int(np.ceil(duration / (trial_time_bin_size))),\n            )\n        elif bin_method == \"fixed\":\n            bins = np.arange(\n                epoch_df.iloc[idx].startTime,\n                epoch_df.iloc[idx].stopTime,\n                int(np.floor(epoch[int(idx)].duration / n_time_bins)),\n            )\n        trials_temp = nel.EpochArray(np.array([bins[:-1], bins[1:]]).T)\n        if epoch_type == \"trials\":\n            temp_ID = trialsID[idx]\n\n        trial_i = 0\n        # loop through possible trials and find when sampled enough\n        for i_interval in range(trials_temp.n_intervals):\n            # compute occupancy map and get proportion of environment sampled for trial\n            trial_occupancy = compute_occupancy_2d(\n                current_position[trials_temp[trial_i : i_interval + 1]],\n                x_edges,\n                y_edges,\n            )\n\n            if method == \"correlation\":\n                # correlate trial_occupancy with overall occupancy\n                r = np.corrcoef(\n                    occupancy.flatten() &gt; 0,\n                    trial_occupancy.flatten() &gt; 0,\n                )[0, 1]\n\n                # if sampled enough, add to trials\n                if r &gt; minimum_correlation:\n                    trials.append(\n                        [\n                            trials_temp[trial_i : i_interval + 1].start,\n                            trials_temp[trial_i : i_interval + 1].stop,\n                        ]\n                    )\n                    if epoch_type == \"trials\":\n                        trial_ID.append(temp_ID + \"_\" + str(idx))\n                    # update trial_i to next interval to start from\n                    trial_i = i_interval + 1\n\n            elif method == \"proportion\":\n                trial_prop_sampled = sum(trial_occupancy.flatten() &gt; 0) / (\n                    (len(x_edges) - 1) * (len(y_edges) - 1)\n                )\n                if trial_prop_sampled &gt; prop_trial_sampled * overall_prop_sampled:\n                    trials.append(\n                        [\n                            trials_temp[trial_i : i_interval + 1].start,\n                            trials_temp[trial_i : i_interval + 1].stop,\n                        ]\n                    )\n                    if epoch_type == \"trials\":\n                        trial_ID.append(temp_ID + \"_\" + str(idx))\n                    # update trial_i to next interval to start from\n                    trial_i = i_interval + 1\n            else:\n                raise ValueError(\"method must be correlation or proportion\")\n\n    # concatenate trials and place in EpochArray\n    if epoch_type == \"trials\":\n        trials = nel.EpochArray(np.vstack(trials), label=np.vstack(trial_ID))\n    else:\n        trials = nel.EpochArray(np.vstack(trials), label=\"session_epochs\")\n\n    return pos, trials\n</code></pre>"},{"location":"reference/neuro_py/behavior/get_trials/#neuro_py.behavior.get_trials.get_t_maze_trials","title":"<code>get_t_maze_trials(basepath, epoch, bypass_standard_behavior=False)</code>","text":"<p>Get trials for T maze.</p> <p>This function retrieves position data and epochs for right and left trials based on the specified epoch. It checks if the number of outbound laps exceeds the number of inbound laps unless bypassed.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the session data.</p> required <code>epoch</code> <code>EpochArray</code> <p>The epoch to get trials for.</p> required <code>bypass_standard_behavior</code> <code>bool</code> <p>If True, allows for more outbound than inbound trials. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>PositionArray or None</code> <p>The position data for the T maze trials.</p> <code>right_epochs</code> <code>EpochArray or None</code> <p>The epochs corresponding to right trials.</p> <code>left_epochs</code> <code>EpochArray or None</code> <p>The epochs corresponding to left trials.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If inbound laps exceed outbound laps and bypass_standard_behavior is False.</p> Notes <p>If there are no valid positions or states in the session data, None is returned for all outputs.</p> Source code in <code>neuro_py/behavior/get_trials.py</code> <pre><code>def get_t_maze_trials(\n    basepath: str, epoch: nel.EpochArray, bypass_standard_behavior: bool = False\n) -&gt; Tuple[\n    Union[nel.PositionArray, None],\n    Union[nel.EpochArray, None],\n    Union[nel.EpochArray, None],\n]:\n    \"\"\"\n    Get trials for T maze.\n\n    This function retrieves position data and epochs for right and left trials\n    based on the specified epoch. It checks if the number of outbound laps exceeds\n    the number of inbound laps unless bypassed.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the session data.\n    epoch : nel.EpochArray\n        The epoch to get trials for.\n    bypass_standard_behavior : bool, optional\n        If True, allows for more outbound than inbound trials. Default is False.\n\n    Returns\n    -------\n    pos : PositionArray or None\n        The position data for the T maze trials.\n    right_epochs : EpochArray or None\n        The epochs corresponding to right trials.\n    left_epochs : EpochArray or None\n        The epochs corresponding to left trials.\n\n    Raises\n    ------\n    TypeError\n        If inbound laps exceed outbound laps and bypass_standard_behavior is False.\n\n    Notes\n    -----\n    If there are no valid positions or states in the session data, None is returned\n    for all outputs.\n    \"\"\"\n\n    def dissociate_laps_by_states(states, dir_epoch, states_of_interest=[1, 2]):\n        # unique_states = np.unique(states.data[~np.isnan(states.data)])\n        lap_id = []\n        for ep in dir_epoch:\n            state_count = []\n            for us in states_of_interest:\n                state_count.append(np.nansum(states[ep].data == us))\n            lap_id.append(states_of_interest[np.argmax(state_count)])\n        return np.array(lap_id).astype(int)\n\n    position_df = loading.load_animal_behavior(basepath)\n    position_df_no_nan = position_df.query(\"not x.isnull() &amp; not y.isnull()\")\n\n    if position_df_no_nan.shape[0] == 0:\n        return None, None, None\n\n    if \"linearized\" not in position_df_no_nan.columns:\n        return None, None, None\n\n    if \"states\" not in position_df_no_nan.columns:\n        return None, None, None\n\n    pos = nel.PositionArray(\n        data=position_df_no_nan[\"linearized\"].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n\n    pos = pos[epoch]\n    if pos.isempty:\n        return None, None, None\n\n    states = nel.AnalogSignalArray(\n        data=position_df_no_nan[\"states\"].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n    states = states[epoch]\n\n    # get outbound and inbound epochs\n    outbound_laps, inbound_laps = linear_positions.get_linear_track_lap_epochs(\n        pos.abscissa_vals, pos.data[0], newLapThreshold=20\n    )\n\n    inbound_laps = linear_positions.find_good_lap_epochs(pos, inbound_laps, min_laps=5)\n    outbound_laps = linear_positions.find_good_lap_epochs(\n        pos, outbound_laps, min_laps=5\n    )\n\n    if outbound_laps.isempty:\n        return None, None, None\n\n    if not inbound_laps.isempty:\n        logging.warning(\"inbound_laps should be empty for tmaze\")\n\n    if (\n        inbound_laps.n_intervals &gt; outbound_laps.n_intervals\n    ) and not bypass_standard_behavior:\n        raise TypeError(\"inbound_laps should be less than outbound_laps for tmaze\")\n\n    # locate laps with the majority in state 1 or 2\n    lap_id = dissociate_laps_by_states(states, outbound_laps, states_of_interest=[1, 2])\n\n    right_epochs = nel.EpochArray(data=outbound_laps.data[lap_id == 1, :])\n    left_epochs = nel.EpochArray(data=outbound_laps.data[lap_id == 2, :])\n\n    position_df_no_nan = position_df_no_nan[\n        position_df_no_nan[\"time\"].between(epoch.start, epoch.stop)\n    ]\n    return pos, right_epochs, left_epochs\n</code></pre>"},{"location":"reference/neuro_py/behavior/get_trials/#neuro_py.behavior.get_trials.get_w_maze_trials","title":"<code>get_w_maze_trials(basepath, max_distance_from_well=20, min_distance_traveled=50)</code>","text":"<p>Get trials for W maze.</p> <p>This function retrieves position data and identifies trials for the W maze based on specified distance criteria.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the session data.</p> required <code>max_distance_from_well</code> <code>int</code> <p>The maximum distance from the well to be considered a trial. Default is 20.</p> <code>20</code> <code>min_distance_traveled</code> <code>int</code> <p>The minimum distance traveled to be considered a trial. Default is 50.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>pos</code> <code>PositionArray or None</code> <p>The position data for the W maze trials.</p> <code>trials</code> <code>ndarray or None</code> <p>The indices of the trials.</p> <code>right_trials</code> <code>ndarray or None</code> <p>The indices of the right trials.</p> <code>left_trials</code> <code>ndarray or None</code> <p>The indices of the left trials.</p> Notes <p>This function requires the following metadata dependencies:</p> <ul> <li><code>animal.behavior.mat</code>: contains center, left, and right x y coordinates.</li> </ul> <p>You can label these with <code>label_key_locations_wmaze.m</code> or manually.</p> Source code in <code>neuro_py/behavior/get_trials.py</code> <pre><code>def get_w_maze_trials(\n    basepath: str, max_distance_from_well: int = 20, min_distance_traveled: int = 50\n) -&gt; Tuple[\n    Union[nel.PositionArray, None],\n    Union[np.ndarray, None],\n    Union[np.ndarray, None],\n    Union[np.ndarray, None],\n]:\n    \"\"\"\n    Get trials for W maze.\n\n    This function retrieves position data and identifies trials for the W maze\n    based on specified distance criteria.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the session data.\n    max_distance_from_well : int, optional\n        The maximum distance from the well to be considered a trial. Default is 20.\n    min_distance_traveled : int, optional\n        The minimum distance traveled to be considered a trial. Default is 50.\n\n    Returns\n    -------\n    pos : PositionArray or None\n        The position data for the W maze trials.\n    trials : ndarray or None\n        The indices of the trials.\n    right_trials : ndarray or None\n        The indices of the right trials.\n    left_trials : ndarray or None\n        The indices of the left trials.\n\n    Notes\n    -----\n    This function requires the following metadata dependencies:\n\n    - `animal.behavior.mat`: contains center, left, and right x y coordinates.\n\n    You can label these with `label_key_locations_wmaze.m` or manually.\n    \"\"\"\n\n    # load position and key location metadata\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".animal.behavior.mat\"\n    )\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    # load epochs and place in array\n    epoch_df = loading.load_epoch(basepath)\n\n    # load position and place in array\n    position_df = loading.load_animal_behavior(basepath)\n    position_df_no_nan = position_df.query(\"not x.isnull() &amp; not y.isnull()\")\n\n    pos = nel.PositionArray(\n        data=position_df_no_nan[\"linearized\"].values.T,\n        timestamps=position_df_no_nan.timestamps.values,\n    )\n    wmaze_idx = np.where(epoch_df.environment == \"wmaze\")[0]\n    for idx in wmaze_idx:\n        # get key locations\n        right_x = data[\"behavior\"][\"epochs\"][idx][\"right_x\"]\n        right_y = data[\"behavior\"][\"epochs\"][idx][\"right_y\"]\n\n        center_x = data[\"behavior\"][\"epochs\"][idx][\"center_x\"]\n        center_y = data[\"behavior\"][\"epochs\"][idx][\"center_y\"]\n\n        left_x = data[\"behavior\"][\"epochs\"][idx][\"left_x\"]\n        left_y = data[\"behavior\"][\"epochs\"][idx][\"left_y\"]\n\n        well_locations = np.array(\n            [[center_x, center_y], [left_x, left_y], [right_x, right_y]]\n        )\n\n        current_ts_idx = position_df_no_nan[\"timestamps\"].between(\n            epoch_df.iloc[idx].startTime, epoch_df.iloc[idx].stopTime\n        )\n\n        # temp_df = position_df[~np.isnan(position_df.x)]\n        segments_df, _ = well_traversal_classification.segment_path(\n            position_df_no_nan[\"timestamps\"].values[current_ts_idx],\n            position_df_no_nan[[\"x\", \"y\"]].values[current_ts_idx],\n            well_locations,\n            max_distance_from_well=max_distance_from_well,\n        )\n\n        segments_df = well_traversal_classification.score_inbound_outbound(\n            segments_df, min_distance_traveled=min_distance_traveled\n        )\n        conditions = [\n            \"from_well == 'Center' &amp; to_well == 'Left'\",\n            \"from_well == 'Left' &amp; to_well == 'Center'\",\n            \"from_well == 'Center' &amp; to_well == 'Right'\",\n            \"from_well == 'Right' &amp; to_well == 'Center'\",\n        ]\n        condition_labels = [\n            \"center_left\",\n            \"left_center\",\n            \"center_right\",\n            \"right_center\",\n        ]\n        trajectories = {}\n        for con, con_label in zip(conditions, condition_labels):\n            trajectories[con_label] = nel.EpochArray(\n                np.array(\n                    [segments_df.query(con).start_time, segments_df.query(con).end_time]\n                ).T\n            )\n\n    return pos, trajectories\n</code></pre>"},{"location":"reference/neuro_py/behavior/kinematics/","title":"neuro_py.behavior.kinematics","text":""},{"location":"reference/neuro_py/behavior/kinematics/#neuro_py.behavior.kinematics.get_speed","title":"<code>get_speed(position, time=None)</code>","text":"<p>Computes the speed from position data.</p> <p>Speed is the magnitude of the velocity vector at each time point. If time is not provided, it assumes a constant time step between position samples.</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>ndarray</code> <p>An array of position data. This can be 1D (for single-dimensional positions) or 2D (for multi-dimensional positions, e.g., x and y coordinates over time).</p> required <code>time</code> <code>Union[ndarray, None]</code> <p>An array of time values corresponding to the position data. If None, the function assumes a constant time step. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of speed values, where each speed is the magnitude of the velocity at the corresponding time point.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; position = np.array([0, 1, 4, 9, 16])\n&gt;&gt;&gt; get_speed(position)\narray([1.41421356, 2.82842712, 5.65685425, 8.48528137, 9.89949494])\n</code></pre> <pre><code>&gt;&gt;&gt; position = np.array([[0, 0], [1, 1], [4, 4], [9, 9], [16, 16]])\n&gt;&gt;&gt; time = np.array([0, 1, 2, 3, 4])\n&gt;&gt;&gt; get_speed(position, time)\narray([1.41421356, 2.82842712, 5.65685425, 8.48528137, 9.89949494])\n</code></pre> Source code in <code>neuro_py/behavior/kinematics.py</code> <pre><code>def get_speed(position: np.ndarray, time: Union[np.ndarray, None] = None) -&gt; np.ndarray:\n    \"\"\"\n    Computes the speed from position data.\n\n    Speed is the magnitude of the velocity vector at each time point. If time is\n    not provided, it assumes a constant time step between position samples.\n\n    Parameters\n    ----------\n    position : np.ndarray\n        An array of position data. This can be 1D (for single-dimensional positions)\n        or 2D (for multi-dimensional positions, e.g., x and y coordinates over time).\n    time : Union[np.ndarray, None], optional\n        An array of time values corresponding to the position data. If None,\n        the function assumes a constant time step. Default is None.\n\n    Returns\n    -------\n    np.ndarray\n        An array of speed values, where each speed is the magnitude of the velocity\n        at the corresponding time point.\n\n    Examples\n    --------\n    &gt;&gt;&gt; position = np.array([0, 1, 4, 9, 16])\n    &gt;&gt;&gt; get_speed(position)\n    array([1.41421356, 2.82842712, 5.65685425, 8.48528137, 9.89949494])\n\n    &gt;&gt;&gt; position = np.array([[0, 0], [1, 1], [4, 4], [9, 9], [16, 16]])\n    &gt;&gt;&gt; time = np.array([0, 1, 2, 3, 4])\n    &gt;&gt;&gt; get_speed(position, time)\n    array([1.41421356, 2.82842712, 5.65685425, 8.48528137, 9.89949494])\n    \"\"\"\n    velocity = get_velocity(position, time=time)\n    return np.sqrt(np.sum(velocity**2, axis=1))\n</code></pre>"},{"location":"reference/neuro_py/behavior/kinematics/#neuro_py.behavior.kinematics.get_velocity","title":"<code>get_velocity(position, time=None)</code>","text":"<p>Computes the velocity from position data.</p> <p>If time is not provided, it assumes a constant time step between position samples. The velocity is calculated as the gradient of the position with respect to time along the first axis.</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>ndarray</code> <p>An array of position data. This can be 1D (for single-dimensional positions) or 2D (for multi-dimensional positions, e.g., x and y coordinates over time).</p> required <code>time</code> <code>Union[ndarray, None]</code> <p>An array of time values corresponding to the position data. If None, the function assumes a constant time step. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of velocity values, where each velocity is the rate of change of position with respect to time.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; position = np.array([0, 1, 4, 9, 16])\n&gt;&gt;&gt; get_velocity(position)\narray([1., 2., 4., 6., 7.])\n</code></pre> <pre><code>&gt;&gt;&gt; position = np.array([[0, 0], [1, 1], [4, 4], [9, 9], [16, 16]])\n&gt;&gt;&gt; time = np.array([0, 1, 2, 3, 4])\n&gt;&gt;&gt; get_velocity(position, time)\narray([[1., 1.],\n    [2., 2.],\n    [4., 4.],\n    [6., 6.],\n    [7., 7.]])\n</code></pre> Source code in <code>neuro_py/behavior/kinematics.py</code> <pre><code>def get_velocity(\n    position: np.ndarray, time: Union[np.ndarray, None] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Computes the velocity from position data.\n\n    If time is not provided, it assumes a constant time step between position\n    samples. The velocity is calculated as the gradient of the position with\n    respect to time along the first axis.\n\n    Parameters\n    ----------\n    position : np.ndarray\n        An array of position data. This can be 1D (for single-dimensional positions)\n        or 2D (for multi-dimensional positions, e.g., x and y coordinates over time).\n    time : Union[np.ndarray, None], optional\n        An array of time values corresponding to the position data. If None,\n        the function assumes a constant time step. Default is None.\n\n    Returns\n    -------\n    np.ndarray\n        An array of velocity values, where each velocity is the rate of change of\n        position with respect to time.\n\n    Examples\n    --------\n    &gt;&gt;&gt; position = np.array([0, 1, 4, 9, 16])\n    &gt;&gt;&gt; get_velocity(position)\n    array([1., 2., 4., 6., 7.])\n\n    &gt;&gt;&gt; position = np.array([[0, 0], [1, 1], [4, 4], [9, 9], [16, 16]])\n    &gt;&gt;&gt; time = np.array([0, 1, 2, 3, 4])\n    &gt;&gt;&gt; get_velocity(position, time)\n    array([[1., 1.],\n        [2., 2.],\n        [4., 4.],\n        [6., 6.],\n        [7., 7.]])\n    \"\"\"\n    if time is None:\n        time = np.arange(position.shape[0])\n    return np.gradient(position, time, axis=0)\n</code></pre>"},{"location":"reference/neuro_py/behavior/linear_positions/","title":"neuro_py.behavior.linear_positions","text":""},{"location":"reference/neuro_py/behavior/linear_positions/#neuro_py.behavior.linear_positions.find_good_lap_epochs","title":"<code>find_good_lap_epochs(pos, dir_epoch, thres=0.5, binsize=6, min_laps=10)</code>","text":"<p>Find good laps in behavior data</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>AnalogSignalArray</code> <p>A nelpy AnalogSignalArray containing the position data with a single dimension.</p> required <code>dir_epoch</code> <code>EpochArray</code> <p>EpochArray defining the laps to analyze for good laps.</p> required <code>thres</code> <code>float</code> <p>Occupancy threshold to determine good laps, by default 0.5.</p> <code>0.5</code> <code>binsize</code> <code>int</code> <p>Size of the bins for calculating occupancy, by default 6.</p> <code>6</code> <code>min_laps</code> <code>int</code> <p>Minimum number of laps required to consider laps as 'good', by default 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>EpochArray</code> <p>An EpochArray containing the good laps based on the occupancy threshold. Returns an empty EpochArray if no good laps are found or if the number of laps is less than <code>min_laps</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; good_laps = find_good_lap_epochs(pos, dir_epoch)\n</code></pre> Notes <p>The function calculates the percent occupancy over position bins per lap, and identifies laps that meet the occupancy threshold criteria. The laps that meet this condition are returned as an EpochArray.</p> Source code in <code>neuro_py/behavior/linear_positions.py</code> <pre><code>def find_good_lap_epochs(\n    pos: nel.AnalogSignalArray,\n    dir_epoch: nel.EpochArray,\n    thres: float = 0.5,\n    binsize: int = 6,\n    min_laps: int = 10,\n) -&gt; nel.EpochArray:\n    \"\"\"\n    Find good laps in behavior data\n\n    Parameters\n    ----------\n    pos : nelpy.AnalogSignalArray\n        A nelpy AnalogSignalArray containing the position data with a single dimension.\n    dir_epoch : nelpy.EpochArray\n        EpochArray defining the laps to analyze for good laps.\n    thres : float, optional\n        Occupancy threshold to determine good laps, by default 0.5.\n    binsize : int, optional\n        Size of the bins for calculating occupancy, by default 6.\n    min_laps : int, optional\n        Minimum number of laps required to consider laps as 'good', by default 10.\n\n    Returns\n    -------\n    nelpy.EpochArray\n        An EpochArray containing the good laps based on the occupancy threshold.\n        Returns an empty EpochArray if no good laps are found or if the number\n        of laps is less than `min_laps`.\n\n    Examples\n    -------\n    &gt;&gt;&gt; good_laps = find_good_lap_epochs(pos, dir_epoch)\n\n    Notes\n    -----\n    The function calculates the percent occupancy over position bins per lap,\n    and identifies laps that meet the occupancy threshold criteria. The laps\n    that meet this condition are returned as an EpochArray.\n    \"\"\"\n    # Ensure the input data is valid\n    if pos.isempty or dir_epoch.isempty:\n        return nel.EpochArray()\n\n    # make bin edges to calc occupancy\n    x_edges = np.arange(np.nanmin(pos.data[0]), np.nanmax(pos.data[0]), binsize)\n    # initialize occupancy matrix (position x time)\n    occ = np.zeros([len(x_edges) - 1, dir_epoch.n_intervals])\n\n    # much faster to not use nelpy objects here, so pull out needed data\n    x_coord = pos.data[0]\n    time = pos.abscissa_vals\n    epochs = dir_epoch.data\n\n    # iterate through laps\n    for i, ep in enumerate(epochs):\n        # bin position per lap\n        occ[:, i], _ = np.histogram(\n            x_coord[(time &gt;= ep[0]) &amp; (time &lt;= ep[1])], bins=x_edges\n        )\n\n    # calc percent occupancy over position bins per lap and find good laps\n    good_laps = np.where(~((np.sum(occ == 0, axis=0) / occ.shape[0]) &gt; thres))[0]\n    # if no good laps, return empty epoch\n    if (len(good_laps) == 0) | (len(good_laps) &lt; min_laps):\n        dir_epoch = nel.EpochArray()\n    else:\n        dir_epoch = dir_epoch[good_laps]\n    return dir_epoch\n</code></pre>"},{"location":"reference/neuro_py/behavior/linear_positions/#neuro_py.behavior.linear_positions.get_linear_track_lap_epochs","title":"<code>get_linear_track_lap_epochs(ts, x, newLapThreshold=15, good_laps=False, edgethresh=0.1, completeprop=0.2, posbins=50)</code>","text":"<p>Identifies lap epochs on a linear track and classifies them into outbound and inbound directions.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>ndarray</code> <p>Array of timestamps corresponding to position data.</p> required <code>x</code> <code>ndarray</code> <p>Array of position data along the linear track.</p> required <code>newLapThreshold</code> <code>float</code> <p>Minimum distance between laps to define a new lap, by default 15.</p> <code>15</code> <code>good_laps</code> <code>bool</code> <p>If True, filter out laps that do not meet certain quality criteria, by default False.</p> <code>False</code> <code>edgethresh</code> <code>float</code> <p>Threshold proportion of the track edge to identify potential boundary errors, by default 0.1.</p> <code>0.1</code> <code>completeprop</code> <code>float</code> <p>Minimum proportion of the track that must be traversed for a lap to be considered complete, by default 0.2.</p> <code>0.2</code> <code>posbins</code> <code>int</code> <p>Number of bins to divide the track into for analysis, by default 50.</p> <code>50</code> <p>Returns:</p> Type Description <code>Tuple[EpochArray, EpochArray]</code> <p>A tuple containing two nelpy EpochArray objects: - outbound_epochs: Epochs representing outbound runs (towards the far end of the track). - inbound_epochs: Epochs representing inbound runs (back towards the start).</p> Notes <ul> <li>This function calls <code>find_laps</code> to determine the lap structure, then segregates epochs into outbound and inbound directions.</li> <li>The EpochArray objects represent the start and stop timestamps for each identified lap.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; outbound_epochs, inbound_epochs = get_linear_track_lap_epochs(ts, x)\n</code></pre> Source code in <code>neuro_py/behavior/linear_positions.py</code> <pre><code>def get_linear_track_lap_epochs(\n    ts: np.ndarray,\n    x: np.ndarray,\n    newLapThreshold: float = 15,\n    good_laps: bool = False,\n    edgethresh: float = 0.1,\n    completeprop: float = 0.2,\n    posbins: int = 50,\n) -&gt; Tuple[nel.EpochArray, nel.EpochArray]:\n    \"\"\"\n    Identifies lap epochs on a linear track and classifies them into outbound and inbound directions.\n\n    Parameters\n    ----------\n    ts : np.ndarray\n        Array of timestamps corresponding to position data.\n    x : np.ndarray\n        Array of position data along the linear track.\n    newLapThreshold : float, optional\n        Minimum distance between laps to define a new lap, by default 15.\n    good_laps : bool, optional\n        If True, filter out laps that do not meet certain quality criteria, by default False.\n    edgethresh : float, optional\n        Threshold proportion of the track edge to identify potential boundary errors, by default 0.1.\n    completeprop : float, optional\n        Minimum proportion of the track that must be traversed for a lap to be considered complete, by default 0.2.\n    posbins : int, optional\n        Number of bins to divide the track into for analysis, by default 50.\n\n    Returns\n    -------\n    Tuple[nel.EpochArray, nel.EpochArray]\n        A tuple containing two nelpy EpochArray objects:\n        - outbound_epochs: Epochs representing outbound runs (towards the far end of the track).\n        - inbound_epochs: Epochs representing inbound runs (back towards the start).\n\n    Notes\n    ------\n    - This function calls `find_laps` to determine the lap structure, then segregates epochs into outbound and inbound directions.\n    - The EpochArray objects represent the start and stop timestamps for each identified lap.\n\n    Examples\n    -------\n    &gt;&gt;&gt; outbound_epochs, inbound_epochs = get_linear_track_lap_epochs(ts, x)\n\n    \"\"\"\n    laps = __find_laps(\n        np.array(ts),\n        np.array(x),\n        newLapThreshold=newLapThreshold,\n        good_laps=good_laps,\n        edgethresh=edgethresh,\n        completeprop=completeprop,\n        posbins=posbins,\n    )\n\n    # Handle no laps\n    if len(laps) == 0:\n        return nel.EpochArray(), nel.EpochArray()\n\n    outbound_start = []\n    outbound_stop = []\n    inbound_start = []\n    inbound_stop = []\n\n    for i in range(len(laps) - 1):\n        if laps.iloc[i].direction == 1:\n            outbound_start.append(laps.iloc[i].start_ts)\n            outbound_stop.append(laps.iloc[i + 1].start_ts)\n\n        if laps.iloc[i].direction == -1:\n            inbound_start.append(laps.iloc[i].start_ts)\n            inbound_stop.append(laps.iloc[i + 1].start_ts)\n\n    outbound_epochs = nel.EpochArray([np.array([outbound_start, outbound_stop]).T])\n    inbound_epochs = nel.EpochArray([np.array([inbound_start, inbound_stop]).T])\n\n    return outbound_epochs, inbound_epochs\n</code></pre>"},{"location":"reference/neuro_py/behavior/linear_positions/#neuro_py.behavior.linear_positions.linearize_position","title":"<code>linearize_position(x, y)</code>","text":"<p>Use PCA (a dimensionality reduction technique) to find the direction of maximal variance in our position data, and use this as the new 1D linear track axis.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>x-coordinates of shape (n, 1)</p> required <code>y</code> <code>ndarray</code> <p>y-coordinates of shape (n, 1)</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>Linearized x and y coordinates, both of shape (n, 1).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; x = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; linear_x, linear_y = npy.behavior.linearize_position(x, y)\n&gt;&gt;&gt; linear_x\narray([0.        , 1.41421356, 2.82842712, 4.24264069, 5.65685425])\n&gt;&gt;&gt; linear_y\narray([3.92192151e-16, 0.00000000e+00, 9.80480378e-17, 1.96096076e-16, 2.94144113e-16])\n</code></pre> Source code in <code>neuro_py/behavior/linear_positions.py</code> <pre><code>def linearize_position(x: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Use PCA (a dimensionality reduction technique) to find the direction of maximal variance\n    in our position data, and use this as the new 1D linear track axis.\n\n    Parameters\n    ----------\n    x : numpy.ndarray\n        x-coordinates of shape (n, 1)\n    y : numpy.ndarray\n        y-coordinates of shape (n, 1)\n\n    Returns\n    -------\n    tuple[numpy.ndarray, numpy.ndarray]\n        Linearized x and y coordinates, both of shape (n, 1).\n\n    Examples\n    --------\n    &gt;&gt;&gt; x = np.array([1, 2, 3, 4, 5])\n    &gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5])\n    &gt;&gt;&gt; linear_x, linear_y = npy.behavior.linearize_position(x, y)\n    &gt;&gt;&gt; linear_x\n    array([0.        , 1.41421356, 2.82842712, 4.24264069, 5.65685425])\n    &gt;&gt;&gt; linear_y\n    array([3.92192151e-16, 0.00000000e+00, 9.80480378e-17, 1.96096076e-16, 2.94144113e-16])\n    \"\"\"\n    # locate and remove nans (sklearn pca does not like nans)\n    badidx = (np.isnan(x)) | (np.isnan(y))\n    badidx_pos = np.where(badidx)\n    goodidx_pos = np.where(~badidx)\n    n = len(x)\n\n    x = x[~badidx]\n    y = y[~badidx]\n\n    # perform pca and return the first 2 components\n    pca = PCA(n_components=2)\n    # transform our coords\n    linear = pca.fit_transform(np.array([x, y]).T)\n\n    # add back nans\n    x = np.zeros([n])\n    x[badidx_pos] = np.nan\n    x[goodidx_pos] = linear[:, 0]\n\n    y = np.zeros([n])\n    y[badidx_pos] = np.nan\n    y[goodidx_pos] = linear[:, 1]\n\n    # pca will center data at 0,0... adjust for this here\n    x = x + np.abs(np.nanmin(x))\n    y = y + np.abs(np.nanmin(y))\n\n    return x, y\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/","title":"neuro_py.behavior.linearization_pipeline","text":""},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker","title":"<code>NodePicker</code>","text":"<p>Interactive creation of track graph by looking at video frames.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The matplotlib axes to draw on, by default None.</p> <code>None</code> <code>basepath</code> <code>str</code> <p>The base path for saving data, by default None.</p> <code>None</code> <code>node_color</code> <code>str</code> <p>The color of the nodes, by default \"#177ee6\".</p> <code>'#177ee6'</code> <code>node_size</code> <code>int</code> <p>The size of the nodes, by default 100.</p> <code>100</code> <code>epoch</code> <code>int</code> <p>The epoch number, by default None.</p> <code>None</code> <code>interval</code> <code>Tuple[float, float]</code> <code>None</code> <p>Attributes:</p> Name Type Description <code>ax</code> <code>Axes</code> <p>The matplotlib axes to draw on.</p> <code>canvas</code> <code>FigureCanvas</code> <p>The matplotlib figure canvas.</p> <code>cid</code> <code>int</code> <p>The connection id for the event handler.</p> <code>_nodes</code> <code>list</code> <p>The list of node positions.</p> <code>node_color</code> <code>str</code> <p>The color of the nodes.</p> <code>_nodes_plot</code> <code>scatter</code> <p>The scatter plot of the nodes.</p> <code>edges</code> <code>list</code> <p>The list of edges.</p> <code>basepath</code> <code>str</code> <p>The base path for saving data.</p> <code>epoch</code> <code>int</code> <p>The epoch number.</p> <code>use_HMM</code> <code>bool</code> <p>Whether to use the hidden markov model.</p> <p>Methods:</p> Name Description <code>node_positions</code> <p>Get the positions of the nodes.</p> <code>connect</code> <p>Connect the event handlers.</p> <code>disconnect</code> <p>Disconnect the event handlers.</p> <code>process_key</code> <p>Process key press events.</p> <code>click_event</code> <p>Process mouse click events.</p> <code>redraw</code> <p>Redraw the nodes and edges.</p> <code>remove_point</code> <p>Remove a point from the nodes.</p> <code>clear</code> <p>Clear all nodes and edges.</p> <code>format_and_save</code> <p>Format the data and save it to disk.</p> <code>save_nodes_edges</code> <p>Save the nodes and edges to a pickle file.</p> <code>save_nodes_edges_to_behavior</code> <p>Store nodes and edges into behavior file.</p> <p>Examples:</p>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker--in-command-line","title":"in command line","text":"<pre><code>&gt;&gt;&gt; python linearization_pipeline.py path/to/session\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker--for-a-specific-epoch","title":"for a specific epoch","text":"<pre><code>&gt;&gt;&gt; python linearization_pipeline.py path/to/session 1\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker--for-a-specific-interval","title":"for a specific interval","text":"<pre><code>&gt;&gt;&gt; python linearization_pipeline.py path/to/session 0 100\n</code></pre> References <p>https://github.com/LorenFrankLab/track_linearization</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>class NodePicker:\n    \"\"\"\n    Interactive creation of track graph by looking at video frames.\n\n    Parameters\n    ----------\n    ax : plt.Axes, optional\n        The matplotlib axes to draw on, by default None.\n    basepath : str, optional\n        The base path for saving data, by default None.\n    node_color : str, optional\n        The color of the nodes, by default \"#177ee6\".\n    node_size : int, optional\n        The size of the nodes, by default 100.\n    epoch : int, optional\n        The epoch number, by default None.\n    interval : Tuple[float, float], optional\n\n    Attributes\n    ----------\n    ax : plt.Axes\n        The matplotlib axes to draw on.\n    canvas : plt.FigureCanvas\n        The matplotlib figure canvas.\n    cid : int\n        The connection id for the event handler.\n    _nodes : list\n        The list of node positions.\n    node_color : str\n        The color of the nodes.\n    _nodes_plot : plt.scatter\n        The scatter plot of the nodes.\n    edges : list\n        The list of edges.\n    basepath : str\n        The base path for saving data.\n    epoch : int\n        The epoch number.\n    use_HMM : bool\n        Whether to use the hidden markov model.\n\n    Methods\n    -------\n    node_positions\n        Get the positions of the nodes.\n    connect\n        Connect the event handlers.\n    disconnect\n        Disconnect the event handlers.\n    process_key\n        Process key press events.\n    click_event\n        Process mouse click events.\n    redraw\n        Redraw the nodes and edges.\n    remove_point\n        Remove a point from the nodes.\n    clear\n        Clear all nodes and edges.\n    format_and_save\n        Format the data and save it to disk.\n    save_nodes_edges\n        Save the nodes and edges to a pickle file.\n    save_nodes_edges_to_behavior\n        Store nodes and edges into behavior file.\n\n    Examples\n    --------\n    # in command line\n    &gt;&gt;&gt; python linearization_pipeline.py path/to/session\n\n    # for a specific epoch\n    &gt;&gt;&gt; python linearization_pipeline.py path/to/session 1\n\n    # for a specific interval\n    &gt;&gt;&gt; python linearization_pipeline.py path/to/session 0 100\n\n    References\n    ----------\n    https://github.com/LorenFrankLab/track_linearization\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ax: Optional[plt.Axes] = None,\n        basepath: Optional[str] = None,\n        node_color: str = \"#177ee6\",\n        node_size: int = 100,\n        epoch: Optional[int] = None,\n        interval: Optional[Tuple[float, float]] = None,\n    ):\n        \"\"\"\n        Initialize the NodePicker.\n\n        Parameters\n        ----------\n        ax : plt.Axes, optional\n            The matplotlib axes to draw on, by default None.\n        basepath : str, optional\n            The base path for saving data, by default None.\n        node_color : str, optional\n            The color of the nodes, by default \"#177ee6\".\n        node_size : int, optional\n            The size of the nodes, by default 100.\n        epoch : int, optional\n            The epoch number, by default None.\n        \"\"\"\n        if ax is None:\n            ax = plt.gca()\n        self.ax = ax\n        self.canvas = ax.get_figure().canvas\n        self.cid = None\n        self._nodes = []\n        self.node_color = node_color\n        self._nodes_plot = ax.scatter([], [], zorder=5, s=node_size, color=node_color)\n        self.edges = [[]]\n        self.basepath = basepath\n        self.epoch = epoch\n        self.interval = interval\n        self.use_HMM = True\n\n        if self.epoch is not None:\n            self.epoch = int(self.epoch)\n\n        ax.set_title(\n            \"Left click to place node.\\nRight click to remove node.\"\n            \"\\nShift+Left click to clear nodes.\\nCntrl+Left click two nodes to place an edge\"\n            \"\\nEnter to save and exit.\",\n            fontsize=8,\n        )\n\n        self.canvas.draw()\n\n        self.connect()\n\n    @property\n    def node_positions(self) -&gt; np.ndarray:\n        \"\"\"\n        Get the positions of the nodes.\n\n        Returns\n        -------\n        np.ndarray\n            An array of node positions.\n        \"\"\"\n        return np.asarray(self._nodes)\n\n    def connect(self) -&gt; None:\n        \"\"\"\n        Connect the event handlers.\n        \"\"\"\n        print(\"Connecting to events\")\n        if self.cid is None:\n            self.cid = self.canvas.mpl_connect(\"button_press_event\", self.click_event)\n            self.canvas.mpl_connect(\"key_press_event\", self.process_key)\n            print(\"Mouse click event connected!\")  # Debugging\n\n    def disconnect(self) -&gt; None:\n        \"\"\"\n        Disconnect the event handlers.\n        \"\"\"\n        if self.cid is not None:\n            self.canvas.mpl_disconnect(self.cid)\n            self.cid = None\n\n    def process_key(self, event: Any) -&gt; None:\n        \"\"\"\n        Process key press events.\n\n        Parameters\n        ----------\n        event : Any\n            The key press event.\n        \"\"\"\n        if event.key == \"enter\":\n            self.format_and_save()\n\n    def click_event(self, event: Any) -&gt; None:\n        \"\"\"\n        Process mouse click events.\n\n        Parameters\n        ----------\n        event : Any\n            The mouse click event.\n        \"\"\"\n\n        print(\n            f\"Mouse clicked at: {event.xdata}, {event.ydata}, button: {event.button}, key: {event.key}\"\n        )  # Debugging\n        if not event.inaxes:\n            return\n\n        if event.key is None:  # Regular mouse clicks\n            if event.button == 1:  # Left click\n                self._nodes.append((event.xdata, event.ydata))\n            elif event.button == 3:  # Right click\n                self.remove_point((event.xdata, event.ydata))\n\n        elif event.key == \"shift\" and event.button == 1:  # Shift + Left click\n            self.clear()\n\n        elif (\n            event.key == \"control\" and event.button == 1\n        ):  # Ctrl + Left click (Edge creation)\n            if len(self._nodes) == 0:\n                return\n            point = (event.xdata, event.ydata)\n            distance_to_nodes = np.linalg.norm(self.node_positions - point, axis=1)\n            closest_node_ind = np.argmin(distance_to_nodes)\n            if len(self.edges[-1]) &lt; 2:\n                self.edges[-1].append(closest_node_ind)\n            else:\n                self.edges.append([closest_node_ind])\n\n        elif event.key == \"enter\":  # Pressing Enter\n            self.format_and_save()\n\n        self.redraw()\n\n    def redraw(self) -&gt; None:\n        \"\"\"\n        Redraw the nodes and edges.\n        \"\"\"\n        # Draw Node Circles\n        if len(self.node_positions) &gt; 0:\n            self._nodes_plot.set_offsets(self.node_positions)\n        else:\n            self._nodes_plot.set_offsets([])\n\n        # Draw Node Numbers\n        for ind, (x, y) in enumerate(self.node_positions):\n            self.ax.text(\n                x,\n                y,\n                ind,\n                zorder=6,\n                fontsize=10,\n                horizontalalignment=\"center\",\n                verticalalignment=\"center\",\n                clip_on=True,\n                bbox=None,\n                transform=self.ax.transData,\n            )\n        # Draw Edges\n        for edge in self.edges:\n            if len(edge) &gt; 1:\n                x1, y1 = self.node_positions[edge[0]]\n                x2, y2 = self.node_positions[edge[1]]\n                self.ax.plot(\n                    [x1, x2], [y1, y2], color=\"#1f8e4f\", linewidth=3, zorder=1000\n                )\n        self.canvas.draw()\n\n    def remove_point(self, point: Tuple[float, float]) -&gt; None:\n        \"\"\"\n        Remove a point from the nodes.\n\n        Parameters\n        ----------\n        point : Tuple[float, float]\n            The point to remove.\n        \"\"\"\n        if len(self._nodes) &gt; 0:\n            distance_to_nodes = np.linalg.norm(self.node_positions - point, axis=1)\n            closest_node_ind = np.argmin(distance_to_nodes)\n            self._nodes.pop(closest_node_ind)\n\n    def clear(self) -&gt; None:\n        \"\"\"\n        Clear all nodes and edges.\n        \"\"\"\n        self._nodes = []\n        self.edges = [[]]\n        self.redraw()\n\n    def format_and_save(self) -&gt; None:\n        \"\"\"\n        Format the data and save it to disk.\n        \"\"\"\n        behave_df = load_animal_behavior(self.basepath)\n\n        if self.epoch is not None:\n            epochs = load_epoch(self.basepath)\n\n            cur_epoch = (\n                ~np.isnan(behave_df.x)\n                &amp; (behave_df.time &gt;= epochs.iloc[self.epoch].startTime)\n                &amp; (behave_df.time &lt;= epochs.iloc[self.epoch].stopTime)\n            )\n        elif self.interval is not None:\n            cur_epoch = (\n                ~np.isnan(behave_df.x)\n                &amp; (behave_df.time &gt;= self.interval[0])\n                &amp; (behave_df.time &lt;= self.interval[1])\n            )\n        else:\n            cur_epoch = ~np.isnan(behave_df.x)\n\n        print(\"running hmm...\")\n        track_graph = make_track_graph(self.node_positions, self.edges)\n\n        position = np.vstack(\n            [behave_df[cur_epoch].x.values, behave_df[cur_epoch].y.values]\n        ).T\n\n        position_df = get_linearized_position(\n            position=position,\n            track_graph=track_graph,\n            edge_order=self.edges,\n            use_HMM=self.use_HMM,\n        )\n\n        print(\"saving to disk...\")\n        behave_df.loc[cur_epoch, \"linearized\"] = position_df.linear_position.values\n        behave_df.loc[cur_epoch, \"states\"] = position_df.track_segment_id.values\n        behave_df.loc[cur_epoch, \"projected_x_position\"] = (\n            position_df.projected_x_position.values\n        )\n        behave_df.loc[cur_epoch, \"projected_y_position\"] = (\n            position_df.projected_y_position.values\n        )\n\n        filename = glob.glob(os.path.join(self.basepath, \"*.animal.behavior.mat\"))[0]\n        data = loadmat(filename, simplify_cells=True)\n\n        data[\"behavior\"][\"position\"][\"linearized\"] = behave_df.linearized.values\n        data[\"behavior\"][\"states\"] = behave_df.states.values\n        data[\"behavior\"][\"position\"][\"projected_x\"] = (\n            behave_df.projected_x_position.values\n        )\n        data[\"behavior\"][\"position\"][\"projected_y\"] = (\n            behave_df.projected_y_position.values\n        )\n\n        # store nodes and edges within behavior file\n        data = self.save_nodes_edges_to_behavior(data, behave_df)\n\n        savemat(filename, data, long_field_names=True)\n\n        self.save_nodes_edges()\n        self.disconnect()\n        plt.close()\n\n    def save_nodes_edges(self) -&gt; None:\n        \"\"\"\n        Save the nodes and edges to a pickle file.\n        \"\"\"\n        results = {\"node_positions\": self.node_positions, \"edges\": self.edges}\n        save_file = os.path.join(self.basepath, \"linearization_nodes_edges.pkl\")\n        with open(save_file, \"wb\") as f:\n            pickle.dump(results, f)\n\n    def save_nodes_edges_to_behavior(self, data: dict, behave_df: pd.DataFrame) -&gt; dict:\n        \"\"\"\n        Store nodes and edges into behavior file.\n        Searches to find epochs with valid linearized coords.\n        Nodes and edges are stored within behavior.epochs{n}.{node_positions and edges}\n\n        Parameters\n        ----------\n        data : dict\n            The behavior data dictionary.\n        behave_df : pd.DataFrame\n            The DataFrame containing behavior data.\n\n        Returns\n        -------\n        dict\n            The updated behavior data dictionary.\n        \"\"\"\n        if self.epoch is None and self.interval is None:\n            # load epochs\n            epochs = load_epoch(self.basepath)\n            # iter over each epoch\n            for epoch_i, ep in enumerate(epochs.itertuples()):\n                # locate index for given epoch\n                idx = behave_df.time.between(ep.startTime, ep.stopTime)\n                # if linearized is not all nan, add nodes and edges\n                if not all(np.isnan(behave_df[idx].linearized)) &amp; (\n                    behave_df[idx].shape[0] != 0\n                ):\n                    # adding nodes and edges\n                    data[\"behavior\"][\"epochs\"][epoch_i][\"node_positions\"] = (\n                        self.node_positions\n                    )\n                    data[\"behavior\"][\"epochs\"][epoch_i][\"edges\"] = self.edges\n        elif self.interval is not None:\n            # if interval was used, add nodes and edges just the epochs within that interval\n            epochs = load_epoch(self.basepath)\n            for epoch_i, ep in enumerate(epochs.itertuples()):\n                # amount of overlap between interval and epoch\n                start_overlap = max(self.interval[0], ep.startTime)\n                end_overlap = min(self.interval[1], ep.stopTime)\n                overlap = max(0, end_overlap - start_overlap)\n\n                # if overlap is greater than 1 second, add nodes and edges\n                if overlap &gt; 1:\n                    data[\"behavior\"][\"epochs\"][epoch_i][\"node_positions\"] = (\n                        self.node_positions\n                    )\n                    data[\"behavior\"][\"epochs\"][epoch_i][\"edges\"] = self.edges\n        else:\n            # if epoch was used, add nodes and edges just that that epoch\n            data[\"behavior\"][\"epochs\"][self.epoch][\"node_positions\"] = (\n                self.node_positions\n            )\n            data[\"behavior\"][\"epochs\"][self.epoch][\"edges\"] = self.edges\n\n        return data\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker.node_positions","title":"<code>node_positions</code>  <code>property</code>","text":"<p>Get the positions of the nodes.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of node positions.</p>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker.clear","title":"<code>clear()</code>","text":"<p>Clear all nodes and edges.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clear all nodes and edges.\n    \"\"\"\n    self._nodes = []\n    self.edges = [[]]\n    self.redraw()\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker.click_event","title":"<code>click_event(event)</code>","text":"<p>Process mouse click events.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Any</code> <p>The mouse click event.</p> required Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def click_event(self, event: Any) -&gt; None:\n    \"\"\"\n    Process mouse click events.\n\n    Parameters\n    ----------\n    event : Any\n        The mouse click event.\n    \"\"\"\n\n    print(\n        f\"Mouse clicked at: {event.xdata}, {event.ydata}, button: {event.button}, key: {event.key}\"\n    )  # Debugging\n    if not event.inaxes:\n        return\n\n    if event.key is None:  # Regular mouse clicks\n        if event.button == 1:  # Left click\n            self._nodes.append((event.xdata, event.ydata))\n        elif event.button == 3:  # Right click\n            self.remove_point((event.xdata, event.ydata))\n\n    elif event.key == \"shift\" and event.button == 1:  # Shift + Left click\n        self.clear()\n\n    elif (\n        event.key == \"control\" and event.button == 1\n    ):  # Ctrl + Left click (Edge creation)\n        if len(self._nodes) == 0:\n            return\n        point = (event.xdata, event.ydata)\n        distance_to_nodes = np.linalg.norm(self.node_positions - point, axis=1)\n        closest_node_ind = np.argmin(distance_to_nodes)\n        if len(self.edges[-1]) &lt; 2:\n            self.edges[-1].append(closest_node_ind)\n        else:\n            self.edges.append([closest_node_ind])\n\n    elif event.key == \"enter\":  # Pressing Enter\n        self.format_and_save()\n\n    self.redraw()\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker.connect","title":"<code>connect()</code>","text":"<p>Connect the event handlers.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"\n    Connect the event handlers.\n    \"\"\"\n    print(\"Connecting to events\")\n    if self.cid is None:\n        self.cid = self.canvas.mpl_connect(\"button_press_event\", self.click_event)\n        self.canvas.mpl_connect(\"key_press_event\", self.process_key)\n        print(\"Mouse click event connected!\")  # Debugging\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker.disconnect","title":"<code>disconnect()</code>","text":"<p>Disconnect the event handlers.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def disconnect(self) -&gt; None:\n    \"\"\"\n    Disconnect the event handlers.\n    \"\"\"\n    if self.cid is not None:\n        self.canvas.mpl_disconnect(self.cid)\n        self.cid = None\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker.format_and_save","title":"<code>format_and_save()</code>","text":"<p>Format the data and save it to disk.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def format_and_save(self) -&gt; None:\n    \"\"\"\n    Format the data and save it to disk.\n    \"\"\"\n    behave_df = load_animal_behavior(self.basepath)\n\n    if self.epoch is not None:\n        epochs = load_epoch(self.basepath)\n\n        cur_epoch = (\n            ~np.isnan(behave_df.x)\n            &amp; (behave_df.time &gt;= epochs.iloc[self.epoch].startTime)\n            &amp; (behave_df.time &lt;= epochs.iloc[self.epoch].stopTime)\n        )\n    elif self.interval is not None:\n        cur_epoch = (\n            ~np.isnan(behave_df.x)\n            &amp; (behave_df.time &gt;= self.interval[0])\n            &amp; (behave_df.time &lt;= self.interval[1])\n        )\n    else:\n        cur_epoch = ~np.isnan(behave_df.x)\n\n    print(\"running hmm...\")\n    track_graph = make_track_graph(self.node_positions, self.edges)\n\n    position = np.vstack(\n        [behave_df[cur_epoch].x.values, behave_df[cur_epoch].y.values]\n    ).T\n\n    position_df = get_linearized_position(\n        position=position,\n        track_graph=track_graph,\n        edge_order=self.edges,\n        use_HMM=self.use_HMM,\n    )\n\n    print(\"saving to disk...\")\n    behave_df.loc[cur_epoch, \"linearized\"] = position_df.linear_position.values\n    behave_df.loc[cur_epoch, \"states\"] = position_df.track_segment_id.values\n    behave_df.loc[cur_epoch, \"projected_x_position\"] = (\n        position_df.projected_x_position.values\n    )\n    behave_df.loc[cur_epoch, \"projected_y_position\"] = (\n        position_df.projected_y_position.values\n    )\n\n    filename = glob.glob(os.path.join(self.basepath, \"*.animal.behavior.mat\"))[0]\n    data = loadmat(filename, simplify_cells=True)\n\n    data[\"behavior\"][\"position\"][\"linearized\"] = behave_df.linearized.values\n    data[\"behavior\"][\"states\"] = behave_df.states.values\n    data[\"behavior\"][\"position\"][\"projected_x\"] = (\n        behave_df.projected_x_position.values\n    )\n    data[\"behavior\"][\"position\"][\"projected_y\"] = (\n        behave_df.projected_y_position.values\n    )\n\n    # store nodes and edges within behavior file\n    data = self.save_nodes_edges_to_behavior(data, behave_df)\n\n    savemat(filename, data, long_field_names=True)\n\n    self.save_nodes_edges()\n    self.disconnect()\n    plt.close()\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker.process_key","title":"<code>process_key(event)</code>","text":"<p>Process key press events.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Any</code> <p>The key press event.</p> required Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def process_key(self, event: Any) -&gt; None:\n    \"\"\"\n    Process key press events.\n\n    Parameters\n    ----------\n    event : Any\n        The key press event.\n    \"\"\"\n    if event.key == \"enter\":\n        self.format_and_save()\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker.redraw","title":"<code>redraw()</code>","text":"<p>Redraw the nodes and edges.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def redraw(self) -&gt; None:\n    \"\"\"\n    Redraw the nodes and edges.\n    \"\"\"\n    # Draw Node Circles\n    if len(self.node_positions) &gt; 0:\n        self._nodes_plot.set_offsets(self.node_positions)\n    else:\n        self._nodes_plot.set_offsets([])\n\n    # Draw Node Numbers\n    for ind, (x, y) in enumerate(self.node_positions):\n        self.ax.text(\n            x,\n            y,\n            ind,\n            zorder=6,\n            fontsize=10,\n            horizontalalignment=\"center\",\n            verticalalignment=\"center\",\n            clip_on=True,\n            bbox=None,\n            transform=self.ax.transData,\n        )\n    # Draw Edges\n    for edge in self.edges:\n        if len(edge) &gt; 1:\n            x1, y1 = self.node_positions[edge[0]]\n            x2, y2 = self.node_positions[edge[1]]\n            self.ax.plot(\n                [x1, x2], [y1, y2], color=\"#1f8e4f\", linewidth=3, zorder=1000\n            )\n    self.canvas.draw()\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker.remove_point","title":"<code>remove_point(point)</code>","text":"<p>Remove a point from the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>point</code> <code>Tuple[float, float]</code> <p>The point to remove.</p> required Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def remove_point(self, point: Tuple[float, float]) -&gt; None:\n    \"\"\"\n    Remove a point from the nodes.\n\n    Parameters\n    ----------\n    point : Tuple[float, float]\n        The point to remove.\n    \"\"\"\n    if len(self._nodes) &gt; 0:\n        distance_to_nodes = np.linalg.norm(self.node_positions - point, axis=1)\n        closest_node_ind = np.argmin(distance_to_nodes)\n        self._nodes.pop(closest_node_ind)\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker.save_nodes_edges","title":"<code>save_nodes_edges()</code>","text":"<p>Save the nodes and edges to a pickle file.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def save_nodes_edges(self) -&gt; None:\n    \"\"\"\n    Save the nodes and edges to a pickle file.\n    \"\"\"\n    results = {\"node_positions\": self.node_positions, \"edges\": self.edges}\n    save_file = os.path.join(self.basepath, \"linearization_nodes_edges.pkl\")\n    with open(save_file, \"wb\") as f:\n        pickle.dump(results, f)\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.NodePicker.save_nodes_edges_to_behavior","title":"<code>save_nodes_edges_to_behavior(data, behave_df)</code>","text":"<p>Store nodes and edges into behavior file. Searches to find epochs with valid linearized coords. Nodes and edges are stored within behavior.epochs{n}.{node_positions and edges}</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The behavior data dictionary.</p> required <code>behave_df</code> <code>DataFrame</code> <p>The DataFrame containing behavior data.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The updated behavior data dictionary.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def save_nodes_edges_to_behavior(self, data: dict, behave_df: pd.DataFrame) -&gt; dict:\n    \"\"\"\n    Store nodes and edges into behavior file.\n    Searches to find epochs with valid linearized coords.\n    Nodes and edges are stored within behavior.epochs{n}.{node_positions and edges}\n\n    Parameters\n    ----------\n    data : dict\n        The behavior data dictionary.\n    behave_df : pd.DataFrame\n        The DataFrame containing behavior data.\n\n    Returns\n    -------\n    dict\n        The updated behavior data dictionary.\n    \"\"\"\n    if self.epoch is None and self.interval is None:\n        # load epochs\n        epochs = load_epoch(self.basepath)\n        # iter over each epoch\n        for epoch_i, ep in enumerate(epochs.itertuples()):\n            # locate index for given epoch\n            idx = behave_df.time.between(ep.startTime, ep.stopTime)\n            # if linearized is not all nan, add nodes and edges\n            if not all(np.isnan(behave_df[idx].linearized)) &amp; (\n                behave_df[idx].shape[0] != 0\n            ):\n                # adding nodes and edges\n                data[\"behavior\"][\"epochs\"][epoch_i][\"node_positions\"] = (\n                    self.node_positions\n                )\n                data[\"behavior\"][\"epochs\"][epoch_i][\"edges\"] = self.edges\n    elif self.interval is not None:\n        # if interval was used, add nodes and edges just the epochs within that interval\n        epochs = load_epoch(self.basepath)\n        for epoch_i, ep in enumerate(epochs.itertuples()):\n            # amount of overlap between interval and epoch\n            start_overlap = max(self.interval[0], ep.startTime)\n            end_overlap = min(self.interval[1], ep.stopTime)\n            overlap = max(0, end_overlap - start_overlap)\n\n            # if overlap is greater than 1 second, add nodes and edges\n            if overlap &gt; 1:\n                data[\"behavior\"][\"epochs\"][epoch_i][\"node_positions\"] = (\n                    self.node_positions\n                )\n                data[\"behavior\"][\"epochs\"][epoch_i][\"edges\"] = self.edges\n    else:\n        # if epoch was used, add nodes and edges just that that epoch\n        data[\"behavior\"][\"epochs\"][self.epoch][\"node_positions\"] = (\n            self.node_positions\n        )\n        data[\"behavior\"][\"epochs\"][self.epoch][\"edges\"] = self.edges\n\n    return data\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.load_animal_behavior","title":"<code>load_animal_behavior(basepath)</code>","text":"<p>Load animal behavior data from a .mat file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path where the .mat file is located.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing the animal behavior data.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def load_animal_behavior(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Load animal behavior data from a .mat file.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path where the .mat file is located.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing the animal behavior data.\n    \"\"\"\n    filename = glob.glob(os.path.join(basepath, \"*.animal.behavior.mat\"))[0]\n    data = loadmat(filename, simplify_cells=True)\n    df = pd.DataFrame()\n    df[\"time\"] = data[\"behavior\"][\"timestamps\"]\n    try:\n        df[\"states\"] = data[\"behavior\"][\"states\"]\n    except Exception:\n        pass\n    for key in data[\"behavior\"][\"position\"].keys():\n        try:\n            df[key] = data[\"behavior\"][\"position\"][key]\n        except Exception:\n            pass\n    return df\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.load_epoch","title":"<code>load_epoch(basepath)</code>","text":"<p>Load epoch info from cell explorer basename.session and store in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path where the .session.mat file is located.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing the epoch information.</p> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def load_epoch(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Load epoch info from cell explorer basename.session and store in a DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path where the .session.mat file is located.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing the epoch information.\n    \"\"\"\n    filename = glob.glob(os.path.join(basepath, \"*.session.mat\"))[0]\n\n    data = loadmat(filename, simplify_cells=True)\n    try:\n        return pd.DataFrame(data[\"session\"][\"epochs\"])\n    except Exception:\n        return pd.DataFrame([data[\"session\"][\"epochs\"]])\n</code></pre>"},{"location":"reference/neuro_py/behavior/linearization_pipeline/#neuro_py.behavior.linearization_pipeline.run","title":"<code>run(basepath, epoch=None, interval=None)</code>","text":"<p>Run the linearization pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path where the data files are located.</p> required <code>epoch</code> <code>int</code> <p>The epoch number to process, by default None.</p> <code>None</code> <code>interval</code> <code>Tuple[float, float]</code> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/behavior/linearization_pipeline.py</code> <pre><code>def run(\n    basepath: str,\n    epoch: Optional[int] = None,\n    interval: Optional[Tuple[float, float]] = None,\n) -&gt; None:\n    \"\"\"\n    Run the linearization pipeline.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path where the data files are located.\n    epoch : int, optional\n        The epoch number to process, by default None.\n    interval : Tuple[float, float], optional\n\n    Returns\n    -------\n    None\n    \"\"\"\n    plt.close(\"all\")\n    print(\"here is the file,\", basepath)\n\n    with plt.style.context(\"dark_background\"):\n        plt.ioff()\n\n        _, ax = plt.subplots(figsize=(5, 5))\n\n        behave_df = load_animal_behavior(basepath)\n\n        if epoch is not None:\n            epochs = load_epoch(basepath)\n\n            behave_df = behave_df[\n                behave_df[\"time\"].between(\n                    epochs.iloc[epoch].startTime, epochs.iloc[epoch].stopTime\n                )\n            ]\n        elif interval is not None:\n            behave_df = behave_df[behave_df[\"time\"].between(interval[0], interval[1])]\n\n        ax.scatter(behave_df.x, behave_df.y, color=\"white\", s=0.5, alpha=0.5)\n        ax.axis(\"equal\")\n        ax.set_axisbelow(True)\n        ax.yaxis.grid(color=\"gray\", linestyle=\"dashed\")\n        ax.xaxis.grid(color=\"gray\", linestyle=\"dashed\")\n        ax.set_ylabel(\"y (cm)\")\n        ax.set_xlabel(\"x (cm)\")\n\n        picker = NodePicker(ax=ax, basepath=basepath, epoch=epoch, interval=interval)\n        picker.connect()  # Ensure connection\n\n        plt.show(block=True)\n</code></pre>"},{"location":"reference/neuro_py/behavior/preprocessing/","title":"neuro_py.behavior.preprocessing","text":""},{"location":"reference/neuro_py/behavior/preprocessing/#neuro_py.behavior.preprocessing.filter_tracker_jumps","title":"<code>filter_tracker_jumps(beh_df, max_speed=100)</code>","text":"<p>Filter out tracker jumps (to NaN) in the behavior data.</p> <p>Parameters:</p> Name Type Description Default <code>beh_df</code> <code>DataFrame</code> <p>Behavior data with columns x, y, and ts.</p> required <code>max_speed</code> <code>Union[int, float]</code> <p>Maximum allowed speed in cm per second.</p> <code>100</code> <p>Returns:</p> Type Description <code>DataFrame</code> Notes <p>Will force dtypes of x and y to float64</p> Source code in <code>neuro_py/behavior/preprocessing.py</code> <pre><code>def filter_tracker_jumps(\n    beh_df: pd.DataFrame, max_speed: Union[int, float] = 100\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter out tracker jumps (to NaN) in the behavior data.\n\n    Parameters\n    ----------\n    beh_df : pd.DataFrame\n        Behavior data with columns x, y, and ts.\n    max_speed : Union[int,float], optional\n        Maximum allowed speed in cm per second.\n\n    Returns\n    -------\n    pd.DataFrame\n\n    Notes\n    -----\n    Will force dtypes of x and y to float64\n    \"\"\"\n\n    # Calculate the Euclidean distance between consecutive points\n    beh_df[\"dx\"] = beh_df[\"x\"].diff()\n    beh_df[\"dy\"] = beh_df[\"y\"].diff()\n    beh_df[\"distance\"] = np.sqrt(beh_df[\"dx\"] ** 2 + beh_df[\"dy\"] ** 2)\n\n    # Calculate the time difference between consecutive timestamps\n    beh_df[\"dt\"] = beh_df[\"ts\"].diff()\n\n    # Calculate the speed between consecutive points (distance / time)\n    beh_df[\"speed\"] = beh_df[\"distance\"] / beh_df[\"dt\"]\n\n    # Identify the start of each jump\n    # A jump starts when the speed exceeds the threshold, and the previous speed did not\n    jump_starts = (beh_df[\"speed\"] &gt; max_speed) &amp; (\n        beh_df[\"speed\"].shift(1) &lt;= max_speed\n    )\n\n    # Mark x and y as NaN only for the first frame of each jump\n    beh_df.loc[jump_starts, [\"x\", \"y\"]] = np.nan\n\n    beh_df = beh_df.drop(columns=[\"dx\", \"dy\", \"distance\", \"dt\", \"speed\"])\n\n    return beh_df\n</code></pre>"},{"location":"reference/neuro_py/behavior/preprocessing/#neuro_py.behavior.preprocessing.filter_tracker_jumps_in_file","title":"<code>filter_tracker_jumps_in_file(basepath, epoch_number=None, epoch_interval=None)</code>","text":"<p>Filter out tracker jumps in the behavior data (to NaN) and save the filtered data back to the file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Basepath to the behavior file.</p> required <code>epoch_number</code> <code>int</code> <p>Epoch number to filter the behavior data to.</p> <code>None</code> <code>epoch_interval</code> <code>tuple</code> <p>Epoch interval to filter the behavior data to.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; basepath = \"path/to/behavior/file\"\n&gt;&gt;&gt; filter_tracker_jumps_in_file(basepath, epoch_number=1)\n</code></pre> Source code in <code>neuro_py/behavior/preprocessing.py</code> <pre><code>def filter_tracker_jumps_in_file(\n    basepath: str, epoch_number=None, epoch_interval=None\n) -&gt; None:\n    \"\"\"\n    Filter out tracker jumps in the behavior data (to NaN) and save the filtered data back to the file.\n\n    Parameters\n    ----------\n    basepath : str\n        Basepath to the behavior file.\n    epoch_number : int, optional\n        Epoch number to filter the behavior data to.\n    epoch_interval : tuple, optional\n        Epoch interval to filter the behavior data to.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; basepath = \"path/to/behavior/file\"\n    &gt;&gt;&gt; filter_tracker_jumps_in_file(basepath, epoch_number=1)\n    \"\"\"\n\n    # Load the behavior data\n    file = os.path.join(basepath, os.path.basename(basepath) + \"animal.behavior.mat\")\n\n    behavior = loadmat(file, simplify_cells=True)\n\n    # Filter the behavior data to remove tracker jumps\n    if epoch_number is not None:\n        epoch_df = npy.io.load_epoch(basepath)\n        idx = (\n            behavior[\"behavior\"][\"timestamps\"] &gt; epoch_df.loc[epoch_number].startTime\n        ) &amp; (behavior[\"behavior\"][\"timestamps\"] &lt; epoch_df.loc[epoch_number].stopTime)\n    elif epoch_interval is not None:\n        idx = (behavior[\"behavior\"][\"timestamps\"] &gt; epoch_interval[0]) &amp; (\n            behavior[\"behavior\"][\"timestamps\"] &lt; epoch_interval[1]\n        )\n    else:\n        # bool length of the same length as the number of timestamps\n        idx = np.ones(len(behavior[\"behavior\"][\"timestamps\"]), dtype=bool)\n\n    # Filter the behavior data and add to dataframe\n    x = behavior[\"behavior\"][\"position\"][\"x\"][idx]\n    y = behavior[\"behavior\"][\"position\"][\"y\"][idx]\n    ts = behavior[\"behavior\"][\"timestamps\"][idx]\n    beh_df = pd.DataFrame({\"x\": x, \"y\": y, \"ts\": ts})\n\n    # Filter out tracker jumps\n    beh_df = filter_tracker_jumps(beh_df)\n\n    # Save the filtered behavior data back to the file\n    behavior[\"behavior\"][\"position\"][\"x\"][idx] = beh_df.x.values\n    behavior[\"behavior\"][\"position\"][\"y\"][idx] = beh_df.y.values\n\n    savemat(file, behavior, long_field_names=True)\n</code></pre>"},{"location":"reference/neuro_py/behavior/well_traversal_classification/","title":"neuro_py.behavior.well_traversal_classification","text":""},{"location":"reference/neuro_py/behavior/well_traversal_classification/#neuro_py.behavior.well_traversal_classification.enter_exit_target","title":"<code>enter_exit_target(position, target, max_distance=1.0)</code>","text":"<p>Marks when a position has reached a target (\"enter\") and when it has left a target (\"exit\").</p> <p>The position is considered to have reached a target when it is less than the <code>max_distance</code> from the target.</p> <p>Enter and exit times are marked as follows:  1: entered the target radius  0: neither -1: exited the target radius</p> <p>Works for 1D position and 2D position.</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>Union[ndarray, list]</code> <p>Array or list of shape (n_time, n_space).</p> required <code>target</code> <code>Union[ndarray, list]</code> <p>Array or list of shape (1, n_space).</p> required <code>max_distance</code> <code>float</code> <p>How close the position is to the target to be considered at the target, by default 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple of two arrays: - The first array contains the enter/exit times. - The second array contains the times when the position is at the target.</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def enter_exit_target(\n    position: Union[np.ndarray, list],\n    target: Union[np.ndarray, list],\n    max_distance: float = 1.0,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Marks when a position has reached a target (\"enter\") and when it has left a target (\"exit\").\n\n    The position is considered to have reached a target when it is less than\n    the `max_distance` from the target.\n\n    Enter and exit times are marked as follows:\n     1: entered the target radius\n     0: neither\n    -1: exited the target radius\n\n    Works for 1D position and 2D position.\n\n    Parameters\n    ----------\n    position : Union[np.ndarray, list]\n        Array or list of shape (n_time, n_space).\n    target : Union[np.ndarray, list]\n        Array or list of shape (1, n_space).\n    max_distance : float, optional\n        How close the position is to the target to be considered at the target, by default 1.0.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        Tuple of two arrays:\n        - The first array contains the enter/exit times.\n        - The second array contains the times when the position is at the target.\n    \"\"\"\n    distance_from_target = paired_distances(position, target)\n    at_target = distance_from_target &lt; max_distance\n    enter_exit = np.r_[0, np.diff(at_target.astype(float))]\n    return enter_exit, at_target\n</code></pre>"},{"location":"reference/neuro_py/behavior/well_traversal_classification/#neuro_py.behavior.well_traversal_classification.enter_exit_target_dio","title":"<code>enter_exit_target_dio(dio_indicator)</code>","text":"<p>Marks when a digital input/output (DIO) indicator has entered or exited a target state.</p> <p>Parameters:</p> Name Type Description Default <code>dio_indicator</code> <code>ndarray</code> <p>Array of DIO indicator values.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple containing: - enter_exit: np.ndarray     Array indicating enter (1) and exit (-1) events. - at_target: np.ndarray     Array indicating whether the target is active (1) or not (0).</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def enter_exit_target_dio(dio_indicator: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Marks when a digital input/output (DIO) indicator has entered or exited a target state.\n\n    Parameters\n    ----------\n    dio_indicator : np.ndarray\n        Array of DIO indicator values.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing:\n        - enter_exit: np.ndarray\n            Array indicating enter (1) and exit (-1) events.\n        - at_target: np.ndarray\n            Array indicating whether the target is active (1) or not (0).\n    \"\"\"\n    at_target = (dio_indicator &gt; 0).astype(np.float16)\n    enter_exit = np.r_[0, np.diff(at_target)]\n    return enter_exit, at_target\n</code></pre>"},{"location":"reference/neuro_py/behavior/well_traversal_classification/#neuro_py.behavior.well_traversal_classification.find_last_non_center_well","title":"<code>find_last_non_center_well(segments_df, segment_ind)</code>","text":"<p>Find the last non-center well before the given segment index.</p> <p>Parameters:</p> Name Type Description Default <code>segments_df</code> <code>DataFrame</code> <p>DataFrame containing segment information.</p> required <code>segment_ind</code> <code>int</code> <p>The segment index to search up to.</p> required <p>Returns:</p> Type Description <code>Union[str, int]</code> <p>The last non-center well before the given segment index. If no non-center wells are found, returns an empty string.</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def find_last_non_center_well(\n    segments_df: pd.DataFrame, segment_ind: int\n) -&gt; Union[str, int]:\n    \"\"\"\n    Find the last non-center well before the given segment index.\n\n    Parameters\n    ----------\n    segments_df : pd.DataFrame\n        DataFrame containing segment information.\n    segment_ind : int\n        The segment index to search up to.\n\n    Returns\n    -------\n    Union[str, int]\n        The last non-center well before the given segment index. If no non-center wells are found,\n        returns an empty string.\n    \"\"\"\n    last_wells = segments_df.iloc[:segment_ind].to_well\n    try:\n        return last_wells[last_wells != \"Center\"].iloc[-1]\n    except IndexError:\n        # There are no non-center wells. Just return current well.\n        return \"\"\n</code></pre>"},{"location":"reference/neuro_py/behavior/well_traversal_classification/#neuro_py.behavior.well_traversal_classification.get_correct_inbound_outbound","title":"<code>get_correct_inbound_outbound(segments_df)</code>","text":"<p>Determine the task type (inbound or outbound), correctness, and turn direction for each segment.</p> <p>Parameters:</p> Name Type Description Default <code>segments_df</code> <code>DataFrame</code> <p>DataFrame containing segment information.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Updated DataFrame with additional columns for task type, correctness, and turn direction.</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def get_correct_inbound_outbound(segments_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Determine the task type (inbound or outbound), correctness, and turn direction for each segment.\n\n    Parameters\n    ----------\n    segments_df : pd.DataFrame\n        DataFrame containing segment information.\n\n    Returns\n    -------\n    pd.DataFrame\n        Updated DataFrame with additional columns for task type, correctness, and turn direction.\n    \"\"\"\n    n_segments = segments_df.shape[0]\n    task = np.empty((n_segments,), dtype=object)\n    turn = np.empty((n_segments,), dtype=object)\n    is_correct = np.zeros((n_segments,), dtype=bool)\n\n    for segment_ind in np.arange(n_segments):\n        cur_segment = segments_df.iloc[segment_ind]\n        if cur_segment.from_well == \"Center\":\n            task[segment_ind] = \"Outbound\"\n            last_non_center_well = find_last_non_center_well(segments_df, segment_ind)\n            is_correct[segment_ind] = (cur_segment.to_well != last_non_center_well) &amp; (\n                cur_segment.to_well != \"Center\"\n            )\n            if (last_non_center_well != \"\") | ~is_correct[segment_ind]:\n                turn[segment_ind] = last_non_center_well\n            else:\n                is_left_turn = (\n                    (cur_segment.from_well == \"Left\")\n                    &amp; (cur_segment.to_well == \"Center\")\n                ) | (\n                    (cur_segment.from_well == \"Center\")\n                    &amp; (cur_segment.to_well == \"Right\")\n                )\n\n                turn[segment_ind] = \"Left\" if is_left_turn else \"Right\"\n        else:\n            task[segment_ind] = \"Inbound\"\n            is_correct[segment_ind] = segments_df.iloc[segment_ind].to_well == \"Center\"\n            turn[segment_ind] = cur_segment.from_well\n\n    segments_df[\"task\"] = task\n    segments_df[\"is_correct\"] = is_correct\n    segments_df[\"turn\"] = turn\n\n    return segments_df\n</code></pre>"},{"location":"reference/neuro_py/behavior/well_traversal_classification/#neuro_py.behavior.well_traversal_classification.paired_distances","title":"<code>paired_distances(x, y)</code>","text":"<p>Euclidean distance between x and y at each time point.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ndarray, list]</code> <p>Array or list of shape (n_time, n_space).</p> required <code>y</code> <code>Union[ndarray, list]</code> <p>Array or list of shape (n_time, n_space).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (n_time,) containing the distances.</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def paired_distances(\n    x: Union[np.ndarray, list], y: Union[np.ndarray, list]\n) -&gt; np.ndarray:\n    \"\"\"\n    Euclidean distance between x and y at each time point.\n\n    Parameters\n    ----------\n    x : Union[np.ndarray, list]\n        Array or list of shape (n_time, n_space).\n    y : Union[np.ndarray, list]\n        Array or list of shape (n_time, n_space).\n\n    Returns\n    -------\n    np.ndarray\n        Array of shape (n_time,) containing the distances.\n    \"\"\"\n    x, y = np.array(x), np.array(y)\n    x = np.atleast_2d(x).T if x.ndim &lt; 2 else x\n    y = np.atleast_2d(y).T if y.ndim &lt; 2 else y\n    return np.linalg.norm(x - y, axis=1)\n</code></pre>"},{"location":"reference/neuro_py/behavior/well_traversal_classification/#neuro_py.behavior.well_traversal_classification.score_inbound_outbound","title":"<code>score_inbound_outbound(segments_df, min_distance_traveled=50, well_names={1: 'Center', 2: 'Left', 3: 'Right'})</code>","text":"<p>In the alternating arm task, determines whether the trial should be inbound (running to the center arm) or outbound (running to the opposite outer arm as before) and if the trial was performed correctly.</p> <p>Parameters:</p> Name Type Description Default <code>segments_df</code> <code>DataFrame</code> <p>Output of <code>segment_path</code> function.</p> required <code>min_distance_traveled</code> <code>float</code> <p>Minimum path length (in cm) while outside of the well radius for a segment to be considered as a trial, by default 50.</p> <code>50</code> <code>well_names</code> <code>Dict[int, str]</code> <p>Dictionary mapping well indices to well names, by default {1: \"Center\", 2: \"Left\", 3: \"Right\"}.</p> <code>{1: 'Center', 2: 'Left', 3: 'Right'}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Same as the input dataframe but with the wells labeled (left, right, center) and columns for <code>task</code> (inbound/outbound) and <code>is_correct</code> (True/False).</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def score_inbound_outbound(\n    segments_df: pd.DataFrame,\n    min_distance_traveled: float = 50,\n    well_names: Dict[int, str] = {1: \"Center\", 2: \"Left\", 3: \"Right\"},\n) -&gt; pd.DataFrame:\n    \"\"\"\n    In the alternating arm task, determines whether the trial should be\n    inbound (running to the center arm) or outbound (running to the opposite\n    outer arm as before) and if the trial was performed correctly.\n\n    Parameters\n    ----------\n    segments_df : pd.DataFrame\n        Output of `segment_path` function.\n    min_distance_traveled : float, optional\n        Minimum path length (in cm) while outside of the well radius for\n        a segment to be considered as a trial, by default 50.\n    well_names : Dict[int, str], optional\n        Dictionary mapping well indices to well names, by default {1: \"Center\", 2: \"Left\", 3: \"Right\"}.\n\n    Returns\n    -------\n    pd.DataFrame\n        Same as the input dataframe but with the wells labeled\n        (left, right, center) and columns for `task` (inbound/outbound) and\n        `is_correct` (True/False).\n    \"\"\"\n    segments_df = (\n        segments_df.copy()\n        .loc[segments_df.distance_traveled &gt; min_distance_traveled]\n        .dropna()\n    )\n    segments_df = segments_df.assign(\n        to_well=lambda df: df.to_well.map(well_names),\n        from_well=lambda df: df.from_well.map(well_names),\n    )\n    return get_correct_inbound_outbound(segments_df)\n</code></pre>"},{"location":"reference/neuro_py/behavior/well_traversal_classification/#neuro_py.behavior.well_traversal_classification.segment_path","title":"<code>segment_path(time, position, well_locations, max_distance_from_well=10)</code>","text":"<p>Label traversals between each well location.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>(ndarray, shape(n_time))</code> <p>Array of time points.</p> required <code>position</code> <code>(ndarray, shape(n_time, n_space))</code> <p>Array of positions at each time point.</p> required <code>well_locations</code> <code>(ndarray, shape(n_wells, n_space))</code> <p>Array of well locations.</p> required <code>max_distance_from_well</code> <code>float</code> <p>The animal is considered at a well location if its position is closer than this value, by default 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <ul> <li>segments_df: DataFrame of shape (n_segments, 6) containing segment information.</li> <li>labeled_segments: DataFrame of shape (n_time,) containing labeled segments.</li> </ul> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def segment_path(\n    time: np.ndarray,\n    position: np.ndarray,\n    well_locations: np.ndarray,\n    max_distance_from_well: float = 10,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Label traversals between each well location.\n\n    Parameters\n    ----------\n    time : np.ndarray, shape (n_time,)\n        Array of time points.\n    position : np.ndarray, shape (n_time, n_space)\n        Array of positions at each time point.\n    well_locations : np.ndarray, shape (n_wells, n_space)\n        Array of well locations.\n    max_distance_from_well : float, optional\n        The animal is considered at a well location if its position is closer\n        than this value, by default 10.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        - segments_df: DataFrame of shape (n_segments, 6) containing segment information.\n        - labeled_segments: DataFrame of shape (n_time,) containing labeled segments.\n    \"\"\"\n\n    well_enter_exit, at_target = np.stack(\n        [\n            enter_exit_target(position, np.atleast_2d(well), max_distance_from_well)\n            for well in well_locations\n        ],\n        axis=1,\n    )\n    n_wells = len(well_locations)\n    well_labels = np.arange(n_wells) + 1\n    well_enter_exit = np.sum(well_enter_exit.T * well_labels, axis=1)\n    shifted_well_enter_exit = shift_well_enters(well_enter_exit)\n    is_segment = ~(np.sum(at_target, axis=0) &gt; 0)\n    labeled_segments, n_segment_labels = label(is_segment)\n    segment_labels = np.arange(n_segment_labels) + 1\n\n    start_time, end_time, duration = [], [], []\n    distance_traveled, from_well, to_well = [], [], []\n\n    for segment_label in segment_labels:\n        is_seg = np.in1d(labeled_segments, segment_label)\n        segment_time = time[is_seg]\n        start_time.append(segment_time.min())\n        end_time.append(segment_time.max())\n        duration.append(segment_time.max() - segment_time.min())\n        try:\n            start, _, end = np.unique(shifted_well_enter_exit[is_seg])\n        except ValueError:\n            start, end = np.nan, np.nan\n\n        from_well.append(np.abs(start))\n        to_well.append(np.abs(end))\n        p = position[is_seg]\n        distance_traveled.append(np.sum(paired_distances(p[1:], p[:-1])))\n\n    data = [\n        (\"start_time\", start_time),\n        (\"end_time\", end_time),\n        (\"duration\", duration),\n        (\"from_well\", from_well),\n        (\"to_well\", to_well),\n        (\"distance_traveled\", distance_traveled),\n    ]\n    index = pd.Index(segment_labels, name=\"segment\")\n    return (\n        pd.DataFrame.from_dict(dict(data)).set_index(index),\n        pd.DataFrame(dict(labeled_segments=labeled_segments), index=time),\n    )\n</code></pre>"},{"location":"reference/neuro_py/behavior/well_traversal_classification/#neuro_py.behavior.well_traversal_classification.shift_well_enters","title":"<code>shift_well_enters(enter_exit)</code>","text":"<p>Shifts the enter times back one time point.</p> <p>Parameters:</p> Name Type Description Default <code>enter_exit</code> <code>ndarray</code> <p>Array indicating enter (positive values) and exit (negative values) events.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array with enter times shifted back by one time point.</p> Source code in <code>neuro_py/behavior/well_traversal_classification.py</code> <pre><code>def shift_well_enters(enter_exit: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Shifts the enter times back one time point.\n\n    Parameters\n    ----------\n    enter_exit : np.ndarray\n        Array indicating enter (positive values) and exit (negative values) events.\n\n    Returns\n    -------\n    np.ndarray\n        Array with enter times shifted back by one time point.\n    \"\"\"\n    shifted_enter_exit = enter_exit.copy()\n    old_ind = np.where(enter_exit &gt; 0)[0]  # positive entries are well-entries\n    new_ind = old_ind - 1\n    shifted_enter_exit[new_ind] = enter_exit[old_ind]\n    shifted_enter_exit[old_ind] = 0\n    return shifted_enter_exit\n</code></pre>"},{"location":"reference/neuro_py/detectors/","title":"neuro_py.detectors","text":""},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.DetectDS","title":"<code>DetectDS</code>","text":"<p>               Bases: <code>object</code></p> <p>Class for detecting dentate spikes</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder containing the data</p> required <code>hilus_ch</code> <code>int</code> <p>Channel number of the hilus signal (0 indexing)</p> required <code>mol_ch</code> <code>int</code> <p>Channel number of the mol signal (0 indexing)</p> required <code>noise_ch</code> <code>int</code> <p>Channel number of the noise signal or signal far from dentate (0 indexing)</p> <code>None</code> <code>lowcut</code> <code>float</code> <p>Low cut frequency for the signal filter</p> <code>10</code> <code>highcut</code> <code>float</code> <p>High cut frequency for the signal filter</p> <code>250</code> <code>filter_signal_bool</code> <code>bool</code> <p>If True, the signal will be filtered</p> <code>True</code> <code>primary_threshold</code> <code>float</code> <p>Primary threshold for detecting the dentate spikes (difference method only)</p> <code>5</code> <code>secondary_threshold</code> <code>float</code> <p>Secondary threshold for detecting the dentate spikes (difference method only)</p> required <code>primary_thres_mol</code> <code>float</code> <p>Primary threshold for detecting the dentate spikes in the mol signal</p> <code>2</code> <code>primary_thres_hilus</code> <code>float</code> <p>Primary threshold for detecting the dentate spikes in the hilus signal</p> <code>5</code> <code>min_duration</code> <code>float</code> <p>Minimum duration of the dentate spikes</p> <code>0.005</code> <code>max_duration</code> <code>float</code> <p>Maximum duration of the dentate spikes</p> <code>0.05</code> <code>filter_order</code> <code>int</code> <p>Order of the filter</p> <code>4</code> <code>filter_rs</code> <code>int</code> <p>Resonance frequency of the filter</p> <code>20</code> <code>method</code> <code>str</code> <p>Method for detecting the dentate spikes. \"difference\" for detecting the dentate spikes by difference between the hilus and mol signal \"seperately\" for detecting the dentate spikes by the hilus and mol signal separately</p> <code>'seperately'</code> <code>clean_lfp</code> <code>bool</code> <p>If True, the LFP signal will be cleaned</p> <code>False</code> <code>emg_threshold</code> <code>float</code> <p>Threshold for the EMG signal to remove dentate spikes</p> <code>0.9</code> <p>Attributes:</p> Name Type Description <code>lfp</code> <code>AnalogSignalArray</code> <p>LFP signal</p> <code>filtered_lfp</code> <code>AnalogSignalArray</code> <p>Filtered LFP signal</p> <code>mol_hilus_diff</code> <code>AnalogSignalArray</code> <p>Difference between the hilus and mol signal</p> <code>ds_epoch</code> <code>EpochArray</code> <p>EpochArray with the dentate spikes</p> <code>peak_val</code> <code>ndarray</code> <p>Peak value of the dentate spikes</p> <p>Methods:</p> Name Description <code>load_lfp</code> <p>Load the LFP signal</p> <code>filter_signal</code> <p>Filter the LFP signal</p> <code>get_filtered_lfp</code> <p>Get the filtered LFP signal</p> <code>get_lfp_diff</code> <p>Get the difference between the hilus and mol signal</p> <code>detect_ds_difference</code> <p>Detect the dentate spikes by difference between the hilus and mol signal</p> <code>detect_ds_seperately</code> <p>Detect the dentate spikes by the hilus and mol signal separately</p> <code>save_ds_epoch</code> <p>Save the dentate spikes as an EpochArray</p> <p>Examples:</p> <p>In IDE or python console</p> <pre><code>&gt;&gt;&gt; from ds_swr.detection.detect_dentate_spike import DetectDS\n&gt;&gt;&gt; from neuro_py.io import loading\n&gt;&gt;&gt; channel_tags = loading.load_channel_tags(basepath)\n&gt;&gt;&gt; dds = DetectDS(\n    basepath,\n    channel_tags[\"hilus\"][\"channels\"] - 1,\n    channel_tags[\"mol\"][\"channels\"] - 1\n)\n&gt;&gt;&gt; dds.detect_ds()\n&gt;&gt;&gt; dds.save_ds_epoch()\n&gt;&gt;&gt; dds\n&lt;DetectDS at 0x17fe787c640: dentate spikes 5,769&gt; of length 1:11:257 minutes\n</code></pre> <p>In command line</p> <pre><code>&gt;&gt;&gt; python detect_dentate_spike.py Z:\\Data\\Can\\OML22\\day20\n</code></pre> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>class DetectDS(object):\n    \"\"\"\n    Class for detecting dentate spikes\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder containing the data\n    hilus_ch : int\n        Channel number of the hilus signal (0 indexing)\n    mol_ch : int\n        Channel number of the mol signal (0 indexing)\n    noise_ch : int, optional\n        Channel number of the noise signal or signal far from dentate (0 indexing)\n    lowcut : float, optional\n        Low cut frequency for the signal filter\n    highcut : float, optional\n        High cut frequency for the signal filter\n    filter_signal_bool : bool, optional\n        If True, the signal will be filtered\n    primary_threshold : float, optional\n        Primary threshold for detecting the dentate spikes (difference method only)\n    secondary_threshold : float, optional\n        Secondary threshold for detecting the dentate spikes (difference method only)\n    primary_thres_mol : float, optional\n        Primary threshold for detecting the dentate spikes in the mol signal\n    primary_thres_hilus : float, optional\n        Primary threshold for detecting the dentate spikes in the hilus signal\n    min_duration : float, optional\n        Minimum duration of the dentate spikes\n    max_duration : float, optional\n        Maximum duration of the dentate spikes\n    filter_order : int, optional\n        Order of the filter\n    filter_rs : int, optional\n        Resonance frequency of the filter\n    method : str, optional\n        Method for detecting the dentate spikes.\n        \"difference\" for detecting the dentate spikes by difference between the hilus and mol signal\n        \"seperately\" for detecting the dentate spikes by the hilus and mol signal separately\n    clean_lfp : bool, optional\n        If True, the LFP signal will be cleaned\n    emg_threshold : float, optional\n        Threshold for the EMG signal to remove dentate spikes\n\n\n    Attributes\n    ----------\n    lfp : nelpy.AnalogSignalArray\n        LFP signal\n    filtered_lfp : nelpy.AnalogSignalArray\n        Filtered LFP signal\n    mol_hilus_diff : nelpy.AnalogSignalArray\n        Difference between the hilus and mol signal\n    ds_epoch : nelpy.EpochArray\n        EpochArray with the dentate spikes\n    peak_val : np.ndarray\n        Peak value of the dentate spikes\n\n\n    Methods\n    -------\n    load_lfp()\n        Load the LFP signal\n    filter_signal()\n        Filter the LFP signal\n    get_filtered_lfp()\n        Get the filtered LFP signal\n    get_lfp_diff()\n        Get the difference between the hilus and mol signal\n    detect_ds_difference()\n        Detect the dentate spikes by difference between the hilus and mol signal\n    detect_ds_seperately()\n        Detect the dentate spikes by the hilus and mol signal separately\n    save_ds_epoch()\n        Save the dentate spikes as an EpochArray\n\n    Examples\n    --------\n    In IDE or python console\n\n    &gt;&gt;&gt; from ds_swr.detection.detect_dentate_spike import DetectDS\n    &gt;&gt;&gt; from neuro_py.io import loading\n    &gt;&gt;&gt; channel_tags = loading.load_channel_tags(basepath)\n    &gt;&gt;&gt; dds = DetectDS(\n        basepath,\n        channel_tags[\"hilus\"][\"channels\"] - 1,\n        channel_tags[\"mol\"][\"channels\"] - 1\n    )\n    &gt;&gt;&gt; dds.detect_ds()\n    &gt;&gt;&gt; dds.save_ds_epoch()\n    &gt;&gt;&gt; dds\n    &lt;DetectDS at 0x17fe787c640: dentate spikes 5,769&gt; of length 1:11:257 minutes\n\n\n    In command line\n\n    &gt;&gt;&gt; python detect_dentate_spike.py Z:\\Data\\Can\\OML22\\day20\n    \"\"\"\n\n    def __init__(\n        self,\n        basepath: str,\n        hilus_ch: int,\n        mol_ch: int,\n        noise_ch: Union[int, None] = None,\n        lowcut: int = 10,\n        highcut: int = 250,\n        filter_signal_bool: bool = True,\n        primary_threshold: Union[int, float] = 5,\n        primary_thres_mol: Union[int, float] = 2,\n        primary_thres_hilus: Union[int, float] = 5,\n        min_duration: float = 0.005,\n        max_duration: float = 0.05,\n        filter_order: int = 4,\n        filter_rs: int = 20,\n        method: str = \"seperately\",\n        clean_lfp: bool = False,\n        emg_threshold: float = 0.9,\n    ) -&gt; None:\n\n        # adding all the parameters to the class\n        self.__dict__.update(locals())\n        del self.__dict__[\"self\"]\n        # setting the type name\n        self.type_name = self.__class__.__name__\n        self.get_xml_data()\n\n    def get_xml_data(self):\n        \"\"\"\n        Load the XML file to get the number of channels, sampling frequency and shank to channel mapping\n        \"\"\"\n        nChannels, fs, fs_dat, shank_to_channel = loading.loadXML(self.basepath)\n        self.nChannels = nChannels\n        self.fs = fs\n        self.fs_dat = fs_dat\n        self.shank_to_channel = shank_to_channel\n\n    def load_lfp(self):\n        \"\"\"\n        Load the LFP signal\n        \"\"\"\n\n        lfp, timestep = loading.loadLFP(\n            self.basepath,\n            n_channels=self.nChannels,\n            frequency=self.fs,\n            ext=\"lfp\",\n        )\n\n        if self.noise_ch is None:\n            channels = [self.hilus_ch, self.mol_ch]\n        else:\n            channels = [self.hilus_ch, self.mol_ch, self.noise_ch]\n\n        self.lfp = nel.AnalogSignalArray(\n            data=lfp[:, channels].T,\n            timestamps=timestep,\n            fs=self.fs,\n            support=nel.EpochArray(np.array([min(timestep), max(timestep)])),\n        )\n        if self.clean_lfp:\n            self.lfp._data = np.array(\n                [\n                    clean_lfp(self.lfp.signals[0]),\n                    clean_lfp(self.lfp.signals[1]),\n                ]\n            )\n\n    def filter_signal(self):\n        \"\"\"\n        Filter the LFP signal\n\n        Returns\n        -------\n        np.ndarray\n            Filtered LFP signal\n        \"\"\"\n        if not hasattr(self, \"lfp\"):\n            self.load_lfp()\n\n        b, a = cheby2(\n            self.filter_order,\n            self.filter_rs,\n            [self.lowcut, self.highcut],\n            fs=self.fs,\n            btype=\"bandpass\",\n        )\n        return filtfilt(b, a, self.lfp.data)\n\n    def get_filtered_lfp(self):\n        if not hasattr(self, \"lfp\"):\n            self.load_lfp()\n\n        self.filtered_lfp = deepcopy(self.lfp)\n        self.filtered_lfp._data = self.filter_signal()\n\n    def get_lfp_diff(self):\n        if self.filter_signal_bool:\n            y = self.filter_signal()\n        else:\n            if not hasattr(self, \"lfp\"):\n                self.load_lfp()\n            y = self.lfp.data\n\n        self.mol_hilus_diff = nel.AnalogSignalArray(\n            data=y[0, :] - y[1, :],\n            timestamps=self.lfp.abscissa_vals,\n            fs=self.fs,\n            support=nel.EpochArray(\n                np.array([min(self.lfp.abscissa_vals), max(self.lfp.abscissa_vals)])\n            ),\n        )\n\n    def detect_ds_difference(self):\n        if not hasattr(self, \"mol_hilus_diff\"):\n            self.get_lfp_diff()\n\n        PrimaryThreshold = (\n            self.mol_hilus_diff.mean()\n            + self.primary_threshold * self.mol_hilus_diff.std()\n        )\n        SecondaryThreshold = (\n            self.mol_hilus_diff.mean()\n            + self.secondary_threshold * self.mol_hilus_diff.std()\n        )\n        bounds, self.peak_val, _ = nel.utils.get_events_boundaries(\n            x=self.mol_hilus_diff.data,\n            PrimaryThreshold=PrimaryThreshold,\n            SecondaryThreshold=SecondaryThreshold,\n            minThresholdLength=0,\n            minLength=self.min_duration,\n            maxLength=self.max_duration,\n            ds=1 / self.mol_hilus_diff.fs,\n        )\n        # convert bounds to time in seconds\n        timebounds = self.mol_hilus_diff.time[bounds]\n        # add 1/fs to stops for open interval\n        timebounds[:, 1] += 1 / self.mol_hilus_diff.fs\n        # create EpochArray with bounds\n        self.ds_epoch = nel.EpochArray(timebounds)\n\n        # remove ds in high emg\n        _, high_emg_epoch, _ = loading.load_emg(self.basepath, self.emg_threshold)\n        if not high_emg_epoch.isempty:\n            idx = find_intersecting_intervals(self.ds_epoch, high_emg_epoch)\n            self.ds_epoch._data = self.ds_epoch.data[~idx]\n            self.peak_val = self.peak_val[~idx]\n\n    def detect_ds_seperately(self):\n        if not hasattr(self, \"filtered_lfp\"):\n            self.get_filtered_lfp()\n\n        # min and max time width of ds (converted to samples for find_peaks)\n        time_widths = [\n            int(self.min_duration * self.filtered_lfp.fs),\n            int(self.max_duration * self.filtered_lfp.fs),\n        ]\n\n        # detect ds in hilus\n        PrimaryThreshold = (\n            self.filtered_lfp.data[0, :].mean()\n            + self.primary_thres_hilus * self.filtered_lfp.data[0, :].std()\n        )\n\n        peaks, properties = find_peaks(\n            self.filtered_lfp.data[0, :],\n            height=PrimaryThreshold,\n            width=time_widths,\n        )\n        self.peaks = peaks / self.filtered_lfp.fs\n        self.peak_val = properties[\"peak_heights\"]\n\n        # create EpochArray with bounds\n        hilus_epoch = nel.EpochArray(\n            np.array([properties[\"left_ips\"], properties[\"right_ips\"]]).T\n            / self.filtered_lfp.fs\n        )\n\n        # detect ds in mol\n        PrimaryThreshold = (\n            self.filtered_lfp.data[1, :].mean()\n            + self.primary_thres_mol * self.filtered_lfp.data[1, :].std()\n        )\n\n        peaks, properties = find_peaks(\n            -self.filtered_lfp.data[1, :],\n            height=PrimaryThreshold,\n            width=time_widths,\n        )\n        mol_epoch_peak = peaks / self.filtered_lfp.fs\n        # create EpochArray with bounds\n        mol_epoch = nel.EpochArray(\n            np.array([properties[\"left_ips\"], properties[\"right_ips\"]]).T\n            / self.filtered_lfp.fs\n        )\n\n        # detect ds in noise channel\n        if self.noise_ch is not None:\n            PrimaryThreshold = (\n                self.filtered_lfp.data[2, :].mean()\n                + self.primary_thres_hilus * self.filtered_lfp.data[2, :].std()\n            )\n\n            peaks, properties = find_peaks(\n                self.filtered_lfp.data[2, :],\n                height=PrimaryThreshold,\n                width=time_widths,\n            )\n\n            # create EpochArray with bounds\n            noise_epoch = nel.EpochArray(\n                np.array([properties[\"left_ips\"], properties[\"right_ips\"]]).T\n                / self.filtered_lfp.fs\n            )\n\n        # remove hilus spikes that are not overlapping with mol spikes\n        # first, find mol peaks that are within hilus epoch\n        idx = in_intervals(mol_epoch_peak, hilus_epoch.data)\n        mol_epoch._data = mol_epoch.data[idx]\n\n        overlap = find_intersecting_intervals(\n            hilus_epoch, mol_epoch, return_indices=True\n        )\n        self.ds_epoch = nel.EpochArray(hilus_epoch.data[overlap])\n        self.peak_val = self.peak_val[overlap]\n        self.peaks = self.peaks[overlap]\n\n        # remove dentate spikes that are overlapping with noise spikes\n        if self.noise_ch is not None:\n            overlap = find_intersecting_intervals(\n                self.ds_epoch, noise_epoch, return_indices=True\n            )\n            self.ds_epoch = nel.EpochArray(self.ds_epoch.data[~overlap])\n            self.peak_val = self.peak_val[~overlap]\n            self.peaks = self.peaks[~overlap]\n\n        # remove ds in high emg\n        _, high_emg_epoch, _ = loading.load_emg(self.basepath, self.emg_threshold)\n        if not high_emg_epoch.isempty:\n            idx = find_intersecting_intervals(self.ds_epoch, high_emg_epoch)\n            self.ds_epoch._data = self.ds_epoch.data[~idx]\n            self.peak_val = self.peak_val[~idx]\n            self.peaks = self.peaks[~idx]\n\n    def detect_ds(self):\n        \"\"\"\n        Detect the dentate spikes based on the method provided\n        \"\"\"\n        if self.method == \"difference\":\n            # deprecated\n            raise NotImplementedError\n            # self.detect_ds_difference()\n        elif self.method == \"seperately\":\n            self.detect_ds_seperately()\n        else:\n            raise ValueError(f\"Method {self.method} not recognized\")\n\n    def save_ds_epoch(self):\n        \"\"\"\n        Save the dentate spikes as a cellexplorer mat file\n        \"\"\"\n\n        filename = os.path.join(\n            self.basepath, os.path.basename(self.basepath) + \".DS2.events.mat\"\n        )\n        data = {}\n        data[\"DS2\"] = {}\n        data[\"DS2\"][\"detectorinfo\"] = {}\n        data[\"DS2\"][\"timestamps\"] = self.ds_epoch.data\n        data[\"DS2\"][\"peaks\"] = self.peaks\n        data[\"DS2\"][\"amplitudes\"] = self.peak_val.T\n        data[\"DS2\"][\"amplitudeUnits\"] = \"mV\"\n        data[\"DS2\"][\"eventID\"] = []\n        data[\"DS2\"][\"eventIDlabels\"] = []\n        data[\"DS2\"][\"eventIDbinary\"] = []\n        data[\"DS2\"][\"duration\"] = self.ds_epoch.durations.T\n        data[\"DS2\"][\"center\"] = np.median(self.ds_epoch.data, axis=1).T\n        data[\"DS2\"][\"detectorinfo\"][\"detectorname\"] = \"DetectDS\"\n        data[\"DS2\"][\"detectorinfo\"][\"detectionparms\"] = []\n        data[\"DS2\"][\"detectorinfo\"][\"detectionintervals\"] = []\n        data[\"DS2\"][\"detectorinfo\"][\"ml_channel\"] = self.mol_ch\n        data[\"DS2\"][\"detectorinfo\"][\"h_channel\"] = self.hilus_ch\n        if self.noise_ch is not None:\n            data[\"DS2\"][\"detectorinfo\"][\"noise_channel\"] = self.noise_ch\n\n        savemat(filename, data, long_field_names=True)\n\n    def get_average_trace(self, shank=None, window=[-0.15, 0.15]):\n        \"\"\"\n        Get the average LFP trace around the dentate spikes\n\n        Parameters\n        ----------\n        shank : int, optional\n            Shank number of the hilus signal\n        window : list, optional\n            Window around the dentate spikes\n\n        Returns\n        -------\n        np.ndarray\n            Average LFP trace around the dentate spikes\n        np.ndarray\n            Time lags around the dentate spikes\n        \"\"\"\n\n        lfp, _ = loading.loadLFP(\n            self.basepath,\n            n_channels=self.nChannels,\n            frequency=self.fs,\n            ext=\"lfp\",\n        )\n\n        if shank is None:\n            hilus_shank = [\n                k for k, v in self.shank_to_channel.items() if self.hilus_ch in v\n            ][0]\n\n        ds_average, time_lags = event_triggered_average_fast(\n            signal=lfp[:, self.shank_to_channel[hilus_shank]].T,\n            events=self.ds_epoch.starts,\n            sampling_rate=self.fs,\n            window=window,\n            return_average=True,\n        )\n        return ds_average, time_lags\n\n    def plot(self, ax=None, window=[-0.15, 0.15], channel_offset=9e4):\n        \"\"\"\n        Plot the average LFP trace around the dentate spikes\n\n        Parameters\n        ----------\n        ax : matplotlib.axes._subplots.AxesSubplot, optional\n            Axis to plot the average LFP trace\n        window : list, optional\n            Window around the dentate spikes\n        channel_offset : float, optional\n            Offset between the channels\n\n        Returns\n        -------\n        matplotlib.axes._subplots.AxesSubplot\n            Axis with the average LFP trace\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        ds_average, time_lags = self.get_average_trace(window=window)\n\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(5, 10))\n\n        ax.plot(\n            time_lags,\n            ds_average.T - np.linspace(0, channel_offset, ds_average.shape[0]),\n            alpha=0.75,\n        )\n        return ax\n\n    def _detach(self):\n        \"\"\"Detach the data from the object to allow for pickling\"\"\"\n        self.filtered_lfp = None\n        self.lfp = None\n        self.mol_hilus_diff = None\n\n    def save(self, filename: str):\n        \"\"\"\n        Save the DetectDS object as a pickle file\n\n        Parameters\n        ----------\n        filename : str\n            Path to the file where the DetectDS object will be saved\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        self._detach()\n        with open(filename, \"wb\") as f:\n            pickle.dump(self, f)\n\n    @classmethod\n    def load(cls, filename: str):\n        \"\"\"\n        Load a DetectDS object from a pickle file\n\n        Parameters\n        ----------\n        filename : str\n            Path to the file where the DetectDS object is saved\n\n        Returns\n        -------\n        DetectDS\n            The loaded DetectDS object\n\n        \"\"\"\n        with open(filename, \"rb\") as f:\n            return pickle.load(f)\n\n    def __repr__(self) -&gt; str:\n\n        address_str = \" at \" + str(hex(id(self)))\n\n        if not hasattr(self, \"ds_epoch\"):\n            return \"&lt;%s%s&gt;\" % (self.type_name, address_str)\n\n        if self.ds_epoch.isempty:\n            return \"&lt;%s%s: empty&gt;\" % self.type_name\n\n        dentate_spikes = f\"dentate spikes {self.ds_epoch.n_intervals}\"\n        dstr = f\"of length {self.ds_epoch.length}\"\n\n        return \"&lt;%s%s: %s&gt; %s\" % (self.type_name, address_str, dentate_spikes, dstr)\n\n    def __str__(self) -&gt; str:\n        return self.__repr__()\n\n    def __len__(self) -&gt; int:\n        if not hasattr(self, \"ds_epoch\"):\n            return 0\n        return self.ds_epoch.n_intervals\n\n    def __getitem__(self, key):\n        if not hasattr(self, \"ds_epoch\"):\n            raise IndexError(\"No dentate spikes detected yet\")\n        return self.ds_epoch[key]\n\n    def __iter__(self):\n        if not hasattr(self, \"ds_epoch\"):\n            raise IndexError(\"No dentate spikes detected yet\")\n        return iter(self.ds_epoch)\n\n    def __contains__(self, item):\n        if not hasattr(self, \"ds_epoch\"):\n            raise IndexError(\"No dentate spikes detected yet\")\n        return item in self.ds_epoch\n</code></pre>"},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.DetectDS._detach","title":"<code>_detach()</code>","text":"<p>Detach the data from the object to allow for pickling</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def _detach(self):\n    \"\"\"Detach the data from the object to allow for pickling\"\"\"\n    self.filtered_lfp = None\n    self.lfp = None\n    self.mol_hilus_diff = None\n</code></pre>"},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.DetectDS.detect_ds","title":"<code>detect_ds()</code>","text":"<p>Detect the dentate spikes based on the method provided</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def detect_ds(self):\n    \"\"\"\n    Detect the dentate spikes based on the method provided\n    \"\"\"\n    if self.method == \"difference\":\n        # deprecated\n        raise NotImplementedError\n        # self.detect_ds_difference()\n    elif self.method == \"seperately\":\n        self.detect_ds_seperately()\n    else:\n        raise ValueError(f\"Method {self.method} not recognized\")\n</code></pre>"},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.DetectDS.filter_signal","title":"<code>filter_signal()</code>","text":"<p>Filter the LFP signal</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered LFP signal</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def filter_signal(self):\n    \"\"\"\n    Filter the LFP signal\n\n    Returns\n    -------\n    np.ndarray\n        Filtered LFP signal\n    \"\"\"\n    if not hasattr(self, \"lfp\"):\n        self.load_lfp()\n\n    b, a = cheby2(\n        self.filter_order,\n        self.filter_rs,\n        [self.lowcut, self.highcut],\n        fs=self.fs,\n        btype=\"bandpass\",\n    )\n    return filtfilt(b, a, self.lfp.data)\n</code></pre>"},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.DetectDS.get_average_trace","title":"<code>get_average_trace(shank=None, window=[-0.15, 0.15])</code>","text":"<p>Get the average LFP trace around the dentate spikes</p> <p>Parameters:</p> Name Type Description Default <code>shank</code> <code>int</code> <p>Shank number of the hilus signal</p> <code>None</code> <code>window</code> <code>list</code> <p>Window around the dentate spikes</p> <code>[-0.15, 0.15]</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Average LFP trace around the dentate spikes</p> <code>ndarray</code> <p>Time lags around the dentate spikes</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def get_average_trace(self, shank=None, window=[-0.15, 0.15]):\n    \"\"\"\n    Get the average LFP trace around the dentate spikes\n\n    Parameters\n    ----------\n    shank : int, optional\n        Shank number of the hilus signal\n    window : list, optional\n        Window around the dentate spikes\n\n    Returns\n    -------\n    np.ndarray\n        Average LFP trace around the dentate spikes\n    np.ndarray\n        Time lags around the dentate spikes\n    \"\"\"\n\n    lfp, _ = loading.loadLFP(\n        self.basepath,\n        n_channels=self.nChannels,\n        frequency=self.fs,\n        ext=\"lfp\",\n    )\n\n    if shank is None:\n        hilus_shank = [\n            k for k, v in self.shank_to_channel.items() if self.hilus_ch in v\n        ][0]\n\n    ds_average, time_lags = event_triggered_average_fast(\n        signal=lfp[:, self.shank_to_channel[hilus_shank]].T,\n        events=self.ds_epoch.starts,\n        sampling_rate=self.fs,\n        window=window,\n        return_average=True,\n    )\n    return ds_average, time_lags\n</code></pre>"},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.DetectDS.get_xml_data","title":"<code>get_xml_data()</code>","text":"<p>Load the XML file to get the number of channels, sampling frequency and shank to channel mapping</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def get_xml_data(self):\n    \"\"\"\n    Load the XML file to get the number of channels, sampling frequency and shank to channel mapping\n    \"\"\"\n    nChannels, fs, fs_dat, shank_to_channel = loading.loadXML(self.basepath)\n    self.nChannels = nChannels\n    self.fs = fs\n    self.fs_dat = fs_dat\n    self.shank_to_channel = shank_to_channel\n</code></pre>"},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.DetectDS.load","title":"<code>load(filename)</code>  <code>classmethod</code>","text":"<p>Load a DetectDS object from a pickle file</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file where the DetectDS object is saved</p> required <p>Returns:</p> Type Description <code>DetectDS</code> <p>The loaded DetectDS object</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>@classmethod\ndef load(cls, filename: str):\n    \"\"\"\n    Load a DetectDS object from a pickle file\n\n    Parameters\n    ----------\n    filename : str\n        Path to the file where the DetectDS object is saved\n\n    Returns\n    -------\n    DetectDS\n        The loaded DetectDS object\n\n    \"\"\"\n    with open(filename, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.DetectDS.load_lfp","title":"<code>load_lfp()</code>","text":"<p>Load the LFP signal</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def load_lfp(self):\n    \"\"\"\n    Load the LFP signal\n    \"\"\"\n\n    lfp, timestep = loading.loadLFP(\n        self.basepath,\n        n_channels=self.nChannels,\n        frequency=self.fs,\n        ext=\"lfp\",\n    )\n\n    if self.noise_ch is None:\n        channels = [self.hilus_ch, self.mol_ch]\n    else:\n        channels = [self.hilus_ch, self.mol_ch, self.noise_ch]\n\n    self.lfp = nel.AnalogSignalArray(\n        data=lfp[:, channels].T,\n        timestamps=timestep,\n        fs=self.fs,\n        support=nel.EpochArray(np.array([min(timestep), max(timestep)])),\n    )\n    if self.clean_lfp:\n        self.lfp._data = np.array(\n            [\n                clean_lfp(self.lfp.signals[0]),\n                clean_lfp(self.lfp.signals[1]),\n            ]\n        )\n</code></pre>"},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.DetectDS.plot","title":"<code>plot(ax=None, window=[-0.15, 0.15], channel_offset=90000.0)</code>","text":"<p>Plot the average LFP trace around the dentate spikes</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>AxesSubplot</code> <p>Axis to plot the average LFP trace</p> <code>None</code> <code>window</code> <code>list</code> <p>Window around the dentate spikes</p> <code>[-0.15, 0.15]</code> <code>channel_offset</code> <code>float</code> <p>Offset between the channels</p> <code>90000.0</code> <p>Returns:</p> Type Description <code>AxesSubplot</code> <p>Axis with the average LFP trace</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def plot(self, ax=None, window=[-0.15, 0.15], channel_offset=9e4):\n    \"\"\"\n    Plot the average LFP trace around the dentate spikes\n\n    Parameters\n    ----------\n    ax : matplotlib.axes._subplots.AxesSubplot, optional\n        Axis to plot the average LFP trace\n    window : list, optional\n        Window around the dentate spikes\n    channel_offset : float, optional\n        Offset between the channels\n\n    Returns\n    -------\n    matplotlib.axes._subplots.AxesSubplot\n        Axis with the average LFP trace\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    ds_average, time_lags = self.get_average_trace(window=window)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(5, 10))\n\n    ax.plot(\n        time_lags,\n        ds_average.T - np.linspace(0, channel_offset, ds_average.shape[0]),\n        alpha=0.75,\n    )\n    return ax\n</code></pre>"},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.DetectDS.save","title":"<code>save(filename)</code>","text":"<p>Save the DetectDS object as a pickle file</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file where the DetectDS object will be saved</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def save(self, filename: str):\n    \"\"\"\n    Save the DetectDS object as a pickle file\n\n    Parameters\n    ----------\n    filename : str\n        Path to the file where the DetectDS object will be saved\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    self._detach()\n    with open(filename, \"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.DetectDS.save_ds_epoch","title":"<code>save_ds_epoch()</code>","text":"<p>Save the dentate spikes as a cellexplorer mat file</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def save_ds_epoch(self):\n    \"\"\"\n    Save the dentate spikes as a cellexplorer mat file\n    \"\"\"\n\n    filename = os.path.join(\n        self.basepath, os.path.basename(self.basepath) + \".DS2.events.mat\"\n    )\n    data = {}\n    data[\"DS2\"] = {}\n    data[\"DS2\"][\"detectorinfo\"] = {}\n    data[\"DS2\"][\"timestamps\"] = self.ds_epoch.data\n    data[\"DS2\"][\"peaks\"] = self.peaks\n    data[\"DS2\"][\"amplitudes\"] = self.peak_val.T\n    data[\"DS2\"][\"amplitudeUnits\"] = \"mV\"\n    data[\"DS2\"][\"eventID\"] = []\n    data[\"DS2\"][\"eventIDlabels\"] = []\n    data[\"DS2\"][\"eventIDbinary\"] = []\n    data[\"DS2\"][\"duration\"] = self.ds_epoch.durations.T\n    data[\"DS2\"][\"center\"] = np.median(self.ds_epoch.data, axis=1).T\n    data[\"DS2\"][\"detectorinfo\"][\"detectorname\"] = \"DetectDS\"\n    data[\"DS2\"][\"detectorinfo\"][\"detectionparms\"] = []\n    data[\"DS2\"][\"detectorinfo\"][\"detectionintervals\"] = []\n    data[\"DS2\"][\"detectorinfo\"][\"ml_channel\"] = self.mol_ch\n    data[\"DS2\"][\"detectorinfo\"][\"h_channel\"] = self.hilus_ch\n    if self.noise_ch is not None:\n        data[\"DS2\"][\"detectorinfo\"][\"noise_channel\"] = self.noise_ch\n\n    savemat(filename, data, long_field_names=True)\n</code></pre>"},{"location":"reference/neuro_py/detectors/#neuro_py.detectors.detect_up_down_states","title":"<code>detect_up_down_states(basepath=None, st=None, nrem_epochs=None, region='ILA|PFC|PL|EC1|EC2|EC3|EC4|EC5|MEC|CTX', min_dur=0.03, max_dur=0.5, percentile=20, bin_size=0.01, smooth_sigma=0.02, min_cells=10, save_mat=True)</code>","text":"<p>Detect UP and DOWN states in neural data.</p> <p>UP and DOWN states are identified by computing the total firing rate of all simultaneously recorded neurons in bins of 10 ms, smoothed with a Gaussian kernel of 20 ms s.d. Epochs with a firing rate below the specified percentile threshold  are considered DOWN states, while the intervals between DOWN states are classified as UP states. Epochs shorter than <code>min_dur</code> or longer than <code>max_dur</code> are discarded.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Base directory path where event files and neural data are stored.</p> <code>None</code> <code>st</code> <code>Optional[SpikeTrain]</code> <p>Spike train data. If None, spike data will be loaded based on specified regions.</p> <code>None</code> <code>nrem_epochs</code> <code>Optional[EpochArray]</code> <p>NREM epochs. If None, epochs will be loaded from the basepath.</p> <code>None</code> <code>region</code> <code>str</code> <p>Brain regions for loading spikes. The first region is prioritized.</p> <code>\"ILA|PFC|PL|EC1|EC2|EC3|EC4|EC5|MEC\"</code> <code>min_dur</code> <code>float</code> <p>Minimum duration for DOWN states, in seconds.</p> <code>0.03</code> <code>max_dur</code> <code>float</code> <p>Maximum duration for DOWN states, in seconds.</p> <code>0.5</code> <code>percentile</code> <code>float</code> <p>Percentile threshold for determining DOWN states based on firing rate.</p> <code>20</code> <code>bin_size</code> <code>float</code> <p>Bin size for computing firing rates, in seconds.</p> <code>0.01</code> <code>smooth_sigma</code> <code>float</code> <p>Standard deviation for Gaussian kernel smoothing, in seconds.</p> <code>0.02</code> <code>min_cells</code> <code>int</code> <p>Minimum number of neurons required for analysis.</p> <code>10</code> <code>save_mat</code> <code>bool</code> <p>Whether to save the detected UP and DOWN states to .mat files.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[Optional[EpochArray], Optional[EpochArray]]</code> <p>A tuple containing the detected DOWN state epochs and UP state epochs. Returns (None, None) if no suitable states are found or insufficient data is available.</p> Notes <p>Detection method based on https://doi.org/10.1038/s41467-020-15842-4</p> Source code in <code>neuro_py/detectors/up_down_state.py</code> <pre><code>def detect_up_down_states(\n    basepath: Optional[str] = None,\n    st: Optional[nel.SpikeTrainArray] = None,\n    nrem_epochs: Optional[nel.EpochArray] = None,\n    region: str = \"ILA|PFC|PL|EC1|EC2|EC3|EC4|EC5|MEC|CTX\",\n    min_dur: float = 0.03,\n    max_dur: float = 0.5,\n    percentile: float = 20,\n    bin_size: float = 0.01,\n    smooth_sigma: float = 0.02,\n    min_cells: int = 10,\n    save_mat: bool = True,\n) -&gt; Tuple[Optional[nel.EpochArray], Optional[nel.EpochArray]]:\n    \"\"\"\n    Detect UP and DOWN states in neural data.\n\n    UP and DOWN states are identified by computing the total firing rate of all\n    simultaneously recorded neurons in bins of 10 ms, smoothed with a Gaussian kernel\n    of 20 ms s.d. Epochs with a firing rate below the specified percentile threshold \n    are considered DOWN states, while the intervals between DOWN states are classified\n    as UP states. Epochs shorter than `min_dur` or longer than `max_dur` are discarded.\n\n    Parameters\n    ----------\n    basepath : str\n        Base directory path where event files and neural data are stored.\n    st : Optional[nel.SpikeTrain], default=None\n        Spike train data. If None, spike data will be loaded based on specified regions.\n    nrem_epochs : Optional[nel.EpochArray], default=None\n        NREM epochs. If None, epochs will be loaded from the basepath.\n    region : str, default=\"ILA|PFC|PL|EC1|EC2|EC3|EC4|EC5|MEC\"\n        Brain regions for loading spikes. The first region is prioritized.\n    min_dur : float, default=0.03\n        Minimum duration for DOWN states, in seconds.\n    max_dur : float, default=0.5\n        Maximum duration for DOWN states, in seconds.\n    percentile : float, default=20\n        Percentile threshold for determining DOWN states based on firing rate.\n    bin_size : float, default=0.01\n        Bin size for computing firing rates, in seconds.\n    smooth_sigma : float, default=0.02\n        Standard deviation for Gaussian kernel smoothing, in seconds.\n    min_cells : int, default=10\n        Minimum number of neurons required for analysis.\n    save_mat : bool, default=True\n        Whether to save the detected UP and DOWN states to .mat files.\n\n    Returns\n    -------\n    Tuple[Optional[nel.EpochArray], Optional[nel.EpochArray]]\n        A tuple containing the detected DOWN state epochs and UP state epochs.\n        Returns (None, None) if no suitable states are found or insufficient data is available.\n\n    Notes\n    -----\n    Detection method based on https://doi.org/10.1038/s41467-020-15842-4\n    \"\"\"\n\n    # check for existance of event files\n    if save_mat:\n        filename_downstate = os.path.join(\n            basepath, os.path.basename(basepath) + \".\" + \"down_state\" + \".events.mat\"\n        )\n        filename_upstate = os.path.join(\n            basepath, os.path.basename(basepath) + \".\" + \"up_state\" + \".events.mat\"\n        )\n        if os.path.exists(filename_downstate) &amp; os.path.exists(filename_upstate):\n            down_state = loading.load_events(basepath=basepath, epoch_name=\"down_state\")\n            up_state = loading.load_events(basepath=basepath, epoch_name=\"up_state\")\n            return down_state, up_state\n\n    # load brain states\n    if nrem_epochs is None:\n        state_dict = loading.load_SleepState_states(basepath)\n        nrem_epochs = nel.EpochArray(state_dict[\"NREMstate\"])\n\n    if nrem_epochs.isempty:\n        print(f\"No NREM epochs found for {basepath}\")\n        return None, None\n\n    # load spikes\n    if st is None:\n        st, _ = loading.load_spikes(basepath, brainRegion=region)\n\n    # check if there are enough cells\n    if st is None or st.isempty or st.data.shape[0] &lt; min_cells:\n        print(f\"No spikes found for {basepath} {region}\")\n\n        # load spikes\n        st, _ = loading.load_spikes(basepath, brainRegion=region[1])\n        # check if there are enough cells\n        if st is None or st.isempty or st.data.shape[0] &lt; min_cells:\n            print(f\"No spikes found for {basepath} {region}\")\n            return None, None\n\n    # flatten spikes\n    st = st[nrem_epochs].flatten()\n\n    # bin and smooth\n    bst = st.bin(ds=bin_size).smooth(sigma=smooth_sigma)\n\n    # find down states, based on percentile\n    down_state_epochs = bst.bin_centers[\n        find_interval(bst.data.flatten() &lt; np.percentile(bst.data.T, percentile))\n    ]\n    if down_state_epochs.shape[0] == 0:\n        print(f\"No down states found for {basepath}\")\n        return None, None\n\n    # remove short and long epochs\n    durations = down_state_epochs[:, 1] - down_state_epochs[:, 0]\n    down_state_epochs = down_state_epochs[\n        ~((durations &lt; min_dur) | (durations &gt; max_dur)), :\n    ]\n    # convert to epoch array with same domain as nrem epochs (this is so compliment will also be in nrem epochs)\n    down_state_epochs = nel.EpochArray(data=down_state_epochs, domain=nrem_epochs)\n\n    # compliment to get up states\n    up_state_epochs = ~down_state_epochs\n\n    # save to cell explorer mat file\n    if save_mat:\n        epoch_to_mat(down_state_epochs, basepath, \"down_state\", \"detect_up_down_states\")\n        epoch_to_mat(up_state_epochs, basepath, \"up_state\", \"detect_up_down_states\")\n\n    return down_state_epochs, up_state_epochs\n</code></pre>"},{"location":"reference/neuro_py/detectors/dentate_spike/","title":"neuro_py.detectors.dentate_spike","text":""},{"location":"reference/neuro_py/detectors/dentate_spike/#neuro_py.detectors.dentate_spike.DetectDS","title":"<code>DetectDS</code>","text":"<p>               Bases: <code>object</code></p> <p>Class for detecting dentate spikes</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder containing the data</p> required <code>hilus_ch</code> <code>int</code> <p>Channel number of the hilus signal (0 indexing)</p> required <code>mol_ch</code> <code>int</code> <p>Channel number of the mol signal (0 indexing)</p> required <code>noise_ch</code> <code>int</code> <p>Channel number of the noise signal or signal far from dentate (0 indexing)</p> <code>None</code> <code>lowcut</code> <code>float</code> <p>Low cut frequency for the signal filter</p> <code>10</code> <code>highcut</code> <code>float</code> <p>High cut frequency for the signal filter</p> <code>250</code> <code>filter_signal_bool</code> <code>bool</code> <p>If True, the signal will be filtered</p> <code>True</code> <code>primary_threshold</code> <code>float</code> <p>Primary threshold for detecting the dentate spikes (difference method only)</p> <code>5</code> <code>secondary_threshold</code> <code>float</code> <p>Secondary threshold for detecting the dentate spikes (difference method only)</p> required <code>primary_thres_mol</code> <code>float</code> <p>Primary threshold for detecting the dentate spikes in the mol signal</p> <code>2</code> <code>primary_thres_hilus</code> <code>float</code> <p>Primary threshold for detecting the dentate spikes in the hilus signal</p> <code>5</code> <code>min_duration</code> <code>float</code> <p>Minimum duration of the dentate spikes</p> <code>0.005</code> <code>max_duration</code> <code>float</code> <p>Maximum duration of the dentate spikes</p> <code>0.05</code> <code>filter_order</code> <code>int</code> <p>Order of the filter</p> <code>4</code> <code>filter_rs</code> <code>int</code> <p>Resonance frequency of the filter</p> <code>20</code> <code>method</code> <code>str</code> <p>Method for detecting the dentate spikes. \"difference\" for detecting the dentate spikes by difference between the hilus and mol signal \"seperately\" for detecting the dentate spikes by the hilus and mol signal separately</p> <code>'seperately'</code> <code>clean_lfp</code> <code>bool</code> <p>If True, the LFP signal will be cleaned</p> <code>False</code> <code>emg_threshold</code> <code>float</code> <p>Threshold for the EMG signal to remove dentate spikes</p> <code>0.9</code> <p>Attributes:</p> Name Type Description <code>lfp</code> <code>AnalogSignalArray</code> <p>LFP signal</p> <code>filtered_lfp</code> <code>AnalogSignalArray</code> <p>Filtered LFP signal</p> <code>mol_hilus_diff</code> <code>AnalogSignalArray</code> <p>Difference between the hilus and mol signal</p> <code>ds_epoch</code> <code>EpochArray</code> <p>EpochArray with the dentate spikes</p> <code>peak_val</code> <code>ndarray</code> <p>Peak value of the dentate spikes</p> <p>Methods:</p> Name Description <code>load_lfp</code> <p>Load the LFP signal</p> <code>filter_signal</code> <p>Filter the LFP signal</p> <code>get_filtered_lfp</code> <p>Get the filtered LFP signal</p> <code>get_lfp_diff</code> <p>Get the difference between the hilus and mol signal</p> <code>detect_ds_difference</code> <p>Detect the dentate spikes by difference between the hilus and mol signal</p> <code>detect_ds_seperately</code> <p>Detect the dentate spikes by the hilus and mol signal separately</p> <code>save_ds_epoch</code> <p>Save the dentate spikes as an EpochArray</p> <p>Examples:</p> <p>In IDE or python console</p> <pre><code>&gt;&gt;&gt; from ds_swr.detection.detect_dentate_spike import DetectDS\n&gt;&gt;&gt; from neuro_py.io import loading\n&gt;&gt;&gt; channel_tags = loading.load_channel_tags(basepath)\n&gt;&gt;&gt; dds = DetectDS(\n    basepath,\n    channel_tags[\"hilus\"][\"channels\"] - 1,\n    channel_tags[\"mol\"][\"channels\"] - 1\n)\n&gt;&gt;&gt; dds.detect_ds()\n&gt;&gt;&gt; dds.save_ds_epoch()\n&gt;&gt;&gt; dds\n&lt;DetectDS at 0x17fe787c640: dentate spikes 5,769&gt; of length 1:11:257 minutes\n</code></pre> <p>In command line</p> <pre><code>&gt;&gt;&gt; python detect_dentate_spike.py Z:\\Data\\Can\\OML22\\day20\n</code></pre> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>class DetectDS(object):\n    \"\"\"\n    Class for detecting dentate spikes\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder containing the data\n    hilus_ch : int\n        Channel number of the hilus signal (0 indexing)\n    mol_ch : int\n        Channel number of the mol signal (0 indexing)\n    noise_ch : int, optional\n        Channel number of the noise signal or signal far from dentate (0 indexing)\n    lowcut : float, optional\n        Low cut frequency for the signal filter\n    highcut : float, optional\n        High cut frequency for the signal filter\n    filter_signal_bool : bool, optional\n        If True, the signal will be filtered\n    primary_threshold : float, optional\n        Primary threshold for detecting the dentate spikes (difference method only)\n    secondary_threshold : float, optional\n        Secondary threshold for detecting the dentate spikes (difference method only)\n    primary_thres_mol : float, optional\n        Primary threshold for detecting the dentate spikes in the mol signal\n    primary_thres_hilus : float, optional\n        Primary threshold for detecting the dentate spikes in the hilus signal\n    min_duration : float, optional\n        Minimum duration of the dentate spikes\n    max_duration : float, optional\n        Maximum duration of the dentate spikes\n    filter_order : int, optional\n        Order of the filter\n    filter_rs : int, optional\n        Resonance frequency of the filter\n    method : str, optional\n        Method for detecting the dentate spikes.\n        \"difference\" for detecting the dentate spikes by difference between the hilus and mol signal\n        \"seperately\" for detecting the dentate spikes by the hilus and mol signal separately\n    clean_lfp : bool, optional\n        If True, the LFP signal will be cleaned\n    emg_threshold : float, optional\n        Threshold for the EMG signal to remove dentate spikes\n\n\n    Attributes\n    ----------\n    lfp : nelpy.AnalogSignalArray\n        LFP signal\n    filtered_lfp : nelpy.AnalogSignalArray\n        Filtered LFP signal\n    mol_hilus_diff : nelpy.AnalogSignalArray\n        Difference between the hilus and mol signal\n    ds_epoch : nelpy.EpochArray\n        EpochArray with the dentate spikes\n    peak_val : np.ndarray\n        Peak value of the dentate spikes\n\n\n    Methods\n    -------\n    load_lfp()\n        Load the LFP signal\n    filter_signal()\n        Filter the LFP signal\n    get_filtered_lfp()\n        Get the filtered LFP signal\n    get_lfp_diff()\n        Get the difference between the hilus and mol signal\n    detect_ds_difference()\n        Detect the dentate spikes by difference between the hilus and mol signal\n    detect_ds_seperately()\n        Detect the dentate spikes by the hilus and mol signal separately\n    save_ds_epoch()\n        Save the dentate spikes as an EpochArray\n\n    Examples\n    --------\n    In IDE or python console\n\n    &gt;&gt;&gt; from ds_swr.detection.detect_dentate_spike import DetectDS\n    &gt;&gt;&gt; from neuro_py.io import loading\n    &gt;&gt;&gt; channel_tags = loading.load_channel_tags(basepath)\n    &gt;&gt;&gt; dds = DetectDS(\n        basepath,\n        channel_tags[\"hilus\"][\"channels\"] - 1,\n        channel_tags[\"mol\"][\"channels\"] - 1\n    )\n    &gt;&gt;&gt; dds.detect_ds()\n    &gt;&gt;&gt; dds.save_ds_epoch()\n    &gt;&gt;&gt; dds\n    &lt;DetectDS at 0x17fe787c640: dentate spikes 5,769&gt; of length 1:11:257 minutes\n\n\n    In command line\n\n    &gt;&gt;&gt; python detect_dentate_spike.py Z:\\Data\\Can\\OML22\\day20\n    \"\"\"\n\n    def __init__(\n        self,\n        basepath: str,\n        hilus_ch: int,\n        mol_ch: int,\n        noise_ch: Union[int, None] = None,\n        lowcut: int = 10,\n        highcut: int = 250,\n        filter_signal_bool: bool = True,\n        primary_threshold: Union[int, float] = 5,\n        primary_thres_mol: Union[int, float] = 2,\n        primary_thres_hilus: Union[int, float] = 5,\n        min_duration: float = 0.005,\n        max_duration: float = 0.05,\n        filter_order: int = 4,\n        filter_rs: int = 20,\n        method: str = \"seperately\",\n        clean_lfp: bool = False,\n        emg_threshold: float = 0.9,\n    ) -&gt; None:\n\n        # adding all the parameters to the class\n        self.__dict__.update(locals())\n        del self.__dict__[\"self\"]\n        # setting the type name\n        self.type_name = self.__class__.__name__\n        self.get_xml_data()\n\n    def get_xml_data(self):\n        \"\"\"\n        Load the XML file to get the number of channels, sampling frequency and shank to channel mapping\n        \"\"\"\n        nChannels, fs, fs_dat, shank_to_channel = loading.loadXML(self.basepath)\n        self.nChannels = nChannels\n        self.fs = fs\n        self.fs_dat = fs_dat\n        self.shank_to_channel = shank_to_channel\n\n    def load_lfp(self):\n        \"\"\"\n        Load the LFP signal\n        \"\"\"\n\n        lfp, timestep = loading.loadLFP(\n            self.basepath,\n            n_channels=self.nChannels,\n            frequency=self.fs,\n            ext=\"lfp\",\n        )\n\n        if self.noise_ch is None:\n            channels = [self.hilus_ch, self.mol_ch]\n        else:\n            channels = [self.hilus_ch, self.mol_ch, self.noise_ch]\n\n        self.lfp = nel.AnalogSignalArray(\n            data=lfp[:, channels].T,\n            timestamps=timestep,\n            fs=self.fs,\n            support=nel.EpochArray(np.array([min(timestep), max(timestep)])),\n        )\n        if self.clean_lfp:\n            self.lfp._data = np.array(\n                [\n                    clean_lfp(self.lfp.signals[0]),\n                    clean_lfp(self.lfp.signals[1]),\n                ]\n            )\n\n    def filter_signal(self):\n        \"\"\"\n        Filter the LFP signal\n\n        Returns\n        -------\n        np.ndarray\n            Filtered LFP signal\n        \"\"\"\n        if not hasattr(self, \"lfp\"):\n            self.load_lfp()\n\n        b, a = cheby2(\n            self.filter_order,\n            self.filter_rs,\n            [self.lowcut, self.highcut],\n            fs=self.fs,\n            btype=\"bandpass\",\n        )\n        return filtfilt(b, a, self.lfp.data)\n\n    def get_filtered_lfp(self):\n        if not hasattr(self, \"lfp\"):\n            self.load_lfp()\n\n        self.filtered_lfp = deepcopy(self.lfp)\n        self.filtered_lfp._data = self.filter_signal()\n\n    def get_lfp_diff(self):\n        if self.filter_signal_bool:\n            y = self.filter_signal()\n        else:\n            if not hasattr(self, \"lfp\"):\n                self.load_lfp()\n            y = self.lfp.data\n\n        self.mol_hilus_diff = nel.AnalogSignalArray(\n            data=y[0, :] - y[1, :],\n            timestamps=self.lfp.abscissa_vals,\n            fs=self.fs,\n            support=nel.EpochArray(\n                np.array([min(self.lfp.abscissa_vals), max(self.lfp.abscissa_vals)])\n            ),\n        )\n\n    def detect_ds_difference(self):\n        if not hasattr(self, \"mol_hilus_diff\"):\n            self.get_lfp_diff()\n\n        PrimaryThreshold = (\n            self.mol_hilus_diff.mean()\n            + self.primary_threshold * self.mol_hilus_diff.std()\n        )\n        SecondaryThreshold = (\n            self.mol_hilus_diff.mean()\n            + self.secondary_threshold * self.mol_hilus_diff.std()\n        )\n        bounds, self.peak_val, _ = nel.utils.get_events_boundaries(\n            x=self.mol_hilus_diff.data,\n            PrimaryThreshold=PrimaryThreshold,\n            SecondaryThreshold=SecondaryThreshold,\n            minThresholdLength=0,\n            minLength=self.min_duration,\n            maxLength=self.max_duration,\n            ds=1 / self.mol_hilus_diff.fs,\n        )\n        # convert bounds to time in seconds\n        timebounds = self.mol_hilus_diff.time[bounds]\n        # add 1/fs to stops for open interval\n        timebounds[:, 1] += 1 / self.mol_hilus_diff.fs\n        # create EpochArray with bounds\n        self.ds_epoch = nel.EpochArray(timebounds)\n\n        # remove ds in high emg\n        _, high_emg_epoch, _ = loading.load_emg(self.basepath, self.emg_threshold)\n        if not high_emg_epoch.isempty:\n            idx = find_intersecting_intervals(self.ds_epoch, high_emg_epoch)\n            self.ds_epoch._data = self.ds_epoch.data[~idx]\n            self.peak_val = self.peak_val[~idx]\n\n    def detect_ds_seperately(self):\n        if not hasattr(self, \"filtered_lfp\"):\n            self.get_filtered_lfp()\n\n        # min and max time width of ds (converted to samples for find_peaks)\n        time_widths = [\n            int(self.min_duration * self.filtered_lfp.fs),\n            int(self.max_duration * self.filtered_lfp.fs),\n        ]\n\n        # detect ds in hilus\n        PrimaryThreshold = (\n            self.filtered_lfp.data[0, :].mean()\n            + self.primary_thres_hilus * self.filtered_lfp.data[0, :].std()\n        )\n\n        peaks, properties = find_peaks(\n            self.filtered_lfp.data[0, :],\n            height=PrimaryThreshold,\n            width=time_widths,\n        )\n        self.peaks = peaks / self.filtered_lfp.fs\n        self.peak_val = properties[\"peak_heights\"]\n\n        # create EpochArray with bounds\n        hilus_epoch = nel.EpochArray(\n            np.array([properties[\"left_ips\"], properties[\"right_ips\"]]).T\n            / self.filtered_lfp.fs\n        )\n\n        # detect ds in mol\n        PrimaryThreshold = (\n            self.filtered_lfp.data[1, :].mean()\n            + self.primary_thres_mol * self.filtered_lfp.data[1, :].std()\n        )\n\n        peaks, properties = find_peaks(\n            -self.filtered_lfp.data[1, :],\n            height=PrimaryThreshold,\n            width=time_widths,\n        )\n        mol_epoch_peak = peaks / self.filtered_lfp.fs\n        # create EpochArray with bounds\n        mol_epoch = nel.EpochArray(\n            np.array([properties[\"left_ips\"], properties[\"right_ips\"]]).T\n            / self.filtered_lfp.fs\n        )\n\n        # detect ds in noise channel\n        if self.noise_ch is not None:\n            PrimaryThreshold = (\n                self.filtered_lfp.data[2, :].mean()\n                + self.primary_thres_hilus * self.filtered_lfp.data[2, :].std()\n            )\n\n            peaks, properties = find_peaks(\n                self.filtered_lfp.data[2, :],\n                height=PrimaryThreshold,\n                width=time_widths,\n            )\n\n            # create EpochArray with bounds\n            noise_epoch = nel.EpochArray(\n                np.array([properties[\"left_ips\"], properties[\"right_ips\"]]).T\n                / self.filtered_lfp.fs\n            )\n\n        # remove hilus spikes that are not overlapping with mol spikes\n        # first, find mol peaks that are within hilus epoch\n        idx = in_intervals(mol_epoch_peak, hilus_epoch.data)\n        mol_epoch._data = mol_epoch.data[idx]\n\n        overlap = find_intersecting_intervals(\n            hilus_epoch, mol_epoch, return_indices=True\n        )\n        self.ds_epoch = nel.EpochArray(hilus_epoch.data[overlap])\n        self.peak_val = self.peak_val[overlap]\n        self.peaks = self.peaks[overlap]\n\n        # remove dentate spikes that are overlapping with noise spikes\n        if self.noise_ch is not None:\n            overlap = find_intersecting_intervals(\n                self.ds_epoch, noise_epoch, return_indices=True\n            )\n            self.ds_epoch = nel.EpochArray(self.ds_epoch.data[~overlap])\n            self.peak_val = self.peak_val[~overlap]\n            self.peaks = self.peaks[~overlap]\n\n        # remove ds in high emg\n        _, high_emg_epoch, _ = loading.load_emg(self.basepath, self.emg_threshold)\n        if not high_emg_epoch.isempty:\n            idx = find_intersecting_intervals(self.ds_epoch, high_emg_epoch)\n            self.ds_epoch._data = self.ds_epoch.data[~idx]\n            self.peak_val = self.peak_val[~idx]\n            self.peaks = self.peaks[~idx]\n\n    def detect_ds(self):\n        \"\"\"\n        Detect the dentate spikes based on the method provided\n        \"\"\"\n        if self.method == \"difference\":\n            # deprecated\n            raise NotImplementedError\n            # self.detect_ds_difference()\n        elif self.method == \"seperately\":\n            self.detect_ds_seperately()\n        else:\n            raise ValueError(f\"Method {self.method} not recognized\")\n\n    def save_ds_epoch(self):\n        \"\"\"\n        Save the dentate spikes as a cellexplorer mat file\n        \"\"\"\n\n        filename = os.path.join(\n            self.basepath, os.path.basename(self.basepath) + \".DS2.events.mat\"\n        )\n        data = {}\n        data[\"DS2\"] = {}\n        data[\"DS2\"][\"detectorinfo\"] = {}\n        data[\"DS2\"][\"timestamps\"] = self.ds_epoch.data\n        data[\"DS2\"][\"peaks\"] = self.peaks\n        data[\"DS2\"][\"amplitudes\"] = self.peak_val.T\n        data[\"DS2\"][\"amplitudeUnits\"] = \"mV\"\n        data[\"DS2\"][\"eventID\"] = []\n        data[\"DS2\"][\"eventIDlabels\"] = []\n        data[\"DS2\"][\"eventIDbinary\"] = []\n        data[\"DS2\"][\"duration\"] = self.ds_epoch.durations.T\n        data[\"DS2\"][\"center\"] = np.median(self.ds_epoch.data, axis=1).T\n        data[\"DS2\"][\"detectorinfo\"][\"detectorname\"] = \"DetectDS\"\n        data[\"DS2\"][\"detectorinfo\"][\"detectionparms\"] = []\n        data[\"DS2\"][\"detectorinfo\"][\"detectionintervals\"] = []\n        data[\"DS2\"][\"detectorinfo\"][\"ml_channel\"] = self.mol_ch\n        data[\"DS2\"][\"detectorinfo\"][\"h_channel\"] = self.hilus_ch\n        if self.noise_ch is not None:\n            data[\"DS2\"][\"detectorinfo\"][\"noise_channel\"] = self.noise_ch\n\n        savemat(filename, data, long_field_names=True)\n\n    def get_average_trace(self, shank=None, window=[-0.15, 0.15]):\n        \"\"\"\n        Get the average LFP trace around the dentate spikes\n\n        Parameters\n        ----------\n        shank : int, optional\n            Shank number of the hilus signal\n        window : list, optional\n            Window around the dentate spikes\n\n        Returns\n        -------\n        np.ndarray\n            Average LFP trace around the dentate spikes\n        np.ndarray\n            Time lags around the dentate spikes\n        \"\"\"\n\n        lfp, _ = loading.loadLFP(\n            self.basepath,\n            n_channels=self.nChannels,\n            frequency=self.fs,\n            ext=\"lfp\",\n        )\n\n        if shank is None:\n            hilus_shank = [\n                k for k, v in self.shank_to_channel.items() if self.hilus_ch in v\n            ][0]\n\n        ds_average, time_lags = event_triggered_average_fast(\n            signal=lfp[:, self.shank_to_channel[hilus_shank]].T,\n            events=self.ds_epoch.starts,\n            sampling_rate=self.fs,\n            window=window,\n            return_average=True,\n        )\n        return ds_average, time_lags\n\n    def plot(self, ax=None, window=[-0.15, 0.15], channel_offset=9e4):\n        \"\"\"\n        Plot the average LFP trace around the dentate spikes\n\n        Parameters\n        ----------\n        ax : matplotlib.axes._subplots.AxesSubplot, optional\n            Axis to plot the average LFP trace\n        window : list, optional\n            Window around the dentate spikes\n        channel_offset : float, optional\n            Offset between the channels\n\n        Returns\n        -------\n        matplotlib.axes._subplots.AxesSubplot\n            Axis with the average LFP trace\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        ds_average, time_lags = self.get_average_trace(window=window)\n\n        if ax is None:\n            fig, ax = plt.subplots(figsize=(5, 10))\n\n        ax.plot(\n            time_lags,\n            ds_average.T - np.linspace(0, channel_offset, ds_average.shape[0]),\n            alpha=0.75,\n        )\n        return ax\n\n    def _detach(self):\n        \"\"\"Detach the data from the object to allow for pickling\"\"\"\n        self.filtered_lfp = None\n        self.lfp = None\n        self.mol_hilus_diff = None\n\n    def save(self, filename: str):\n        \"\"\"\n        Save the DetectDS object as a pickle file\n\n        Parameters\n        ----------\n        filename : str\n            Path to the file where the DetectDS object will be saved\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        self._detach()\n        with open(filename, \"wb\") as f:\n            pickle.dump(self, f)\n\n    @classmethod\n    def load(cls, filename: str):\n        \"\"\"\n        Load a DetectDS object from a pickle file\n\n        Parameters\n        ----------\n        filename : str\n            Path to the file where the DetectDS object is saved\n\n        Returns\n        -------\n        DetectDS\n            The loaded DetectDS object\n\n        \"\"\"\n        with open(filename, \"rb\") as f:\n            return pickle.load(f)\n\n    def __repr__(self) -&gt; str:\n\n        address_str = \" at \" + str(hex(id(self)))\n\n        if not hasattr(self, \"ds_epoch\"):\n            return \"&lt;%s%s&gt;\" % (self.type_name, address_str)\n\n        if self.ds_epoch.isempty:\n            return \"&lt;%s%s: empty&gt;\" % self.type_name\n\n        dentate_spikes = f\"dentate spikes {self.ds_epoch.n_intervals}\"\n        dstr = f\"of length {self.ds_epoch.length}\"\n\n        return \"&lt;%s%s: %s&gt; %s\" % (self.type_name, address_str, dentate_spikes, dstr)\n\n    def __str__(self) -&gt; str:\n        return self.__repr__()\n\n    def __len__(self) -&gt; int:\n        if not hasattr(self, \"ds_epoch\"):\n            return 0\n        return self.ds_epoch.n_intervals\n\n    def __getitem__(self, key):\n        if not hasattr(self, \"ds_epoch\"):\n            raise IndexError(\"No dentate spikes detected yet\")\n        return self.ds_epoch[key]\n\n    def __iter__(self):\n        if not hasattr(self, \"ds_epoch\"):\n            raise IndexError(\"No dentate spikes detected yet\")\n        return iter(self.ds_epoch)\n\n    def __contains__(self, item):\n        if not hasattr(self, \"ds_epoch\"):\n            raise IndexError(\"No dentate spikes detected yet\")\n        return item in self.ds_epoch\n</code></pre>"},{"location":"reference/neuro_py/detectors/dentate_spike/#neuro_py.detectors.dentate_spike.DetectDS._detach","title":"<code>_detach()</code>","text":"<p>Detach the data from the object to allow for pickling</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def _detach(self):\n    \"\"\"Detach the data from the object to allow for pickling\"\"\"\n    self.filtered_lfp = None\n    self.lfp = None\n    self.mol_hilus_diff = None\n</code></pre>"},{"location":"reference/neuro_py/detectors/dentate_spike/#neuro_py.detectors.dentate_spike.DetectDS.detect_ds","title":"<code>detect_ds()</code>","text":"<p>Detect the dentate spikes based on the method provided</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def detect_ds(self):\n    \"\"\"\n    Detect the dentate spikes based on the method provided\n    \"\"\"\n    if self.method == \"difference\":\n        # deprecated\n        raise NotImplementedError\n        # self.detect_ds_difference()\n    elif self.method == \"seperately\":\n        self.detect_ds_seperately()\n    else:\n        raise ValueError(f\"Method {self.method} not recognized\")\n</code></pre>"},{"location":"reference/neuro_py/detectors/dentate_spike/#neuro_py.detectors.dentate_spike.DetectDS.filter_signal","title":"<code>filter_signal()</code>","text":"<p>Filter the LFP signal</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered LFP signal</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def filter_signal(self):\n    \"\"\"\n    Filter the LFP signal\n\n    Returns\n    -------\n    np.ndarray\n        Filtered LFP signal\n    \"\"\"\n    if not hasattr(self, \"lfp\"):\n        self.load_lfp()\n\n    b, a = cheby2(\n        self.filter_order,\n        self.filter_rs,\n        [self.lowcut, self.highcut],\n        fs=self.fs,\n        btype=\"bandpass\",\n    )\n    return filtfilt(b, a, self.lfp.data)\n</code></pre>"},{"location":"reference/neuro_py/detectors/dentate_spike/#neuro_py.detectors.dentate_spike.DetectDS.get_average_trace","title":"<code>get_average_trace(shank=None, window=[-0.15, 0.15])</code>","text":"<p>Get the average LFP trace around the dentate spikes</p> <p>Parameters:</p> Name Type Description Default <code>shank</code> <code>int</code> <p>Shank number of the hilus signal</p> <code>None</code> <code>window</code> <code>list</code> <p>Window around the dentate spikes</p> <code>[-0.15, 0.15]</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Average LFP trace around the dentate spikes</p> <code>ndarray</code> <p>Time lags around the dentate spikes</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def get_average_trace(self, shank=None, window=[-0.15, 0.15]):\n    \"\"\"\n    Get the average LFP trace around the dentate spikes\n\n    Parameters\n    ----------\n    shank : int, optional\n        Shank number of the hilus signal\n    window : list, optional\n        Window around the dentate spikes\n\n    Returns\n    -------\n    np.ndarray\n        Average LFP trace around the dentate spikes\n    np.ndarray\n        Time lags around the dentate spikes\n    \"\"\"\n\n    lfp, _ = loading.loadLFP(\n        self.basepath,\n        n_channels=self.nChannels,\n        frequency=self.fs,\n        ext=\"lfp\",\n    )\n\n    if shank is None:\n        hilus_shank = [\n            k for k, v in self.shank_to_channel.items() if self.hilus_ch in v\n        ][0]\n\n    ds_average, time_lags = event_triggered_average_fast(\n        signal=lfp[:, self.shank_to_channel[hilus_shank]].T,\n        events=self.ds_epoch.starts,\n        sampling_rate=self.fs,\n        window=window,\n        return_average=True,\n    )\n    return ds_average, time_lags\n</code></pre>"},{"location":"reference/neuro_py/detectors/dentate_spike/#neuro_py.detectors.dentate_spike.DetectDS.get_xml_data","title":"<code>get_xml_data()</code>","text":"<p>Load the XML file to get the number of channels, sampling frequency and shank to channel mapping</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def get_xml_data(self):\n    \"\"\"\n    Load the XML file to get the number of channels, sampling frequency and shank to channel mapping\n    \"\"\"\n    nChannels, fs, fs_dat, shank_to_channel = loading.loadXML(self.basepath)\n    self.nChannels = nChannels\n    self.fs = fs\n    self.fs_dat = fs_dat\n    self.shank_to_channel = shank_to_channel\n</code></pre>"},{"location":"reference/neuro_py/detectors/dentate_spike/#neuro_py.detectors.dentate_spike.DetectDS.load","title":"<code>load(filename)</code>  <code>classmethod</code>","text":"<p>Load a DetectDS object from a pickle file</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file where the DetectDS object is saved</p> required <p>Returns:</p> Type Description <code>DetectDS</code> <p>The loaded DetectDS object</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>@classmethod\ndef load(cls, filename: str):\n    \"\"\"\n    Load a DetectDS object from a pickle file\n\n    Parameters\n    ----------\n    filename : str\n        Path to the file where the DetectDS object is saved\n\n    Returns\n    -------\n    DetectDS\n        The loaded DetectDS object\n\n    \"\"\"\n    with open(filename, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"reference/neuro_py/detectors/dentate_spike/#neuro_py.detectors.dentate_spike.DetectDS.load_lfp","title":"<code>load_lfp()</code>","text":"<p>Load the LFP signal</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def load_lfp(self):\n    \"\"\"\n    Load the LFP signal\n    \"\"\"\n\n    lfp, timestep = loading.loadLFP(\n        self.basepath,\n        n_channels=self.nChannels,\n        frequency=self.fs,\n        ext=\"lfp\",\n    )\n\n    if self.noise_ch is None:\n        channels = [self.hilus_ch, self.mol_ch]\n    else:\n        channels = [self.hilus_ch, self.mol_ch, self.noise_ch]\n\n    self.lfp = nel.AnalogSignalArray(\n        data=lfp[:, channels].T,\n        timestamps=timestep,\n        fs=self.fs,\n        support=nel.EpochArray(np.array([min(timestep), max(timestep)])),\n    )\n    if self.clean_lfp:\n        self.lfp._data = np.array(\n            [\n                clean_lfp(self.lfp.signals[0]),\n                clean_lfp(self.lfp.signals[1]),\n            ]\n        )\n</code></pre>"},{"location":"reference/neuro_py/detectors/dentate_spike/#neuro_py.detectors.dentate_spike.DetectDS.plot","title":"<code>plot(ax=None, window=[-0.15, 0.15], channel_offset=90000.0)</code>","text":"<p>Plot the average LFP trace around the dentate spikes</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>AxesSubplot</code> <p>Axis to plot the average LFP trace</p> <code>None</code> <code>window</code> <code>list</code> <p>Window around the dentate spikes</p> <code>[-0.15, 0.15]</code> <code>channel_offset</code> <code>float</code> <p>Offset between the channels</p> <code>90000.0</code> <p>Returns:</p> Type Description <code>AxesSubplot</code> <p>Axis with the average LFP trace</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def plot(self, ax=None, window=[-0.15, 0.15], channel_offset=9e4):\n    \"\"\"\n    Plot the average LFP trace around the dentate spikes\n\n    Parameters\n    ----------\n    ax : matplotlib.axes._subplots.AxesSubplot, optional\n        Axis to plot the average LFP trace\n    window : list, optional\n        Window around the dentate spikes\n    channel_offset : float, optional\n        Offset between the channels\n\n    Returns\n    -------\n    matplotlib.axes._subplots.AxesSubplot\n        Axis with the average LFP trace\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    ds_average, time_lags = self.get_average_trace(window=window)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(5, 10))\n\n    ax.plot(\n        time_lags,\n        ds_average.T - np.linspace(0, channel_offset, ds_average.shape[0]),\n        alpha=0.75,\n    )\n    return ax\n</code></pre>"},{"location":"reference/neuro_py/detectors/dentate_spike/#neuro_py.detectors.dentate_spike.DetectDS.save","title":"<code>save(filename)</code>","text":"<p>Save the DetectDS object as a pickle file</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file where the DetectDS object will be saved</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def save(self, filename: str):\n    \"\"\"\n    Save the DetectDS object as a pickle file\n\n    Parameters\n    ----------\n    filename : str\n        Path to the file where the DetectDS object will be saved\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    self._detach()\n    with open(filename, \"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"reference/neuro_py/detectors/dentate_spike/#neuro_py.detectors.dentate_spike.DetectDS.save_ds_epoch","title":"<code>save_ds_epoch()</code>","text":"<p>Save the dentate spikes as a cellexplorer mat file</p> Source code in <code>neuro_py/detectors/dentate_spike.py</code> <pre><code>def save_ds_epoch(self):\n    \"\"\"\n    Save the dentate spikes as a cellexplorer mat file\n    \"\"\"\n\n    filename = os.path.join(\n        self.basepath, os.path.basename(self.basepath) + \".DS2.events.mat\"\n    )\n    data = {}\n    data[\"DS2\"] = {}\n    data[\"DS2\"][\"detectorinfo\"] = {}\n    data[\"DS2\"][\"timestamps\"] = self.ds_epoch.data\n    data[\"DS2\"][\"peaks\"] = self.peaks\n    data[\"DS2\"][\"amplitudes\"] = self.peak_val.T\n    data[\"DS2\"][\"amplitudeUnits\"] = \"mV\"\n    data[\"DS2\"][\"eventID\"] = []\n    data[\"DS2\"][\"eventIDlabels\"] = []\n    data[\"DS2\"][\"eventIDbinary\"] = []\n    data[\"DS2\"][\"duration\"] = self.ds_epoch.durations.T\n    data[\"DS2\"][\"center\"] = np.median(self.ds_epoch.data, axis=1).T\n    data[\"DS2\"][\"detectorinfo\"][\"detectorname\"] = \"DetectDS\"\n    data[\"DS2\"][\"detectorinfo\"][\"detectionparms\"] = []\n    data[\"DS2\"][\"detectorinfo\"][\"detectionintervals\"] = []\n    data[\"DS2\"][\"detectorinfo\"][\"ml_channel\"] = self.mol_ch\n    data[\"DS2\"][\"detectorinfo\"][\"h_channel\"] = self.hilus_ch\n    if self.noise_ch is not None:\n        data[\"DS2\"][\"detectorinfo\"][\"noise_channel\"] = self.noise_ch\n\n    savemat(filename, data, long_field_names=True)\n</code></pre>"},{"location":"reference/neuro_py/detectors/up_down_state/","title":"neuro_py.detectors.up_down_state","text":""},{"location":"reference/neuro_py/detectors/up_down_state/#neuro_py.detectors.up_down_state.detect_up_down_states","title":"<code>detect_up_down_states(basepath=None, st=None, nrem_epochs=None, region='ILA|PFC|PL|EC1|EC2|EC3|EC4|EC5|MEC|CTX', min_dur=0.03, max_dur=0.5, percentile=20, bin_size=0.01, smooth_sigma=0.02, min_cells=10, save_mat=True)</code>","text":"<p>Detect UP and DOWN states in neural data.</p> <p>UP and DOWN states are identified by computing the total firing rate of all simultaneously recorded neurons in bins of 10 ms, smoothed with a Gaussian kernel of 20 ms s.d. Epochs with a firing rate below the specified percentile threshold  are considered DOWN states, while the intervals between DOWN states are classified as UP states. Epochs shorter than <code>min_dur</code> or longer than <code>max_dur</code> are discarded.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Base directory path where event files and neural data are stored.</p> <code>None</code> <code>st</code> <code>Optional[SpikeTrain]</code> <p>Spike train data. If None, spike data will be loaded based on specified regions.</p> <code>None</code> <code>nrem_epochs</code> <code>Optional[EpochArray]</code> <p>NREM epochs. If None, epochs will be loaded from the basepath.</p> <code>None</code> <code>region</code> <code>str</code> <p>Brain regions for loading spikes. The first region is prioritized.</p> <code>\"ILA|PFC|PL|EC1|EC2|EC3|EC4|EC5|MEC\"</code> <code>min_dur</code> <code>float</code> <p>Minimum duration for DOWN states, in seconds.</p> <code>0.03</code> <code>max_dur</code> <code>float</code> <p>Maximum duration for DOWN states, in seconds.</p> <code>0.5</code> <code>percentile</code> <code>float</code> <p>Percentile threshold for determining DOWN states based on firing rate.</p> <code>20</code> <code>bin_size</code> <code>float</code> <p>Bin size for computing firing rates, in seconds.</p> <code>0.01</code> <code>smooth_sigma</code> <code>float</code> <p>Standard deviation for Gaussian kernel smoothing, in seconds.</p> <code>0.02</code> <code>min_cells</code> <code>int</code> <p>Minimum number of neurons required for analysis.</p> <code>10</code> <code>save_mat</code> <code>bool</code> <p>Whether to save the detected UP and DOWN states to .mat files.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[Optional[EpochArray], Optional[EpochArray]]</code> <p>A tuple containing the detected DOWN state epochs and UP state epochs. Returns (None, None) if no suitable states are found or insufficient data is available.</p> Notes <p>Detection method based on https://doi.org/10.1038/s41467-020-15842-4</p> Source code in <code>neuro_py/detectors/up_down_state.py</code> <pre><code>def detect_up_down_states(\n    basepath: Optional[str] = None,\n    st: Optional[nel.SpikeTrainArray] = None,\n    nrem_epochs: Optional[nel.EpochArray] = None,\n    region: str = \"ILA|PFC|PL|EC1|EC2|EC3|EC4|EC5|MEC|CTX\",\n    min_dur: float = 0.03,\n    max_dur: float = 0.5,\n    percentile: float = 20,\n    bin_size: float = 0.01,\n    smooth_sigma: float = 0.02,\n    min_cells: int = 10,\n    save_mat: bool = True,\n) -&gt; Tuple[Optional[nel.EpochArray], Optional[nel.EpochArray]]:\n    \"\"\"\n    Detect UP and DOWN states in neural data.\n\n    UP and DOWN states are identified by computing the total firing rate of all\n    simultaneously recorded neurons in bins of 10 ms, smoothed with a Gaussian kernel\n    of 20 ms s.d. Epochs with a firing rate below the specified percentile threshold \n    are considered DOWN states, while the intervals between DOWN states are classified\n    as UP states. Epochs shorter than `min_dur` or longer than `max_dur` are discarded.\n\n    Parameters\n    ----------\n    basepath : str\n        Base directory path where event files and neural data are stored.\n    st : Optional[nel.SpikeTrain], default=None\n        Spike train data. If None, spike data will be loaded based on specified regions.\n    nrem_epochs : Optional[nel.EpochArray], default=None\n        NREM epochs. If None, epochs will be loaded from the basepath.\n    region : str, default=\"ILA|PFC|PL|EC1|EC2|EC3|EC4|EC5|MEC\"\n        Brain regions for loading spikes. The first region is prioritized.\n    min_dur : float, default=0.03\n        Minimum duration for DOWN states, in seconds.\n    max_dur : float, default=0.5\n        Maximum duration for DOWN states, in seconds.\n    percentile : float, default=20\n        Percentile threshold for determining DOWN states based on firing rate.\n    bin_size : float, default=0.01\n        Bin size for computing firing rates, in seconds.\n    smooth_sigma : float, default=0.02\n        Standard deviation for Gaussian kernel smoothing, in seconds.\n    min_cells : int, default=10\n        Minimum number of neurons required for analysis.\n    save_mat : bool, default=True\n        Whether to save the detected UP and DOWN states to .mat files.\n\n    Returns\n    -------\n    Tuple[Optional[nel.EpochArray], Optional[nel.EpochArray]]\n        A tuple containing the detected DOWN state epochs and UP state epochs.\n        Returns (None, None) if no suitable states are found or insufficient data is available.\n\n    Notes\n    -----\n    Detection method based on https://doi.org/10.1038/s41467-020-15842-4\n    \"\"\"\n\n    # check for existance of event files\n    if save_mat:\n        filename_downstate = os.path.join(\n            basepath, os.path.basename(basepath) + \".\" + \"down_state\" + \".events.mat\"\n        )\n        filename_upstate = os.path.join(\n            basepath, os.path.basename(basepath) + \".\" + \"up_state\" + \".events.mat\"\n        )\n        if os.path.exists(filename_downstate) &amp; os.path.exists(filename_upstate):\n            down_state = loading.load_events(basepath=basepath, epoch_name=\"down_state\")\n            up_state = loading.load_events(basepath=basepath, epoch_name=\"up_state\")\n            return down_state, up_state\n\n    # load brain states\n    if nrem_epochs is None:\n        state_dict = loading.load_SleepState_states(basepath)\n        nrem_epochs = nel.EpochArray(state_dict[\"NREMstate\"])\n\n    if nrem_epochs.isempty:\n        print(f\"No NREM epochs found for {basepath}\")\n        return None, None\n\n    # load spikes\n    if st is None:\n        st, _ = loading.load_spikes(basepath, brainRegion=region)\n\n    # check if there are enough cells\n    if st is None or st.isempty or st.data.shape[0] &lt; min_cells:\n        print(f\"No spikes found for {basepath} {region}\")\n\n        # load spikes\n        st, _ = loading.load_spikes(basepath, brainRegion=region[1])\n        # check if there are enough cells\n        if st is None or st.isempty or st.data.shape[0] &lt; min_cells:\n            print(f\"No spikes found for {basepath} {region}\")\n            return None, None\n\n    # flatten spikes\n    st = st[nrem_epochs].flatten()\n\n    # bin and smooth\n    bst = st.bin(ds=bin_size).smooth(sigma=smooth_sigma)\n\n    # find down states, based on percentile\n    down_state_epochs = bst.bin_centers[\n        find_interval(bst.data.flatten() &lt; np.percentile(bst.data.T, percentile))\n    ]\n    if down_state_epochs.shape[0] == 0:\n        print(f\"No down states found for {basepath}\")\n        return None, None\n\n    # remove short and long epochs\n    durations = down_state_epochs[:, 1] - down_state_epochs[:, 0]\n    down_state_epochs = down_state_epochs[\n        ~((durations &lt; min_dur) | (durations &gt; max_dur)), :\n    ]\n    # convert to epoch array with same domain as nrem epochs (this is so compliment will also be in nrem epochs)\n    down_state_epochs = nel.EpochArray(data=down_state_epochs, domain=nrem_epochs)\n\n    # compliment to get up states\n    up_state_epochs = ~down_state_epochs\n\n    # save to cell explorer mat file\n    if save_mat:\n        epoch_to_mat(down_state_epochs, basepath, \"down_state\", \"detect_up_down_states\")\n        epoch_to_mat(up_state_epochs, basepath, \"up_state\", \"detect_up_down_states\")\n\n    return down_state_epochs, up_state_epochs\n</code></pre>"},{"location":"reference/neuro_py/ensemble/","title":"neuro_py.ensemble","text":""},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact","title":"<code>AssemblyReact</code>","text":"<p>Class for running assembly reactivation analysis</p> <p>Core assembly methods come from assembly.py by V\u00edtor Lopes dos Santos     https://doi.org/10.1016/j.jneumeth.2013.04.010</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder</p> <code>None</code> <code>brainRegion</code> <code>str</code> <p>Brain region to restrict to. Can be multi ex. \"CA1|CA2\"</p> <code>'CA1'</code> <code>putativeCellType</code> <code>str</code> <p>Cell type to restrict to</p> <code>'Pyramidal Cell'</code> <code>weight_dt</code> <code>float</code> <p>Time resolution of the weight matrix</p> <code>0.025</code> <code>z_mat_dt</code> <code>float</code> <p>Time resolution of the z matrix</p> <code>0.002</code> <code>method</code> <code>str</code> <p>Defines how to extract assembly patterns (ica,pca).</p> <code>'ica'</code> <code>nullhyp</code> <code>str</code> <p>Defines how to generate statistical threshold for assembly detection (bin,circ,mp).</p> <code>'mp'</code> <code>nshu</code> <code>int</code> <p>Number of shuffles for bin and circ null hypothesis.</p> <code>1000</code> <code>percentile</code> <code>int</code> <p>Percentile for mp null hypothesis.</p> <code>99</code> <code>tracywidom</code> <code>bool</code> <p>If true, uses Tracy-Widom distribution for mp null hypothesis.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>st</code> <code>SpikeTrainArray</code> <p>Spike train</p> <code>cell_metrics</code> <code>DataFrame</code> <p>Cell metrics</p> <code>ripples</code> <code>EpochArray</code> <p>Ripples</p> <code>patterns</code> <code>ndarray</code> <p>Assembly patterns</p> <code>assembly_act</code> <code>AnalogSignalArray</code> <p>Assembly activity</p> <p>Methods:</p> Name Description <code>load_data</code> <p>Load data (st, ripples, epochs)</p> <code>restrict_to_epoch</code> <p>Restrict to a specific epoch</p> <code>get_z_mat</code> <p>Get z matrix</p> <code>get_weights</code> <p>Get assembly weights</p> <code>get_assembly_act</code> <p>Get assembly activity</p> <code>n_assemblies</code> <p>Number of detected assemblies</p> <code>isempty</code> <p>Check if empty</p> <code>copy</code> <p>Returns copy of class</p> <code>plot</code> <p>Stem plot of assembly weights</p> <code>find_members</code> <p>Find members of an assembly</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # create the object assembly_react\n&gt;&gt;&gt; assembly_react = assembly_reactivation.AssemblyReact(\n...    basepath=basepath,\n...    )\n</code></pre> <pre><code>&gt;&gt;&gt; # load need data (spikes, ripples, epochs)\n&gt;&gt;&gt; assembly_react.load_data()\n</code></pre> <pre><code>&gt;&gt;&gt; # detect assemblies\n&gt;&gt;&gt; assembly_react.get_weights()\n</code></pre> <pre><code>&gt;&gt;&gt; # visually inspect weights for each assembly\n&gt;&gt;&gt; assembly_react.plot()\n</code></pre> <pre><code>&gt;&gt;&gt; # compute time resolved signal for each assembly\n&gt;&gt;&gt; assembly_act = assembly_react.get_assembly_act()\n</code></pre> <pre><code>&gt;&gt;&gt; # locate members of assemblies\n&gt;&gt;&gt; assembly_members = assembly_react.find_members()\n</code></pre> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>class AssemblyReact:\n    \"\"\"\n    Class for running assembly reactivation analysis\n\n    Core assembly methods come from assembly.py by V\u00edtor Lopes dos Santos\n        https://doi.org/10.1016/j.jneumeth.2013.04.010\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder\n    brainRegion : str\n        Brain region to restrict to. Can be multi ex. \"CA1|CA2\"\n    putativeCellType : str\n        Cell type to restrict to\n    weight_dt : float\n        Time resolution of the weight matrix\n    z_mat_dt : float\n        Time resolution of the z matrix\n    method : str\n        Defines how to extract assembly patterns (ica,pca).\n    nullhyp : str\n        Defines how to generate statistical threshold for assembly detection (bin,circ,mp).\n    nshu : int\n        Number of shuffles for bin and circ null hypothesis.\n    percentile : int\n        Percentile for mp null hypothesis.\n    tracywidom : bool\n        If true, uses Tracy-Widom distribution for mp null hypothesis.\n\n    Attributes\n    ----------\n    st : nelpy.SpikeTrainArray\n        Spike train\n    cell_metrics : pd.DataFrame\n        Cell metrics\n    ripples : nelpy.EpochArray\n        Ripples\n    patterns : np.ndarray\n        Assembly patterns\n    assembly_act : nelpy.AnalogSignalArray\n        Assembly activity\n\n    Methods\n    -------\n    load_data()\n        Load data (st, ripples, epochs)\n    restrict_to_epoch(epoch)\n        Restrict to a specific epoch\n    get_z_mat(st)\n        Get z matrix\n    get_weights(epoch=None)\n        Get assembly weights\n    get_assembly_act(epoch=None)\n        Get assembly activity\n    n_assemblies()\n        Number of detected assemblies\n    isempty()\n        Check if empty\n    copy()\n        Returns copy of class\n    plot()\n        Stem plot of assembly weights\n    find_members()\n        Find members of an assembly\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; # create the object assembly_react\n    &gt;&gt;&gt; assembly_react = assembly_reactivation.AssemblyReact(\n    ...    basepath=basepath,\n    ...    )\n\n    &gt;&gt;&gt; # load need data (spikes, ripples, epochs)\n    &gt;&gt;&gt; assembly_react.load_data()\n\n    &gt;&gt;&gt; # detect assemblies\n    &gt;&gt;&gt; assembly_react.get_weights()\n\n    &gt;&gt;&gt; # visually inspect weights for each assembly\n    &gt;&gt;&gt; assembly_react.plot()\n\n    &gt;&gt;&gt; # compute time resolved signal for each assembly\n    &gt;&gt;&gt; assembly_act = assembly_react.get_assembly_act()\n\n    &gt;&gt;&gt; # locate members of assemblies\n    &gt;&gt;&gt; assembly_members = assembly_react.find_members()\n\n    \"\"\"\n\n    def __init__(\n        self,\n        basepath: Union[str, None] = None,\n        brainRegion: str = \"CA1\",\n        putativeCellType: str = \"Pyramidal Cell\",\n        weight_dt: float = 0.025,\n        z_mat_dt: float = 0.002,\n        method: str = \"ica\",\n        nullhyp: str = \"mp\",\n        nshu: int = 1000,\n        percentile: int = 99,\n        tracywidom: bool = False,\n        whiten: str = \"unit-variance\",\n    ):\n        self.basepath = basepath\n        self.brainRegion = brainRegion\n        self.putativeCellType = putativeCellType\n        self.weight_dt = weight_dt\n        self.z_mat_dt = z_mat_dt\n        self.method = method\n        self.nullhyp = nullhyp\n        self.nshu = nshu\n        self.percentile = percentile\n        self.tracywidom = tracywidom\n        self.whiten = whiten\n        self.type_name = self.__class__.__name__\n\n    def add_st(self, st: nel.SpikeTrainArray) -&gt; None:\n        self.st = st\n\n    def add_ripples(self, ripples: nel.EpochArray) -&gt; None:\n        self.ripples = ripples\n\n    def add_epoch_df(self, epoch_df: pd.DataFrame) -&gt; None:\n        self.epoch_df = epoch_df\n\n    def load_spikes(self) -&gt; None:\n        \"\"\"\n        loads spikes from the session folder\n        \"\"\"\n        self.st, self.cell_metrics = loading.load_spikes(\n            self.basepath,\n            brainRegion=self.brainRegion,\n            putativeCellType=self.putativeCellType,\n            support=self.time_support,\n        )\n\n    def load_ripples(self) -&gt; None:\n        \"\"\"\n        loads ripples from the session folder\n        \"\"\"\n        ripples = loading.load_ripples_events(self.basepath)\n        self.ripples = nel.EpochArray(\n            [np.array([ripples.start, ripples.stop]).T], domain=self.time_support\n        )\n\n    def load_epoch(self) -&gt; None:\n        \"\"\"\n        loads epochs from the session folder\n        \"\"\"\n        epoch_df = loading.load_epoch(self.basepath)\n        epoch_df = compress_repeated_epochs(epoch_df)\n        self.time_support = nel.EpochArray(\n            [epoch_df.iloc[0].startTime, epoch_df.iloc[-1].stopTime]\n        )\n        self.epochs = nel.EpochArray(\n            [np.array([epoch_df.startTime, epoch_df.stopTime]).T],\n            domain=self.time_support,\n        )\n        self.epoch_df = epoch_df\n\n    def load_data(self) -&gt; None:\n        \"\"\"\n        loads data (spikes,ripples,epochs) from the session folder\n        \"\"\"\n        self.load_epoch()\n        self.load_spikes()\n        self.load_ripples()\n\n    def restrict_epochs_to_pre_task_post(self) -&gt; None:\n        \"\"\"\n        Restricts the epochs to the specified epochs\n        \"\"\"\n        # fetch data\n        epoch_df = loading.load_epoch(self.basepath)\n        # compress back to back sleep epochs (an issue further up the pipeline)\n        epoch_df = compress_repeated_epochs(epoch_df)\n        # restrict to pre task post epochs\n        idx = find_pre_task_post(epoch_df.environment)\n        self.epoch_df = epoch_df[idx[0]]\n        # convert to epoch array and add to object\n        self.epochs = nel.EpochArray(\n            [np.array([self.epoch_df.startTime, self.epoch_df.stopTime]).T],\n            label=\"session_epochs\",\n            domain=self.time_support,\n        )\n\n    def restrict_to_epoch(self, epoch) -&gt; None:\n        \"\"\"\n        Restricts the spike data to a specific epoch.\n\n        Parameters\n        ----------\n        epoch : nel.EpochArray\n            The epoch to restrict to.\n        \"\"\"\n        self.st_resticted = self.st[epoch]\n\n    def get_z_mat(self, st: nel.SpikeTrainArray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Get z matrix.\n\n        Parameters\n        ----------\n        st : nel.SpikeTrainArray\n            Spike train array.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray]\n            Z-scored binned spike train and bin centers.\n        \"\"\"\n        # binning the spike train\n        z_t = st.bin(ds=self.z_mat_dt)\n        # gaussian kernel to match the bin-size used to identify the assembly patterns\n        sigma = self.weight_dt / np.sqrt(int(1000 * self.weight_dt / 2))\n        z_t.smooth(sigma=sigma, inplace=True)\n        # zscore the z matrix\n        z_scored_bst = stats.zscore(z_t.data, axis=1)\n        # make sure there are no nans, important as strengths will all be nan otherwise\n        z_scored_bst[np.isnan(z_scored_bst).any(axis=1)] = 0\n\n        return z_scored_bst, z_t.bin_centers\n\n    def get_weights(self, epoch: Optional[nel.EpochArray] = None) -&gt; None:\n        \"\"\"\n        Gets the assembly weights.\n\n        Parameters\n        ----------\n        epoch : nel.EpochArray, optional\n            The epoch to restrict to, by default None.\n        \"\"\"\n\n        # check if st has any neurons\n        if self.st.isempty:\n            self.patterns = None\n            return\n\n        if epoch is not None:\n            bst = self.st[epoch].bin(ds=self.weight_dt).data\n        else:\n            bst = self.st.bin(ds=self.weight_dt).data\n\n        if (bst == 0).all():\n            self.patterns = None\n            return\n        else:\n            patterns, _, _ = assembly.runPatterns(\n                bst,\n                method=self.method,\n                nullhyp=self.nullhyp,\n                nshu=self.nshu,\n                percentile=self.percentile,\n                tracywidom=self.tracywidom,\n                whiten=self.whiten,\n            )\n\n            if patterns is None: \n                self.patterns = None\n                return \n\n            # flip patterns to have positive max\n            self.patterns = np.array(\n                [\n                    (\n                        patterns[i, :]\n                        if patterns[i, np.argmax(np.abs(patterns[i, :]))] &gt; 0\n                        else -patterns[i, :]\n                    )\n                    for i in range(patterns.shape[0])\n                ]\n            )\n\n    def get_assembly_act(\n        self, epoch: Optional[nel.EpochArray] = None\n    ) -&gt; nel.AnalogSignalArray:\n        \"\"\"\n        Get assembly activity.\n\n        Parameters\n        ----------\n        epoch : nel.EpochArray, optional\n            The epoch to restrict to, by default None.\n\n        Returns\n        -------\n        nel.AnalogSignalArray\n            Assembly activity.\n        \"\"\"\n        # check for num of assemblies first\n        if self.n_assemblies() == 0:\n            return nel.AnalogSignalArray(empty=True)\n\n        if epoch is not None:\n            zactmat, ts = self.get_z_mat(self.st[epoch])\n        else:\n            zactmat, ts = self.get_z_mat(self.st)\n\n        assembly_act = nel.AnalogSignalArray(\n            data=assembly.computeAssemblyActivity(self.patterns, zactmat),\n            timestamps=ts,\n            fs=1 / self.z_mat_dt,\n        )\n        return assembly_act\n\n    def plot(\n        self,\n        plot_members: bool = True,\n        central_line_color: str = \"grey\",\n        marker_color: str = \"k\",\n        member_color: Union[str, list] = \"#6768ab\",\n        line_width: float = 1.25,\n        markersize: float = 4,\n        x_padding: float = 0.2,\n        figsize: Union[tuple, None] = None,\n    ) -&gt; Union[Tuple[plt.Figure, np.ndarray], str, None]:\n        \"\"\"\n        Plots basic stem plot to display assembly weights.\n\n        Parameters\n        ----------\n        plot_members : bool, optional\n            Whether to plot assembly members, by default True.\n        central_line_color : str, optional\n            Color of the central line, by default \"grey\".\n        marker_color : str, optional\n            Color of the markers, by default \"k\".\n        member_color : Union[str, List[str]], optional\n            Color of the members, by default \"#6768ab\".\n        line_width : float, optional\n            Width of the lines, by default 1.25.\n        markersize : float, optional\n            Size of the markers, by default 4.\n        x_padding : float, optional\n            Padding on the x-axis, by default 0.2.\n        figsize : Optional[Tuple[float, float]], optional\n            Size of the figure, by default None.\n\n        Returns\n        -------\n        Union[Tuple[plt.Figure, np.ndarray], str, None]\n            The figure and axes if successful, otherwise a message or None.\n        \"\"\"\n        if not hasattr(self, \"patterns\"):\n            return \"run get_weights first\"\n        else:\n            if self.patterns is None:\n                return None, None\n            if plot_members:\n                self.find_members()\n            if figsize is None:\n                figsize = (self.n_assemblies() + 1, np.round(self.n_assemblies() / 2))\n            # set up figure with size relative to assembly matrix\n            fig, axes = plt.subplots(\n                1,\n                self.n_assemblies(),\n                figsize=figsize,\n                sharey=True,\n                sharex=True,\n            )\n            # iter over each assembly and plot the weight per cell\n            for i in range(self.n_assemblies()):\n                markerline, stemlines, baseline = axes[i].stem(\n                    self.patterns[i, :], orientation=\"horizontal\"\n                )\n                markerline._color = marker_color\n                baseline._color = central_line_color\n                baseline.zorder = -1000\n                plt.setp(stemlines, \"color\", plt.getp(markerline, \"color\"))\n                plt.setp(stemlines, linewidth=line_width)\n                plt.setp(markerline, markersize=markersize)\n\n                if plot_members:\n                    current_pattern = self.patterns[i, :].copy()\n                    current_pattern[~self.assembly_members[i, :]] = np.nan\n                    markerline, stemlines, baseline = axes[i].stem(\n                        current_pattern, orientation=\"horizontal\"\n                    )\n                    if isinstance(\n                        member_color, sns.palettes._ColorPalette\n                    ) or isinstance(member_color, list):\n                        markerline._color = member_color[i]\n                    else:\n                        markerline._color = member_color\n                    baseline._color = \"#00000000\"\n                    baseline.zorder = -1000\n                    plt.setp(stemlines, \"color\", plt.getp(markerline, \"color\"))\n                    plt.setp(stemlines, linewidth=line_width)\n                    plt.setp(markerline, markersize=markersize)\n\n                axes[i].spines[\"top\"].set_visible(False)\n                axes[i].spines[\"right\"].set_visible(False)\n\n            # give room for marker\n            axes[0].set_xlim(\n                -self.patterns.max() - x_padding, self.patterns.max() + x_padding\n            )\n\n            axes[0].set_ylabel(\"Neurons #\")\n            axes[0].set_xlabel(\"Weights (a.u.)\")\n\n            return fig, axes\n\n    def n_assemblies(self) -&gt; int:\n        \"\"\"\n        Get the number of detected assemblies.\n\n        Returns\n        -------\n        int\n            Number of detected assemblies.\n        \"\"\"\n        if hasattr(self, \"patterns\"):\n            if self.patterns is None:\n                return 0\n            return self.patterns.shape[0]\n\n    @property\n    def isempty(self) -&gt; bool:\n        \"\"\"\n        Check if the object is empty.\n\n        Returns\n        -------\n        bool\n            True if empty, False otherwise.\n        \"\"\"\n        if hasattr(self, \"st\"):\n            return False\n        elif not hasattr(self, \"st\"):\n            return True\n\n    def copy(self) -&gt; \"AssemblyReact\":\n        \"\"\"\n        Returns a copy of the current class.\n\n        Returns\n        -------\n        AssemblyReact\n            A copy of the current class.\n        \"\"\"\n        newcopy = copy.deepcopy(self)\n        return newcopy\n\n    def __repr__(self) -&gt; str:\n        if self.isempty:\n            return f\"&lt;{self.type_name}: empty&gt;\"\n\n        # if st data as been loaded and patterns have been computed\n        if hasattr(self, \"patterns\"):\n            n_units = f\"{self.st.n_active} units\"\n            n_patterns = f\"{self.n_assemblies()} assemblies\"\n            dstr = f\"of length {self.st.support.length}\"\n            return \"&lt;%s: %s, %s&gt; %s\" % (self.type_name, n_units, n_patterns, dstr)\n\n        # if st data as been loaded\n        if hasattr(self, \"st\"):\n            n_units = f\"{self.st.n_active} units\"\n            dstr = f\"of length {self.st.support.length}\"\n            return \"&lt;%s: %s&gt; %s\" % (self.type_name, n_units, dstr)\n\n    def find_members(self) -&gt; np.ndarray:\n        \"\"\"\n        Finds significant assembly patterns and significant assembly members.\n\n        Returns\n        -------\n        np.ndarray\n            A ndarray of booleans indicating whether each unit is a significant member of an assembly.\n\n        Notes\n        -----\n        also, sets self.assembly_members and self.valid_assembly\n\n        self.valid_assembly: a ndarray of booleans indicating an assembly has members with the same sign (Boucly et al. 2022)\n        \"\"\"\n\n        def Otsu(vector: np.ndarray) -&gt; Tuple[np.ndarray, float, float]:\n            \"\"\"\n            The Otsu method for splitting data into two groups.\n\n            Parameters\n            ----------\n            vector : np.ndarray\n                Arbitrary vector.\n\n            Returns\n            -------\n            Tuple[np.ndarray, float, float]\n                Group, threshold used for classification, and effectiveness metric.\n            \"\"\"\n            sorted = np.sort(vector)\n            n = len(vector)\n            intraClassVariance = [np.nan] * n\n            for i in np.arange(n):\n                p = (i + 1) / n\n                p0 = 1 - p\n                if i + 1 == n:\n                    intraClassVariance[i] = np.nan\n                else:\n                    intraClassVariance[i] = p * np.var(sorted[0 : i + 1]) + p0 * np.var(\n                        sorted[i + 1 :]\n                    )\n\n            minIntraVariance = np.nanmin(intraClassVariance)\n            idx = np.nanargmin(intraClassVariance)\n            threshold = sorted[idx]\n            group = vector &gt; threshold\n\n            em = 1 - (minIntraVariance / np.var(vector))\n\n            return group, threshold, em\n\n        is_member = []\n        keep_assembly = []\n        for pat in self.patterns:\n            isMember, _, _ = Otsu(np.abs(pat))\n            is_member.append(isMember)\n\n            if np.any(pat[isMember] &lt; 0) &amp; np.any(pat[isMember] &gt; 0):\n                keep_assembly.append(False)\n            elif sum(isMember) == 0:\n                keep_assembly.append(False)\n            else:\n                keep_assembly.append(True)\n\n        self.assembly_members = np.array(is_member)\n        self.valid_assembly = np.array(keep_assembly)\n\n        return self.assembly_members\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>Check if the object is empty.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if empty, False otherwise.</p>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the current class.</p> <p>Returns:</p> Type Description <code>AssemblyReact</code> <p>A copy of the current class.</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def copy(self) -&gt; \"AssemblyReact\":\n    \"\"\"\n    Returns a copy of the current class.\n\n    Returns\n    -------\n    AssemblyReact\n        A copy of the current class.\n    \"\"\"\n    newcopy = copy.deepcopy(self)\n    return newcopy\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.find_members","title":"<code>find_members()</code>","text":"<p>Finds significant assembly patterns and significant assembly members.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A ndarray of booleans indicating whether each unit is a significant member of an assembly.</p> Notes <p>also, sets self.assembly_members and self.valid_assembly</p> <p>self.valid_assembly: a ndarray of booleans indicating an assembly has members with the same sign (Boucly et al. 2022)</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def find_members(self) -&gt; np.ndarray:\n    \"\"\"\n    Finds significant assembly patterns and significant assembly members.\n\n    Returns\n    -------\n    np.ndarray\n        A ndarray of booleans indicating whether each unit is a significant member of an assembly.\n\n    Notes\n    -----\n    also, sets self.assembly_members and self.valid_assembly\n\n    self.valid_assembly: a ndarray of booleans indicating an assembly has members with the same sign (Boucly et al. 2022)\n    \"\"\"\n\n    def Otsu(vector: np.ndarray) -&gt; Tuple[np.ndarray, float, float]:\n        \"\"\"\n        The Otsu method for splitting data into two groups.\n\n        Parameters\n        ----------\n        vector : np.ndarray\n            Arbitrary vector.\n\n        Returns\n        -------\n        Tuple[np.ndarray, float, float]\n            Group, threshold used for classification, and effectiveness metric.\n        \"\"\"\n        sorted = np.sort(vector)\n        n = len(vector)\n        intraClassVariance = [np.nan] * n\n        for i in np.arange(n):\n            p = (i + 1) / n\n            p0 = 1 - p\n            if i + 1 == n:\n                intraClassVariance[i] = np.nan\n            else:\n                intraClassVariance[i] = p * np.var(sorted[0 : i + 1]) + p0 * np.var(\n                    sorted[i + 1 :]\n                )\n\n        minIntraVariance = np.nanmin(intraClassVariance)\n        idx = np.nanargmin(intraClassVariance)\n        threshold = sorted[idx]\n        group = vector &gt; threshold\n\n        em = 1 - (minIntraVariance / np.var(vector))\n\n        return group, threshold, em\n\n    is_member = []\n    keep_assembly = []\n    for pat in self.patterns:\n        isMember, _, _ = Otsu(np.abs(pat))\n        is_member.append(isMember)\n\n        if np.any(pat[isMember] &lt; 0) &amp; np.any(pat[isMember] &gt; 0):\n            keep_assembly.append(False)\n        elif sum(isMember) == 0:\n            keep_assembly.append(False)\n        else:\n            keep_assembly.append(True)\n\n    self.assembly_members = np.array(is_member)\n    self.valid_assembly = np.array(keep_assembly)\n\n    return self.assembly_members\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.get_assembly_act","title":"<code>get_assembly_act(epoch=None)</code>","text":"<p>Get assembly activity.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The epoch to restrict to, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>AnalogSignalArray</code> <p>Assembly activity.</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def get_assembly_act(\n    self, epoch: Optional[nel.EpochArray] = None\n) -&gt; nel.AnalogSignalArray:\n    \"\"\"\n    Get assembly activity.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray, optional\n        The epoch to restrict to, by default None.\n\n    Returns\n    -------\n    nel.AnalogSignalArray\n        Assembly activity.\n    \"\"\"\n    # check for num of assemblies first\n    if self.n_assemblies() == 0:\n        return nel.AnalogSignalArray(empty=True)\n\n    if epoch is not None:\n        zactmat, ts = self.get_z_mat(self.st[epoch])\n    else:\n        zactmat, ts = self.get_z_mat(self.st)\n\n    assembly_act = nel.AnalogSignalArray(\n        data=assembly.computeAssemblyActivity(self.patterns, zactmat),\n        timestamps=ts,\n        fs=1 / self.z_mat_dt,\n    )\n    return assembly_act\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.get_weights","title":"<code>get_weights(epoch=None)</code>","text":"<p>Gets the assembly weights.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The epoch to restrict to, by default None.</p> <code>None</code> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def get_weights(self, epoch: Optional[nel.EpochArray] = None) -&gt; None:\n    \"\"\"\n    Gets the assembly weights.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray, optional\n        The epoch to restrict to, by default None.\n    \"\"\"\n\n    # check if st has any neurons\n    if self.st.isempty:\n        self.patterns = None\n        return\n\n    if epoch is not None:\n        bst = self.st[epoch].bin(ds=self.weight_dt).data\n    else:\n        bst = self.st.bin(ds=self.weight_dt).data\n\n    if (bst == 0).all():\n        self.patterns = None\n        return\n    else:\n        patterns, _, _ = assembly.runPatterns(\n            bst,\n            method=self.method,\n            nullhyp=self.nullhyp,\n            nshu=self.nshu,\n            percentile=self.percentile,\n            tracywidom=self.tracywidom,\n            whiten=self.whiten,\n        )\n\n        if patterns is None: \n            self.patterns = None\n            return \n\n        # flip patterns to have positive max\n        self.patterns = np.array(\n            [\n                (\n                    patterns[i, :]\n                    if patterns[i, np.argmax(np.abs(patterns[i, :]))] &gt; 0\n                    else -patterns[i, :]\n                )\n                for i in range(patterns.shape[0])\n            ]\n        )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.get_z_mat","title":"<code>get_z_mat(st)</code>","text":"<p>Get z matrix.</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>SpikeTrainArray</code> <p>Spike train array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Z-scored binned spike train and bin centers.</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def get_z_mat(self, st: nel.SpikeTrainArray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Get z matrix.\n\n    Parameters\n    ----------\n    st : nel.SpikeTrainArray\n        Spike train array.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        Z-scored binned spike train and bin centers.\n    \"\"\"\n    # binning the spike train\n    z_t = st.bin(ds=self.z_mat_dt)\n    # gaussian kernel to match the bin-size used to identify the assembly patterns\n    sigma = self.weight_dt / np.sqrt(int(1000 * self.weight_dt / 2))\n    z_t.smooth(sigma=sigma, inplace=True)\n    # zscore the z matrix\n    z_scored_bst = stats.zscore(z_t.data, axis=1)\n    # make sure there are no nans, important as strengths will all be nan otherwise\n    z_scored_bst[np.isnan(z_scored_bst).any(axis=1)] = 0\n\n    return z_scored_bst, z_t.bin_centers\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.load_data","title":"<code>load_data()</code>","text":"<p>loads data (spikes,ripples,epochs) from the session folder</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def load_data(self) -&gt; None:\n    \"\"\"\n    loads data (spikes,ripples,epochs) from the session folder\n    \"\"\"\n    self.load_epoch()\n    self.load_spikes()\n    self.load_ripples()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.load_epoch","title":"<code>load_epoch()</code>","text":"<p>loads epochs from the session folder</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def load_epoch(self) -&gt; None:\n    \"\"\"\n    loads epochs from the session folder\n    \"\"\"\n    epoch_df = loading.load_epoch(self.basepath)\n    epoch_df = compress_repeated_epochs(epoch_df)\n    self.time_support = nel.EpochArray(\n        [epoch_df.iloc[0].startTime, epoch_df.iloc[-1].stopTime]\n    )\n    self.epochs = nel.EpochArray(\n        [np.array([epoch_df.startTime, epoch_df.stopTime]).T],\n        domain=self.time_support,\n    )\n    self.epoch_df = epoch_df\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.load_ripples","title":"<code>load_ripples()</code>","text":"<p>loads ripples from the session folder</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def load_ripples(self) -&gt; None:\n    \"\"\"\n    loads ripples from the session folder\n    \"\"\"\n    ripples = loading.load_ripples_events(self.basepath)\n    self.ripples = nel.EpochArray(\n        [np.array([ripples.start, ripples.stop]).T], domain=self.time_support\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.load_spikes","title":"<code>load_spikes()</code>","text":"<p>loads spikes from the session folder</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def load_spikes(self) -&gt; None:\n    \"\"\"\n    loads spikes from the session folder\n    \"\"\"\n    self.st, self.cell_metrics = loading.load_spikes(\n        self.basepath,\n        brainRegion=self.brainRegion,\n        putativeCellType=self.putativeCellType,\n        support=self.time_support,\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.n_assemblies","title":"<code>n_assemblies()</code>","text":"<p>Get the number of detected assemblies.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of detected assemblies.</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def n_assemblies(self) -&gt; int:\n    \"\"\"\n    Get the number of detected assemblies.\n\n    Returns\n    -------\n    int\n        Number of detected assemblies.\n    \"\"\"\n    if hasattr(self, \"patterns\"):\n        if self.patterns is None:\n            return 0\n        return self.patterns.shape[0]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.plot","title":"<code>plot(plot_members=True, central_line_color='grey', marker_color='k', member_color='#6768ab', line_width=1.25, markersize=4, x_padding=0.2, figsize=None)</code>","text":"<p>Plots basic stem plot to display assembly weights.</p> <p>Parameters:</p> Name Type Description Default <code>plot_members</code> <code>bool</code> <p>Whether to plot assembly members, by default True.</p> <code>True</code> <code>central_line_color</code> <code>str</code> <p>Color of the central line, by default \"grey\".</p> <code>'grey'</code> <code>marker_color</code> <code>str</code> <p>Color of the markers, by default \"k\".</p> <code>'k'</code> <code>member_color</code> <code>Union[str, List[str]]</code> <p>Color of the members, by default \"#6768ab\".</p> <code>'#6768ab'</code> <code>line_width</code> <code>float</code> <p>Width of the lines, by default 1.25.</p> <code>1.25</code> <code>markersize</code> <code>float</code> <p>Size of the markers, by default 4.</p> <code>4</code> <code>x_padding</code> <code>float</code> <p>Padding on the x-axis, by default 0.2.</p> <code>0.2</code> <code>figsize</code> <code>Optional[Tuple[float, float]]</code> <p>Size of the figure, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Figure, ndarray], str, None]</code> <p>The figure and axes if successful, otherwise a message or None.</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def plot(\n    self,\n    plot_members: bool = True,\n    central_line_color: str = \"grey\",\n    marker_color: str = \"k\",\n    member_color: Union[str, list] = \"#6768ab\",\n    line_width: float = 1.25,\n    markersize: float = 4,\n    x_padding: float = 0.2,\n    figsize: Union[tuple, None] = None,\n) -&gt; Union[Tuple[plt.Figure, np.ndarray], str, None]:\n    \"\"\"\n    Plots basic stem plot to display assembly weights.\n\n    Parameters\n    ----------\n    plot_members : bool, optional\n        Whether to plot assembly members, by default True.\n    central_line_color : str, optional\n        Color of the central line, by default \"grey\".\n    marker_color : str, optional\n        Color of the markers, by default \"k\".\n    member_color : Union[str, List[str]], optional\n        Color of the members, by default \"#6768ab\".\n    line_width : float, optional\n        Width of the lines, by default 1.25.\n    markersize : float, optional\n        Size of the markers, by default 4.\n    x_padding : float, optional\n        Padding on the x-axis, by default 0.2.\n    figsize : Optional[Tuple[float, float]], optional\n        Size of the figure, by default None.\n\n    Returns\n    -------\n    Union[Tuple[plt.Figure, np.ndarray], str, None]\n        The figure and axes if successful, otherwise a message or None.\n    \"\"\"\n    if not hasattr(self, \"patterns\"):\n        return \"run get_weights first\"\n    else:\n        if self.patterns is None:\n            return None, None\n        if plot_members:\n            self.find_members()\n        if figsize is None:\n            figsize = (self.n_assemblies() + 1, np.round(self.n_assemblies() / 2))\n        # set up figure with size relative to assembly matrix\n        fig, axes = plt.subplots(\n            1,\n            self.n_assemblies(),\n            figsize=figsize,\n            sharey=True,\n            sharex=True,\n        )\n        # iter over each assembly and plot the weight per cell\n        for i in range(self.n_assemblies()):\n            markerline, stemlines, baseline = axes[i].stem(\n                self.patterns[i, :], orientation=\"horizontal\"\n            )\n            markerline._color = marker_color\n            baseline._color = central_line_color\n            baseline.zorder = -1000\n            plt.setp(stemlines, \"color\", plt.getp(markerline, \"color\"))\n            plt.setp(stemlines, linewidth=line_width)\n            plt.setp(markerline, markersize=markersize)\n\n            if plot_members:\n                current_pattern = self.patterns[i, :].copy()\n                current_pattern[~self.assembly_members[i, :]] = np.nan\n                markerline, stemlines, baseline = axes[i].stem(\n                    current_pattern, orientation=\"horizontal\"\n                )\n                if isinstance(\n                    member_color, sns.palettes._ColorPalette\n                ) or isinstance(member_color, list):\n                    markerline._color = member_color[i]\n                else:\n                    markerline._color = member_color\n                baseline._color = \"#00000000\"\n                baseline.zorder = -1000\n                plt.setp(stemlines, \"color\", plt.getp(markerline, \"color\"))\n                plt.setp(stemlines, linewidth=line_width)\n                plt.setp(markerline, markersize=markersize)\n\n            axes[i].spines[\"top\"].set_visible(False)\n            axes[i].spines[\"right\"].set_visible(False)\n\n        # give room for marker\n        axes[0].set_xlim(\n            -self.patterns.max() - x_padding, self.patterns.max() + x_padding\n        )\n\n        axes[0].set_ylabel(\"Neurons #\")\n        axes[0].set_xlabel(\"Weights (a.u.)\")\n\n        return fig, axes\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.restrict_epochs_to_pre_task_post","title":"<code>restrict_epochs_to_pre_task_post()</code>","text":"<p>Restricts the epochs to the specified epochs</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def restrict_epochs_to_pre_task_post(self) -&gt; None:\n    \"\"\"\n    Restricts the epochs to the specified epochs\n    \"\"\"\n    # fetch data\n    epoch_df = loading.load_epoch(self.basepath)\n    # compress back to back sleep epochs (an issue further up the pipeline)\n    epoch_df = compress_repeated_epochs(epoch_df)\n    # restrict to pre task post epochs\n    idx = find_pre_task_post(epoch_df.environment)\n    self.epoch_df = epoch_df[idx[0]]\n    # convert to epoch array and add to object\n    self.epochs = nel.EpochArray(\n        [np.array([self.epoch_df.startTime, self.epoch_df.stopTime]).T],\n        label=\"session_epochs\",\n        domain=self.time_support,\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.AssemblyReact.restrict_to_epoch","title":"<code>restrict_to_epoch(epoch)</code>","text":"<p>Restricts the spike data to a specific epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The epoch to restrict to.</p> required Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def restrict_to_epoch(self, epoch) -&gt; None:\n    \"\"\"\n    Restricts the spike data to a specific epoch.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray\n        The epoch to restrict to.\n    \"\"\"\n    self.st_resticted = self.st[epoch]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.ExplainedVariance","title":"<code>ExplainedVariance</code>","text":"<p>               Bases: <code>object</code></p> <p>Explained variance measure for assessing reactivation of neuronal activity using pairwise correlations.</p> References <p>1) Kudrimoti, H. S., Barnes, C. A., &amp; McNaughton, B. L. (1999).     Reactivation of Hippocampal Cell Assemblies: Effects of Behavioral State, Experience, and EEG Dynamics.     Journal of Neuroscience, 19(10), 4090-4101. https://doi.org/10/4090 2) Tatsuno, M., Lipa, P., &amp; McNaughton, B. L. (2006).     Methodological Considerations on the Use of Template Matching to Study Long-Lasting Memory Trace Replay.     Journal of Neuroscience, 26(42), 10727-10742. https://doi.org/10.1523/JNEUROSCI.3317-06.2006</p> <p>Adapted from https://github.com/diba-lab/NeuroPy/blob/main/neuropy/analyses/reactivation.py</p> <p>Attributes:</p> Name Type Description <code>st</code> <code>SpikeTrainArray</code> <p>obj that holds spiketrains</p> <code>template</code> <code>EpochArray</code> <p>time in seconds, pairwise correlation calculated from this period will be compared to matching period (task-period)</p> <code>matching</code> <code>EpochArray</code> <p>time in seconds, template-correlations will be correlated with pariwise correlations of this period (post-task period)</p> <code>control</code> <code>EpochArray</code> <p>time in seconds, control for pairwise correlations within this period (pre-task period)</p> <code>bin_size</code> <code>float</code> <p>in seconds, binning size for spike counts</p> <code>window</code> <code>int</code> <p>window over which pairwise correlations will be calculated in matching and control time periods,     if window is None entire time period is considered, in seconds</p> <code>slideby</code> <code>int</code> <p>slide window by this much, in seconds</p> <code>matching_windows</code> <code>array</code> <p>windows for matching period</p> <code>control_windows</code> <code>array</code> <p>windows for control period</p> <code>template_corr</code> <code>array</code> <p>pairwise correlations for template period</p> <code>matching_paircorr</code> <code>array</code> <p>pairwise correlations for matching period</p> <code>control_paircorr</code> <code>array</code> <p>pairwise correlations for control period</p> <code>ev</code> <code>array</code> <p>explained variance for each time point</p> <code>rev</code> <code>array</code> <p>reverse explained variance for each time point</p> <code>ev_std</code> <code>array</code> <p>explained variance standard deviation for each time point</p> <code>rev_std</code> <code>array</code> <p>reverse explained variance standard deviation for each time point</p> <code>partial_corr</code> <code>array</code> <p>partial correlations for each time point</p> <code>rev_partial_corr</code> <code>array</code> <p>reverse partial correlations for each time point</p> <code>n_pairs</code> <code>int</code> <p>number of pairs</p> <code>matching_time</code> <code>array</code> <p>time points for matching period</p> <code>control_time</code> <code>array</code> <p>time points for control period</p> <code>ev_signal</code> <code>AnalogSignalArray</code> <p>explained variance signal</p> <code>rev_signal</code> <code>AnalogSignalArray</code> <p>reverse explained variance signal</p> <code>plot</code> <code>function</code> <p>plot explained variance</p> <code>pvalue</code> <code>function</code> <p>calculate p-value for explained variance by shuffling the template correlations</p> <p>Examples:</p>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.ExplainedVariance--load-data","title":"Load data","text":"<pre><code>&gt;&gt;&gt; basepath = r\"U:\\data\\HMC\\HMC1\\day8\"\n&gt;&gt;&gt; st,cm = loading.load_spikes(basepath,brainRegion=\"CA1\",putativeCellType=\"Pyr\")\n</code></pre> <pre><code>&gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n&gt;&gt;&gt; beh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.ExplainedVariance--most-simple-case-returns-single-explained-variance-value","title":"Most simple case, returns single explained variance value","text":"<pre><code>&gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n&gt;&gt;&gt;        st=st,\n&gt;&gt;&gt;        template=beh_epochs[1],\n&gt;&gt;&gt;        matching=beh_epochs[2],\n&gt;&gt;&gt;        control=beh_epochs[0],\n&gt;&gt;&gt;        window=None,\n&gt;&gt;&gt;    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.ExplainedVariance--get-time-resolved-explained-variance-across-entire-session-in-200sec-bins","title":"Get time resolved explained variance across entire session in 200sec bins","text":"<pre><code>&gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n&gt;&gt;&gt;        st=st,\n&gt;&gt;&gt;        template=beh_epochs[1],\n&gt;&gt;&gt;        matching=nel.EpochArray([beh_epochs.start, beh_epochs.stop]),\n&gt;&gt;&gt;        control=beh_epochs[0],\n&gt;&gt;&gt;        window=200\n&gt;&gt;&gt;    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.ExplainedVariance--get-time-resolved-explained-variance-across-entire-session-in-200sec-bins-sliding-by-100sec","title":"Get time resolved explained variance across entire session in 200sec bins sliding by 100sec","text":"<pre><code>&gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n&gt;&gt;&gt;        st=st,\n&gt;&gt;&gt;        template=beh_epochs[1],\n&gt;&gt;&gt;        matching=nel.EpochArray([beh_epochs.start, beh_epochs.stop]),\n&gt;&gt;&gt;        control=beh_epochs[0],\n&gt;&gt;&gt;        window=200,\n&gt;&gt;&gt;        slideby=100\n&gt;&gt;&gt;    )\n</code></pre> Source code in <code>neuro_py/ensemble/explained_variance.py</code> <pre><code>class ExplainedVariance(object):\n    \"\"\"Explained variance measure for assessing reactivation of neuronal activity using pairwise correlations.\n\n    References\n    -------\n    1) Kudrimoti, H. S., Barnes, C. A., &amp; McNaughton, B. L. (1999).\n        Reactivation of Hippocampal Cell Assemblies: Effects of Behavioral State, Experience, and EEG Dynamics.\n        Journal of Neuroscience, 19(10), 4090-4101. https://doi.org/10/4090\n    2) Tatsuno, M., Lipa, P., &amp; McNaughton, B. L. (2006).\n        Methodological Considerations on the Use of Template Matching to Study Long-Lasting Memory Trace Replay.\n        Journal of Neuroscience, 26(42), 10727-10742. https://doi.org/10.1523/JNEUROSCI.3317-06.2006\n\n    Adapted from https://github.com/diba-lab/NeuroPy/blob/main/neuropy/analyses/reactivation.py\n\n    Attributes\n    ----------\n    st : SpikeTrainArray\n        obj that holds spiketrains\n    template : EpochArray\n        time in seconds, pairwise correlation calculated from this period will be compared to matching period (task-period)\n    matching : EpochArray\n        time in seconds, template-correlations will be correlated with pariwise correlations of this period (post-task period)\n    control : EpochArray\n        time in seconds, control for pairwise correlations within this period (pre-task period)\n    bin_size : float\n        in seconds, binning size for spike counts\n    window : int\n        window over which pairwise correlations will be calculated in matching and control time periods,\n            if window is None entire time period is considered, in seconds\n    slideby : int\n        slide window by this much, in seconds\n    matching_windows : array\n        windows for matching period\n    control_windows : array\n        windows for control period\n    template_corr : array\n        pairwise correlations for template period\n    matching_paircorr : array\n        pairwise correlations for matching period\n    control_paircorr : array\n        pairwise correlations for control period\n    ev : array\n        explained variance for each time point\n    rev : array\n        reverse explained variance for each time point\n    ev_std : array\n        explained variance standard deviation for each time point\n    rev_std : array\n        reverse explained variance standard deviation for each time point\n    partial_corr : array\n        partial correlations for each time point\n    rev_partial_corr : array\n        reverse partial correlations for each time point\n    n_pairs : int\n        number of pairs\n    matching_time : array\n        time points for matching period\n    control_time : array\n        time points for control period\n    ev_signal : AnalogSignalArray\n        explained variance signal\n    rev_signal : AnalogSignalArray\n        reverse explained variance signal\n    plot : function\n        plot explained variance\n    pvalue : function\n        calculate p-value for explained variance by shuffling the template correlations\n\n    Examples\n    --------\n    # Load data\n    &gt;&gt;&gt; basepath = r\"U:\\data\\HMC\\HMC1\\day8\"\n    &gt;&gt;&gt; st,cm = loading.load_spikes(basepath,brainRegion=\"CA1\",putativeCellType=\"Pyr\")\n\n    &gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n    &gt;&gt;&gt; beh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values)\n\n\n    # Most simple case, returns single explained variance value\n    &gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n    &gt;&gt;&gt;        st=st,\n    &gt;&gt;&gt;        template=beh_epochs[1],\n    &gt;&gt;&gt;        matching=beh_epochs[2],\n    &gt;&gt;&gt;        control=beh_epochs[0],\n    &gt;&gt;&gt;        window=None,\n    &gt;&gt;&gt;    )\n\n    # Get time resolved explained variance across entire session in 200sec bins\n    &gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n    &gt;&gt;&gt;        st=st,\n    &gt;&gt;&gt;        template=beh_epochs[1],\n    &gt;&gt;&gt;        matching=nel.EpochArray([beh_epochs.start, beh_epochs.stop]),\n    &gt;&gt;&gt;        control=beh_epochs[0],\n    &gt;&gt;&gt;        window=200\n    &gt;&gt;&gt;    )\n\n    # Get time resolved explained variance across entire session in 200sec bins sliding by 100sec\n    &gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n    &gt;&gt;&gt;        st=st,\n    &gt;&gt;&gt;        template=beh_epochs[1],\n    &gt;&gt;&gt;        matching=nel.EpochArray([beh_epochs.start, beh_epochs.stop]),\n    &gt;&gt;&gt;        control=beh_epochs[0],\n    &gt;&gt;&gt;        window=200,\n    &gt;&gt;&gt;        slideby=100\n    &gt;&gt;&gt;    )\n    \"\"\"\n\n    def __init__(\n        self,\n        st: SpikeTrainArray,\n        template: EpochArray,\n        matching: EpochArray,\n        control: EpochArray,\n        bin_size: float = 0.2,\n        window: int = 900,\n        slideby: int = None,\n    ):\n        \"\"\"Explained variance measure for assessing reactivation of neuronal activity using pairwise correlations.\n\n        Parameters\n        ----------\n        st : SpikeTrainArray\n            obj that holds spiketrains\n        template : EpochArray\n            time in seconds, pairwise correlation calculated from this period will be compared to matching period (task-period)\n        matching : EpochArray\n            time in seconds, template-correlations will be correlated with pariwise correlations of this period (post-task period)\n        control : EpochArray\n            time in seconds, control for pairwise correlations within this period (pre-task period)\n        bin_size : float, optional\n            in seconds, binning size for spike counts, by default 0.2\n        window : int, optional\n            window over which pairwise correlations will be calculated in matching and control time periods,\n                if window is None entire time period is considered, in seconds, by default 900\n        slideby : int, optional\n            slide window by this much, in seconds, by default None\n        \"\"\"\n        self.__dict__.update(locals())\n        del self.__dict__[\"self\"]\n\n        self.__validate_input()\n        self.__calculate()\n\n    def __validate_input(self):\n        \"\"\"Validate input parameters.\"\"\"\n        assert isinstance(self.st, SpikeTrainArray)\n        assert isinstance(self.template, EpochArray)\n        assert isinstance(self.matching, EpochArray)\n        assert isinstance(self.control, EpochArray)\n        assert isinstance(self.bin_size, (float, int))\n        assert isinstance(self.window, (int, type(None)))\n        assert isinstance(self.slideby, (int, type(None)))\n\n    def __calculate(self):\n        \"\"\"processing steps for explained variance calculation.\"\"\"\n        control_window_size, matching_window_size, slideby = self.__get_window_sizes()\n\n        self.matching_windows = self.__get_windows_array(\n            self.matching, matching_window_size, slideby\n        )\n        self.control_windows = self.__get_windows_array(\n            self.control, control_window_size, slideby\n        )\n        self.__validate_window_sizes(control_window_size, matching_window_size)\n        self.template_corr = self.__get_template_corr()\n        self.__calculate_pairwise_correlations()\n        self.__calculate_partial_correlations()\n\n    def __get_window_sizes(self):\n        \"\"\"Get window sizes for control and matching periods.\"\"\"\n        if self.window is None:\n            control_window_size = np.array(self.control.duration).astype(int)\n            matching_window_size = np.array(self.matching.duration).astype(int)\n            slideby = None\n        elif self.slideby is None:\n            control_window_size = self.window\n            matching_window_size = self.window\n            slideby = None\n        else:\n            control_window_size = self.window\n            matching_window_size = self.window\n            slideby = self.slideby\n        return control_window_size, matching_window_size, slideby\n\n    def __get_windows_array(self, epoch_array, window_size, slideby):\n        \"\"\"Get windows array for control and matching periods.\"\"\"\n        if slideby is not None:\n            array = np.arange(epoch_array.start, epoch_array.stop)\n            windows = np.lib.stride_tricks.sliding_window_view(array, window_size)\n            windows = windows[::slideby, [0, -1]]\n        elif np.array(epoch_array.duration) == window_size:\n            windows = np.array([[epoch_array.start, epoch_array.stop]])\n        else:\n            array = np.arange(epoch_array.start, epoch_array.stop, window_size)\n            windows = np.array([array[:-1], array[1:]]).T\n        return windows\n\n    def __validate_window_sizes(self, control_window_size, matching_window_size):\n        \"\"\"Validate window sizes.\"\"\"\n        assert (\n            control_window_size &lt;= self.control.duration\n        ), \"window is bigger than matching\"\n        assert (\n            matching_window_size &lt;= self.matching.duration\n        ), \"window is bigger than matching\"\n\n    def __get_template_corr(self):\n        \"\"\"Get pairwise correlations for template period.\"\"\"\n        self.bst = self.st.bin(ds=self.bin_size)\n        return self.__get_pairwise_corr(self.bst[self.template].data)\n\n    def __calculate_pairwise_correlations(self):\n        \"\"\"Calculate pairwise correlations for matching and control periods.\"\"\"\n        self.matching_paircorr = self.__time_resolved_correlation(self.matching_windows)\n        self.control_paircorr = self.__time_resolved_correlation(self.control_windows)\n\n    @staticmethod\n    def __get_pairwise_corr(bst_data):\n        \"\"\"Calculate pairwise correlations.\"\"\"\n        corr = np.corrcoef(bst_data)\n        return corr[np.tril_indices(corr.shape[0], k=-1)]\n\n    def __time_resolved_correlation(self, windows):\n        \"\"\"Calculate pairwise correlations for given windows.\"\"\"\n        paircorr = []\n        bst_data = self.bst.data\n        bin_centers = self.bst.bin_centers\n\n        for w in windows:\n            start, stop = w\n            idx = (bin_centers &gt; start) &amp; (bin_centers &lt; stop)\n            corr = np.corrcoef(bst_data[:, idx])\n            paircorr.append(corr[np.tril_indices(corr.shape[0], k=-1)])\n\n        return np.array(paircorr)\n\n    def __calculate_partial_correlations(self):\n        \"\"\"Calculate partial correlations.\"\"\"\n        partial_corr, rev_partial_corr = self.__calculate_partial_correlations_(\n            self.matching_paircorr, self.control_paircorr, self.template_corr\n        )\n        self.__calculate_statistics(partial_corr, rev_partial_corr)\n\n    @staticmethod\n    @jit(nopython=True)\n    def __calculate_partial_correlations_(\n        matching_paircorr, control_paircorr, template_corr\n    ):\n        \"\"\"Calculate partial correlations.\"\"\"\n\n        def __explained_variance(x, y, covar):\n            \"\"\"Calculate explained variance and reverse explained variance.\"\"\"\n\n            # Calculate covariance matrix\n            n = len(covar)\n            valid = np.zeros(n, dtype=np.bool_)\n            for i in range(n):\n                valid[i] = not (np.isnan(covar[i]) or np.isnan(x[i]) or np.isnan(y[i]))\n            mat = np.empty((3, len(x)))\n            mat[0] = covar\n            mat[1] = x\n            mat[2] = y\n            cov = np.corrcoef(mat[:, valid])\n\n            # Calculate explained variance\n            EV = (cov[1, 2] - cov[0, 1] * cov[0, 2]) / (\n                np.sqrt((1 - cov[0, 1] ** 2) * (1 - cov[0, 2] ** 2)) + 1e-10\n            )\n\n            # Calculate reverse explained variance\n            rEV = (cov[0, 1] - cov[1, 2] * cov[0, 2]) / (\n                np.sqrt((1 - cov[1, 2] ** 2) * (1 - cov[0, 2] ** 2)) + 1e-10\n            )\n\n            return EV, rEV\n\n        n_matching = len(matching_paircorr)\n        n_control = len(control_paircorr)\n        partial_corr = np.zeros((n_control, n_matching))\n        rev_partial_corr = np.zeros((n_control, n_matching))\n\n        for m_i, m_pairs in enumerate(matching_paircorr):\n            for c_i, c_pairs in enumerate(control_paircorr):\n                partial_corr[c_i, m_i], rev_partial_corr[c_i, m_i] = (\n                    __explained_variance(template_corr, m_pairs, c_pairs)\n                )\n        return partial_corr, rev_partial_corr\n\n    def __calculate_statistics(self, partial_corr, rev_partial_corr):\n        \"\"\"Calculate explained variance statistics.\"\"\"\n        self.ev = np.nanmean(partial_corr**2, axis=0)\n        self.rev = np.nanmean(rev_partial_corr**2, axis=0)\n        self.ev_std = np.nanstd(partial_corr**2, axis=0)\n        self.rev_std = np.nanstd(rev_partial_corr**2, axis=0)\n        self.partial_corr = partial_corr**2\n        self.rev_partial_corr = rev_partial_corr**2\n        self.n_pairs = len(self.template_corr)\n        self.matching_time = np.mean(self.matching_windows, axis=1)\n        self.control_time = np.mean(self.control_windows, axis=1)\n\n    @property\n    def ev_signal(self):\n        \"\"\"Return explained variance signal.\"\"\"\n        return AnalogSignalArray(\n            data=self.ev,\n            timestamps=self.matching_time,\n            fs=1 / np.diff(self.matching_time)[0],\n            support=EpochArray(data=[self.matching.start, self.matching.stop]),\n        )\n\n    @property\n    def rev_signal(self):\n        \"\"\"Return reverse explained variance signal.\"\"\"\n        return AnalogSignalArray(\n            data=self.rev,\n            timestamps=self.matching_time,\n            fs=1 / np.diff(self.matching_time)[0],\n            support=EpochArray(data=[self.matching.start, self.matching.stop]),\n        )\n\n    def pvalue(self, n_shuffles=1000):\n        \"\"\"\n        Calculate p-value for explained variance by shuffling the template correlations.\n        \"\"\"\n        from copy import deepcopy\n\n        def shuffle_template(self):\n            template_corr = deepcopy(self.template_corr)\n            np.random.shuffle(template_corr)\n\n            partial_corr, _ = self.__calculate_partial_correlations_(\n                self.matching_paircorr, self.control_paircorr, template_corr\n            )\n            ev = np.nanmean(partial_corr**2, axis=0)\n            return ev.flatten()\n\n        if len(self.ev) &gt; 1:\n            print(\"Multiple time points, p-values are not supported\")\n            return\n\n        ev_shuffle = [shuffle_template(self) for _ in range(n_shuffles)]\n\n        ev_shuffle = np.array(ev_shuffle)\n\n        n = len(ev_shuffle)\n        r = np.sum(ev_shuffle &gt; self.ev)\n        pvalues = (r + 1) / (n + 1)\n        return pvalues\n\n    def plot(self):\n        \"\"\"Plot explained variance.\"\"\"\n        if self.matching_time.size == 1:\n            print(\"Only single time point, cannot plot\")\n            return\n        import matplotlib.pyplot as plt\n\n        fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n        ax.plot(self.matching_time, self.ev, label=\"EV\")\n        ax.fill_between(\n            self.matching_time,\n            self.ev - self.ev_std,\n            self.ev + self.ev_std,\n            alpha=0.5,\n        )\n        ax.plot(self.matching_time, self.rev, label=\"rEV\", color=\"grey\")\n        ax.fill_between(\n            self.matching_time,\n            self.rev - self.rev_std,\n            self.rev + self.rev_std,\n            alpha=0.5,\n            color=\"grey\",\n        )\n        # check if matching time overlaps with control time and plot control time\n        if np.any(\n            (self.control_time &gt;= self.matching_time[0])\n            &amp; (self.control_time &lt;= self.matching_time[-1])\n        ):\n            ax.axvspan(\n                self.control.start,\n                self.control.stop,\n                color=\"green\",\n                alpha=0.3,\n                label=\"Control\",\n                zorder=-10,\n            )\n        # check if matching time overlaps with template time and plot template time\n        if np.any(\n            (self.template.start &gt;= self.matching_time[0])\n            &amp; (self.template.stop &lt;= self.matching_time[-1])\n        ):\n            ax.axvspan(\n                self.template.start,\n                self.template.stop,\n                color=\"purple\",\n                alpha=0.4,\n                label=\"Template\",\n                zorder=-10,\n            )\n        # remove axis spines\n        ax.spines[\"right\"].set_visible(False)\n        ax.spines[\"top\"].set_visible(False)\n\n        ax.legend(frameon=False)\n        ax.set_xlabel(\"Time (s)\")\n        ax.set_ylabel(\"Explained Variance\")\n        ax.set_title(\"Explained Variance\")\n        plt.show()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.ExplainedVariance.ev_signal","title":"<code>ev_signal</code>  <code>property</code>","text":"<p>Return explained variance signal.</p>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.ExplainedVariance.rev_signal","title":"<code>rev_signal</code>  <code>property</code>","text":"<p>Return reverse explained variance signal.</p>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.ExplainedVariance.plot","title":"<code>plot()</code>","text":"<p>Plot explained variance.</p> Source code in <code>neuro_py/ensemble/explained_variance.py</code> <pre><code>def plot(self):\n    \"\"\"Plot explained variance.\"\"\"\n    if self.matching_time.size == 1:\n        print(\"Only single time point, cannot plot\")\n        return\n    import matplotlib.pyplot as plt\n\n    fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n    ax.plot(self.matching_time, self.ev, label=\"EV\")\n    ax.fill_between(\n        self.matching_time,\n        self.ev - self.ev_std,\n        self.ev + self.ev_std,\n        alpha=0.5,\n    )\n    ax.plot(self.matching_time, self.rev, label=\"rEV\", color=\"grey\")\n    ax.fill_between(\n        self.matching_time,\n        self.rev - self.rev_std,\n        self.rev + self.rev_std,\n        alpha=0.5,\n        color=\"grey\",\n    )\n    # check if matching time overlaps with control time and plot control time\n    if np.any(\n        (self.control_time &gt;= self.matching_time[0])\n        &amp; (self.control_time &lt;= self.matching_time[-1])\n    ):\n        ax.axvspan(\n            self.control.start,\n            self.control.stop,\n            color=\"green\",\n            alpha=0.3,\n            label=\"Control\",\n            zorder=-10,\n        )\n    # check if matching time overlaps with template time and plot template time\n    if np.any(\n        (self.template.start &gt;= self.matching_time[0])\n        &amp; (self.template.stop &lt;= self.matching_time[-1])\n    ):\n        ax.axvspan(\n            self.template.start,\n            self.template.stop,\n            color=\"purple\",\n            alpha=0.4,\n            label=\"Template\",\n            zorder=-10,\n        )\n    # remove axis spines\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n\n    ax.legend(frameon=False)\n    ax.set_xlabel(\"Time (s)\")\n    ax.set_ylabel(\"Explained Variance\")\n    ax.set_title(\"Explained Variance\")\n    plt.show()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.ExplainedVariance.pvalue","title":"<code>pvalue(n_shuffles=1000)</code>","text":"<p>Calculate p-value for explained variance by shuffling the template correlations.</p> Source code in <code>neuro_py/ensemble/explained_variance.py</code> <pre><code>def pvalue(self, n_shuffles=1000):\n    \"\"\"\n    Calculate p-value for explained variance by shuffling the template correlations.\n    \"\"\"\n    from copy import deepcopy\n\n    def shuffle_template(self):\n        template_corr = deepcopy(self.template_corr)\n        np.random.shuffle(template_corr)\n\n        partial_corr, _ = self.__calculate_partial_correlations_(\n            self.matching_paircorr, self.control_paircorr, template_corr\n        )\n        ev = np.nanmean(partial_corr**2, axis=0)\n        return ev.flatten()\n\n    if len(self.ev) &gt; 1:\n        print(\"Multiple time points, p-values are not supported\")\n        return\n\n    ev_shuffle = [shuffle_template(self) for _ in range(n_shuffles)]\n\n    ev_shuffle = np.array(ev_shuffle)\n\n    n = len(ev_shuffle)\n    r = np.sum(ev_shuffle &gt; self.ev)\n    pvalues = (r + 1) / (n + 1)\n    return pvalues\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.PairwiseBias","title":"<code>PairwiseBias</code>","text":"<p>               Bases: <code>object</code></p> <p>Pairwise bias analysis for comparing task and post-task spike sequences.</p> <p>Parameters:</p> Name Type Description Default <code>num_shuffles</code> <code>int</code> <p>Number of shuffles to perform for significance testing. Default is 300.</p> <code>300</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to run for computing correlations. Default is 10.</p> <code>10</code> <p>Attributes:</p> Name Type Description <code>total_neurons</code> <code>int, or None</code> <p>Total number of neurons in the dataset.</p> <code>task_skew_bias</code> <code>np.ndarray, or None</code> <p>Normalized skew-bias matrix for the task data.</p> <code>observed_correlation_</code> <code>np.ndarray, or None</code> <p>Observed cosine similarity between task and post-task bias matrices.</p> <code>shuffled_correlations_</code> <code>np.ndarray, or None</code> <p>Shuffled cosine similarities for significance testing.</p> <code>z_score_</code> <code>np.ndarray, or None</code> <p>Z-score of the observed correlation compared to the shuffled distribution.</p> <code>p_value_</code> <code>np.ndarray, or None</code> <p>p-value for significance test.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Fit the model using the task spike data.</p> <code>transform</code> <p>Transform the post-task data to compute z-scores and p-values.</p> <code>fit_transform</code> <p>Fit the model with task data and transform the post-task data.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>class PairwiseBias(object):\n    \"\"\"\n    Pairwise bias analysis for comparing task and post-task spike sequences.\n\n    Parameters\n    ----------\n    num_shuffles : int, optional\n        Number of shuffles to perform for significance testing. Default is 300.\n    n_jobs : int, optional\n        Number of parallel jobs to run for computing correlations. Default is 10.\n\n    Attributes\n    ----------\n    total_neurons : int, or None\n        Total number of neurons in the dataset.\n    task_skew_bias : np.ndarray, or None\n        Normalized skew-bias matrix for the task data.\n    observed_correlation_ : np.ndarray, or None\n        Observed cosine similarity between task and post-task bias matrices.\n    shuffled_correlations_ : np.ndarray, or None\n        Shuffled cosine similarities for significance testing.\n    z_score_ : np.ndarray, or None\n        Z-score of the observed correlation compared to the shuffled distribution.\n    p_value_ : np.ndarray, or None\n        p-value for significance test.\n\n    Methods\n    -------\n    fit(task_spikes: Union[List[float], np.ndarray], task_neurons: Union[List[int], np.ndarray]) -&gt; 'PairwiseBias'\n        Fit the model using the task spike data.\n    transform(post_spikes: Union[List[float], np.ndarray], post_neurons: Union[List[int], np.ndarray], post_intervals: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]\n        Transform the post-task data to compute z-scores and p-values.\n    fit_transform(task_spikes: Union[List[float], np.ndarray], task_neurons: Union[List[int], np.ndarray], post_spikes: Union[List[float], np.ndarray], post_neurons: Union[List[int], np.ndarray], post_intervals: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]\n        Fit the model with task data and transform the post-task data.\n    \"\"\"\n\n    def __init__(\n        self, num_shuffles: int = 300, n_jobs: int = 10, fillneutral: float = np.nan\n    ):\n        self.num_shuffles = num_shuffles\n        self.n_jobs = n_jobs\n        self.fillneutral = fillneutral\n        self.total_neurons = None\n        self.task_skew_bias = None\n        self.observed_correlation_ = None\n        self.shuffled_correlations_ = None\n        self.z_score_ = None\n        self.p_value_ = None\n\n    @staticmethod\n    def bias_matrix(\n        spike_times: np.ndarray,\n        neuron_ids: np.ndarray,\n        total_neurons: int,\n        fillneutral: float = np.nan,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Optimized computation of the bias matrix B_k for a given sequence of spikes using vectorized operations.\n\n        Parameters\n        ----------\n        spike_times : np.ndarray\n            Spike times for the sequence.\n        neuron_ids : np.ndarray\n            Neuron identifiers corresponding to spike_times.\n        total_neurons : int\n            Total number of neurons being considered.\n        fillneutral : float, optional\n            Value to fill the diagonal of the bias matrix and other empty\n            combinations, by default np.nan.\n\n        Returns\n        -------\n        np.ndarray\n            A matrix of size (total_neurons, total_neurons) representing the bias.\n        \"\"\"\n        return skew_bias_matrix(spike_times, neuron_ids, total_neurons, fillneutral)\n\n    @staticmethod\n    def cosine_similarity_matrices(matrix1: np.ndarray, matrix2: np.ndarray) -&gt; float:\n        \"\"\"\n        Computes the cosine similarity between two flattened bias matrices.\n\n        Parameters\n        ----------\n        matrix1 : np.ndarray\n            A normalized bias matrix.\n        matrix2 : np.ndarray\n            Another normalized bias matrix.\n\n        Returns\n        -------\n        float\n            The cosine similarity between the two matrices.\n        \"\"\"\n        return cosine_similarity_matrices(matrix1, matrix2)\n\n    def observed_and_shuffled_correlation(\n        self,\n        post_spikes: np.ndarray,\n        post_neurons: np.ndarray,\n        task_skew_bias: np.ndarray,\n        post_intervals: np.ndarray,\n        interval_i: int,\n    ) -&gt; Tuple[float, List[float]]:\n        \"\"\"\n        Compute observed and shuffled correlation for a given post-task interval.\n\n        Parameters\n        ----------\n        post_spikes : np.ndarray\n            Spike times during post-task (e.g., sleep).\n        post_neurons : np.ndarray\n            Neuron identifiers for post-task spikes.\n        task_normalized : np.ndarray\n            Normalized task bias matrix.\n        post_intervals : np.ndarray\n            Intervals for post-task epochs.\n        interval_i : int\n            Index of the current post-task interval.\n\n        Returns\n        -------\n        Tuple[float, List[float]]\n            The observed correlation and a list of shuffled correlations.\n        \"\"\"\n        post_neurons = np.asarray(post_neurons, dtype=int)\n\n        start, end = post_intervals[interval_i]\n        start_idx = np.searchsorted(post_spikes, start, side=\"left\")\n        end_idx = np.searchsorted(post_spikes, end, side=\"right\")\n\n        filtered_spikes = post_spikes[start_idx:end_idx]\n        filtered_neurons = post_neurons[start_idx:end_idx]\n\n        post_skew_bias = self.bias_matrix(\n            filtered_spikes,\n            filtered_neurons,\n            self.total_neurons,\n            fillneutral=self.fillneutral,\n        )\n\n        observed_correlation = self.cosine_similarity_matrices(\n            task_skew_bias, post_skew_bias\n        )\n\n        shuffled_correlation = []\n        for _ in range(self.num_shuffles):\n            shuffled_neurons = np.random.permutation(filtered_neurons)\n            shuffled_skew_bias = self.bias_matrix(\n                filtered_spikes,\n                shuffled_neurons,\n                self.total_neurons,\n                fillneutral=self.fillneutral,\n            )\n            shuffled_correlation.append(\n                self.cosine_similarity_matrices(task_skew_bias, shuffled_skew_bias)\n            )\n\n        return observed_correlation, shuffled_correlation\n\n    def fit(\n        self,\n        task_spikes: np.ndarray,\n        task_neurons: np.ndarray,\n        task_intervals: np.ndarray = None,\n    ) -&gt; \"PairwiseBias\":\n        \"\"\"\n        Fit the model using the task spike data.\n\n        Parameters\n        ----------\n        task_spikes : np.ndarray\n            Spike times during the task.\n        task_neurons : np.ndarray\n            Neuron identifiers for task spikes.\n        task_intervals : np.ndarray, optional\n            Intervals for task epochs, by default None. If None, the entire task\n            data is used. Otherwise, the average bias matrix is computed across\n            all task intervals. Shape: (n_intervals, 2).\n\n        Returns\n        -------\n        PairwiseBias\n            Returns the instance itself.\n        \"\"\"\n        # Convert task_neurons to numpy array of integers\n        task_neurons = np.asarray(task_neurons, dtype=int)\n\n        # Calculate the total number of neurons based on unique entries in task_neurons\n        self.total_neurons = len(np.unique(task_neurons))\n\n        if task_intervals is None:\n            # Compute bias matrix for task data and normalize\n            task_skew_bias = self.bias_matrix(\n                task_spikes,\n                task_neurons,\n                self.total_neurons,\n                fillneutral=self.fillneutral,\n            )\n            self.task_skew_bias = task_skew_bias\n        else:\n            # Compute bias matrices for each task interval\n            task_skew_biases = []\n\n            for interval in task_intervals:\n                # find the indices of spikes within the interval\n                start_idx = np.searchsorted(task_spikes, interval[0], side=\"left\")\n                end_idx = np.searchsorted(task_spikes, interval[1], side=\"right\")\n\n                # Extract spikes and neurons within the interval\n                interval_spikes = task_spikes[start_idx:end_idx]\n                interval_neurons = task_neurons[start_idx:end_idx]\n\n                # Compute the bias matrix for the interval\n                interval_skew_bias = self.bias_matrix(\n                    interval_spikes,\n                    interval_neurons,\n                    self.total_neurons,\n                    fillneutral=self.fillneutral,\n                )\n                task_skew_biases.append(interval_skew_bias)\n\n            # Average the normalized bias matrices\n            # I expect to see RuntimeWarnings in this block\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n                self.task_skew_bias = np.nanmean(task_skew_biases, axis=0)\n        return self\n\n    def transform(\n        self,\n        post_spikes: np.ndarray,\n        post_neurons: np.ndarray,\n        post_intervals: np.ndarray,\n        allow_reverse_replay: bool = False,\n        parallel: bool = True,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Transform the post-task data to compute z-scores and p-values.\n\n        Parameters\n        ----------\n        post_spikes : np.ndarray\n            Spike times during post-task (e.g., sleep).\n        post_neurons : np.ndarray\n            Neuron identifiers for post-task spikes.\n        post_intervals : np.ndarray\n            Intervals for post-task epochs. Shape: (n_intervals, 2).\n        allow_reverse_replay : bool, optional\n            Whether to allow reverse sequences, by default False.\n        parallel : bool, optional\n            Whether to run in parallel, by default True.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray, np.ndarray]\n            z_score: The z-score of the observed correlation compared to the shuffled distribution.\n            p_value: p-value for significance test.\n            observed_correlation_: The observed correlation for each interval.\n        \"\"\"\n        # Check if the number of jobs is less than the number of intervals\n        if post_intervals.shape[0] &lt; self.n_jobs:\n            self.n_jobs = post_intervals.shape[0]\n\n        if parallel:\n            observed_correlation, shuffled_correlations = zip(\n                *Parallel(n_jobs=self.n_jobs)(\n                    delayed(self.observed_and_shuffled_correlation)(\n                        post_spikes,\n                        post_neurons,\n                        self.task_skew_bias,\n                        post_intervals,\n                        interval_i,\n                    )\n                    for interval_i in range(post_intervals.shape[0])\n                )\n            )\n        else:  # Run in serial for debugging\n            observed_correlation, shuffled_correlations = zip(\n                *[\n                    self.observed_and_shuffled_correlation(\n                        post_spikes,\n                        post_neurons,\n                        self.task_skew_bias,\n                        post_intervals,\n                        interval_i,\n                    )\n                    for interval_i in range(post_intervals.shape[0])\n                ]\n            )\n\n        self.observed_correlation_ = np.array(\n            observed_correlation\n        )  # Shape: (n_intervals,)\n        self.shuffled_correlations_ = np.array(\n            shuffled_correlations\n        )  # Shape: (n_intervals, n_shuffles)\n\n        shuffled_mean = np.mean(self.shuffled_correlations_, axis=1)\n        shuffled_std = np.std(self.shuffled_correlations_, axis=1)\n        self.z_score_ = (self.observed_correlation_ - shuffled_mean) / shuffled_std\n\n        observed_correlation = self.observed_correlation_\n        shuffled_correlations = self.shuffled_correlations_\n        if allow_reverse_replay:\n            observed_correlation = np.abs(observed_correlation)\n            shuffled_correlations = np.abs(shuffled_correlations)\n\n        self.p_value_ = (\n            np.sum(\n                shuffled_correlations.T &gt; observed_correlation,\n                axis=0,\n            )\n            + 1\n        ) / (self.num_shuffles + 1)\n\n        return self.z_score_, self.p_value_, self.observed_correlation_\n\n    def fit_transform(\n        self,\n        task_spikes: np.ndarray,\n        task_neurons: np.ndarray,\n        task_intervals: np.ndarray,\n        post_spikes: np.ndarray,\n        post_neurons: np.ndarray,\n        post_intervals: np.ndarray,\n        allow_reverse_replay: bool = False,\n        parallel: bool = True,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Fit the model with task data and transform the post-task data.\n\n        Parameters\n        ----------\n        task_spikes : np.ndarray\n            Spike times during the task.\n        task_neurons : np.ndarray\n            Neuron identifiers for task spikes.\n        task_intervals : np.ndarray\n            Intervals for task epochs. Shape: (n_intervals, 2).\n        post_spikes : np.ndarray\n            Spike times during post-task (e.g., sleep).\n        post_neurons : np.ndarray\n            Neuron identifiers for post-task spikes.\n        post_intervals : np.ndarray\n            Intervals for post-task epochs. Shape: (n_intervals, 2).\n        allow_reverse_replay : bool, optional\n            Whether to allow reverse sequences, by default False.\n        parallel : bool, optional\n            Whether to run in parallel, by default True.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray, np.ndarray]\n            z_score: The z-score of the observed correlation compared to the shuffled distribution.\n            p_value: p-value for significance test.\n            observed_correlation_: The observed correlation for each interval.\n        \"\"\"\n        self.fit(task_spikes, task_neurons, task_intervals)\n        return self.transform(\n            post_spikes, post_neurons, post_intervals, allow_reverse_replay, parallel\n        )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.PairwiseBias.bias_matrix","title":"<code>bias_matrix(spike_times, neuron_ids, total_neurons, fillneutral=np.nan)</code>  <code>staticmethod</code>","text":"<p>Optimized computation of the bias matrix B_k for a given sequence of spikes using vectorized operations.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>ndarray</code> <p>Spike times for the sequence.</p> required <code>neuron_ids</code> <code>ndarray</code> <p>Neuron identifiers corresponding to spike_times.</p> required <code>total_neurons</code> <code>int</code> <p>Total number of neurons being considered.</p> required <code>fillneutral</code> <code>float</code> <p>Value to fill the diagonal of the bias matrix and other empty combinations, by default np.nan.</p> <code>nan</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A matrix of size (total_neurons, total_neurons) representing the bias.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>@staticmethod\ndef bias_matrix(\n    spike_times: np.ndarray,\n    neuron_ids: np.ndarray,\n    total_neurons: int,\n    fillneutral: float = np.nan,\n) -&gt; np.ndarray:\n    \"\"\"\n    Optimized computation of the bias matrix B_k for a given sequence of spikes using vectorized operations.\n\n    Parameters\n    ----------\n    spike_times : np.ndarray\n        Spike times for the sequence.\n    neuron_ids : np.ndarray\n        Neuron identifiers corresponding to spike_times.\n    total_neurons : int\n        Total number of neurons being considered.\n    fillneutral : float, optional\n        Value to fill the diagonal of the bias matrix and other empty\n        combinations, by default np.nan.\n\n    Returns\n    -------\n    np.ndarray\n        A matrix of size (total_neurons, total_neurons) representing the bias.\n    \"\"\"\n    return skew_bias_matrix(spike_times, neuron_ids, total_neurons, fillneutral)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.PairwiseBias.cosine_similarity_matrices","title":"<code>cosine_similarity_matrices(matrix1, matrix2)</code>  <code>staticmethod</code>","text":"<p>Computes the cosine similarity between two flattened bias matrices.</p> <p>Parameters:</p> Name Type Description Default <code>matrix1</code> <code>ndarray</code> <p>A normalized bias matrix.</p> required <code>matrix2</code> <code>ndarray</code> <p>Another normalized bias matrix.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The cosine similarity between the two matrices.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>@staticmethod\ndef cosine_similarity_matrices(matrix1: np.ndarray, matrix2: np.ndarray) -&gt; float:\n    \"\"\"\n    Computes the cosine similarity between two flattened bias matrices.\n\n    Parameters\n    ----------\n    matrix1 : np.ndarray\n        A normalized bias matrix.\n    matrix2 : np.ndarray\n        Another normalized bias matrix.\n\n    Returns\n    -------\n    float\n        The cosine similarity between the two matrices.\n    \"\"\"\n    return cosine_similarity_matrices(matrix1, matrix2)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.PairwiseBias.fit","title":"<code>fit(task_spikes, task_neurons, task_intervals=None)</code>","text":"<p>Fit the model using the task spike data.</p> <p>Parameters:</p> Name Type Description Default <code>task_spikes</code> <code>ndarray</code> <p>Spike times during the task.</p> required <code>task_neurons</code> <code>ndarray</code> <p>Neuron identifiers for task spikes.</p> required <code>task_intervals</code> <code>ndarray</code> <p>Intervals for task epochs, by default None. If None, the entire task data is used. Otherwise, the average bias matrix is computed across all task intervals. Shape: (n_intervals, 2).</p> <code>None</code> <p>Returns:</p> Type Description <code>PairwiseBias</code> <p>Returns the instance itself.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def fit(\n    self,\n    task_spikes: np.ndarray,\n    task_neurons: np.ndarray,\n    task_intervals: np.ndarray = None,\n) -&gt; \"PairwiseBias\":\n    \"\"\"\n    Fit the model using the task spike data.\n\n    Parameters\n    ----------\n    task_spikes : np.ndarray\n        Spike times during the task.\n    task_neurons : np.ndarray\n        Neuron identifiers for task spikes.\n    task_intervals : np.ndarray, optional\n        Intervals for task epochs, by default None. If None, the entire task\n        data is used. Otherwise, the average bias matrix is computed across\n        all task intervals. Shape: (n_intervals, 2).\n\n    Returns\n    -------\n    PairwiseBias\n        Returns the instance itself.\n    \"\"\"\n    # Convert task_neurons to numpy array of integers\n    task_neurons = np.asarray(task_neurons, dtype=int)\n\n    # Calculate the total number of neurons based on unique entries in task_neurons\n    self.total_neurons = len(np.unique(task_neurons))\n\n    if task_intervals is None:\n        # Compute bias matrix for task data and normalize\n        task_skew_bias = self.bias_matrix(\n            task_spikes,\n            task_neurons,\n            self.total_neurons,\n            fillneutral=self.fillneutral,\n        )\n        self.task_skew_bias = task_skew_bias\n    else:\n        # Compute bias matrices for each task interval\n        task_skew_biases = []\n\n        for interval in task_intervals:\n            # find the indices of spikes within the interval\n            start_idx = np.searchsorted(task_spikes, interval[0], side=\"left\")\n            end_idx = np.searchsorted(task_spikes, interval[1], side=\"right\")\n\n            # Extract spikes and neurons within the interval\n            interval_spikes = task_spikes[start_idx:end_idx]\n            interval_neurons = task_neurons[start_idx:end_idx]\n\n            # Compute the bias matrix for the interval\n            interval_skew_bias = self.bias_matrix(\n                interval_spikes,\n                interval_neurons,\n                self.total_neurons,\n                fillneutral=self.fillneutral,\n            )\n            task_skew_biases.append(interval_skew_bias)\n\n        # Average the normalized bias matrices\n        # I expect to see RuntimeWarnings in this block\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n            self.task_skew_bias = np.nanmean(task_skew_biases, axis=0)\n    return self\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.PairwiseBias.fit_transform","title":"<code>fit_transform(task_spikes, task_neurons, task_intervals, post_spikes, post_neurons, post_intervals, allow_reverse_replay=False, parallel=True)</code>","text":"<p>Fit the model with task data and transform the post-task data.</p> <p>Parameters:</p> Name Type Description Default <code>task_spikes</code> <code>ndarray</code> <p>Spike times during the task.</p> required <code>task_neurons</code> <code>ndarray</code> <p>Neuron identifiers for task spikes.</p> required <code>task_intervals</code> <code>ndarray</code> <p>Intervals for task epochs. Shape: (n_intervals, 2).</p> required <code>post_spikes</code> <code>ndarray</code> <p>Spike times during post-task (e.g., sleep).</p> required <code>post_neurons</code> <code>ndarray</code> <p>Neuron identifiers for post-task spikes.</p> required <code>post_intervals</code> <code>ndarray</code> <p>Intervals for post-task epochs. Shape: (n_intervals, 2).</p> required <code>allow_reverse_replay</code> <code>bool</code> <p>Whether to allow reverse sequences, by default False.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to run in parallel, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>z_score: The z-score of the observed correlation compared to the shuffled distribution. p_value: p-value for significance test. observed_correlation_: The observed correlation for each interval.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def fit_transform(\n    self,\n    task_spikes: np.ndarray,\n    task_neurons: np.ndarray,\n    task_intervals: np.ndarray,\n    post_spikes: np.ndarray,\n    post_neurons: np.ndarray,\n    post_intervals: np.ndarray,\n    allow_reverse_replay: bool = False,\n    parallel: bool = True,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Fit the model with task data and transform the post-task data.\n\n    Parameters\n    ----------\n    task_spikes : np.ndarray\n        Spike times during the task.\n    task_neurons : np.ndarray\n        Neuron identifiers for task spikes.\n    task_intervals : np.ndarray\n        Intervals for task epochs. Shape: (n_intervals, 2).\n    post_spikes : np.ndarray\n        Spike times during post-task (e.g., sleep).\n    post_neurons : np.ndarray\n        Neuron identifiers for post-task spikes.\n    post_intervals : np.ndarray\n        Intervals for post-task epochs. Shape: (n_intervals, 2).\n    allow_reverse_replay : bool, optional\n        Whether to allow reverse sequences, by default False.\n    parallel : bool, optional\n        Whether to run in parallel, by default True.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, np.ndarray]\n        z_score: The z-score of the observed correlation compared to the shuffled distribution.\n        p_value: p-value for significance test.\n        observed_correlation_: The observed correlation for each interval.\n    \"\"\"\n    self.fit(task_spikes, task_neurons, task_intervals)\n    return self.transform(\n        post_spikes, post_neurons, post_intervals, allow_reverse_replay, parallel\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.PairwiseBias.observed_and_shuffled_correlation","title":"<code>observed_and_shuffled_correlation(post_spikes, post_neurons, task_skew_bias, post_intervals, interval_i)</code>","text":"<p>Compute observed and shuffled correlation for a given post-task interval.</p> <p>Parameters:</p> Name Type Description Default <code>post_spikes</code> <code>ndarray</code> <p>Spike times during post-task (e.g., sleep).</p> required <code>post_neurons</code> <code>ndarray</code> <p>Neuron identifiers for post-task spikes.</p> required <code>task_normalized</code> <code>ndarray</code> <p>Normalized task bias matrix.</p> required <code>post_intervals</code> <code>ndarray</code> <p>Intervals for post-task epochs.</p> required <code>interval_i</code> <code>int</code> <p>Index of the current post-task interval.</p> required <p>Returns:</p> Type Description <code>Tuple[float, List[float]]</code> <p>The observed correlation and a list of shuffled correlations.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def observed_and_shuffled_correlation(\n    self,\n    post_spikes: np.ndarray,\n    post_neurons: np.ndarray,\n    task_skew_bias: np.ndarray,\n    post_intervals: np.ndarray,\n    interval_i: int,\n) -&gt; Tuple[float, List[float]]:\n    \"\"\"\n    Compute observed and shuffled correlation for a given post-task interval.\n\n    Parameters\n    ----------\n    post_spikes : np.ndarray\n        Spike times during post-task (e.g., sleep).\n    post_neurons : np.ndarray\n        Neuron identifiers for post-task spikes.\n    task_normalized : np.ndarray\n        Normalized task bias matrix.\n    post_intervals : np.ndarray\n        Intervals for post-task epochs.\n    interval_i : int\n        Index of the current post-task interval.\n\n    Returns\n    -------\n    Tuple[float, List[float]]\n        The observed correlation and a list of shuffled correlations.\n    \"\"\"\n    post_neurons = np.asarray(post_neurons, dtype=int)\n\n    start, end = post_intervals[interval_i]\n    start_idx = np.searchsorted(post_spikes, start, side=\"left\")\n    end_idx = np.searchsorted(post_spikes, end, side=\"right\")\n\n    filtered_spikes = post_spikes[start_idx:end_idx]\n    filtered_neurons = post_neurons[start_idx:end_idx]\n\n    post_skew_bias = self.bias_matrix(\n        filtered_spikes,\n        filtered_neurons,\n        self.total_neurons,\n        fillneutral=self.fillneutral,\n    )\n\n    observed_correlation = self.cosine_similarity_matrices(\n        task_skew_bias, post_skew_bias\n    )\n\n    shuffled_correlation = []\n    for _ in range(self.num_shuffles):\n        shuffled_neurons = np.random.permutation(filtered_neurons)\n        shuffled_skew_bias = self.bias_matrix(\n            filtered_spikes,\n            shuffled_neurons,\n            self.total_neurons,\n            fillneutral=self.fillneutral,\n        )\n        shuffled_correlation.append(\n            self.cosine_similarity_matrices(task_skew_bias, shuffled_skew_bias)\n        )\n\n    return observed_correlation, shuffled_correlation\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.PairwiseBias.transform","title":"<code>transform(post_spikes, post_neurons, post_intervals, allow_reverse_replay=False, parallel=True)</code>","text":"<p>Transform the post-task data to compute z-scores and p-values.</p> <p>Parameters:</p> Name Type Description Default <code>post_spikes</code> <code>ndarray</code> <p>Spike times during post-task (e.g., sleep).</p> required <code>post_neurons</code> <code>ndarray</code> <p>Neuron identifiers for post-task spikes.</p> required <code>post_intervals</code> <code>ndarray</code> <p>Intervals for post-task epochs. Shape: (n_intervals, 2).</p> required <code>allow_reverse_replay</code> <code>bool</code> <p>Whether to allow reverse sequences, by default False.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to run in parallel, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>z_score: The z-score of the observed correlation compared to the shuffled distribution. p_value: p-value for significance test. observed_correlation_: The observed correlation for each interval.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def transform(\n    self,\n    post_spikes: np.ndarray,\n    post_neurons: np.ndarray,\n    post_intervals: np.ndarray,\n    allow_reverse_replay: bool = False,\n    parallel: bool = True,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Transform the post-task data to compute z-scores and p-values.\n\n    Parameters\n    ----------\n    post_spikes : np.ndarray\n        Spike times during post-task (e.g., sleep).\n    post_neurons : np.ndarray\n        Neuron identifiers for post-task spikes.\n    post_intervals : np.ndarray\n        Intervals for post-task epochs. Shape: (n_intervals, 2).\n    allow_reverse_replay : bool, optional\n        Whether to allow reverse sequences, by default False.\n    parallel : bool, optional\n        Whether to run in parallel, by default True.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, np.ndarray]\n        z_score: The z-score of the observed correlation compared to the shuffled distribution.\n        p_value: p-value for significance test.\n        observed_correlation_: The observed correlation for each interval.\n    \"\"\"\n    # Check if the number of jobs is less than the number of intervals\n    if post_intervals.shape[0] &lt; self.n_jobs:\n        self.n_jobs = post_intervals.shape[0]\n\n    if parallel:\n        observed_correlation, shuffled_correlations = zip(\n            *Parallel(n_jobs=self.n_jobs)(\n                delayed(self.observed_and_shuffled_correlation)(\n                    post_spikes,\n                    post_neurons,\n                    self.task_skew_bias,\n                    post_intervals,\n                    interval_i,\n                )\n                for interval_i in range(post_intervals.shape[0])\n            )\n        )\n    else:  # Run in serial for debugging\n        observed_correlation, shuffled_correlations = zip(\n            *[\n                self.observed_and_shuffled_correlation(\n                    post_spikes,\n                    post_neurons,\n                    self.task_skew_bias,\n                    post_intervals,\n                    interval_i,\n                )\n                for interval_i in range(post_intervals.shape[0])\n            ]\n        )\n\n    self.observed_correlation_ = np.array(\n        observed_correlation\n    )  # Shape: (n_intervals,)\n    self.shuffled_correlations_ = np.array(\n        shuffled_correlations\n    )  # Shape: (n_intervals, n_shuffles)\n\n    shuffled_mean = np.mean(self.shuffled_correlations_, axis=1)\n    shuffled_std = np.std(self.shuffled_correlations_, axis=1)\n    self.z_score_ = (self.observed_correlation_ - shuffled_mean) / shuffled_std\n\n    observed_correlation = self.observed_correlation_\n    shuffled_correlations = self.shuffled_correlations_\n    if allow_reverse_replay:\n        observed_correlation = np.abs(observed_correlation)\n        shuffled_correlations = np.abs(shuffled_correlations)\n\n    self.p_value_ = (\n        np.sum(\n            shuffled_correlations.T &gt; observed_correlation,\n            axis=0,\n        )\n        + 1\n    ) / (self.num_shuffles + 1)\n\n    return self.z_score_, self.p_value_, self.observed_correlation_\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.WeightedCorr","title":"<code>WeightedCorr(weights, x=None, y=None)</code>","text":"<p>Calculate the weighted correlation between the X and Y dimensions of the matrix.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>ndarray</code> <p>A matrix of weights.</p> required <code>x</code> <code>Optional[ndarray]</code> <p>X-values for each column and row, by default None.</p> <code>None</code> <code>y</code> <code>Optional[ndarray]</code> <p>Y-values for each column and row, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The weighted correlation coefficient.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def WeightedCorr(\n    weights: np.ndarray, x: Optional[np.ndarray] = None, y: Optional[np.ndarray] = None\n) -&gt; float:\n    \"\"\"\n    Calculate the weighted correlation between the X and Y dimensions of the matrix.\n\n    Parameters\n    ----------\n    weights : np.ndarray\n        A matrix of weights.\n    x : Optional[np.ndarray], optional\n        X-values for each column and row, by default None.\n    y : Optional[np.ndarray], optional\n        Y-values for each column and row, by default None.\n\n    Returns\n    -------\n    float\n        The weighted correlation coefficient.\n    \"\"\"\n    weights[np.isnan(weights)] = 0.0\n\n    if x is not None and x.size &gt; 0:\n        if np.ndim(x) == 1:\n            x = np.tile(x, (weights.shape[0], 1))\n    else:\n        x, _ = np.meshgrid(\n            np.arange(1, weights.shape[1] + 1), np.arange(1, weights.shape[0] + 1)\n        )\n\n    if y is not None and y.size &gt; 0:\n        if np.ndim(y) == 1:\n            y = np.tile(y, (weights.shape[0], 1))\n    else:\n        _, y = np.meshgrid(\n            np.arange(1, weights.shape[1] + 1), np.arange(1, weights.shape[0] + 1)\n        )\n\n    x = x.flatten()\n    y = y.flatten()\n    w = weights.flatten()\n\n    mX = np.nansum(w * x) / np.nansum(w)\n    mY = np.nansum(w * y) / np.nansum(w)\n\n    covXY = np.nansum(w * (x - mX) * (y - mY)) / np.nansum(w)\n    covXX = np.nansum(w * (x - mX) ** 2) / np.nansum(w)\n    covYY = np.nansum(w * (y - mY) ** 2) / np.nansum(w)\n\n    c = covXY / np.sqrt(covXX * covYY)\n\n    return c\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.WeightedCorrCirc","title":"<code>WeightedCorrCirc(weights, x=None, alpha=None)</code>","text":"<p>Compute the correlation between x and y dimensions of a matrix with angular (circular) values.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>ndarray</code> <p>A 2D numpy array of weights.</p> required <code>x</code> <code>Optional[ndarray]</code> <p>A 2D numpy array of x-values, by default None.</p> <code>None</code> <code>alpha</code> <code>Optional[ndarray]</code> <p>A 2D numpy array of angular (circular) y-values, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The correlation between x and y dimensions.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def WeightedCorrCirc(\n    weights: np.ndarray,\n    x: Optional[np.ndarray] = None,\n    alpha: Optional[np.ndarray] = None,\n) -&gt; float:\n    \"\"\"\n    Compute the correlation between x and y dimensions of a matrix with angular (circular) values.\n\n    Parameters\n    ----------\n    weights : np.ndarray\n        A 2D numpy array of weights.\n    x : Optional[np.ndarray], optional\n        A 2D numpy array of x-values, by default None.\n    alpha : Optional[np.ndarray], optional\n        A 2D numpy array of angular (circular) y-values, by default None.\n\n    Returns\n    -------\n    float\n        The correlation between x and y dimensions.\n    \"\"\"\n    weights[np.isnan(weights)] = 0.0\n\n    if x is not None and x.size &gt; 0:\n        if np.ndim(x) == 1:\n            x = np.tile(x, (weights.shape[0], 1))\n    else:\n        x, _ = np.meshgrid(\n            np.arange(1, weights.shape[1] + 1), np.arange(1, weights.shape[0] + 1)\n        )\n    if alpha is None:\n        alpha = np.tile(\n            np.linspace(0, 2 * np.pi, weights.shape[0], endpoint=False),\n            (weights.shape[1], 1),\n        ).T\n\n    rxs = WeightedCorr(weights, x, np.sin(alpha))\n    rxc = WeightedCorr(weights, x, np.cos(alpha))\n    rcs = WeightedCorr(weights, np.sin(alpha), np.cos(alpha))\n\n    # Compute angular-linear correlation\n    rho = np.sqrt((rxc**2 + rxs**2 - 2 * rxc * rxs * rcs) / (1 - rcs**2))\n    return rho\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.binshuffling","title":"<code>binshuffling(zactmat, significance)</code>","text":"<p>Perform bin shuffling to generate statistical threshold.</p> <p>Parameters:</p> Name Type Description Default <code>zactmat</code> <code>ndarray</code> <p>Z-scored activity matrix.</p> required <code>significance</code> <code>object</code> <p>Object containing significance parameters.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Statistical threshold.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def binshuffling(zactmat: np.ndarray, significance: object) -&gt; float:\n    \"\"\"\n    Perform bin shuffling to generate statistical threshold.\n\n    Parameters\n    ----------\n    zactmat : np.ndarray\n        Z-scored activity matrix.\n    significance : object\n        Object containing significance parameters.\n\n    Returns\n    -------\n    float\n        Statistical threshold.\n    \"\"\"\n    np.random.seed()\n\n    lambdamax_ = np.zeros(significance.nshu)\n    for shui in range(significance.nshu):\n        zactmat_ = np.copy(zactmat)\n        for neuroni, activity in enumerate(zactmat_):\n            randomorder = np.argsort(np.random.rand(significance.nbins))\n            zactmat_[neuroni, :] = activity[randomorder]\n        lambdamax_[shui] = getlambdacontrol(zactmat_)\n\n    lambdaMax = np.percentile(lambdamax_, significance.percentile)\n\n    return lambdaMax\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.circshuffling","title":"<code>circshuffling(zactmat, significance)</code>","text":"<p>Perform circular shuffling to generate statistical threshold.</p> <p>Parameters:</p> Name Type Description Default <code>zactmat</code> <code>ndarray</code> <p>Z-scored activity matrix.</p> required <code>significance</code> <code>object</code> <p>Object containing significance parameters.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Statistical threshold.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def circshuffling(zactmat: np.ndarray, significance: object) -&gt; float:\n    \"\"\"\n    Perform circular shuffling to generate statistical threshold.\n\n    Parameters\n    ----------\n    zactmat : np.ndarray\n        Z-scored activity matrix.\n    significance : object\n        Object containing significance parameters.\n\n    Returns\n    -------\n    float\n        Statistical threshold.\n    \"\"\"\n    np.random.seed()\n\n    lambdamax_ = np.zeros(significance.nshu)\n    for shui in range(significance.nshu):\n        zactmat_ = np.copy(zactmat)\n        for neuroni, activity in enumerate(zactmat_):\n            cut = int(np.random.randint(significance.nbins * 2))\n            zactmat_[neuroni, :] = np.roll(activity, cut)\n        lambdamax_[shui] = getlambdacontrol(zactmat_)\n\n    lambdaMax = np.percentile(lambdamax_, significance.percentile)\n\n    return lambdaMax\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.computeAssemblyActivity","title":"<code>computeAssemblyActivity(patterns, zactmat, zerodiag=True)</code>","text":"<p>Compute assembly activity.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>ndarray</code> <p>Co-activation patterns (assemblies, neurons).</p> required <code>zactmat</code> <code>ndarray</code> <p>Z-scored activity matrix (neurons, time bins).</p> required <code>zerodiag</code> <code>bool</code> <p>If True, diagonal of projection matrix is set to zero, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Assembly activity matrix (assemblies, time bins).</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def computeAssemblyActivity(\n    patterns: np.ndarray,\n    zactmat: np.ndarray,\n    zerodiag: bool = True,\n) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Compute assembly activity.\n\n    Parameters\n    ----------\n    patterns : np.ndarray\n        Co-activation patterns (assemblies, neurons).\n    zactmat : np.ndarray\n        Z-scored activity matrix (neurons, time bins).\n    zerodiag : bool, optional\n        If True, diagonal of projection matrix is set to zero, by default True.\n\n    Returns\n    -------\n    Optional[np.ndarray]\n        Assembly activity matrix (assemblies, time bins).\n    \"\"\"\n    # check if patterns is empty (no assembly detected) and return None if so\n    if len(patterns) == 0:\n        return None\n\n    # number of assemblies and time bins\n    nassemblies = len(patterns)\n    nbins = np.size(zactmat, 1)\n\n    # transpose for later matrix multiplication\n    zactmat = zactmat.T\n\n    # preallocate assembly activity matrix (nassemblies, nbins)\n    assemblyAct = np.zeros((nassemblies, nbins))\n\n    # loop over assemblies\n    for assemblyi, pattern in enumerate(patterns):\n        # compute projection matrix (neurons, neurons)\n        projMat = np.outer(pattern, pattern)\n\n        # set the diagonal to zero to not count coactivation of i and j when i=j\n        if zerodiag:\n            np.fill_diagonal(projMat, 0)\n\n        # project assembly pattern onto z-scored activity matrix\n        assemblyAct[assemblyi, :] = np.nansum(zactmat @ projMat * zactmat, axis=1)\n\n    return assemblyAct\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.cosine_similarity","title":"<code>cosine_similarity(pv1, pv2)</code>","text":"<p>Cosine similarity between temporal difference vectors of two firing rate vector trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>pv1</code> <code>ndarray</code> <p>Temporal difference of firing rate vector trajectory in one context. Shape: (num_bins, num_neurons)</p> required <code>pv2</code> <code>ndarray</code> <p>Temporal difference of firing rate vector trajectory in another context. Shape: (num_bins, num_neurons)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Cosine similarity between the two contexts.</p> References <p>.. [1] Guidera, J. A., Gramling, D. P., Comrie, A. E., Joshi, A., Denovellis, E. L., Lee, K. H., Zhou, J., Thompson, P., Hernandez, J., Yorita, A., Haque, R., Kirst, C., &amp; Frank, L. M. (2024). Regional specialization manifests in the reliability of neural population codes. bioRxiv : the preprint server for biology, 2024.01.25.576941. https://doi.org/10.1101/2024.01.25.576941</p> Source code in <code>neuro_py/ensemble/dynamics.py</code> <pre><code>def cosine_similarity(pv1: np.ndarray, pv2: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Cosine similarity between temporal difference vectors of two firing rate\n    vector trajectories.\n\n    Parameters\n    ----------\n    pv1 : numpy.ndarray\n        Temporal difference of firing rate vector trajectory in one context.\n        Shape: (num_bins, num_neurons)\n\n    pv2 : numpy.ndarray\n        Temporal difference of firing rate vector trajectory in another context.\n        Shape: (num_bins, num_neurons)\n\n    Returns\n    -------\n    numpy.ndarray\n        Cosine similarity between the two contexts.\n\n    References\n    ----------\n    .. [1] Guidera, J. A., Gramling, D. P., Comrie, A. E., Joshi, A.,\n    Denovellis, E. L., Lee, K. H., Zhou, J., Thompson, P., Hernandez, J.,\n    Yorita, A., Haque, R., Kirst, C., &amp; Frank, L. M. (2024). Regional\n    specialization manifests in the reliability of neural population codes.\n    bioRxiv : the preprint server for biology, 2024.01.25.576941.\n    https://doi.org/10.1101/2024.01.25.576941\n    \"\"\"\n    cosine_mat = sklearn.metrics.pairwise.cosine_similarity(pv1, pv2)\n    cosine_sim = np.diag(cosine_mat)\n\n    return cosine_sim\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.cosine_similarity_matrices","title":"<code>cosine_similarity_matrices(matrix1, matrix2)</code>","text":"<p>Compute the cosine similarity between two flattened matrices</p> <p>Parameters:</p> Name Type Description Default <code>matrix1</code> <code>ndarray</code> <p>A normalized bias matrix</p> required <code>matrix2</code> <code>ndarray</code> <p>Another normalized bias matrix</p> required <p>Returns:</p> Type Description <code>float</code> <p>The cosine similarity between the two matrices.</p> Source code in <code>neuro_py/ensemble/pairwise_bias_correlation.py</code> <pre><code>def cosine_similarity_matrices(\n    matrix1: np.ndarray,\n    matrix2: np.ndarray\n) -&gt; float:\n    \"\"\"\n    Compute the cosine similarity between two flattened matrices\n\n    Parameters\n    ----------\n    matrix1 : numpy.ndarray\n        A normalized bias matrix\n    matrix2 : numpy.ndarray\n        Another normalized bias matrix\n\n    Returns\n    -------\n    float\n        The cosine similarity between the two matrices.\n    \"\"\"\n    # Flatten matrices\n    x = matrix1.flatten().reshape(1, -1)\n    y = matrix2.flatten().reshape(1, -1)\n\n    if np.all(np.isnan(x)) or np.all(np.isnan(y)):\n        return np.nan\n\n    # handle nan values\n    x = np.nan_to_num(x)\n    y = np.nan_to_num(y)\n\n    cossim = sklearn.metrics.pairwise.cosine_similarity(x, y)\n\n    # Compute cosine similarity\n    return cossim.item()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.extractPatterns","title":"<code>extractPatterns(actmat, significance, method, whiten='unit-variance')</code>","text":"<p>Extract co-activation patterns (assemblies).</p> <p>Parameters:</p> Name Type Description Default <code>actmat</code> <code>ndarray</code> <p>Activity matrix.</p> required <code>significance</code> <code>object</code> <p>Object containing significance parameters.</p> required <code>method</code> <code>str</code> <p>Method to extract assembly patterns (ica, pca).</p> required <code>whiten</code> <code>str</code> <p>Whitening method, by default \"unit-variance\".</p> <code>'unit-variance'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Co-activation patterns (assemblies).</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def extractPatterns(\n    actmat: np.ndarray, significance: object, method: str, whiten: str = \"unit-variance\"\n) -&gt; np.ndarray:\n    \"\"\"\n    Extract co-activation patterns (assemblies).\n\n    Parameters\n    ----------\n    actmat : np.ndarray\n        Activity matrix.\n    significance : object\n        Object containing significance parameters.\n    method : str\n        Method to extract assembly patterns (ica, pca).\n    whiten : str, optional\n        Whitening method, by default \"unit-variance\".\n\n    Returns\n    -------\n    np.ndarray\n        Co-activation patterns (assemblies).\n    \"\"\"\n    nassemblies = significance.nassemblies\n\n    if method == \"pca\":\n        idxs = np.argsort(-significance.explained_variance_)[0:nassemblies]\n        patterns = significance.components_[idxs, :]\n    elif method == \"ica\":\n        ica = FastICA(n_components=nassemblies, random_state=0, whiten=whiten)\n        ica.fit(actmat.T)\n        patterns = ica.components_\n    else:\n        raise ValueError(\n            \"assembly extraction method \" + str(method) + \" not understood\"\n        )\n\n    if patterns is not np.nan:\n        patterns = patterns.reshape(nassemblies, -1)\n\n        # sets norm of assembly vectors to 1\n        norms = np.linalg.norm(patterns, axis=1)\n        patterns /= np.tile(norms, [np.size(patterns, 1), 1]).T\n\n    return patterns\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.getlambdacontrol","title":"<code>getlambdacontrol(zactmat_)</code>","text":"<p>Get the maximum eigenvalue from PCA.</p> <p>Parameters:</p> Name Type Description Default <code>zactmat_</code> <code>ndarray</code> <p>Z-scored activity matrix.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Maximum eigenvalue.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def getlambdacontrol(zactmat_: np.ndarray) -&gt; float:\n    \"\"\"\n    Get the maximum eigenvalue from PCA.\n\n    Parameters\n    ----------\n    zactmat_ : np.ndarray\n        Z-scored activity matrix.\n\n    Returns\n    -------\n    float\n        Maximum eigenvalue.\n    \"\"\"\n    significance_ = PCA()\n    significance_.fit(zactmat_.T)\n    lambdamax_ = np.max(significance_.explained_variance_)\n\n    return lambdamax_\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.marcenkopastur","title":"<code>marcenkopastur(significance)</code>","text":"<p>Calculate statistical threshold from Marcenko-Pastur distribution.</p> <p>Parameters:</p> Name Type Description Default <code>significance</code> <code>object</code> <p>Object containing significance parameters.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Statistical threshold.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def marcenkopastur(significance: object) -&gt; float:\n    \"\"\"\n    Calculate statistical threshold from Marcenko-Pastur distribution.\n\n    Parameters\n    ----------\n    significance : object\n        Object containing significance parameters.\n\n    Returns\n    -------\n    float\n        Statistical threshold.\n    \"\"\"\n    nbins = significance.nbins\n    nneurons = significance.nneurons\n    tracywidom = significance.tracywidom\n\n    # calculates statistical threshold from Marcenko-Pastur distribution\n    q = float(nbins) / float(nneurons)  # note that silent neurons are counted too\n    lambdaMax = pow((1 + np.sqrt(1 / q)), 2)\n    lambdaMax += tracywidom * pow(nneurons, -2.0 / 3)  # Tracy-Widom correction\n\n    return lambdaMax\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.observed_and_shuffled_correlation","title":"<code>observed_and_shuffled_correlation(post_spikes, post_neurons, total_neurons, task_normalized, post_intervals, interval_i, num_shuffles=100)</code>","text":"<p>Calculate observed and shuffled correlations between task and post-task neural activity.</p> <p>This function computes the correlation between normalized task bias matrix and post-task bias matrix, as well as correlations with shuffled post-task data.</p> <p>Parameters:</p> Name Type Description Default <code>post_spikes</code> <code>ndarray</code> <p>Array of post-task spike times.</p> required <code>post_neurons</code> <code>ndarray</code> <p>Array of neuron IDs corresponding to post_spikes.</p> required <code>total_neurons</code> <code>int</code> <p>Total number of neurons in the dataset.</p> required <code>task_normalized</code> <code>ndarray</code> <p>Normalized bias matrix from task period.</p> required <code>post_intervals</code> <code>ndarray</code> <p>Array of post-task intervals, shape (n_intervals, 2).</p> required <code>interval_i</code> <code>int</code> <p>Index of the current interval to analyze.</p> required <code>num_shuffles</code> <code>int</code> <p>Number of times to shuffle post-task data for null distribution, by default 100.</p> <code>100</code> <p>Returns:</p> Type Description <code>Tuple[float, List[float]]</code> <p>A tuple containing: - observed_correlation: float     Cosine similarity between task and post-task bias matrices. - shuffled_correlation: List[float]     List of cosine similarities between task and shuffled post-task bias matrices.</p> Source code in <code>neuro_py/ensemble/pairwise_bias_correlation.py</code> <pre><code>def observed_and_shuffled_correlation(\n    post_spikes: np.ndarray,\n    post_neurons: np.ndarray,\n    total_neurons: int,\n    task_normalized: np.ndarray,\n    post_intervals: np.ndarray,\n    interval_i: int,\n    num_shuffles: int = 100,\n) -&gt; Tuple[float, List[float]]:\n    \"\"\"\n    Calculate observed and shuffled correlations between task and post-task neural activity.\n\n    This function computes the correlation between normalized task bias matrix and\n    post-task bias matrix, as well as correlations with shuffled post-task data.\n\n    Parameters\n    ----------\n    post_spikes : np.ndarray\n        Array of post-task spike times.\n    post_neurons : np.ndarray\n        Array of neuron IDs corresponding to post_spikes.\n    total_neurons : int\n        Total number of neurons in the dataset.\n    task_normalized : np.ndarray\n        Normalized bias matrix from task period.\n    post_intervals : np.ndarray\n        Array of post-task intervals, shape (n_intervals, 2).\n    interval_i : int\n        Index of the current interval to analyze.\n    num_shuffles : int, optional\n        Number of times to shuffle post-task data for null distribution, by default 100.\n\n    Returns\n    -------\n    Tuple[float, List[float]]\n        A tuple containing:\n        - observed_correlation: float\n            Cosine similarity between task and post-task bias matrices.\n        - shuffled_correlation: List[float]\n            List of cosine similarities between task and shuffled post-task bias matrices.\n    \"\"\"\n    # for i_interval in range(post_intervals.shape[0]):\n    idx = (post_spikes &gt; post_intervals[interval_i][0]) &amp; (\n        post_spikes &lt; post_intervals[interval_i][1]\n    )\n\n    post_bias_matrix = skew_bias_matrix(\n        post_spikes[idx], post_neurons[idx], total_neurons\n    )\n\n    # Compute cosine similarity between task and post-task bias matrices\n    observed_correlation = cosine_similarity_matrices(task_normalized, post_bias_matrix)\n\n    # Shuffle post-task spikes and compute bias matrix\n    shuffled_correlation = [\n        cosine_similarity_matrices(\n            task_normalized,\n            skew_bias_matrix(\n                post_spikes[idx],\n                np.random.permutation(post_neurons[idx]),\n                total_neurons,\n            ),\n        )\n        for _ in range(num_shuffles)\n    ]\n\n    return observed_correlation, shuffled_correlation\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.potential_landscape","title":"<code>potential_landscape(X_dyn, projbins, domainbins=None)</code>","text":"<p>Compute numerical approximation of potential energy landscape across 1D state and domain (e.g. time, position, etc.).</p> <p>Potential landscape is defined as the integral of the flow vectors.</p> <p>Parameters:</p> Name Type Description Default <code>X_dyn</code> <code>ndarray</code> <p>State vectors of shape (trials, bins).</p> required <code>projbins</code> <code>int or array - like</code> <p>Number of bins for projection axis or bin edges</p> required <code>domainbins</code> <code>int or array - like</code> <p>Number of bins for domain axis or bin edges, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Potential energy landscape across state and domain</p> <code>ndarray</code> <p>Temporal gradient of potential energy landscape across state and domain</p> <code>ndarray</code> <p>Histogram of state vectors across state and domain</p> <code>ndarray</code> <p>Bin edges of state vectors</p> <code>ndarray</code> <p>Bin edges of domain</p> References <p>.. [1] Wang, S., Falcone, R., Richmond, B. et al. Attractor dynamics reflect        decision confidence in macaque prefrontal cortex. Nat Neurosci 26,        1970\u20131980 (2023).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X_dyn = np.array([[0.1, 0.2, 0.4], [0.0, 0.3, 0.6]])\n&gt;&gt;&gt; projbins = 3\n&gt;&gt;&gt; domainbins = 3\n&gt;&gt;&gt; potential_landscape(X_dyn, projbins, domainbins)\n(array([[ 0.  ,  0.  ,   nan],\n        [-0.1 ,  0.  ,   nan],\n        [  nan,  0.  , -0.25]]),\narray([[0.3 ,  nan,  nan],\n       [0.1 ,  nan,  nan],\n       [ nan,  nan, 0.25]]),\narray([[1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 2.]]),\narray([0. , 0.1, 0.2, 0.3]),\narray([0.        , 0.33333333, 0.66666667, 1.        ]))\n</code></pre> Source code in <code>neuro_py/ensemble/dynamics.py</code> <pre><code>def potential_landscape(\n    X_dyn: np.ndarray,\n    projbins: Union[int, np.ndarray],\n    domainbins: Union[int, np.ndarray, None] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Compute numerical approximation of potential energy landscape across\n    1D state and domain (e.g. time, position, etc.).\n\n    Potential landscape is defined as the integral of the flow vectors.\n\n    Parameters\n    ----------\n    X_dyn : np.ndarray\n        State vectors of shape (trials, bins).\n    projbins : int or array-like\n        Number of bins for projection axis or bin edges\n    domainbins : int or array-like, optional\n        Number of bins for domain axis or bin edges, by default None\n\n    Returns\n    -------\n    np.ndarray\n        Potential energy landscape across state and domain\n    np.ndarray\n        Temporal gradient of potential energy landscape across state and domain\n    np.ndarray\n        Histogram of state vectors across state and domain\n    np.ndarray\n        Bin edges of state vectors\n    np.ndarray\n        Bin edges of domain\n\n    References\n    ----------\n    .. [1] Wang, S., Falcone, R., Richmond, B. et al. Attractor dynamics reflect\n           decision confidence in macaque prefrontal cortex. Nat Neurosci 26,\n           1970\u20131980 (2023).\n\n    Examples\n    --------\n    &gt;&gt;&gt; X_dyn = np.array([[0.1, 0.2, 0.4], [0.0, 0.3, 0.6]])\n    &gt;&gt;&gt; projbins = 3\n    &gt;&gt;&gt; domainbins = 3\n    &gt;&gt;&gt; potential_landscape(X_dyn, projbins, domainbins)\n    (array([[ 0.  ,  0.  ,   nan],\n            [-0.1 ,  0.  ,   nan],\n            [  nan,  0.  , -0.25]]),\n    array([[0.3 ,  nan,  nan],\n           [0.1 ,  nan,  nan],\n           [ nan,  nan, 0.25]]),\n    array([[1., 0., 0.],\n           [1., 0., 0.],\n           [0., 0., 2.]]),\n    array([0. , 0.1, 0.2, 0.3]),\n    array([0.        , 0.33333333, 0.66666667, 1.        ]))\n    \"\"\"\n    # _t suffix is following notation of paper but applicable across any domain\n    nnrns = 1\n    ntrials, nbins = X_dyn.shape\n    delta_t = np.diff(X_dyn, axis=1)  # time derivatives: ntrials x nbins-1 x nnrns\n\n    X_t_flat = np.reshape(\n        X_dyn[:, :-1], (-1, nnrns), order=\"F\"\n    ).ravel()  # skip last bin as no displacement exists for last time point\n    delta_t_flat = np.reshape(\n        delta_t, (-1, nnrns), order=\"F\"\n    ).ravel()  # column-major order\n    norm_tpts = np.repeat(np.arange(nbins - 1), ntrials)\n\n    nbins_domain = (\n        nbins - 1 if domainbins is None else domainbins\n    )  # downsample domain bins\n\n    # 1D state space binning of time derivatives across domain\n    # assumes landscape may morph across domain\n    H, bin_edges, _ = binned_statistic_dd(  # posbins x time\n        np.asarray((X_t_flat, norm_tpts)).T,\n        delta_t_flat,\n        statistic=\"count\",\n        bins=(projbins, nbins_domain),\n    )\n    latentedges, domainedges = bin_edges\n\n    grad_pos_t_svm = binned_statistic_dd(\n        np.asarray((X_t_flat, norm_tpts)).T,\n        delta_t_flat,\n        statistic=\"sum\",\n        bins=(projbins, nbins_domain),\n    ).statistic\n    # average derivative, a.k.a. flow/vector field for dynamics underlying\n    # population activity\n    grad_pos_t_svm = np.divide(grad_pos_t_svm, H, where=H != 0)\n    grad_pos_t_svm[H == 0] = np.nan  # crucial to handle division by zero\n    # spatial integration via nnancumsum treats nan as zero for cumulative sum\n    potential_pos_t = -np.nancumsum(grad_pos_t_svm, axis=0)  # projbins x domainbins\n\n    idx_zero_X_t = np.searchsorted(latentedges, 0)\n    offset = potential_pos_t[idx_zero_X_t, :]  # use potential at X_t = 0 as reference\n    potential_pos_t = potential_pos_t - offset  # potential difference\n\n    nonzero_mask = H != 0\n    idx_first_nonzero, idx_last_nonzero = find_terminal_masked_indices(\n        nonzero_mask, axis=0\n    )  # each have shape: time\n    # along axis 0 set all values from start to idx_first_nonzero to nan\n    for t in range(H.shape[1]):\n        potential_pos_t[: idx_first_nonzero[t], t] = np.nan\n        potential_pos_t[idx_last_nonzero[t] + 1 :, t] = np.nan\n\n    return potential_pos_t, grad_pos_t_svm, H, latentedges, domainedges\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.potential_landscape_nd","title":"<code>potential_landscape_nd(X_dyn, projbins, domainbins=None, nanborderempty=True)</code>","text":"<p>Compute numerical approximation of potential energy landscape across n-dimensional state and domain (e.g. time, position, etc.).</p> <p>Potential landscape is defined as the integral of the flow vectors.</p> <p>Parameters:</p> Name Type Description Default <code>X_dyn</code> <code>ndarray</code> <p>State vectors of shape (trials, bins, neurons)</p> required <code>projbins</code> <code>int or array - like</code> <p>Number of bins for projection axis or bin edges for each neuron</p> required <code>domainbins</code> <code>int or array - like</code> <p>Number of bins for domain axis or bin edges, by default None</p> <code>None</code> <code>nanborderempty</code> <code>bool</code> <p>Whether to set border values to nan if they are empty, by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Potential energy landscape across state averaged across domain for each neuron. Shape: nnrns x projbins times nnrns</p> <code>ndarray</code> <p>Potential energy landscape across state and domain for each neuron. Shape: projbins times nnrns x domainbins x nnrns</p> <code>ndarray</code> <p>Temporal gradient of potential energy landscape across state and domain for each neuron. Shape: projbins times nnrns x domainbins x nnrns</p> <code>ndarray</code> <p>Histogram of state vectors across state and domain for each neuron. Shape: projbins times nnrns x domainbins x nnrns</p> <code>ndarray</code> <p>Bin edges of state vectors for each neuron</p> <code>ndarray</code> <p>Bin edges of domain for each neuron</p> References <p>.. [1] Wang, S., Falcone, R., Richmond, B. et al. Attractor dynamics reflect        decision confidence in macaque prefrontal cortex. Nat Neurosci 26,        1970\u20131980 (2023).</p> Source code in <code>neuro_py/ensemble/dynamics.py</code> <pre><code>def potential_landscape_nd(\n    X_dyn: np.ndarray,\n    projbins: Union[int, np.ndarray],\n    domainbins: Union[int, np.ndarray, None] = None,\n    nanborderempty: bool = True,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Compute numerical approximation of potential energy landscape across\n    n-dimensional state and domain (e.g. time, position, etc.).\n\n    Potential landscape is defined as the integral of the flow vectors.\n\n    Parameters\n    ----------\n    X_dyn : np.ndarray\n        State vectors of shape (trials, bins, neurons)\n    projbins : int or array-like\n        Number of bins for projection axis or bin edges for each neuron\n    domainbins : int or array-like, optional\n        Number of bins for domain axis or bin edges, by default None\n    nanborderempty : bool, optional\n        Whether to set border values to nan if they are empty, by default True\n\n    Returns\n    -------\n    np.ndarray\n        Potential energy landscape across state averaged across domain for each\n        neuron. Shape: nnrns x projbins times nnrns\n    np.ndarray\n        Potential energy landscape across state and domain for each neuron.\n        Shape: projbins times nnrns x domainbins x nnrns\n    np.ndarray\n        Temporal gradient of potential energy landscape across state and domain\n        for each neuron. Shape: projbins times nnrns x domainbins x nnrns\n    np.ndarray\n        Histogram of state vectors across state and domain for each neuron.\n        Shape: projbins times nnrns x domainbins x nnrns\n    np.ndarray\n        Bin edges of state vectors for each neuron\n    np.ndarray\n        Bin edges of domain for each neuron\n\n    References\n    ----------\n    .. [1] Wang, S., Falcone, R., Richmond, B. et al. Attractor dynamics reflect\n           decision confidence in macaque prefrontal cortex. Nat Neurosci 26,\n           1970\u20131980 (2023).\n    \"\"\"\n    # _t suffix is following notation of paper but applicable across any domain\n    ntrials, nbins, nnrns = X_dyn.shape\n    delta_t = np.diff(\n        X_dyn, axis=1\n    )  # time derivatives: ntrials x ndomainbins-1 x nnrns\n\n    X_t_flat = np.reshape(\n        X_dyn[:, :-1], (-1, nnrns), order=\"F\"\n    )  # skip last bin as no displacement exists for last time point\n    delta_t_flat = np.reshape(delta_t, (-1, nnrns), order=\"F\")  # column-major order\n    norm_tpts = np.repeat(np.arange(nbins - 1), ntrials)\n\n    nbins_domain = (\n        nbins - 1 if domainbins is None else domainbins\n    )  # downsample domain bins\n\n    potential_pos_t_nrns = []\n    grad_pos_t_svm_nrns = []\n    hist_nrns = []\n    latentedges_nrns = []\n    domainedges_nrns = []\n    for nnrn in range(nnrns):\n        # 1D state space binning of time derivatives across domain\n        # assumes landscape may morph across domain\n        H, bin_edges, _ = binned_statistic_dd(  # (nnrns times projbins) x time\n            np.asarray((*X_t_flat.T, norm_tpts)).T,\n            delta_t_flat[:, nnrn],\n            statistic=\"count\",\n            bins=(\n                *[\n                    projbins if isinstance(projbins, int) else projbins[idx]\n                    for idx in range(nnrns)\n                ],\n                nbins_domain,\n            ),\n        )\n        latentedges = bin_edges[nnrn]\n        domainedges = bin_edges[-1]\n\n        grad_pos_t_svm = binned_statistic_dd(\n            np.asarray((*X_t_flat.T, norm_tpts)).T,\n            delta_t_flat[:, nnrn],\n            statistic=\"sum\",\n            bins=(\n                *[\n                    projbins if isinstance(projbins, int) else projbins[idx]\n                    for idx in range(nnrns)\n                ],\n                nbins_domain,\n            ),\n        ).statistic\n        # average derivative, a.k.a. flow/vector field for dynamics underlying\n        # population activity\n        grad_pos_t_svm = np.divide(grad_pos_t_svm, H, where=H != 0)\n        grad_pos_t_svm[H == 0] = np.nan  # crucial to handle division by zero\n        # spatial integration via nnancumsum treats nan as zero for cumulative sum\n        potential_pos_t = -np.nancumsum(\n            grad_pos_t_svm, axis=nnrn\n        )  # (nnrns times projbins) x domainbins\n\n        if nanborderempty:\n            nonzero_mask = H != 0\n\n            for t in range(nbins_domain):\n                nrndimslices = [slice(None)] * nnrns\n                nrndimslices.append(t)\n                peripheral_zeros_nanmask = ~np.isnan(\n                    replace_border_zeros_with_nan(nonzero_mask[tuple(nrndimslices)])\n                )\n                peripheral_zeros_nanmask = np.where(\n                    peripheral_zeros_nanmask, peripheral_zeros_nanmask, np.nan\n                )\n                potential_pos_t[tuple(nrndimslices)] *= peripheral_zeros_nanmask\n\n        potential_pos_t_nrns.append(potential_pos_t)\n        grad_pos_t_svm_nrns.append(grad_pos_t_svm)\n        hist_nrns.append(H)\n        latentedges_nrns.append(latentedges)\n        domainedges_nrns.append(domainedges)\n\n    potential_pos_t_nrns = np.stack(\n        potential_pos_t_nrns, axis=-1\n    )  # projbins x domainbins x nnrns\n    grad_pos_t_svm_nrns = np.stack(\n        grad_pos_t_svm_nrns, axis=-1\n    )  # projbins x domainbins x nnrns\n    hist = np.stack(hist_nrns, axis=-1)  # projbins x domainbins x nnrns\n    latentedges_nrns = np.stack(latentedges_nrns, axis=-1)  # projbins x nnrns\n    domainedges_nrns = np.stack(domainedges_nrns, axis=-1)  # domainbins x nnrns\n    nrndimslices = [slice(None)] * (nnrns + 1)\n    nrndimslices.append(0)\n    potential_nrns_pos = []\n    for nrn in range(nnrns):\n        nrndimslices[-1] = nrn\n        potential_nrns_pos.append(\n            np.nanmean(\n                potential_pos_t_nrns[tuple(nrndimslices)], axis=-1\n            )  # average across domainbins\n        )\n    potential_nrns_pos = np.asarray(potential_nrns_pos)  # nnrns x nnrns times projbins\n\n    return (\n        potential_nrns_pos,\n        potential_pos_t_nrns,\n        grad_pos_t_svm_nrns,\n        hist,\n        latentedges_nrns,\n        domainedges_nrns,\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.proximity","title":"<code>proximity(pv1, pv2)</code>","text":"<p>Proximity between two firing rate vector trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>pv1</code> <code>ndarray</code> <p>Firing rate vector trajectory in one context. Shape: (num_bins, num_neurons)</p> required <code>pv2</code> <code>ndarray</code> <p>Firing rate vector trajectory in another context. Shape: (num_bins, num_neurons)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Proximity between the two contexts.</p> References <p>.. [1] Guidera, J. A., Gramling, D. P., Comrie, A. E., Joshi, A.,     Denovellis, E. L., Lee, K. H., Zhou, J., Thompson, P., Hernandez, J.,     Yorita, A., Haque, R., Kirst, C., &amp; Frank, L. M. (2024). Regional     specialization manifests in the reliability of neural population codes.     bioRxiv : the preprint server for biology, 2024.01.25.576941.     https://doi.org/10.1101/2024.01.25.576941</p> Source code in <code>neuro_py/ensemble/geometry.py</code> <pre><code>def proximity(pv1: np.ndarray, pv2: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Proximity between two firing rate vector trajectories.\n\n    Parameters\n    ----------\n    pv1 : numpy.ndarray\n        Firing rate vector trajectory in one context.\n        Shape: (num_bins, num_neurons)\n\n    pv2 : numpy.ndarray\n        Firing rate vector trajectory in another context.\n        Shape: (num_bins, num_neurons)\n\n    Returns\n    -------\n    numpy.ndarray\n        Proximity between the two contexts.\n\n    References\n    ----------\n    .. [1] Guidera, J. A., Gramling, D. P., Comrie, A. E., Joshi, A.,\n        Denovellis, E. L., Lee, K. H., Zhou, J., Thompson, P., Hernandez, J.,\n        Yorita, A., Haque, R., Kirst, C., &amp; Frank, L. M. (2024). Regional\n        specialization manifests in the reliability of neural population codes.\n        bioRxiv : the preprint server for biology, 2024.01.25.576941.\n        https://doi.org/10.1101/2024.01.25.576941\n    \"\"\"\n    # Calculate the norms\n    norm_diff = np.linalg.norm(pv1 - pv2, axis=1)\n\n    norm_diff_mean = np.apply_along_axis(\n        lambda e: np.mean(np.linalg.norm(e - pv2, axis=1)),\n        arr=pv1,\n        axis=1\n    )\n\n    # Calculate proximity\n    prox = 1 - (norm_diff / norm_diff_mean)\n\n    return prox\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.runPatterns","title":"<code>runPatterns(actmat, method='ica', nullhyp='mp', nshu=1000, percentile=99, tracywidom=False, whiten='unit-variance', nassemblies=None)</code>","text":"<p>Run pattern detection to identify cell assemblies.</p> <p>Parameters:</p> Name Type Description Default <code>actmat</code> <code>ndarray</code> <p>Activity matrix (neurons, time bins).</p> required <code>method</code> <code>str</code> <p>Method to extract assembly patterns (ica, pca), by default \"ica\".</p> <code>'ica'</code> <code>nullhyp</code> <code>str</code> <p>Null hypothesis method (bin, circ, mp), by default \"mp\".</p> <code>'mp'</code> <code>nshu</code> <code>int</code> <p>Number of shuffling controls, by default 1000.</p> <code>1000</code> <code>percentile</code> <code>int</code> <p>Percentile for shuffling methods, by default 99.</p> <code>99</code> <code>tracywidom</code> <code>bool</code> <p>Use Tracy-Widom correction, by default False.</p> <code>False</code> <code>whiten</code> <code>str</code> <p>Whitening method, by default \"unit-variance\".</p> <code>'unit-variance'</code> <code>nassemblies</code> <code>Optional[int]</code> <p>Number of assemblies, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Union[ndarray, None], object, Union[ndarray, None]], None]</code> <p>Patterns, significance object, and z-scored activity matrix.</p> Notes <p>nullhyp     'bin' - bin shuffling, will shuffle time bins of each neuron independently     'circ' - circular shuffling, will shift time bins of each neuron independently     'mp' - Marcenko-Pastur distribution - analytical threshold</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def runPatterns(\n    actmat: np.ndarray,\n    method: str = \"ica\",\n    nullhyp: str = \"mp\",\n    nshu: int = 1000,\n    percentile: int = 99,\n    tracywidom: bool = False,\n    whiten: str = \"unit-variance\",\n    nassemblies: int = None,\n) -&gt; Union[Tuple[Union[np.ndarray, None], object, Union[np.ndarray, None]], None]:\n    \"\"\"\n    Run pattern detection to identify cell assemblies.\n\n    Parameters\n    ----------\n    actmat : np.ndarray\n        Activity matrix (neurons, time bins).\n    method : str, optional\n        Method to extract assembly patterns (ica, pca), by default \"ica\".\n    nullhyp : str, optional\n        Null hypothesis method (bin, circ, mp), by default \"mp\".\n    nshu : int, optional\n        Number of shuffling controls, by default 1000.\n    percentile : int, optional\n        Percentile for shuffling methods, by default 99.\n    tracywidom : bool, optional\n        Use Tracy-Widom correction, by default False.\n    whiten : str, optional\n        Whitening method, by default \"unit-variance\".\n    nassemblies : Optional[int], optional\n        Number of assemblies, by default None.\n\n    Returns\n    -------\n    Union[Tuple[Union[np.ndarray, None], object, Union[np.ndarray, None]], None]\n        Patterns, significance object, and z-scored activity matrix.\n\n    Notes\n    -----\n    nullhyp\n        'bin' - bin shuffling, will shuffle time bins of each neuron independently\n        'circ' - circular shuffling, will shift time bins of each neuron independently\n        'mp' - Marcenko-Pastur distribution - analytical threshold\n    \"\"\"\n\n    nneurons = np.size(actmat, 0)\n    nbins = np.size(actmat, 1)\n\n    silentneurons = np.var(actmat, axis=1) == 0\n    actmat_ = actmat[~silentneurons, :]\n    if actmat_.shape[0] == 0:\n        warnings.warn(\"no active neurons\")\n        return None, None, None\n\n    # z-scoring activity matrix\n    zactmat_ = stats.zscore(actmat_, axis=1)\n\n    # running significance (estimating number of assemblies)\n    significance = PCA()\n    significance.fit(zactmat_.T)\n    significance.nneurons = nneurons\n    significance.nbins = nbins\n    significance.nshu = nshu\n    significance.percentile = percentile\n    significance.tracywidom = tracywidom\n    significance.nullhyp = nullhyp\n    significance = runSignificance(zactmat_, significance)\n\n    if nassemblies is not None:\n        significance.nassemblies = nassemblies\n\n    if np.isnan(significance.nassemblies):\n        return None, significance, None\n\n    if significance.nassemblies &lt; 1:\n        warnings.warn(\"no assembly detected\")\n\n        patterns = None\n        zactmat = None\n    else:\n        # extracting co-activation patterns\n        patterns_ = extractPatterns(zactmat_, significance, method, whiten=whiten)\n        if patterns_ is np.nan:\n            return None\n\n        # putting eventual silent neurons back (their assembly weights are defined as zero)\n        patterns = np.zeros((np.size(patterns_, 0), nneurons))\n        patterns[:, ~silentneurons] = patterns_\n        zactmat = np.copy(actmat)\n        zactmat[~silentneurons, :] = zactmat_\n\n    return patterns, significance, zactmat\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.runSignificance","title":"<code>runSignificance(zactmat, significance)</code>","text":"<p>Run significance tests to estimate the number of assemblies.</p> <p>Parameters:</p> Name Type Description Default <code>zactmat</code> <code>ndarray</code> <p>Z-scored activity matrix.</p> required <code>significance</code> <code>object</code> <p>Object containing significance parameters.</p> required <p>Returns:</p> Type Description <code>object</code> <p>Updated significance object with the number of assemblies.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def runSignificance(zactmat: np.ndarray, significance: object) -&gt; object:\n    \"\"\"\n    Run significance tests to estimate the number of assemblies.\n\n    Parameters\n    ----------\n    zactmat : np.ndarray\n        Z-scored activity matrix.\n    significance : object\n        Object containing significance parameters.\n\n    Returns\n    -------\n    object\n        Updated significance object with the number of assemblies.\n    \"\"\"\n    if significance.nullhyp == \"mp\":\n        lambdaMax = marcenkopastur(significance)\n    elif significance.nullhyp == \"bin\":\n        lambdaMax = binshuffling(zactmat, significance)\n    elif significance.nullhyp == \"circ\":\n        lambdaMax = circshuffling(zactmat, significance)\n    else:\n        raise ValueError(\n            \"nyll hypothesis method \" + str(significance.nullhyp) + \" not understood\"\n        )\n\n    nassemblies = np.sum(significance.explained_variance_ &gt; lambdaMax)\n    significance.nassemblies = nassemblies\n\n    return significance\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.shuffle_and_score","title":"<code>shuffle_and_score(posterior_array, w, normalize, tc, ds, dp)</code>","text":"<p>Shuffle the posterior array and compute scores and weighted correlations.</p> <p>Parameters:</p> Name Type Description Default <code>posterior_array</code> <code>ndarray</code> <p>The posterior probability array.</p> required <code>w</code> <code>ndarray</code> <p>Weights array.</p> required <code>normalize</code> <code>bool</code> <p>Whether to normalize the scores.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, float, float]</code> <p>Scores and weighted correlations for time-swapped and column-cycled arrays.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def shuffle_and_score(\n    posterior_array: np.ndarray,\n    w: np.ndarray,\n    normalize: bool,\n    tc: float,\n    ds: float,\n    dp: float,\n) -&gt; Tuple[np.ndarray, np.ndarray, float, float]:\n    \"\"\"\n    Shuffle the posterior array and compute scores and weighted correlations.\n\n    Parameters\n    ----------\n    posterior_array : np.ndarray\n        The posterior probability array.\n    w : np.ndarray\n        Weights array.\n    normalize : bool\n        Whether to normalize the scores.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, float, float]\n        Scores and weighted correlations for time-swapped and column-cycled arrays.\n    \"\"\"\n\n    posterior_ts = replay.time_swap_array(posterior_array)\n    posterior_cs = replay.column_cycle_array(posterior_array)\n\n    scores_time_swap = replay.trajectory_score_array(\n        posterior=posterior_ts, w=w, normalize=normalize\n    )\n    scores_col_cycle = replay.trajectory_score_array(\n        posterior=posterior_cs, w=w, normalize=normalize\n    )\n\n    weighted_corr_time_swap = weighted_correlation(posterior_ts)\n    weighted_corr_col_cycle = weighted_correlation(posterior_cs)\n\n    return (\n        scores_time_swap,\n        scores_col_cycle,\n        weighted_corr_time_swap,\n        weighted_corr_col_cycle,\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.shuffled_significance","title":"<code>shuffled_significance(task_spikes, task_neurons, post_spikes, post_neurons, total_neurons, post_intervals=np.array([[-np.inf, np.inf]]), num_shuffles=100, n_jobs=-1)</code>","text":"<p>Computes the significance of the task-post correlation by comparing against shuffled distributions.</p> <p>Parameters:</p> Name Type Description Default <code>task_spikes</code> <code>ndarray</code> <p>Spike timestamps during the task. Shape is (n_spikes_task,)</p> required <code>task_neurons</code> <code>ndarray</code> <p>Neuron identifiers corresponding to each of <code>task_spikes</code>. Shape is (n_spikes_task,)</p> required <code>post_spikes</code> <code>ndarray</code> <p>Spike timestamps during post-task (e.g., sleep). Shape is (n_spikes_post,)</p> required <code>post_neurons</code> <code>ndarray</code> <p>Neuron identifiers corresponding to <code>post_spikes</code>. Shape is (n_spikes_post,)</p> required <code>total_neurons</code> <code>int</code> <p>Total number of neurons being considered</p> required <code>post_intervals</code> <code>ndarray</code> <p>Intervals for post-task epochs, with shape (n_intervals, 2). Each row defines the start and end of an interval. May correspond to specific sleep states. Default is <code>np.array([[-np.inf, np.inf]])</code>, representing the entire range of post-task epochs</p> <code>array([[-inf, inf]])</code> <code>num_shuffles</code> <code>int</code> <p>Number of shuffles to compute the significance. Default is 100</p> <code>100</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to use for shuffling. Default is -1 (use all available cores).</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>z_score</code> <code>ndarray</code> <p>Z-scores of the observed correlations compared to the shuffled distributions.  Shape is (n_intervals,).</p> <code>p_value</code> <code>ndarray</code> <p>P-values indicating the significance of the observed correlation.  Shape is (n_intervals,).</p> Notes <p>The function uses parallel processing to compute observed and shuffled  correlations for each post-task interval. The z-score is calculated as:</p> <pre><code>z_score = (observed_correlation - mean(shuffled_correlations)) / std(shuffled_correlations)\n</code></pre> <p>The p-value is computed as the proportion of shuffled correlations greater than  the observed correlation, with a small constant added for numerical stability.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; task_spikes = np.array([1.2, 3.4, 5.6])\n&gt;&gt;&gt; task_neurons = np.array([0, 1, 0])\n&gt;&gt;&gt; post_spikes = np.array([2.3, 4.5, 6.7])\n&gt;&gt;&gt; post_neurons = np.array([1, 0, 1])\n&gt;&gt;&gt; total_neurons = 2\n&gt;&gt;&gt; post_intervals = np.array([[0, 10]])\n&gt;&gt;&gt; z_score, p_value = shuffled_significance(task_spikes, task_neurons, post_spikes, post_neurons, total_neurons, post_intervals)\n&gt;&gt;&gt; z_score\narray([1.23])\n&gt;&gt;&gt; p_value\narray([0.04])\n</code></pre> Source code in <code>neuro_py/ensemble/pairwise_bias_correlation.py</code> <pre><code>def shuffled_significance(\n    task_spikes: np.ndarray,\n    task_neurons: np.ndarray,\n    post_spikes: np.ndarray,\n    post_neurons: np.ndarray,\n    total_neurons: int,\n    post_intervals: np.ndarray = np.array([[-np.inf, np.inf]]),\n    num_shuffles: int = 100,\n    n_jobs: int = -1,\n):\n    \"\"\"\n    Computes the significance of the task-post correlation by comparing against shuffled distributions.\n\n    Parameters\n    ----------\n    task_spikes : np.ndarray\n        Spike timestamps during the task. Shape is (n_spikes_task,)\n    task_neurons : np.ndarray\n        Neuron identifiers corresponding to each of `task_spikes`. Shape is\n        (n_spikes_task,)\n    post_spikes : np.ndarray\n        Spike timestamps during post-task (e.g., sleep). Shape is\n        (n_spikes_post,)\n    post_neurons : np.ndarray\n        Neuron identifiers corresponding to `post_spikes`. Shape is\n        (n_spikes_post,)\n    total_neurons : int\n        Total number of neurons being considered\n    post_intervals : np.ndarray, optional\n        Intervals for post-task epochs, with shape (n_intervals, 2).\n        Each row defines the start and end of an interval. May correspond to\n        specific sleep states. Default is `np.array([[-np.inf, np.inf]])`,\n        representing the entire range of post-task epochs\n    num_shuffles : int, optional\n        Number of shuffles to compute the significance. Default is 100\n    n_jobs : int, optional\n        Number of parallel jobs to use for shuffling. Default is -1 (use all\n        available cores).\n\n    Returns\n    -------\n    z_score : np.ndarray\n        Z-scores of the observed correlations compared to the shuffled distributions. \n        Shape is (n_intervals,).\n    p_value : np.ndarray\n        P-values indicating the significance of the observed correlation. \n        Shape is (n_intervals,).\n\n    Notes\n    -----\n    The function uses parallel processing to compute observed and shuffled \n    correlations for each post-task interval. The z-score is calculated as:\n\n        z_score = (observed_correlation - mean(shuffled_correlations)) / std(shuffled_correlations)\n\n    The p-value is computed as the proportion of shuffled correlations greater than \n    the observed correlation, with a small constant added for numerical stability.\n\n    Examples\n    --------\n    &gt;&gt;&gt; task_spikes = np.array([1.2, 3.4, 5.6])\n    &gt;&gt;&gt; task_neurons = np.array([0, 1, 0])\n    &gt;&gt;&gt; post_spikes = np.array([2.3, 4.5, 6.7])\n    &gt;&gt;&gt; post_neurons = np.array([1, 0, 1])\n    &gt;&gt;&gt; total_neurons = 2\n    &gt;&gt;&gt; post_intervals = np.array([[0, 10]])\n    &gt;&gt;&gt; z_score, p_value = shuffled_significance(task_spikes, task_neurons, post_spikes, post_neurons, total_neurons, post_intervals)\n    &gt;&gt;&gt; z_score\n    array([1.23])\n    &gt;&gt;&gt; p_value\n    array([0.04])\n    \"\"\"\n    # set random seed for reproducibility\n    np.random.seed(0)\n\n    # Compute bias matrices for task epochs\n    task_bias_matrix = skew_bias_matrix(\n        task_spikes, task_neurons, total_neurons\n    )\n\n    # Get shuffled and observed correlations using parallel processing\n    observed_correlation, shuffled_correlations = zip(\n        *Parallel(n_jobs=n_jobs)(\n            delayed(observed_and_shuffled_correlation)(\n                post_spikes,\n                post_neurons,\n                total_neurons,\n                task_bias_matrix,\n                post_intervals,\n                interval_i,\n                num_shuffles,\n            )\n            for interval_i in range(post_intervals.shape[0])\n        )\n    )\n    observed_correlation, shuffled_correlations = np.array(\n        observed_correlation\n    ), np.array(shuffled_correlations)\n    # Compute z-score\n    shuffled_mean = np.mean(shuffled_correlations, axis=1)\n    shuffled_std = np.std(shuffled_correlations, axis=1)\n    z_score = (observed_correlation - shuffled_mean) / shuffled_std\n\n    # significance test between the observed correlation and the shuffled distribution\n    p_value = (np.sum(shuffled_correlations.T &gt; observed_correlation, axis=0) + 1) / (\n        num_shuffles + 1\n    )\n\n    return z_score, p_value\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.skew_bias_matrix","title":"<code>skew_bias_matrix(spike_times, neuron_ids, total_neurons, fillneutral=0)</code>","text":"<p>Compute the pairwise skew-bias matrix for a given sequence of spikes.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>ndarray</code> <p>Spike times for the sequence, assumed to be sorted.</p> required <code>neuron_ids</code> <code>ndarray</code> <p>Neuron identifiers corresponding to <code>spike_times</code>. Values should be integers between 0 and <code>total_neurons - 1</code>.</p> required <code>total_neurons</code> <code>int</code> <p>Total number of neurons being considered.</p> required <code>fillneutral</code> <code>float</code> <p>Value to fill for neutral bias, by default 0</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Skew-bias matrix of size <code>(total_neurons, total_neurons)</code> where each entry represents the normalized bias between neuron pairs.</p> Notes <p>The probability-bias ( B_{ij} ) for neurons ( i ) and ( j ) is computed as: [ B_{ij} = \\frac{nspikes_{ij}}{nspikes_i \\cdot nspikes_j} ] where ( nspikes_{ij} ) is the count of spikes from neuron ( i ) occurring before spikes from neuron ( j ). If there are no spikes for either neuron, the bias is set to 0.5 (neutral bias).</p> <p>The skew-bias matrix is computed as: [ S_{ij} = 2 \\cdot B_{ij} - 1 ] where ( B_{ij} ) is the probability-bias matrix.</p> <p>The skew-bias matrix is a skew-symmetric matrix as ( S_{ij} = -S_{ji} ). The values are normalized between -1 and 1. A value of 1 indicates that neuron ( i ) spikes before neuron ( j ) in all cases, while -1 indicates the opposite. A value of 0 indicates that the order of spikes is random.</p> References <p>.. [1] Roth, Z. (2016). Analysis of neuronal sequences using pairwise     biases. arXiv, 11-16. https://arxiv.org/abs/1603.02916</p> Source code in <code>neuro_py/ensemble/pairwise_bias_correlation.py</code> <pre><code>@njit\ndef skew_bias_matrix(\n    spike_times: np.ndarray,\n    neuron_ids: np.ndarray,\n    total_neurons: int,\n    fillneutral: float = 0\n) -&gt; np.ndarray:\n    r\"\"\"\n    Compute the pairwise skew-bias matrix for a given sequence of spikes.\n\n    Parameters\n    ----------\n    spike_times : numpy.ndarray\n        Spike times for the sequence, assumed to be sorted.\n    neuron_ids : numpy.ndarray\n        Neuron identifiers corresponding to `spike_times`.\n        Values should be integers between 0 and `total_neurons - 1`.\n    total_neurons : int\n        Total number of neurons being considered.\n    fillneutral : float, optional\n        Value to fill for neutral bias, by default 0\n\n    Returns\n    -------\n    numpy.ndarray\n        Skew-bias matrix of size `(total_neurons, total_neurons)` where\n        each entry represents the normalized bias between neuron pairs.\n\n    Notes\n    -----\n    The probability-bias \\( B_{ij} \\) for neurons \\( i \\) and \\( j \\) is\n    computed as:\n    \\[\n    B_{ij} = \\frac{nspikes_{ij}}{nspikes_i \\cdot nspikes_j}\n    \\]\n    where \\( nspikes_{ij} \\) is the count of spikes from neuron \\( i \\)\n    occurring before spikes from neuron \\( j \\). If there are no spikes for\n    either neuron, the bias is set to 0.5 (neutral bias).\n\n    The skew-bias matrix is computed as:\n    \\[\n    S_{ij} = 2 \\cdot B_{ij} - 1\n    \\]\n    where \\( B_{ij} \\) is the probability-bias matrix.\n\n    The skew-bias matrix is a skew-symmetric matrix as \\( S_{ij} = -S_{ji} \\).\n    The values are normalized between -1 and 1. A value of 1 indicates that\n    neuron \\( i \\) spikes before neuron \\( j \\) in all cases, while -1 indicates\n    the opposite. A value of 0 indicates that the order of spikes is random.\n\n    References\n    ----------\n    .. [1] Roth, Z. (2016). Analysis of neuronal sequences using pairwise\n        biases. arXiv, 11-16. https://arxiv.org/abs/1603.02916\n    \"\"\"\n    bias = np.empty((total_neurons, total_neurons))\n    nrn_spk_rindices = np.empty(total_neurons+1, dtype=np.int64)\n    nrn_spk_rindices[0] = 0\n\n    nrns_st = numba.typed.List()\n    for _ in range(total_neurons):\n        nrns_st.append(numba.typed.List.empty_list(np.float64))\n    for i, nrn_id in enumerate(neuron_ids):\n        nrns_st[nrn_id].append(spike_times[i])\n    for nnrn in range(total_neurons):\n        nrn_spk_rindices[nnrn+1] = nrn_spk_rindices[nnrn] + len(nrns_st[nnrn])\n\n    nrns_st_all = np.empty(nrn_spk_rindices[-1], dtype=np.float64)\n    for nnrn in range(total_neurons):\n        nrns_st_all[nrn_spk_rindices[nnrn]:nrn_spk_rindices[nnrn+1]] = np.asarray(nrns_st[nnrn])\n\n    # Build bias matrix\n    for i in range(total_neurons):\n        spikes_i = nrns_st_all[nrn_spk_rindices[i]:nrn_spk_rindices[i+1]]\n        nspikes_i = len(spikes_i)\n\n        for j in range(i + 1, total_neurons):\n            nspikes_j = len(nrns_st[j])\n            if (nspikes_i == 0) or (nspikes_j == 0):\n                bias[i, j] = bias[j, i] = fillneutral\n            else:\n                spikes_j = nrns_st_all[\n                    nrn_spk_rindices[j]:nrn_spk_rindices[j+1]]\n\n                nspikes_ij = np.searchsorted(\n                    spikes_i, spikes_j, side='right').sum()\n                bias[i, j] = 2 * (nspikes_ij / (nspikes_i * nspikes_j)) - 1\n                bias[j, i] = -bias[i, j]\n\n    # set diagonal to fillneutral\n    for i in range(total_neurons):\n        bias[i, i] = fillneutral\n\n    return bias\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.toyExample","title":"<code>toyExample(assemblies, nneurons=10, nbins=1000, rate=1.0)</code>","text":"<p>Generate a toy example activity matrix with assemblies.</p> <p>Parameters:</p> Name Type Description Default <code>assemblies</code> <code>ToyAssemblies</code> <p>The toy assemblies.</p> required <code>nneurons</code> <code>int</code> <p>Number of neurons, by default 10.</p> <code>10</code> <code>nbins</code> <code>int</code> <p>Number of time bins, by default 1000.</p> <code>1000</code> <code>rate</code> <code>float</code> <p>Poisson rate, by default 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Activity matrix.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def toyExample(\n    assemblies: \"ToyAssemblies\",\n    nneurons: int = 10,\n    nbins: int = 1000,\n    rate: float = 1.0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a toy example activity matrix with assemblies.\n\n    Parameters\n    ----------\n    assemblies : ToyAssemblies\n        The toy assemblies.\n    nneurons : int, optional\n        Number of neurons, by default 10.\n    nbins : int, optional\n        Number of time bins, by default 1000.\n    rate : float, optional\n        Poisson rate, by default 1.0.\n\n    Returns\n    -------\n    np.ndarray\n        Activity matrix.\n    \"\"\"\n    np.random.seed(42)\n\n    actmat = np.random.poisson(rate, nneurons * nbins).reshape(nneurons, nbins)\n    assemblies.actbins = [None] * len(assemblies.membership)\n    for ai, members in enumerate(assemblies.membership):\n        members = np.array(members)\n        nact = int(nbins * assemblies.actrate[ai])\n        actstrength_ = rate * assemblies.actstrength[ai]\n\n        actbins = np.argsort(np.random.rand(nbins))[0:nact]\n\n        actmat[members.reshape(-1, 1), actbins] = (\n            np.ones((len(members), nact)) + actstrength_\n        )\n\n        assemblies.actbins[ai] = np.sort(actbins)\n\n    return actmat\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.trajectory_score_bst","title":"<code>trajectory_score_bst(bst, tuningcurve, w=None, n_shuffles=1000, weights=None, normalize=False, parallel=True)</code>","text":"<p>Calculate trajectory scores and weighted correlations for Bayesian spike train decoding.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train object.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve object.</p> required <code>w</code> <code>Optional[int]</code> <p>Window size, by default None.</p> <code>None</code> <code>n_shuffles</code> <code>int</code> <p>Number of shuffles, by default 1000.</p> <code>1000</code> <code>weights</code> <code>Optional[ndarray]</code> <p>Weights array, by default None.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the scores, by default False.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to run in parallel, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray],</p> <code>]</code> <p>Scores and weighted correlations for original, time-swapped, and column-cycled arrays.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def trajectory_score_bst(\n    bst: BinnedSpikeTrainArray,\n    tuningcurve: TuningCurve1D,\n    w: Optional[int] = None,\n    n_shuffles: int = 1000,\n    weights: Optional[np.ndarray] = None,\n    normalize: bool = False,\n    parallel: bool = True,\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n    Tuple[np.ndarray, np.ndarray],\n]:\n    \"\"\"\n    Calculate trajectory scores and weighted correlations for Bayesian spike train decoding.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train object.\n    tuningcurve : TuningCurve1D\n        Tuning curve object.\n    w : Optional[int], optional\n        Window size, by default None.\n    n_shuffles : int, optional\n        Number of shuffles, by default 1000.\n    weights : Optional[np.ndarray], optional\n        Weights array, by default None.\n    normalize : bool, optional\n        Whether to normalize the scores, by default False.\n    parallel : bool, optional\n        Whether to run in parallel, by default True.\n\n    Returns\n    -------\n    Union[\n        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n        Tuple[np.ndarray, np.ndarray],\n    ]\n        Scores and weighted correlations for original, time-swapped, and column-cycled arrays.\n    \"\"\"\n\n    if w is None:\n        w = 0\n    if not float(w).is_integer:\n        raise ValueError(\"w has to be an integer!\")\n\n    if float(n_shuffles).is_integer:\n        n_shuffles = int(n_shuffles)\n    else:\n        raise ValueError(\"n_shuffles must be an integer!\")\n\n    posterior, bdries, _, _ = decode(bst=bst, ratemap=tuningcurve)\n\n    num_cores = 1\n\n    if parallel:\n        # all but one core\n        num_cores = multiprocessing.cpu_count() - 1\n\n    ds, dp = bst.ds, np.diff(tuningcurve.bins)[0]\n\n    (\n        scores,\n        weighted_corr,\n        scores_time_swap,\n        scores_col_cycle,\n        weighted_corr_time_swap,\n        weighted_corr_col_cycle,\n    ) = zip(\n        *Parallel(n_jobs=num_cores)(\n            delayed(_shuffle_and_score)(\n                posterior[:, bdries[idx] : bdries[idx + 1]],\n                tuningcurve,\n                w,\n                normalize,\n                ds,\n                dp,\n                n_shuffles,\n            )\n            for idx in range(bst.n_epochs)\n        )\n    )\n\n    if n_shuffles &gt; 0:\n        return (\n            np.array(scores),\n            np.array(weighted_corr),\n            np.array(scores_time_swap).T,\n            np.array(scores_col_cycle).T,\n            np.array(weighted_corr_time_swap).T,\n            np.array(weighted_corr_col_cycle).T,\n        )\n    return scores, weighted_corr\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.weighted_corr_2d","title":"<code>weighted_corr_2d(weights, x_coords=None, y_coords=None, time_coords=None)</code>","text":"<p>Calculate the weighted correlation between the X and Y dimensions of the matrix.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>ndarray</code> <p>A matrix of weights.</p> required <code>x_coords</code> <code>Optional[ndarray]</code> <p>X-values for each column and row, by default None.</p> <code>None</code> <code>y_coords</code> <code>Optional[ndarray]</code> <p>Y-values for each column and row, by default None.</p> <code>None</code> <code>time_coords</code> <code>Optional[ndarray]</code> <p>Time-values for each column and row, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[float, ndarray, ndarray, float, float, float, float]</code> <p>The weighted correlation coefficient, x trajectory, y trajectory, slope_x, slope_y, mean_x, mean_y.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; weights = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n&gt;&gt;&gt; x_coords = np.array([0, 1])\n&gt;&gt;&gt; y_coords = np.array([0, 1])\n&gt;&gt;&gt; time_coords = np.array([0, 1, 2])\n&gt;&gt;&gt; weighted_corr_2d(weights, x_coords, y_coords, time_coords)\n</code></pre> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def weighted_corr_2d(\n    weights: np.ndarray,\n    x_coords: Optional[np.ndarray] = None,\n    y_coords: Optional[np.ndarray] = None,\n    time_coords: Optional[np.ndarray] = None,\n) -&gt; Tuple[float, np.ndarray, np.ndarray, float, float, float, float]:\n    \"\"\"\n    Calculate the weighted correlation between the X and Y dimensions of the matrix.\n\n    Parameters\n    ----------\n    weights : np.ndarray\n        A matrix of weights.\n    x_coords : Optional[np.ndarray], optional\n        X-values for each column and row, by default None.\n    y_coords : Optional[np.ndarray], optional\n        Y-values for each column and row, by default None.\n    time_coords : Optional[np.ndarray], optional\n        Time-values for each column and row, by default None.\n\n    Returns\n    -------\n    Tuple[float, np.ndarray, np.ndarray, float, float, float, float]\n        The weighted correlation coefficient, x trajectory, y trajectory,\n        slope_x, slope_y, mean_x, mean_y.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; weights = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n    &gt;&gt;&gt; x_coords = np.array([0, 1])\n    &gt;&gt;&gt; y_coords = np.array([0, 1])\n    &gt;&gt;&gt; time_coords = np.array([0, 1, 2])\n    &gt;&gt;&gt; weighted_corr_2d(weights, x_coords, y_coords, time_coords)\n\n    \"\"\"\n    x_dim, y_dim, t_dim = weights.shape\n    if x_coords is None:\n        x_coords = np.arange(x_dim)\n    if y_coords is None:\n        y_coords = np.arange(y_dim)\n    if time_coords is None:\n        time_coords = np.arange(t_dim)\n    return __weighted_corr_2d_jit(weights, x_coords, y_coords, time_coords)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/#neuro_py.ensemble.weighted_correlation","title":"<code>weighted_correlation(posterior, time=None, place_bin_centers=None)</code>","text":"<p>Calculate the weighted correlation between time and place bin centers using a posterior probability matrix.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>A 2D numpy array representing the posterior probability matrix.</p> required <code>time</code> <code>Optional[ndarray]</code> <p>A 1D numpy array representing the time bins, by default None.</p> <code>None</code> <code>place_bin_centers</code> <code>Optional[ndarray]</code> <p>A 1D numpy array representing the place bin centers, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The weighted correlation coefficient.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def weighted_correlation(\n    posterior: np.ndarray,\n    time: Optional[np.ndarray] = None,\n    place_bin_centers: Optional[np.ndarray] = None,\n) -&gt; float:\n    \"\"\"\n    Calculate the weighted correlation between time and place bin centers using a posterior probability matrix.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        A 2D numpy array representing the posterior probability matrix.\n    time : Optional[np.ndarray], optional\n        A 1D numpy array representing the time bins, by default None.\n    place_bin_centers : Optional[np.ndarray], optional\n        A 1D numpy array representing the place bin centers, by default None.\n\n    Returns\n    -------\n    float\n        The weighted correlation coefficient.\n    \"\"\"\n\n    def _m(x, w) -&gt; float:\n        \"\"\"Weighted Mean\"\"\"\n        return np.sum(x * w) / np.sum(w)\n\n    def _cov(x, y, w) -&gt; float:\n        \"\"\"Weighted Covariance\"\"\"\n        return np.sum(w * (x - _m(x, w)) * (y - _m(y, w))) / np.sum(w)\n\n    def _corr(x, y, w) -&gt; float:\n        \"\"\"Weighted Correlation\"\"\"\n        return _cov(x, y, w) / np.sqrt(_cov(x, x, w) * _cov(y, y, w))\n\n    if time is None:\n        time = np.arange(posterior.shape[1])\n    if place_bin_centers is None:\n        place_bin_centers = np.arange(posterior.shape[0])\n\n    place_bin_centers = place_bin_centers.squeeze()\n    posterior[np.isnan(posterior)] = 0.0\n\n    return _corr(time[:, np.newaxis], place_bin_centers[np.newaxis, :], posterior.T)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly/","title":"neuro_py.ensemble.assembly","text":"<p>Codes for PCA/ICA methods described in Detecting cell assemblies in large neuronal populations, Lopes-dos-Santos et al (2013).  https://doi.org/10.1016/j.jneumeth.2013.04.010 This implementation was written in Feb 2019. Please e-mail me if you have comments, doubts, bug reports or criticism (V\u00edtor, vtlsantos@gmail.com /  vitor.lopesdossantos@pharm.ox.ac.uk).</p>"},{"location":"reference/neuro_py/ensemble/assembly/#neuro_py.ensemble.assembly.ToyAssemblies","title":"<code>ToyAssemblies</code>","text":"Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>class ToyAssemblies:\n    def __init__(\n        self,\n        membership: List[List[int]],\n        actrate: List[float],\n        actstrength: List[float],\n    ):\n        \"\"\"\n        Initialize ToyAssemblies.\n\n        Parameters\n        ----------\n        membership : List[List[int]]\n            List of lists containing neuron memberships for each assembly.\n        actrate : List[float]\n            List of activation rates for each assembly.\n        actstrength : List[float]\n            List of activation strengths for each assembly.\n        \"\"\"\n        self.membership = membership\n        self.actrate = actrate\n        self.actstrength = actstrength\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly/#neuro_py.ensemble.assembly.binshuffling","title":"<code>binshuffling(zactmat, significance)</code>","text":"<p>Perform bin shuffling to generate statistical threshold.</p> <p>Parameters:</p> Name Type Description Default <code>zactmat</code> <code>ndarray</code> <p>Z-scored activity matrix.</p> required <code>significance</code> <code>object</code> <p>Object containing significance parameters.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Statistical threshold.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def binshuffling(zactmat: np.ndarray, significance: object) -&gt; float:\n    \"\"\"\n    Perform bin shuffling to generate statistical threshold.\n\n    Parameters\n    ----------\n    zactmat : np.ndarray\n        Z-scored activity matrix.\n    significance : object\n        Object containing significance parameters.\n\n    Returns\n    -------\n    float\n        Statistical threshold.\n    \"\"\"\n    np.random.seed()\n\n    lambdamax_ = np.zeros(significance.nshu)\n    for shui in range(significance.nshu):\n        zactmat_ = np.copy(zactmat)\n        for neuroni, activity in enumerate(zactmat_):\n            randomorder = np.argsort(np.random.rand(significance.nbins))\n            zactmat_[neuroni, :] = activity[randomorder]\n        lambdamax_[shui] = getlambdacontrol(zactmat_)\n\n    lambdaMax = np.percentile(lambdamax_, significance.percentile)\n\n    return lambdaMax\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly/#neuro_py.ensemble.assembly.circshuffling","title":"<code>circshuffling(zactmat, significance)</code>","text":"<p>Perform circular shuffling to generate statistical threshold.</p> <p>Parameters:</p> Name Type Description Default <code>zactmat</code> <code>ndarray</code> <p>Z-scored activity matrix.</p> required <code>significance</code> <code>object</code> <p>Object containing significance parameters.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Statistical threshold.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def circshuffling(zactmat: np.ndarray, significance: object) -&gt; float:\n    \"\"\"\n    Perform circular shuffling to generate statistical threshold.\n\n    Parameters\n    ----------\n    zactmat : np.ndarray\n        Z-scored activity matrix.\n    significance : object\n        Object containing significance parameters.\n\n    Returns\n    -------\n    float\n        Statistical threshold.\n    \"\"\"\n    np.random.seed()\n\n    lambdamax_ = np.zeros(significance.nshu)\n    for shui in range(significance.nshu):\n        zactmat_ = np.copy(zactmat)\n        for neuroni, activity in enumerate(zactmat_):\n            cut = int(np.random.randint(significance.nbins * 2))\n            zactmat_[neuroni, :] = np.roll(activity, cut)\n        lambdamax_[shui] = getlambdacontrol(zactmat_)\n\n    lambdaMax = np.percentile(lambdamax_, significance.percentile)\n\n    return lambdaMax\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly/#neuro_py.ensemble.assembly.computeAssemblyActivity","title":"<code>computeAssemblyActivity(patterns, zactmat, zerodiag=True)</code>","text":"<p>Compute assembly activity.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>ndarray</code> <p>Co-activation patterns (assemblies, neurons).</p> required <code>zactmat</code> <code>ndarray</code> <p>Z-scored activity matrix (neurons, time bins).</p> required <code>zerodiag</code> <code>bool</code> <p>If True, diagonal of projection matrix is set to zero, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Assembly activity matrix (assemblies, time bins).</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def computeAssemblyActivity(\n    patterns: np.ndarray,\n    zactmat: np.ndarray,\n    zerodiag: bool = True,\n) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Compute assembly activity.\n\n    Parameters\n    ----------\n    patterns : np.ndarray\n        Co-activation patterns (assemblies, neurons).\n    zactmat : np.ndarray\n        Z-scored activity matrix (neurons, time bins).\n    zerodiag : bool, optional\n        If True, diagonal of projection matrix is set to zero, by default True.\n\n    Returns\n    -------\n    Optional[np.ndarray]\n        Assembly activity matrix (assemblies, time bins).\n    \"\"\"\n    # check if patterns is empty (no assembly detected) and return None if so\n    if len(patterns) == 0:\n        return None\n\n    # number of assemblies and time bins\n    nassemblies = len(patterns)\n    nbins = np.size(zactmat, 1)\n\n    # transpose for later matrix multiplication\n    zactmat = zactmat.T\n\n    # preallocate assembly activity matrix (nassemblies, nbins)\n    assemblyAct = np.zeros((nassemblies, nbins))\n\n    # loop over assemblies\n    for assemblyi, pattern in enumerate(patterns):\n        # compute projection matrix (neurons, neurons)\n        projMat = np.outer(pattern, pattern)\n\n        # set the diagonal to zero to not count coactivation of i and j when i=j\n        if zerodiag:\n            np.fill_diagonal(projMat, 0)\n\n        # project assembly pattern onto z-scored activity matrix\n        assemblyAct[assemblyi, :] = np.nansum(zactmat @ projMat * zactmat, axis=1)\n\n    return assemblyAct\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly/#neuro_py.ensemble.assembly.extractPatterns","title":"<code>extractPatterns(actmat, significance, method, whiten='unit-variance')</code>","text":"<p>Extract co-activation patterns (assemblies).</p> <p>Parameters:</p> Name Type Description Default <code>actmat</code> <code>ndarray</code> <p>Activity matrix.</p> required <code>significance</code> <code>object</code> <p>Object containing significance parameters.</p> required <code>method</code> <code>str</code> <p>Method to extract assembly patterns (ica, pca).</p> required <code>whiten</code> <code>str</code> <p>Whitening method, by default \"unit-variance\".</p> <code>'unit-variance'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Co-activation patterns (assemblies).</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def extractPatterns(\n    actmat: np.ndarray, significance: object, method: str, whiten: str = \"unit-variance\"\n) -&gt; np.ndarray:\n    \"\"\"\n    Extract co-activation patterns (assemblies).\n\n    Parameters\n    ----------\n    actmat : np.ndarray\n        Activity matrix.\n    significance : object\n        Object containing significance parameters.\n    method : str\n        Method to extract assembly patterns (ica, pca).\n    whiten : str, optional\n        Whitening method, by default \"unit-variance\".\n\n    Returns\n    -------\n    np.ndarray\n        Co-activation patterns (assemblies).\n    \"\"\"\n    nassemblies = significance.nassemblies\n\n    if method == \"pca\":\n        idxs = np.argsort(-significance.explained_variance_)[0:nassemblies]\n        patterns = significance.components_[idxs, :]\n    elif method == \"ica\":\n        ica = FastICA(n_components=nassemblies, random_state=0, whiten=whiten)\n        ica.fit(actmat.T)\n        patterns = ica.components_\n    else:\n        raise ValueError(\n            \"assembly extraction method \" + str(method) + \" not understood\"\n        )\n\n    if patterns is not np.nan:\n        patterns = patterns.reshape(nassemblies, -1)\n\n        # sets norm of assembly vectors to 1\n        norms = np.linalg.norm(patterns, axis=1)\n        patterns /= np.tile(norms, [np.size(patterns, 1), 1]).T\n\n    return patterns\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly/#neuro_py.ensemble.assembly.getlambdacontrol","title":"<code>getlambdacontrol(zactmat_)</code>","text":"<p>Get the maximum eigenvalue from PCA.</p> <p>Parameters:</p> Name Type Description Default <code>zactmat_</code> <code>ndarray</code> <p>Z-scored activity matrix.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Maximum eigenvalue.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def getlambdacontrol(zactmat_: np.ndarray) -&gt; float:\n    \"\"\"\n    Get the maximum eigenvalue from PCA.\n\n    Parameters\n    ----------\n    zactmat_ : np.ndarray\n        Z-scored activity matrix.\n\n    Returns\n    -------\n    float\n        Maximum eigenvalue.\n    \"\"\"\n    significance_ = PCA()\n    significance_.fit(zactmat_.T)\n    lambdamax_ = np.max(significance_.explained_variance_)\n\n    return lambdamax_\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly/#neuro_py.ensemble.assembly.marcenkopastur","title":"<code>marcenkopastur(significance)</code>","text":"<p>Calculate statistical threshold from Marcenko-Pastur distribution.</p> <p>Parameters:</p> Name Type Description Default <code>significance</code> <code>object</code> <p>Object containing significance parameters.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Statistical threshold.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def marcenkopastur(significance: object) -&gt; float:\n    \"\"\"\n    Calculate statistical threshold from Marcenko-Pastur distribution.\n\n    Parameters\n    ----------\n    significance : object\n        Object containing significance parameters.\n\n    Returns\n    -------\n    float\n        Statistical threshold.\n    \"\"\"\n    nbins = significance.nbins\n    nneurons = significance.nneurons\n    tracywidom = significance.tracywidom\n\n    # calculates statistical threshold from Marcenko-Pastur distribution\n    q = float(nbins) / float(nneurons)  # note that silent neurons are counted too\n    lambdaMax = pow((1 + np.sqrt(1 / q)), 2)\n    lambdaMax += tracywidom * pow(nneurons, -2.0 / 3)  # Tracy-Widom correction\n\n    return lambdaMax\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly/#neuro_py.ensemble.assembly.runPatterns","title":"<code>runPatterns(actmat, method='ica', nullhyp='mp', nshu=1000, percentile=99, tracywidom=False, whiten='unit-variance', nassemblies=None)</code>","text":"<p>Run pattern detection to identify cell assemblies.</p> <p>Parameters:</p> Name Type Description Default <code>actmat</code> <code>ndarray</code> <p>Activity matrix (neurons, time bins).</p> required <code>method</code> <code>str</code> <p>Method to extract assembly patterns (ica, pca), by default \"ica\".</p> <code>'ica'</code> <code>nullhyp</code> <code>str</code> <p>Null hypothesis method (bin, circ, mp), by default \"mp\".</p> <code>'mp'</code> <code>nshu</code> <code>int</code> <p>Number of shuffling controls, by default 1000.</p> <code>1000</code> <code>percentile</code> <code>int</code> <p>Percentile for shuffling methods, by default 99.</p> <code>99</code> <code>tracywidom</code> <code>bool</code> <p>Use Tracy-Widom correction, by default False.</p> <code>False</code> <code>whiten</code> <code>str</code> <p>Whitening method, by default \"unit-variance\".</p> <code>'unit-variance'</code> <code>nassemblies</code> <code>Optional[int]</code> <p>Number of assemblies, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Union[ndarray, None], object, Union[ndarray, None]], None]</code> <p>Patterns, significance object, and z-scored activity matrix.</p> Notes <p>nullhyp     'bin' - bin shuffling, will shuffle time bins of each neuron independently     'circ' - circular shuffling, will shift time bins of each neuron independently     'mp' - Marcenko-Pastur distribution - analytical threshold</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def runPatterns(\n    actmat: np.ndarray,\n    method: str = \"ica\",\n    nullhyp: str = \"mp\",\n    nshu: int = 1000,\n    percentile: int = 99,\n    tracywidom: bool = False,\n    whiten: str = \"unit-variance\",\n    nassemblies: int = None,\n) -&gt; Union[Tuple[Union[np.ndarray, None], object, Union[np.ndarray, None]], None]:\n    \"\"\"\n    Run pattern detection to identify cell assemblies.\n\n    Parameters\n    ----------\n    actmat : np.ndarray\n        Activity matrix (neurons, time bins).\n    method : str, optional\n        Method to extract assembly patterns (ica, pca), by default \"ica\".\n    nullhyp : str, optional\n        Null hypothesis method (bin, circ, mp), by default \"mp\".\n    nshu : int, optional\n        Number of shuffling controls, by default 1000.\n    percentile : int, optional\n        Percentile for shuffling methods, by default 99.\n    tracywidom : bool, optional\n        Use Tracy-Widom correction, by default False.\n    whiten : str, optional\n        Whitening method, by default \"unit-variance\".\n    nassemblies : Optional[int], optional\n        Number of assemblies, by default None.\n\n    Returns\n    -------\n    Union[Tuple[Union[np.ndarray, None], object, Union[np.ndarray, None]], None]\n        Patterns, significance object, and z-scored activity matrix.\n\n    Notes\n    -----\n    nullhyp\n        'bin' - bin shuffling, will shuffle time bins of each neuron independently\n        'circ' - circular shuffling, will shift time bins of each neuron independently\n        'mp' - Marcenko-Pastur distribution - analytical threshold\n    \"\"\"\n\n    nneurons = np.size(actmat, 0)\n    nbins = np.size(actmat, 1)\n\n    silentneurons = np.var(actmat, axis=1) == 0\n    actmat_ = actmat[~silentneurons, :]\n    if actmat_.shape[0] == 0:\n        warnings.warn(\"no active neurons\")\n        return None, None, None\n\n    # z-scoring activity matrix\n    zactmat_ = stats.zscore(actmat_, axis=1)\n\n    # running significance (estimating number of assemblies)\n    significance = PCA()\n    significance.fit(zactmat_.T)\n    significance.nneurons = nneurons\n    significance.nbins = nbins\n    significance.nshu = nshu\n    significance.percentile = percentile\n    significance.tracywidom = tracywidom\n    significance.nullhyp = nullhyp\n    significance = runSignificance(zactmat_, significance)\n\n    if nassemblies is not None:\n        significance.nassemblies = nassemblies\n\n    if np.isnan(significance.nassemblies):\n        return None, significance, None\n\n    if significance.nassemblies &lt; 1:\n        warnings.warn(\"no assembly detected\")\n\n        patterns = None\n        zactmat = None\n    else:\n        # extracting co-activation patterns\n        patterns_ = extractPatterns(zactmat_, significance, method, whiten=whiten)\n        if patterns_ is np.nan:\n            return None\n\n        # putting eventual silent neurons back (their assembly weights are defined as zero)\n        patterns = np.zeros((np.size(patterns_, 0), nneurons))\n        patterns[:, ~silentneurons] = patterns_\n        zactmat = np.copy(actmat)\n        zactmat[~silentneurons, :] = zactmat_\n\n    return patterns, significance, zactmat\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly/#neuro_py.ensemble.assembly.runSignificance","title":"<code>runSignificance(zactmat, significance)</code>","text":"<p>Run significance tests to estimate the number of assemblies.</p> <p>Parameters:</p> Name Type Description Default <code>zactmat</code> <code>ndarray</code> <p>Z-scored activity matrix.</p> required <code>significance</code> <code>object</code> <p>Object containing significance parameters.</p> required <p>Returns:</p> Type Description <code>object</code> <p>Updated significance object with the number of assemblies.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def runSignificance(zactmat: np.ndarray, significance: object) -&gt; object:\n    \"\"\"\n    Run significance tests to estimate the number of assemblies.\n\n    Parameters\n    ----------\n    zactmat : np.ndarray\n        Z-scored activity matrix.\n    significance : object\n        Object containing significance parameters.\n\n    Returns\n    -------\n    object\n        Updated significance object with the number of assemblies.\n    \"\"\"\n    if significance.nullhyp == \"mp\":\n        lambdaMax = marcenkopastur(significance)\n    elif significance.nullhyp == \"bin\":\n        lambdaMax = binshuffling(zactmat, significance)\n    elif significance.nullhyp == \"circ\":\n        lambdaMax = circshuffling(zactmat, significance)\n    else:\n        raise ValueError(\n            \"nyll hypothesis method \" + str(significance.nullhyp) + \" not understood\"\n        )\n\n    nassemblies = np.sum(significance.explained_variance_ &gt; lambdaMax)\n    significance.nassemblies = nassemblies\n\n    return significance\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly/#neuro_py.ensemble.assembly.toyExample","title":"<code>toyExample(assemblies, nneurons=10, nbins=1000, rate=1.0)</code>","text":"<p>Generate a toy example activity matrix with assemblies.</p> <p>Parameters:</p> Name Type Description Default <code>assemblies</code> <code>ToyAssemblies</code> <p>The toy assemblies.</p> required <code>nneurons</code> <code>int</code> <p>Number of neurons, by default 10.</p> <code>10</code> <code>nbins</code> <code>int</code> <p>Number of time bins, by default 1000.</p> <code>1000</code> <code>rate</code> <code>float</code> <p>Poisson rate, by default 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Activity matrix.</p> Source code in <code>neuro_py/ensemble/assembly.py</code> <pre><code>def toyExample(\n    assemblies: \"ToyAssemblies\",\n    nneurons: int = 10,\n    nbins: int = 1000,\n    rate: float = 1.0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate a toy example activity matrix with assemblies.\n\n    Parameters\n    ----------\n    assemblies : ToyAssemblies\n        The toy assemblies.\n    nneurons : int, optional\n        Number of neurons, by default 10.\n    nbins : int, optional\n        Number of time bins, by default 1000.\n    rate : float, optional\n        Poisson rate, by default 1.0.\n\n    Returns\n    -------\n    np.ndarray\n        Activity matrix.\n    \"\"\"\n    np.random.seed(42)\n\n    actmat = np.random.poisson(rate, nneurons * nbins).reshape(nneurons, nbins)\n    assemblies.actbins = [None] * len(assemblies.membership)\n    for ai, members in enumerate(assemblies.membership):\n        members = np.array(members)\n        nact = int(nbins * assemblies.actrate[ai])\n        actstrength_ = rate * assemblies.actstrength[ai]\n\n        actbins = np.argsort(np.random.rand(nbins))[0:nact]\n\n        actmat[members.reshape(-1, 1), actbins] = (\n            np.ones((len(members), nact)) + actstrength_\n        )\n\n        assemblies.actbins[ai] = np.sort(actbins)\n\n    return actmat\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/","title":"neuro_py.ensemble.assembly_reactivation","text":""},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact","title":"<code>AssemblyReact</code>","text":"<p>Class for running assembly reactivation analysis</p> <p>Core assembly methods come from assembly.py by V\u00edtor Lopes dos Santos     https://doi.org/10.1016/j.jneumeth.2013.04.010</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder</p> <code>None</code> <code>brainRegion</code> <code>str</code> <p>Brain region to restrict to. Can be multi ex. \"CA1|CA2\"</p> <code>'CA1'</code> <code>putativeCellType</code> <code>str</code> <p>Cell type to restrict to</p> <code>'Pyramidal Cell'</code> <code>weight_dt</code> <code>float</code> <p>Time resolution of the weight matrix</p> <code>0.025</code> <code>z_mat_dt</code> <code>float</code> <p>Time resolution of the z matrix</p> <code>0.002</code> <code>method</code> <code>str</code> <p>Defines how to extract assembly patterns (ica,pca).</p> <code>'ica'</code> <code>nullhyp</code> <code>str</code> <p>Defines how to generate statistical threshold for assembly detection (bin,circ,mp).</p> <code>'mp'</code> <code>nshu</code> <code>int</code> <p>Number of shuffles for bin and circ null hypothesis.</p> <code>1000</code> <code>percentile</code> <code>int</code> <p>Percentile for mp null hypothesis.</p> <code>99</code> <code>tracywidom</code> <code>bool</code> <p>If true, uses Tracy-Widom distribution for mp null hypothesis.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>st</code> <code>SpikeTrainArray</code> <p>Spike train</p> <code>cell_metrics</code> <code>DataFrame</code> <p>Cell metrics</p> <code>ripples</code> <code>EpochArray</code> <p>Ripples</p> <code>patterns</code> <code>ndarray</code> <p>Assembly patterns</p> <code>assembly_act</code> <code>AnalogSignalArray</code> <p>Assembly activity</p> <p>Methods:</p> Name Description <code>load_data</code> <p>Load data (st, ripples, epochs)</p> <code>restrict_to_epoch</code> <p>Restrict to a specific epoch</p> <code>get_z_mat</code> <p>Get z matrix</p> <code>get_weights</code> <p>Get assembly weights</p> <code>get_assembly_act</code> <p>Get assembly activity</p> <code>n_assemblies</code> <p>Number of detected assemblies</p> <code>isempty</code> <p>Check if empty</p> <code>copy</code> <p>Returns copy of class</p> <code>plot</code> <p>Stem plot of assembly weights</p> <code>find_members</code> <p>Find members of an assembly</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # create the object assembly_react\n&gt;&gt;&gt; assembly_react = assembly_reactivation.AssemblyReact(\n...    basepath=basepath,\n...    )\n</code></pre> <pre><code>&gt;&gt;&gt; # load need data (spikes, ripples, epochs)\n&gt;&gt;&gt; assembly_react.load_data()\n</code></pre> <pre><code>&gt;&gt;&gt; # detect assemblies\n&gt;&gt;&gt; assembly_react.get_weights()\n</code></pre> <pre><code>&gt;&gt;&gt; # visually inspect weights for each assembly\n&gt;&gt;&gt; assembly_react.plot()\n</code></pre> <pre><code>&gt;&gt;&gt; # compute time resolved signal for each assembly\n&gt;&gt;&gt; assembly_act = assembly_react.get_assembly_act()\n</code></pre> <pre><code>&gt;&gt;&gt; # locate members of assemblies\n&gt;&gt;&gt; assembly_members = assembly_react.find_members()\n</code></pre> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>class AssemblyReact:\n    \"\"\"\n    Class for running assembly reactivation analysis\n\n    Core assembly methods come from assembly.py by V\u00edtor Lopes dos Santos\n        https://doi.org/10.1016/j.jneumeth.2013.04.010\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder\n    brainRegion : str\n        Brain region to restrict to. Can be multi ex. \"CA1|CA2\"\n    putativeCellType : str\n        Cell type to restrict to\n    weight_dt : float\n        Time resolution of the weight matrix\n    z_mat_dt : float\n        Time resolution of the z matrix\n    method : str\n        Defines how to extract assembly patterns (ica,pca).\n    nullhyp : str\n        Defines how to generate statistical threshold for assembly detection (bin,circ,mp).\n    nshu : int\n        Number of shuffles for bin and circ null hypothesis.\n    percentile : int\n        Percentile for mp null hypothesis.\n    tracywidom : bool\n        If true, uses Tracy-Widom distribution for mp null hypothesis.\n\n    Attributes\n    ----------\n    st : nelpy.SpikeTrainArray\n        Spike train\n    cell_metrics : pd.DataFrame\n        Cell metrics\n    ripples : nelpy.EpochArray\n        Ripples\n    patterns : np.ndarray\n        Assembly patterns\n    assembly_act : nelpy.AnalogSignalArray\n        Assembly activity\n\n    Methods\n    -------\n    load_data()\n        Load data (st, ripples, epochs)\n    restrict_to_epoch(epoch)\n        Restrict to a specific epoch\n    get_z_mat(st)\n        Get z matrix\n    get_weights(epoch=None)\n        Get assembly weights\n    get_assembly_act(epoch=None)\n        Get assembly activity\n    n_assemblies()\n        Number of detected assemblies\n    isempty()\n        Check if empty\n    copy()\n        Returns copy of class\n    plot()\n        Stem plot of assembly weights\n    find_members()\n        Find members of an assembly\n\n\n    Examples\n    --------\n    &gt;&gt;&gt; # create the object assembly_react\n    &gt;&gt;&gt; assembly_react = assembly_reactivation.AssemblyReact(\n    ...    basepath=basepath,\n    ...    )\n\n    &gt;&gt;&gt; # load need data (spikes, ripples, epochs)\n    &gt;&gt;&gt; assembly_react.load_data()\n\n    &gt;&gt;&gt; # detect assemblies\n    &gt;&gt;&gt; assembly_react.get_weights()\n\n    &gt;&gt;&gt; # visually inspect weights for each assembly\n    &gt;&gt;&gt; assembly_react.plot()\n\n    &gt;&gt;&gt; # compute time resolved signal for each assembly\n    &gt;&gt;&gt; assembly_act = assembly_react.get_assembly_act()\n\n    &gt;&gt;&gt; # locate members of assemblies\n    &gt;&gt;&gt; assembly_members = assembly_react.find_members()\n\n    \"\"\"\n\n    def __init__(\n        self,\n        basepath: Union[str, None] = None,\n        brainRegion: str = \"CA1\",\n        putativeCellType: str = \"Pyramidal Cell\",\n        weight_dt: float = 0.025,\n        z_mat_dt: float = 0.002,\n        method: str = \"ica\",\n        nullhyp: str = \"mp\",\n        nshu: int = 1000,\n        percentile: int = 99,\n        tracywidom: bool = False,\n        whiten: str = \"unit-variance\",\n    ):\n        self.basepath = basepath\n        self.brainRegion = brainRegion\n        self.putativeCellType = putativeCellType\n        self.weight_dt = weight_dt\n        self.z_mat_dt = z_mat_dt\n        self.method = method\n        self.nullhyp = nullhyp\n        self.nshu = nshu\n        self.percentile = percentile\n        self.tracywidom = tracywidom\n        self.whiten = whiten\n        self.type_name = self.__class__.__name__\n\n    def add_st(self, st: nel.SpikeTrainArray) -&gt; None:\n        self.st = st\n\n    def add_ripples(self, ripples: nel.EpochArray) -&gt; None:\n        self.ripples = ripples\n\n    def add_epoch_df(self, epoch_df: pd.DataFrame) -&gt; None:\n        self.epoch_df = epoch_df\n\n    def load_spikes(self) -&gt; None:\n        \"\"\"\n        loads spikes from the session folder\n        \"\"\"\n        self.st, self.cell_metrics = loading.load_spikes(\n            self.basepath,\n            brainRegion=self.brainRegion,\n            putativeCellType=self.putativeCellType,\n            support=self.time_support,\n        )\n\n    def load_ripples(self) -&gt; None:\n        \"\"\"\n        loads ripples from the session folder\n        \"\"\"\n        ripples = loading.load_ripples_events(self.basepath)\n        self.ripples = nel.EpochArray(\n            [np.array([ripples.start, ripples.stop]).T], domain=self.time_support\n        )\n\n    def load_epoch(self) -&gt; None:\n        \"\"\"\n        loads epochs from the session folder\n        \"\"\"\n        epoch_df = loading.load_epoch(self.basepath)\n        epoch_df = compress_repeated_epochs(epoch_df)\n        self.time_support = nel.EpochArray(\n            [epoch_df.iloc[0].startTime, epoch_df.iloc[-1].stopTime]\n        )\n        self.epochs = nel.EpochArray(\n            [np.array([epoch_df.startTime, epoch_df.stopTime]).T],\n            domain=self.time_support,\n        )\n        self.epoch_df = epoch_df\n\n    def load_data(self) -&gt; None:\n        \"\"\"\n        loads data (spikes,ripples,epochs) from the session folder\n        \"\"\"\n        self.load_epoch()\n        self.load_spikes()\n        self.load_ripples()\n\n    def restrict_epochs_to_pre_task_post(self) -&gt; None:\n        \"\"\"\n        Restricts the epochs to the specified epochs\n        \"\"\"\n        # fetch data\n        epoch_df = loading.load_epoch(self.basepath)\n        # compress back to back sleep epochs (an issue further up the pipeline)\n        epoch_df = compress_repeated_epochs(epoch_df)\n        # restrict to pre task post epochs\n        idx = find_pre_task_post(epoch_df.environment)\n        self.epoch_df = epoch_df[idx[0]]\n        # convert to epoch array and add to object\n        self.epochs = nel.EpochArray(\n            [np.array([self.epoch_df.startTime, self.epoch_df.stopTime]).T],\n            label=\"session_epochs\",\n            domain=self.time_support,\n        )\n\n    def restrict_to_epoch(self, epoch) -&gt; None:\n        \"\"\"\n        Restricts the spike data to a specific epoch.\n\n        Parameters\n        ----------\n        epoch : nel.EpochArray\n            The epoch to restrict to.\n        \"\"\"\n        self.st_resticted = self.st[epoch]\n\n    def get_z_mat(self, st: nel.SpikeTrainArray) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Get z matrix.\n\n        Parameters\n        ----------\n        st : nel.SpikeTrainArray\n            Spike train array.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray]\n            Z-scored binned spike train and bin centers.\n        \"\"\"\n        # binning the spike train\n        z_t = st.bin(ds=self.z_mat_dt)\n        # gaussian kernel to match the bin-size used to identify the assembly patterns\n        sigma = self.weight_dt / np.sqrt(int(1000 * self.weight_dt / 2))\n        z_t.smooth(sigma=sigma, inplace=True)\n        # zscore the z matrix\n        z_scored_bst = stats.zscore(z_t.data, axis=1)\n        # make sure there are no nans, important as strengths will all be nan otherwise\n        z_scored_bst[np.isnan(z_scored_bst).any(axis=1)] = 0\n\n        return z_scored_bst, z_t.bin_centers\n\n    def get_weights(self, epoch: Optional[nel.EpochArray] = None) -&gt; None:\n        \"\"\"\n        Gets the assembly weights.\n\n        Parameters\n        ----------\n        epoch : nel.EpochArray, optional\n            The epoch to restrict to, by default None.\n        \"\"\"\n\n        # check if st has any neurons\n        if self.st.isempty:\n            self.patterns = None\n            return\n\n        if epoch is not None:\n            bst = self.st[epoch].bin(ds=self.weight_dt).data\n        else:\n            bst = self.st.bin(ds=self.weight_dt).data\n\n        if (bst == 0).all():\n            self.patterns = None\n            return\n        else:\n            patterns, _, _ = assembly.runPatterns(\n                bst,\n                method=self.method,\n                nullhyp=self.nullhyp,\n                nshu=self.nshu,\n                percentile=self.percentile,\n                tracywidom=self.tracywidom,\n                whiten=self.whiten,\n            )\n\n            if patterns is None: \n                self.patterns = None\n                return \n\n            # flip patterns to have positive max\n            self.patterns = np.array(\n                [\n                    (\n                        patterns[i, :]\n                        if patterns[i, np.argmax(np.abs(patterns[i, :]))] &gt; 0\n                        else -patterns[i, :]\n                    )\n                    for i in range(patterns.shape[0])\n                ]\n            )\n\n    def get_assembly_act(\n        self, epoch: Optional[nel.EpochArray] = None\n    ) -&gt; nel.AnalogSignalArray:\n        \"\"\"\n        Get assembly activity.\n\n        Parameters\n        ----------\n        epoch : nel.EpochArray, optional\n            The epoch to restrict to, by default None.\n\n        Returns\n        -------\n        nel.AnalogSignalArray\n            Assembly activity.\n        \"\"\"\n        # check for num of assemblies first\n        if self.n_assemblies() == 0:\n            return nel.AnalogSignalArray(empty=True)\n\n        if epoch is not None:\n            zactmat, ts = self.get_z_mat(self.st[epoch])\n        else:\n            zactmat, ts = self.get_z_mat(self.st)\n\n        assembly_act = nel.AnalogSignalArray(\n            data=assembly.computeAssemblyActivity(self.patterns, zactmat),\n            timestamps=ts,\n            fs=1 / self.z_mat_dt,\n        )\n        return assembly_act\n\n    def plot(\n        self,\n        plot_members: bool = True,\n        central_line_color: str = \"grey\",\n        marker_color: str = \"k\",\n        member_color: Union[str, list] = \"#6768ab\",\n        line_width: float = 1.25,\n        markersize: float = 4,\n        x_padding: float = 0.2,\n        figsize: Union[tuple, None] = None,\n    ) -&gt; Union[Tuple[plt.Figure, np.ndarray], str, None]:\n        \"\"\"\n        Plots basic stem plot to display assembly weights.\n\n        Parameters\n        ----------\n        plot_members : bool, optional\n            Whether to plot assembly members, by default True.\n        central_line_color : str, optional\n            Color of the central line, by default \"grey\".\n        marker_color : str, optional\n            Color of the markers, by default \"k\".\n        member_color : Union[str, List[str]], optional\n            Color of the members, by default \"#6768ab\".\n        line_width : float, optional\n            Width of the lines, by default 1.25.\n        markersize : float, optional\n            Size of the markers, by default 4.\n        x_padding : float, optional\n            Padding on the x-axis, by default 0.2.\n        figsize : Optional[Tuple[float, float]], optional\n            Size of the figure, by default None.\n\n        Returns\n        -------\n        Union[Tuple[plt.Figure, np.ndarray], str, None]\n            The figure and axes if successful, otherwise a message or None.\n        \"\"\"\n        if not hasattr(self, \"patterns\"):\n            return \"run get_weights first\"\n        else:\n            if self.patterns is None:\n                return None, None\n            if plot_members:\n                self.find_members()\n            if figsize is None:\n                figsize = (self.n_assemblies() + 1, np.round(self.n_assemblies() / 2))\n            # set up figure with size relative to assembly matrix\n            fig, axes = plt.subplots(\n                1,\n                self.n_assemblies(),\n                figsize=figsize,\n                sharey=True,\n                sharex=True,\n            )\n            # iter over each assembly and plot the weight per cell\n            for i in range(self.n_assemblies()):\n                markerline, stemlines, baseline = axes[i].stem(\n                    self.patterns[i, :], orientation=\"horizontal\"\n                )\n                markerline._color = marker_color\n                baseline._color = central_line_color\n                baseline.zorder = -1000\n                plt.setp(stemlines, \"color\", plt.getp(markerline, \"color\"))\n                plt.setp(stemlines, linewidth=line_width)\n                plt.setp(markerline, markersize=markersize)\n\n                if plot_members:\n                    current_pattern = self.patterns[i, :].copy()\n                    current_pattern[~self.assembly_members[i, :]] = np.nan\n                    markerline, stemlines, baseline = axes[i].stem(\n                        current_pattern, orientation=\"horizontal\"\n                    )\n                    if isinstance(\n                        member_color, sns.palettes._ColorPalette\n                    ) or isinstance(member_color, list):\n                        markerline._color = member_color[i]\n                    else:\n                        markerline._color = member_color\n                    baseline._color = \"#00000000\"\n                    baseline.zorder = -1000\n                    plt.setp(stemlines, \"color\", plt.getp(markerline, \"color\"))\n                    plt.setp(stemlines, linewidth=line_width)\n                    plt.setp(markerline, markersize=markersize)\n\n                axes[i].spines[\"top\"].set_visible(False)\n                axes[i].spines[\"right\"].set_visible(False)\n\n            # give room for marker\n            axes[0].set_xlim(\n                -self.patterns.max() - x_padding, self.patterns.max() + x_padding\n            )\n\n            axes[0].set_ylabel(\"Neurons #\")\n            axes[0].set_xlabel(\"Weights (a.u.)\")\n\n            return fig, axes\n\n    def n_assemblies(self) -&gt; int:\n        \"\"\"\n        Get the number of detected assemblies.\n\n        Returns\n        -------\n        int\n            Number of detected assemblies.\n        \"\"\"\n        if hasattr(self, \"patterns\"):\n            if self.patterns is None:\n                return 0\n            return self.patterns.shape[0]\n\n    @property\n    def isempty(self) -&gt; bool:\n        \"\"\"\n        Check if the object is empty.\n\n        Returns\n        -------\n        bool\n            True if empty, False otherwise.\n        \"\"\"\n        if hasattr(self, \"st\"):\n            return False\n        elif not hasattr(self, \"st\"):\n            return True\n\n    def copy(self) -&gt; \"AssemblyReact\":\n        \"\"\"\n        Returns a copy of the current class.\n\n        Returns\n        -------\n        AssemblyReact\n            A copy of the current class.\n        \"\"\"\n        newcopy = copy.deepcopy(self)\n        return newcopy\n\n    def __repr__(self) -&gt; str:\n        if self.isempty:\n            return f\"&lt;{self.type_name}: empty&gt;\"\n\n        # if st data as been loaded and patterns have been computed\n        if hasattr(self, \"patterns\"):\n            n_units = f\"{self.st.n_active} units\"\n            n_patterns = f\"{self.n_assemblies()} assemblies\"\n            dstr = f\"of length {self.st.support.length}\"\n            return \"&lt;%s: %s, %s&gt; %s\" % (self.type_name, n_units, n_patterns, dstr)\n\n        # if st data as been loaded\n        if hasattr(self, \"st\"):\n            n_units = f\"{self.st.n_active} units\"\n            dstr = f\"of length {self.st.support.length}\"\n            return \"&lt;%s: %s&gt; %s\" % (self.type_name, n_units, dstr)\n\n    def find_members(self) -&gt; np.ndarray:\n        \"\"\"\n        Finds significant assembly patterns and significant assembly members.\n\n        Returns\n        -------\n        np.ndarray\n            A ndarray of booleans indicating whether each unit is a significant member of an assembly.\n\n        Notes\n        -----\n        also, sets self.assembly_members and self.valid_assembly\n\n        self.valid_assembly: a ndarray of booleans indicating an assembly has members with the same sign (Boucly et al. 2022)\n        \"\"\"\n\n        def Otsu(vector: np.ndarray) -&gt; Tuple[np.ndarray, float, float]:\n            \"\"\"\n            The Otsu method for splitting data into two groups.\n\n            Parameters\n            ----------\n            vector : np.ndarray\n                Arbitrary vector.\n\n            Returns\n            -------\n            Tuple[np.ndarray, float, float]\n                Group, threshold used for classification, and effectiveness metric.\n            \"\"\"\n            sorted = np.sort(vector)\n            n = len(vector)\n            intraClassVariance = [np.nan] * n\n            for i in np.arange(n):\n                p = (i + 1) / n\n                p0 = 1 - p\n                if i + 1 == n:\n                    intraClassVariance[i] = np.nan\n                else:\n                    intraClassVariance[i] = p * np.var(sorted[0 : i + 1]) + p0 * np.var(\n                        sorted[i + 1 :]\n                    )\n\n            minIntraVariance = np.nanmin(intraClassVariance)\n            idx = np.nanargmin(intraClassVariance)\n            threshold = sorted[idx]\n            group = vector &gt; threshold\n\n            em = 1 - (minIntraVariance / np.var(vector))\n\n            return group, threshold, em\n\n        is_member = []\n        keep_assembly = []\n        for pat in self.patterns:\n            isMember, _, _ = Otsu(np.abs(pat))\n            is_member.append(isMember)\n\n            if np.any(pat[isMember] &lt; 0) &amp; np.any(pat[isMember] &gt; 0):\n                keep_assembly.append(False)\n            elif sum(isMember) == 0:\n                keep_assembly.append(False)\n            else:\n                keep_assembly.append(True)\n\n        self.assembly_members = np.array(is_member)\n        self.valid_assembly = np.array(keep_assembly)\n\n        return self.assembly_members\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.isempty","title":"<code>isempty</code>  <code>property</code>","text":"<p>Check if the object is empty.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if empty, False otherwise.</p>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the current class.</p> <p>Returns:</p> Type Description <code>AssemblyReact</code> <p>A copy of the current class.</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def copy(self) -&gt; \"AssemblyReact\":\n    \"\"\"\n    Returns a copy of the current class.\n\n    Returns\n    -------\n    AssemblyReact\n        A copy of the current class.\n    \"\"\"\n    newcopy = copy.deepcopy(self)\n    return newcopy\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.find_members","title":"<code>find_members()</code>","text":"<p>Finds significant assembly patterns and significant assembly members.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A ndarray of booleans indicating whether each unit is a significant member of an assembly.</p> Notes <p>also, sets self.assembly_members and self.valid_assembly</p> <p>self.valid_assembly: a ndarray of booleans indicating an assembly has members with the same sign (Boucly et al. 2022)</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def find_members(self) -&gt; np.ndarray:\n    \"\"\"\n    Finds significant assembly patterns and significant assembly members.\n\n    Returns\n    -------\n    np.ndarray\n        A ndarray of booleans indicating whether each unit is a significant member of an assembly.\n\n    Notes\n    -----\n    also, sets self.assembly_members and self.valid_assembly\n\n    self.valid_assembly: a ndarray of booleans indicating an assembly has members with the same sign (Boucly et al. 2022)\n    \"\"\"\n\n    def Otsu(vector: np.ndarray) -&gt; Tuple[np.ndarray, float, float]:\n        \"\"\"\n        The Otsu method for splitting data into two groups.\n\n        Parameters\n        ----------\n        vector : np.ndarray\n            Arbitrary vector.\n\n        Returns\n        -------\n        Tuple[np.ndarray, float, float]\n            Group, threshold used for classification, and effectiveness metric.\n        \"\"\"\n        sorted = np.sort(vector)\n        n = len(vector)\n        intraClassVariance = [np.nan] * n\n        for i in np.arange(n):\n            p = (i + 1) / n\n            p0 = 1 - p\n            if i + 1 == n:\n                intraClassVariance[i] = np.nan\n            else:\n                intraClassVariance[i] = p * np.var(sorted[0 : i + 1]) + p0 * np.var(\n                    sorted[i + 1 :]\n                )\n\n        minIntraVariance = np.nanmin(intraClassVariance)\n        idx = np.nanargmin(intraClassVariance)\n        threshold = sorted[idx]\n        group = vector &gt; threshold\n\n        em = 1 - (minIntraVariance / np.var(vector))\n\n        return group, threshold, em\n\n    is_member = []\n    keep_assembly = []\n    for pat in self.patterns:\n        isMember, _, _ = Otsu(np.abs(pat))\n        is_member.append(isMember)\n\n        if np.any(pat[isMember] &lt; 0) &amp; np.any(pat[isMember] &gt; 0):\n            keep_assembly.append(False)\n        elif sum(isMember) == 0:\n            keep_assembly.append(False)\n        else:\n            keep_assembly.append(True)\n\n    self.assembly_members = np.array(is_member)\n    self.valid_assembly = np.array(keep_assembly)\n\n    return self.assembly_members\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.get_assembly_act","title":"<code>get_assembly_act(epoch=None)</code>","text":"<p>Get assembly activity.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The epoch to restrict to, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>AnalogSignalArray</code> <p>Assembly activity.</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def get_assembly_act(\n    self, epoch: Optional[nel.EpochArray] = None\n) -&gt; nel.AnalogSignalArray:\n    \"\"\"\n    Get assembly activity.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray, optional\n        The epoch to restrict to, by default None.\n\n    Returns\n    -------\n    nel.AnalogSignalArray\n        Assembly activity.\n    \"\"\"\n    # check for num of assemblies first\n    if self.n_assemblies() == 0:\n        return nel.AnalogSignalArray(empty=True)\n\n    if epoch is not None:\n        zactmat, ts = self.get_z_mat(self.st[epoch])\n    else:\n        zactmat, ts = self.get_z_mat(self.st)\n\n    assembly_act = nel.AnalogSignalArray(\n        data=assembly.computeAssemblyActivity(self.patterns, zactmat),\n        timestamps=ts,\n        fs=1 / self.z_mat_dt,\n    )\n    return assembly_act\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.get_weights","title":"<code>get_weights(epoch=None)</code>","text":"<p>Gets the assembly weights.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The epoch to restrict to, by default None.</p> <code>None</code> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def get_weights(self, epoch: Optional[nel.EpochArray] = None) -&gt; None:\n    \"\"\"\n    Gets the assembly weights.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray, optional\n        The epoch to restrict to, by default None.\n    \"\"\"\n\n    # check if st has any neurons\n    if self.st.isempty:\n        self.patterns = None\n        return\n\n    if epoch is not None:\n        bst = self.st[epoch].bin(ds=self.weight_dt).data\n    else:\n        bst = self.st.bin(ds=self.weight_dt).data\n\n    if (bst == 0).all():\n        self.patterns = None\n        return\n    else:\n        patterns, _, _ = assembly.runPatterns(\n            bst,\n            method=self.method,\n            nullhyp=self.nullhyp,\n            nshu=self.nshu,\n            percentile=self.percentile,\n            tracywidom=self.tracywidom,\n            whiten=self.whiten,\n        )\n\n        if patterns is None: \n            self.patterns = None\n            return \n\n        # flip patterns to have positive max\n        self.patterns = np.array(\n            [\n                (\n                    patterns[i, :]\n                    if patterns[i, np.argmax(np.abs(patterns[i, :]))] &gt; 0\n                    else -patterns[i, :]\n                )\n                for i in range(patterns.shape[0])\n            ]\n        )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.get_z_mat","title":"<code>get_z_mat(st)</code>","text":"<p>Get z matrix.</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>SpikeTrainArray</code> <p>Spike train array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Z-scored binned spike train and bin centers.</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def get_z_mat(self, st: nel.SpikeTrainArray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Get z matrix.\n\n    Parameters\n    ----------\n    st : nel.SpikeTrainArray\n        Spike train array.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        Z-scored binned spike train and bin centers.\n    \"\"\"\n    # binning the spike train\n    z_t = st.bin(ds=self.z_mat_dt)\n    # gaussian kernel to match the bin-size used to identify the assembly patterns\n    sigma = self.weight_dt / np.sqrt(int(1000 * self.weight_dt / 2))\n    z_t.smooth(sigma=sigma, inplace=True)\n    # zscore the z matrix\n    z_scored_bst = stats.zscore(z_t.data, axis=1)\n    # make sure there are no nans, important as strengths will all be nan otherwise\n    z_scored_bst[np.isnan(z_scored_bst).any(axis=1)] = 0\n\n    return z_scored_bst, z_t.bin_centers\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.load_data","title":"<code>load_data()</code>","text":"<p>loads data (spikes,ripples,epochs) from the session folder</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def load_data(self) -&gt; None:\n    \"\"\"\n    loads data (spikes,ripples,epochs) from the session folder\n    \"\"\"\n    self.load_epoch()\n    self.load_spikes()\n    self.load_ripples()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.load_epoch","title":"<code>load_epoch()</code>","text":"<p>loads epochs from the session folder</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def load_epoch(self) -&gt; None:\n    \"\"\"\n    loads epochs from the session folder\n    \"\"\"\n    epoch_df = loading.load_epoch(self.basepath)\n    epoch_df = compress_repeated_epochs(epoch_df)\n    self.time_support = nel.EpochArray(\n        [epoch_df.iloc[0].startTime, epoch_df.iloc[-1].stopTime]\n    )\n    self.epochs = nel.EpochArray(\n        [np.array([epoch_df.startTime, epoch_df.stopTime]).T],\n        domain=self.time_support,\n    )\n    self.epoch_df = epoch_df\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.load_ripples","title":"<code>load_ripples()</code>","text":"<p>loads ripples from the session folder</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def load_ripples(self) -&gt; None:\n    \"\"\"\n    loads ripples from the session folder\n    \"\"\"\n    ripples = loading.load_ripples_events(self.basepath)\n    self.ripples = nel.EpochArray(\n        [np.array([ripples.start, ripples.stop]).T], domain=self.time_support\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.load_spikes","title":"<code>load_spikes()</code>","text":"<p>loads spikes from the session folder</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def load_spikes(self) -&gt; None:\n    \"\"\"\n    loads spikes from the session folder\n    \"\"\"\n    self.st, self.cell_metrics = loading.load_spikes(\n        self.basepath,\n        brainRegion=self.brainRegion,\n        putativeCellType=self.putativeCellType,\n        support=self.time_support,\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.n_assemblies","title":"<code>n_assemblies()</code>","text":"<p>Get the number of detected assemblies.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of detected assemblies.</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def n_assemblies(self) -&gt; int:\n    \"\"\"\n    Get the number of detected assemblies.\n\n    Returns\n    -------\n    int\n        Number of detected assemblies.\n    \"\"\"\n    if hasattr(self, \"patterns\"):\n        if self.patterns is None:\n            return 0\n        return self.patterns.shape[0]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.plot","title":"<code>plot(plot_members=True, central_line_color='grey', marker_color='k', member_color='#6768ab', line_width=1.25, markersize=4, x_padding=0.2, figsize=None)</code>","text":"<p>Plots basic stem plot to display assembly weights.</p> <p>Parameters:</p> Name Type Description Default <code>plot_members</code> <code>bool</code> <p>Whether to plot assembly members, by default True.</p> <code>True</code> <code>central_line_color</code> <code>str</code> <p>Color of the central line, by default \"grey\".</p> <code>'grey'</code> <code>marker_color</code> <code>str</code> <p>Color of the markers, by default \"k\".</p> <code>'k'</code> <code>member_color</code> <code>Union[str, List[str]]</code> <p>Color of the members, by default \"#6768ab\".</p> <code>'#6768ab'</code> <code>line_width</code> <code>float</code> <p>Width of the lines, by default 1.25.</p> <code>1.25</code> <code>markersize</code> <code>float</code> <p>Size of the markers, by default 4.</p> <code>4</code> <code>x_padding</code> <code>float</code> <p>Padding on the x-axis, by default 0.2.</p> <code>0.2</code> <code>figsize</code> <code>Optional[Tuple[float, float]]</code> <p>Size of the figure, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[Figure, ndarray], str, None]</code> <p>The figure and axes if successful, otherwise a message or None.</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def plot(\n    self,\n    plot_members: bool = True,\n    central_line_color: str = \"grey\",\n    marker_color: str = \"k\",\n    member_color: Union[str, list] = \"#6768ab\",\n    line_width: float = 1.25,\n    markersize: float = 4,\n    x_padding: float = 0.2,\n    figsize: Union[tuple, None] = None,\n) -&gt; Union[Tuple[plt.Figure, np.ndarray], str, None]:\n    \"\"\"\n    Plots basic stem plot to display assembly weights.\n\n    Parameters\n    ----------\n    plot_members : bool, optional\n        Whether to plot assembly members, by default True.\n    central_line_color : str, optional\n        Color of the central line, by default \"grey\".\n    marker_color : str, optional\n        Color of the markers, by default \"k\".\n    member_color : Union[str, List[str]], optional\n        Color of the members, by default \"#6768ab\".\n    line_width : float, optional\n        Width of the lines, by default 1.25.\n    markersize : float, optional\n        Size of the markers, by default 4.\n    x_padding : float, optional\n        Padding on the x-axis, by default 0.2.\n    figsize : Optional[Tuple[float, float]], optional\n        Size of the figure, by default None.\n\n    Returns\n    -------\n    Union[Tuple[plt.Figure, np.ndarray], str, None]\n        The figure and axes if successful, otherwise a message or None.\n    \"\"\"\n    if not hasattr(self, \"patterns\"):\n        return \"run get_weights first\"\n    else:\n        if self.patterns is None:\n            return None, None\n        if plot_members:\n            self.find_members()\n        if figsize is None:\n            figsize = (self.n_assemblies() + 1, np.round(self.n_assemblies() / 2))\n        # set up figure with size relative to assembly matrix\n        fig, axes = plt.subplots(\n            1,\n            self.n_assemblies(),\n            figsize=figsize,\n            sharey=True,\n            sharex=True,\n        )\n        # iter over each assembly and plot the weight per cell\n        for i in range(self.n_assemblies()):\n            markerline, stemlines, baseline = axes[i].stem(\n                self.patterns[i, :], orientation=\"horizontal\"\n            )\n            markerline._color = marker_color\n            baseline._color = central_line_color\n            baseline.zorder = -1000\n            plt.setp(stemlines, \"color\", plt.getp(markerline, \"color\"))\n            plt.setp(stemlines, linewidth=line_width)\n            plt.setp(markerline, markersize=markersize)\n\n            if plot_members:\n                current_pattern = self.patterns[i, :].copy()\n                current_pattern[~self.assembly_members[i, :]] = np.nan\n                markerline, stemlines, baseline = axes[i].stem(\n                    current_pattern, orientation=\"horizontal\"\n                )\n                if isinstance(\n                    member_color, sns.palettes._ColorPalette\n                ) or isinstance(member_color, list):\n                    markerline._color = member_color[i]\n                else:\n                    markerline._color = member_color\n                baseline._color = \"#00000000\"\n                baseline.zorder = -1000\n                plt.setp(stemlines, \"color\", plt.getp(markerline, \"color\"))\n                plt.setp(stemlines, linewidth=line_width)\n                plt.setp(markerline, markersize=markersize)\n\n            axes[i].spines[\"top\"].set_visible(False)\n            axes[i].spines[\"right\"].set_visible(False)\n\n        # give room for marker\n        axes[0].set_xlim(\n            -self.patterns.max() - x_padding, self.patterns.max() + x_padding\n        )\n\n        axes[0].set_ylabel(\"Neurons #\")\n        axes[0].set_xlabel(\"Weights (a.u.)\")\n\n        return fig, axes\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.restrict_epochs_to_pre_task_post","title":"<code>restrict_epochs_to_pre_task_post()</code>","text":"<p>Restricts the epochs to the specified epochs</p> Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def restrict_epochs_to_pre_task_post(self) -&gt; None:\n    \"\"\"\n    Restricts the epochs to the specified epochs\n    \"\"\"\n    # fetch data\n    epoch_df = loading.load_epoch(self.basepath)\n    # compress back to back sleep epochs (an issue further up the pipeline)\n    epoch_df = compress_repeated_epochs(epoch_df)\n    # restrict to pre task post epochs\n    idx = find_pre_task_post(epoch_df.environment)\n    self.epoch_df = epoch_df[idx[0]]\n    # convert to epoch array and add to object\n    self.epochs = nel.EpochArray(\n        [np.array([self.epoch_df.startTime, self.epoch_df.stopTime]).T],\n        label=\"session_epochs\",\n        domain=self.time_support,\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/assembly_reactivation/#neuro_py.ensemble.assembly_reactivation.AssemblyReact.restrict_to_epoch","title":"<code>restrict_to_epoch(epoch)</code>","text":"<p>Restricts the spike data to a specific epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The epoch to restrict to.</p> required Source code in <code>neuro_py/ensemble/assembly_reactivation.py</code> <pre><code>def restrict_to_epoch(self, epoch) -&gt; None:\n    \"\"\"\n    Restricts the spike data to a specific epoch.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray\n        The epoch to restrict to.\n    \"\"\"\n    self.st_resticted = self.st[epoch]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/dynamics/","title":"neuro_py.ensemble.dynamics","text":""},{"location":"reference/neuro_py/ensemble/dynamics/#neuro_py.ensemble.dynamics.cosine_similarity","title":"<code>cosine_similarity(pv1, pv2)</code>","text":"<p>Cosine similarity between temporal difference vectors of two firing rate vector trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>pv1</code> <code>ndarray</code> <p>Temporal difference of firing rate vector trajectory in one context. Shape: (num_bins, num_neurons)</p> required <code>pv2</code> <code>ndarray</code> <p>Temporal difference of firing rate vector trajectory in another context. Shape: (num_bins, num_neurons)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Cosine similarity between the two contexts.</p> References <p>.. [1] Guidera, J. A., Gramling, D. P., Comrie, A. E., Joshi, A., Denovellis, E. L., Lee, K. H., Zhou, J., Thompson, P., Hernandez, J., Yorita, A., Haque, R., Kirst, C., &amp; Frank, L. M. (2024). Regional specialization manifests in the reliability of neural population codes. bioRxiv : the preprint server for biology, 2024.01.25.576941. https://doi.org/10.1101/2024.01.25.576941</p> Source code in <code>neuro_py/ensemble/dynamics.py</code> <pre><code>def cosine_similarity(pv1: np.ndarray, pv2: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Cosine similarity between temporal difference vectors of two firing rate\n    vector trajectories.\n\n    Parameters\n    ----------\n    pv1 : numpy.ndarray\n        Temporal difference of firing rate vector trajectory in one context.\n        Shape: (num_bins, num_neurons)\n\n    pv2 : numpy.ndarray\n        Temporal difference of firing rate vector trajectory in another context.\n        Shape: (num_bins, num_neurons)\n\n    Returns\n    -------\n    numpy.ndarray\n        Cosine similarity between the two contexts.\n\n    References\n    ----------\n    .. [1] Guidera, J. A., Gramling, D. P., Comrie, A. E., Joshi, A.,\n    Denovellis, E. L., Lee, K. H., Zhou, J., Thompson, P., Hernandez, J.,\n    Yorita, A., Haque, R., Kirst, C., &amp; Frank, L. M. (2024). Regional\n    specialization manifests in the reliability of neural population codes.\n    bioRxiv : the preprint server for biology, 2024.01.25.576941.\n    https://doi.org/10.1101/2024.01.25.576941\n    \"\"\"\n    cosine_mat = sklearn.metrics.pairwise.cosine_similarity(pv1, pv2)\n    cosine_sim = np.diag(cosine_mat)\n\n    return cosine_sim\n</code></pre>"},{"location":"reference/neuro_py/ensemble/dynamics/#neuro_py.ensemble.dynamics.potential_landscape","title":"<code>potential_landscape(X_dyn, projbins, domainbins=None)</code>","text":"<p>Compute numerical approximation of potential energy landscape across 1D state and domain (e.g. time, position, etc.).</p> <p>Potential landscape is defined as the integral of the flow vectors.</p> <p>Parameters:</p> Name Type Description Default <code>X_dyn</code> <code>ndarray</code> <p>State vectors of shape (trials, bins).</p> required <code>projbins</code> <code>int or array - like</code> <p>Number of bins for projection axis or bin edges</p> required <code>domainbins</code> <code>int or array - like</code> <p>Number of bins for domain axis or bin edges, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Potential energy landscape across state and domain</p> <code>ndarray</code> <p>Temporal gradient of potential energy landscape across state and domain</p> <code>ndarray</code> <p>Histogram of state vectors across state and domain</p> <code>ndarray</code> <p>Bin edges of state vectors</p> <code>ndarray</code> <p>Bin edges of domain</p> References <p>.. [1] Wang, S., Falcone, R., Richmond, B. et al. Attractor dynamics reflect        decision confidence in macaque prefrontal cortex. Nat Neurosci 26,        1970\u20131980 (2023).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X_dyn = np.array([[0.1, 0.2, 0.4], [0.0, 0.3, 0.6]])\n&gt;&gt;&gt; projbins = 3\n&gt;&gt;&gt; domainbins = 3\n&gt;&gt;&gt; potential_landscape(X_dyn, projbins, domainbins)\n(array([[ 0.  ,  0.  ,   nan],\n        [-0.1 ,  0.  ,   nan],\n        [  nan,  0.  , -0.25]]),\narray([[0.3 ,  nan,  nan],\n       [0.1 ,  nan,  nan],\n       [ nan,  nan, 0.25]]),\narray([[1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 2.]]),\narray([0. , 0.1, 0.2, 0.3]),\narray([0.        , 0.33333333, 0.66666667, 1.        ]))\n</code></pre> Source code in <code>neuro_py/ensemble/dynamics.py</code> <pre><code>def potential_landscape(\n    X_dyn: np.ndarray,\n    projbins: Union[int, np.ndarray],\n    domainbins: Union[int, np.ndarray, None] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Compute numerical approximation of potential energy landscape across\n    1D state and domain (e.g. time, position, etc.).\n\n    Potential landscape is defined as the integral of the flow vectors.\n\n    Parameters\n    ----------\n    X_dyn : np.ndarray\n        State vectors of shape (trials, bins).\n    projbins : int or array-like\n        Number of bins for projection axis or bin edges\n    domainbins : int or array-like, optional\n        Number of bins for domain axis or bin edges, by default None\n\n    Returns\n    -------\n    np.ndarray\n        Potential energy landscape across state and domain\n    np.ndarray\n        Temporal gradient of potential energy landscape across state and domain\n    np.ndarray\n        Histogram of state vectors across state and domain\n    np.ndarray\n        Bin edges of state vectors\n    np.ndarray\n        Bin edges of domain\n\n    References\n    ----------\n    .. [1] Wang, S., Falcone, R., Richmond, B. et al. Attractor dynamics reflect\n           decision confidence in macaque prefrontal cortex. Nat Neurosci 26,\n           1970\u20131980 (2023).\n\n    Examples\n    --------\n    &gt;&gt;&gt; X_dyn = np.array([[0.1, 0.2, 0.4], [0.0, 0.3, 0.6]])\n    &gt;&gt;&gt; projbins = 3\n    &gt;&gt;&gt; domainbins = 3\n    &gt;&gt;&gt; potential_landscape(X_dyn, projbins, domainbins)\n    (array([[ 0.  ,  0.  ,   nan],\n            [-0.1 ,  0.  ,   nan],\n            [  nan,  0.  , -0.25]]),\n    array([[0.3 ,  nan,  nan],\n           [0.1 ,  nan,  nan],\n           [ nan,  nan, 0.25]]),\n    array([[1., 0., 0.],\n           [1., 0., 0.],\n           [0., 0., 2.]]),\n    array([0. , 0.1, 0.2, 0.3]),\n    array([0.        , 0.33333333, 0.66666667, 1.        ]))\n    \"\"\"\n    # _t suffix is following notation of paper but applicable across any domain\n    nnrns = 1\n    ntrials, nbins = X_dyn.shape\n    delta_t = np.diff(X_dyn, axis=1)  # time derivatives: ntrials x nbins-1 x nnrns\n\n    X_t_flat = np.reshape(\n        X_dyn[:, :-1], (-1, nnrns), order=\"F\"\n    ).ravel()  # skip last bin as no displacement exists for last time point\n    delta_t_flat = np.reshape(\n        delta_t, (-1, nnrns), order=\"F\"\n    ).ravel()  # column-major order\n    norm_tpts = np.repeat(np.arange(nbins - 1), ntrials)\n\n    nbins_domain = (\n        nbins - 1 if domainbins is None else domainbins\n    )  # downsample domain bins\n\n    # 1D state space binning of time derivatives across domain\n    # assumes landscape may morph across domain\n    H, bin_edges, _ = binned_statistic_dd(  # posbins x time\n        np.asarray((X_t_flat, norm_tpts)).T,\n        delta_t_flat,\n        statistic=\"count\",\n        bins=(projbins, nbins_domain),\n    )\n    latentedges, domainedges = bin_edges\n\n    grad_pos_t_svm = binned_statistic_dd(\n        np.asarray((X_t_flat, norm_tpts)).T,\n        delta_t_flat,\n        statistic=\"sum\",\n        bins=(projbins, nbins_domain),\n    ).statistic\n    # average derivative, a.k.a. flow/vector field for dynamics underlying\n    # population activity\n    grad_pos_t_svm = np.divide(grad_pos_t_svm, H, where=H != 0)\n    grad_pos_t_svm[H == 0] = np.nan  # crucial to handle division by zero\n    # spatial integration via nnancumsum treats nan as zero for cumulative sum\n    potential_pos_t = -np.nancumsum(grad_pos_t_svm, axis=0)  # projbins x domainbins\n\n    idx_zero_X_t = np.searchsorted(latentedges, 0)\n    offset = potential_pos_t[idx_zero_X_t, :]  # use potential at X_t = 0 as reference\n    potential_pos_t = potential_pos_t - offset  # potential difference\n\n    nonzero_mask = H != 0\n    idx_first_nonzero, idx_last_nonzero = find_terminal_masked_indices(\n        nonzero_mask, axis=0\n    )  # each have shape: time\n    # along axis 0 set all values from start to idx_first_nonzero to nan\n    for t in range(H.shape[1]):\n        potential_pos_t[: idx_first_nonzero[t], t] = np.nan\n        potential_pos_t[idx_last_nonzero[t] + 1 :, t] = np.nan\n\n    return potential_pos_t, grad_pos_t_svm, H, latentedges, domainedges\n</code></pre>"},{"location":"reference/neuro_py/ensemble/dynamics/#neuro_py.ensemble.dynamics.potential_landscape_nd","title":"<code>potential_landscape_nd(X_dyn, projbins, domainbins=None, nanborderempty=True)</code>","text":"<p>Compute numerical approximation of potential energy landscape across n-dimensional state and domain (e.g. time, position, etc.).</p> <p>Potential landscape is defined as the integral of the flow vectors.</p> <p>Parameters:</p> Name Type Description Default <code>X_dyn</code> <code>ndarray</code> <p>State vectors of shape (trials, bins, neurons)</p> required <code>projbins</code> <code>int or array - like</code> <p>Number of bins for projection axis or bin edges for each neuron</p> required <code>domainbins</code> <code>int or array - like</code> <p>Number of bins for domain axis or bin edges, by default None</p> <code>None</code> <code>nanborderempty</code> <code>bool</code> <p>Whether to set border values to nan if they are empty, by default True</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Potential energy landscape across state averaged across domain for each neuron. Shape: nnrns x projbins times nnrns</p> <code>ndarray</code> <p>Potential energy landscape across state and domain for each neuron. Shape: projbins times nnrns x domainbins x nnrns</p> <code>ndarray</code> <p>Temporal gradient of potential energy landscape across state and domain for each neuron. Shape: projbins times nnrns x domainbins x nnrns</p> <code>ndarray</code> <p>Histogram of state vectors across state and domain for each neuron. Shape: projbins times nnrns x domainbins x nnrns</p> <code>ndarray</code> <p>Bin edges of state vectors for each neuron</p> <code>ndarray</code> <p>Bin edges of domain for each neuron</p> References <p>.. [1] Wang, S., Falcone, R., Richmond, B. et al. Attractor dynamics reflect        decision confidence in macaque prefrontal cortex. Nat Neurosci 26,        1970\u20131980 (2023).</p> Source code in <code>neuro_py/ensemble/dynamics.py</code> <pre><code>def potential_landscape_nd(\n    X_dyn: np.ndarray,\n    projbins: Union[int, np.ndarray],\n    domainbins: Union[int, np.ndarray, None] = None,\n    nanborderempty: bool = True,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Compute numerical approximation of potential energy landscape across\n    n-dimensional state and domain (e.g. time, position, etc.).\n\n    Potential landscape is defined as the integral of the flow vectors.\n\n    Parameters\n    ----------\n    X_dyn : np.ndarray\n        State vectors of shape (trials, bins, neurons)\n    projbins : int or array-like\n        Number of bins for projection axis or bin edges for each neuron\n    domainbins : int or array-like, optional\n        Number of bins for domain axis or bin edges, by default None\n    nanborderempty : bool, optional\n        Whether to set border values to nan if they are empty, by default True\n\n    Returns\n    -------\n    np.ndarray\n        Potential energy landscape across state averaged across domain for each\n        neuron. Shape: nnrns x projbins times nnrns\n    np.ndarray\n        Potential energy landscape across state and domain for each neuron.\n        Shape: projbins times nnrns x domainbins x nnrns\n    np.ndarray\n        Temporal gradient of potential energy landscape across state and domain\n        for each neuron. Shape: projbins times nnrns x domainbins x nnrns\n    np.ndarray\n        Histogram of state vectors across state and domain for each neuron.\n        Shape: projbins times nnrns x domainbins x nnrns\n    np.ndarray\n        Bin edges of state vectors for each neuron\n    np.ndarray\n        Bin edges of domain for each neuron\n\n    References\n    ----------\n    .. [1] Wang, S., Falcone, R., Richmond, B. et al. Attractor dynamics reflect\n           decision confidence in macaque prefrontal cortex. Nat Neurosci 26,\n           1970\u20131980 (2023).\n    \"\"\"\n    # _t suffix is following notation of paper but applicable across any domain\n    ntrials, nbins, nnrns = X_dyn.shape\n    delta_t = np.diff(\n        X_dyn, axis=1\n    )  # time derivatives: ntrials x ndomainbins-1 x nnrns\n\n    X_t_flat = np.reshape(\n        X_dyn[:, :-1], (-1, nnrns), order=\"F\"\n    )  # skip last bin as no displacement exists for last time point\n    delta_t_flat = np.reshape(delta_t, (-1, nnrns), order=\"F\")  # column-major order\n    norm_tpts = np.repeat(np.arange(nbins - 1), ntrials)\n\n    nbins_domain = (\n        nbins - 1 if domainbins is None else domainbins\n    )  # downsample domain bins\n\n    potential_pos_t_nrns = []\n    grad_pos_t_svm_nrns = []\n    hist_nrns = []\n    latentedges_nrns = []\n    domainedges_nrns = []\n    for nnrn in range(nnrns):\n        # 1D state space binning of time derivatives across domain\n        # assumes landscape may morph across domain\n        H, bin_edges, _ = binned_statistic_dd(  # (nnrns times projbins) x time\n            np.asarray((*X_t_flat.T, norm_tpts)).T,\n            delta_t_flat[:, nnrn],\n            statistic=\"count\",\n            bins=(\n                *[\n                    projbins if isinstance(projbins, int) else projbins[idx]\n                    for idx in range(nnrns)\n                ],\n                nbins_domain,\n            ),\n        )\n        latentedges = bin_edges[nnrn]\n        domainedges = bin_edges[-1]\n\n        grad_pos_t_svm = binned_statistic_dd(\n            np.asarray((*X_t_flat.T, norm_tpts)).T,\n            delta_t_flat[:, nnrn],\n            statistic=\"sum\",\n            bins=(\n                *[\n                    projbins if isinstance(projbins, int) else projbins[idx]\n                    for idx in range(nnrns)\n                ],\n                nbins_domain,\n            ),\n        ).statistic\n        # average derivative, a.k.a. flow/vector field for dynamics underlying\n        # population activity\n        grad_pos_t_svm = np.divide(grad_pos_t_svm, H, where=H != 0)\n        grad_pos_t_svm[H == 0] = np.nan  # crucial to handle division by zero\n        # spatial integration via nnancumsum treats nan as zero for cumulative sum\n        potential_pos_t = -np.nancumsum(\n            grad_pos_t_svm, axis=nnrn\n        )  # (nnrns times projbins) x domainbins\n\n        if nanborderempty:\n            nonzero_mask = H != 0\n\n            for t in range(nbins_domain):\n                nrndimslices = [slice(None)] * nnrns\n                nrndimslices.append(t)\n                peripheral_zeros_nanmask = ~np.isnan(\n                    replace_border_zeros_with_nan(nonzero_mask[tuple(nrndimslices)])\n                )\n                peripheral_zeros_nanmask = np.where(\n                    peripheral_zeros_nanmask, peripheral_zeros_nanmask, np.nan\n                )\n                potential_pos_t[tuple(nrndimslices)] *= peripheral_zeros_nanmask\n\n        potential_pos_t_nrns.append(potential_pos_t)\n        grad_pos_t_svm_nrns.append(grad_pos_t_svm)\n        hist_nrns.append(H)\n        latentedges_nrns.append(latentedges)\n        domainedges_nrns.append(domainedges)\n\n    potential_pos_t_nrns = np.stack(\n        potential_pos_t_nrns, axis=-1\n    )  # projbins x domainbins x nnrns\n    grad_pos_t_svm_nrns = np.stack(\n        grad_pos_t_svm_nrns, axis=-1\n    )  # projbins x domainbins x nnrns\n    hist = np.stack(hist_nrns, axis=-1)  # projbins x domainbins x nnrns\n    latentedges_nrns = np.stack(latentedges_nrns, axis=-1)  # projbins x nnrns\n    domainedges_nrns = np.stack(domainedges_nrns, axis=-1)  # domainbins x nnrns\n    nrndimslices = [slice(None)] * (nnrns + 1)\n    nrndimslices.append(0)\n    potential_nrns_pos = []\n    for nrn in range(nnrns):\n        nrndimslices[-1] = nrn\n        potential_nrns_pos.append(\n            np.nanmean(\n                potential_pos_t_nrns[tuple(nrndimslices)], axis=-1\n            )  # average across domainbins\n        )\n    potential_nrns_pos = np.asarray(potential_nrns_pos)  # nnrns x nnrns times projbins\n\n    return (\n        potential_nrns_pos,\n        potential_pos_t_nrns,\n        grad_pos_t_svm_nrns,\n        hist,\n        latentedges_nrns,\n        domainedges_nrns,\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/explained_variance/","title":"neuro_py.ensemble.explained_variance","text":""},{"location":"reference/neuro_py/ensemble/explained_variance/#neuro_py.ensemble.explained_variance.ExplainedVariance","title":"<code>ExplainedVariance</code>","text":"<p>               Bases: <code>object</code></p> <p>Explained variance measure for assessing reactivation of neuronal activity using pairwise correlations.</p> References <p>1) Kudrimoti, H. S., Barnes, C. A., &amp; McNaughton, B. L. (1999).     Reactivation of Hippocampal Cell Assemblies: Effects of Behavioral State, Experience, and EEG Dynamics.     Journal of Neuroscience, 19(10), 4090-4101. https://doi.org/10/4090 2) Tatsuno, M., Lipa, P., &amp; McNaughton, B. L. (2006).     Methodological Considerations on the Use of Template Matching to Study Long-Lasting Memory Trace Replay.     Journal of Neuroscience, 26(42), 10727-10742. https://doi.org/10.1523/JNEUROSCI.3317-06.2006</p> <p>Adapted from https://github.com/diba-lab/NeuroPy/blob/main/neuropy/analyses/reactivation.py</p> <p>Attributes:</p> Name Type Description <code>st</code> <code>SpikeTrainArray</code> <p>obj that holds spiketrains</p> <code>template</code> <code>EpochArray</code> <p>time in seconds, pairwise correlation calculated from this period will be compared to matching period (task-period)</p> <code>matching</code> <code>EpochArray</code> <p>time in seconds, template-correlations will be correlated with pariwise correlations of this period (post-task period)</p> <code>control</code> <code>EpochArray</code> <p>time in seconds, control for pairwise correlations within this period (pre-task period)</p> <code>bin_size</code> <code>float</code> <p>in seconds, binning size for spike counts</p> <code>window</code> <code>int</code> <p>window over which pairwise correlations will be calculated in matching and control time periods,     if window is None entire time period is considered, in seconds</p> <code>slideby</code> <code>int</code> <p>slide window by this much, in seconds</p> <code>matching_windows</code> <code>array</code> <p>windows for matching period</p> <code>control_windows</code> <code>array</code> <p>windows for control period</p> <code>template_corr</code> <code>array</code> <p>pairwise correlations for template period</p> <code>matching_paircorr</code> <code>array</code> <p>pairwise correlations for matching period</p> <code>control_paircorr</code> <code>array</code> <p>pairwise correlations for control period</p> <code>ev</code> <code>array</code> <p>explained variance for each time point</p> <code>rev</code> <code>array</code> <p>reverse explained variance for each time point</p> <code>ev_std</code> <code>array</code> <p>explained variance standard deviation for each time point</p> <code>rev_std</code> <code>array</code> <p>reverse explained variance standard deviation for each time point</p> <code>partial_corr</code> <code>array</code> <p>partial correlations for each time point</p> <code>rev_partial_corr</code> <code>array</code> <p>reverse partial correlations for each time point</p> <code>n_pairs</code> <code>int</code> <p>number of pairs</p> <code>matching_time</code> <code>array</code> <p>time points for matching period</p> <code>control_time</code> <code>array</code> <p>time points for control period</p> <code>ev_signal</code> <code>AnalogSignalArray</code> <p>explained variance signal</p> <code>rev_signal</code> <code>AnalogSignalArray</code> <p>reverse explained variance signal</p> <code>plot</code> <code>function</code> <p>plot explained variance</p> <code>pvalue</code> <code>function</code> <p>calculate p-value for explained variance by shuffling the template correlations</p> <p>Examples:</p>"},{"location":"reference/neuro_py/ensemble/explained_variance/#neuro_py.ensemble.explained_variance.ExplainedVariance--load-data","title":"Load data","text":"<pre><code>&gt;&gt;&gt; basepath = r\"U:\\data\\HMC\\HMC1\\day8\"\n&gt;&gt;&gt; st,cm = loading.load_spikes(basepath,brainRegion=\"CA1\",putativeCellType=\"Pyr\")\n</code></pre> <pre><code>&gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n&gt;&gt;&gt; beh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/explained_variance/#neuro_py.ensemble.explained_variance.ExplainedVariance--most-simple-case-returns-single-explained-variance-value","title":"Most simple case, returns single explained variance value","text":"<pre><code>&gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n&gt;&gt;&gt;        st=st,\n&gt;&gt;&gt;        template=beh_epochs[1],\n&gt;&gt;&gt;        matching=beh_epochs[2],\n&gt;&gt;&gt;        control=beh_epochs[0],\n&gt;&gt;&gt;        window=None,\n&gt;&gt;&gt;    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/explained_variance/#neuro_py.ensemble.explained_variance.ExplainedVariance--get-time-resolved-explained-variance-across-entire-session-in-200sec-bins","title":"Get time resolved explained variance across entire session in 200sec bins","text":"<pre><code>&gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n&gt;&gt;&gt;        st=st,\n&gt;&gt;&gt;        template=beh_epochs[1],\n&gt;&gt;&gt;        matching=nel.EpochArray([beh_epochs.start, beh_epochs.stop]),\n&gt;&gt;&gt;        control=beh_epochs[0],\n&gt;&gt;&gt;        window=200\n&gt;&gt;&gt;    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/explained_variance/#neuro_py.ensemble.explained_variance.ExplainedVariance--get-time-resolved-explained-variance-across-entire-session-in-200sec-bins-sliding-by-100sec","title":"Get time resolved explained variance across entire session in 200sec bins sliding by 100sec","text":"<pre><code>&gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n&gt;&gt;&gt;        st=st,\n&gt;&gt;&gt;        template=beh_epochs[1],\n&gt;&gt;&gt;        matching=nel.EpochArray([beh_epochs.start, beh_epochs.stop]),\n&gt;&gt;&gt;        control=beh_epochs[0],\n&gt;&gt;&gt;        window=200,\n&gt;&gt;&gt;        slideby=100\n&gt;&gt;&gt;    )\n</code></pre> Source code in <code>neuro_py/ensemble/explained_variance.py</code> <pre><code>class ExplainedVariance(object):\n    \"\"\"Explained variance measure for assessing reactivation of neuronal activity using pairwise correlations.\n\n    References\n    -------\n    1) Kudrimoti, H. S., Barnes, C. A., &amp; McNaughton, B. L. (1999).\n        Reactivation of Hippocampal Cell Assemblies: Effects of Behavioral State, Experience, and EEG Dynamics.\n        Journal of Neuroscience, 19(10), 4090-4101. https://doi.org/10/4090\n    2) Tatsuno, M., Lipa, P., &amp; McNaughton, B. L. (2006).\n        Methodological Considerations on the Use of Template Matching to Study Long-Lasting Memory Trace Replay.\n        Journal of Neuroscience, 26(42), 10727-10742. https://doi.org/10.1523/JNEUROSCI.3317-06.2006\n\n    Adapted from https://github.com/diba-lab/NeuroPy/blob/main/neuropy/analyses/reactivation.py\n\n    Attributes\n    ----------\n    st : SpikeTrainArray\n        obj that holds spiketrains\n    template : EpochArray\n        time in seconds, pairwise correlation calculated from this period will be compared to matching period (task-period)\n    matching : EpochArray\n        time in seconds, template-correlations will be correlated with pariwise correlations of this period (post-task period)\n    control : EpochArray\n        time in seconds, control for pairwise correlations within this period (pre-task period)\n    bin_size : float\n        in seconds, binning size for spike counts\n    window : int\n        window over which pairwise correlations will be calculated in matching and control time periods,\n            if window is None entire time period is considered, in seconds\n    slideby : int\n        slide window by this much, in seconds\n    matching_windows : array\n        windows for matching period\n    control_windows : array\n        windows for control period\n    template_corr : array\n        pairwise correlations for template period\n    matching_paircorr : array\n        pairwise correlations for matching period\n    control_paircorr : array\n        pairwise correlations for control period\n    ev : array\n        explained variance for each time point\n    rev : array\n        reverse explained variance for each time point\n    ev_std : array\n        explained variance standard deviation for each time point\n    rev_std : array\n        reverse explained variance standard deviation for each time point\n    partial_corr : array\n        partial correlations for each time point\n    rev_partial_corr : array\n        reverse partial correlations for each time point\n    n_pairs : int\n        number of pairs\n    matching_time : array\n        time points for matching period\n    control_time : array\n        time points for control period\n    ev_signal : AnalogSignalArray\n        explained variance signal\n    rev_signal : AnalogSignalArray\n        reverse explained variance signal\n    plot : function\n        plot explained variance\n    pvalue : function\n        calculate p-value for explained variance by shuffling the template correlations\n\n    Examples\n    --------\n    # Load data\n    &gt;&gt;&gt; basepath = r\"U:\\data\\HMC\\HMC1\\day8\"\n    &gt;&gt;&gt; st,cm = loading.load_spikes(basepath,brainRegion=\"CA1\",putativeCellType=\"Pyr\")\n\n    &gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n    &gt;&gt;&gt; beh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values)\n\n\n    # Most simple case, returns single explained variance value\n    &gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n    &gt;&gt;&gt;        st=st,\n    &gt;&gt;&gt;        template=beh_epochs[1],\n    &gt;&gt;&gt;        matching=beh_epochs[2],\n    &gt;&gt;&gt;        control=beh_epochs[0],\n    &gt;&gt;&gt;        window=None,\n    &gt;&gt;&gt;    )\n\n    # Get time resolved explained variance across entire session in 200sec bins\n    &gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n    &gt;&gt;&gt;        st=st,\n    &gt;&gt;&gt;        template=beh_epochs[1],\n    &gt;&gt;&gt;        matching=nel.EpochArray([beh_epochs.start, beh_epochs.stop]),\n    &gt;&gt;&gt;        control=beh_epochs[0],\n    &gt;&gt;&gt;        window=200\n    &gt;&gt;&gt;    )\n\n    # Get time resolved explained variance across entire session in 200sec bins sliding by 100sec\n    &gt;&gt;&gt; expvar = explained_variance.ExplainedVariance(\n    &gt;&gt;&gt;        st=st,\n    &gt;&gt;&gt;        template=beh_epochs[1],\n    &gt;&gt;&gt;        matching=nel.EpochArray([beh_epochs.start, beh_epochs.stop]),\n    &gt;&gt;&gt;        control=beh_epochs[0],\n    &gt;&gt;&gt;        window=200,\n    &gt;&gt;&gt;        slideby=100\n    &gt;&gt;&gt;    )\n    \"\"\"\n\n    def __init__(\n        self,\n        st: SpikeTrainArray,\n        template: EpochArray,\n        matching: EpochArray,\n        control: EpochArray,\n        bin_size: float = 0.2,\n        window: int = 900,\n        slideby: int = None,\n    ):\n        \"\"\"Explained variance measure for assessing reactivation of neuronal activity using pairwise correlations.\n\n        Parameters\n        ----------\n        st : SpikeTrainArray\n            obj that holds spiketrains\n        template : EpochArray\n            time in seconds, pairwise correlation calculated from this period will be compared to matching period (task-period)\n        matching : EpochArray\n            time in seconds, template-correlations will be correlated with pariwise correlations of this period (post-task period)\n        control : EpochArray\n            time in seconds, control for pairwise correlations within this period (pre-task period)\n        bin_size : float, optional\n            in seconds, binning size for spike counts, by default 0.2\n        window : int, optional\n            window over which pairwise correlations will be calculated in matching and control time periods,\n                if window is None entire time period is considered, in seconds, by default 900\n        slideby : int, optional\n            slide window by this much, in seconds, by default None\n        \"\"\"\n        self.__dict__.update(locals())\n        del self.__dict__[\"self\"]\n\n        self.__validate_input()\n        self.__calculate()\n\n    def __validate_input(self):\n        \"\"\"Validate input parameters.\"\"\"\n        assert isinstance(self.st, SpikeTrainArray)\n        assert isinstance(self.template, EpochArray)\n        assert isinstance(self.matching, EpochArray)\n        assert isinstance(self.control, EpochArray)\n        assert isinstance(self.bin_size, (float, int))\n        assert isinstance(self.window, (int, type(None)))\n        assert isinstance(self.slideby, (int, type(None)))\n\n    def __calculate(self):\n        \"\"\"processing steps for explained variance calculation.\"\"\"\n        control_window_size, matching_window_size, slideby = self.__get_window_sizes()\n\n        self.matching_windows = self.__get_windows_array(\n            self.matching, matching_window_size, slideby\n        )\n        self.control_windows = self.__get_windows_array(\n            self.control, control_window_size, slideby\n        )\n        self.__validate_window_sizes(control_window_size, matching_window_size)\n        self.template_corr = self.__get_template_corr()\n        self.__calculate_pairwise_correlations()\n        self.__calculate_partial_correlations()\n\n    def __get_window_sizes(self):\n        \"\"\"Get window sizes for control and matching periods.\"\"\"\n        if self.window is None:\n            control_window_size = np.array(self.control.duration).astype(int)\n            matching_window_size = np.array(self.matching.duration).astype(int)\n            slideby = None\n        elif self.slideby is None:\n            control_window_size = self.window\n            matching_window_size = self.window\n            slideby = None\n        else:\n            control_window_size = self.window\n            matching_window_size = self.window\n            slideby = self.slideby\n        return control_window_size, matching_window_size, slideby\n\n    def __get_windows_array(self, epoch_array, window_size, slideby):\n        \"\"\"Get windows array for control and matching periods.\"\"\"\n        if slideby is not None:\n            array = np.arange(epoch_array.start, epoch_array.stop)\n            windows = np.lib.stride_tricks.sliding_window_view(array, window_size)\n            windows = windows[::slideby, [0, -1]]\n        elif np.array(epoch_array.duration) == window_size:\n            windows = np.array([[epoch_array.start, epoch_array.stop]])\n        else:\n            array = np.arange(epoch_array.start, epoch_array.stop, window_size)\n            windows = np.array([array[:-1], array[1:]]).T\n        return windows\n\n    def __validate_window_sizes(self, control_window_size, matching_window_size):\n        \"\"\"Validate window sizes.\"\"\"\n        assert (\n            control_window_size &lt;= self.control.duration\n        ), \"window is bigger than matching\"\n        assert (\n            matching_window_size &lt;= self.matching.duration\n        ), \"window is bigger than matching\"\n\n    def __get_template_corr(self):\n        \"\"\"Get pairwise correlations for template period.\"\"\"\n        self.bst = self.st.bin(ds=self.bin_size)\n        return self.__get_pairwise_corr(self.bst[self.template].data)\n\n    def __calculate_pairwise_correlations(self):\n        \"\"\"Calculate pairwise correlations for matching and control periods.\"\"\"\n        self.matching_paircorr = self.__time_resolved_correlation(self.matching_windows)\n        self.control_paircorr = self.__time_resolved_correlation(self.control_windows)\n\n    @staticmethod\n    def __get_pairwise_corr(bst_data):\n        \"\"\"Calculate pairwise correlations.\"\"\"\n        corr = np.corrcoef(bst_data)\n        return corr[np.tril_indices(corr.shape[0], k=-1)]\n\n    def __time_resolved_correlation(self, windows):\n        \"\"\"Calculate pairwise correlations for given windows.\"\"\"\n        paircorr = []\n        bst_data = self.bst.data\n        bin_centers = self.bst.bin_centers\n\n        for w in windows:\n            start, stop = w\n            idx = (bin_centers &gt; start) &amp; (bin_centers &lt; stop)\n            corr = np.corrcoef(bst_data[:, idx])\n            paircorr.append(corr[np.tril_indices(corr.shape[0], k=-1)])\n\n        return np.array(paircorr)\n\n    def __calculate_partial_correlations(self):\n        \"\"\"Calculate partial correlations.\"\"\"\n        partial_corr, rev_partial_corr = self.__calculate_partial_correlations_(\n            self.matching_paircorr, self.control_paircorr, self.template_corr\n        )\n        self.__calculate_statistics(partial_corr, rev_partial_corr)\n\n    @staticmethod\n    @jit(nopython=True)\n    def __calculate_partial_correlations_(\n        matching_paircorr, control_paircorr, template_corr\n    ):\n        \"\"\"Calculate partial correlations.\"\"\"\n\n        def __explained_variance(x, y, covar):\n            \"\"\"Calculate explained variance and reverse explained variance.\"\"\"\n\n            # Calculate covariance matrix\n            n = len(covar)\n            valid = np.zeros(n, dtype=np.bool_)\n            for i in range(n):\n                valid[i] = not (np.isnan(covar[i]) or np.isnan(x[i]) or np.isnan(y[i]))\n            mat = np.empty((3, len(x)))\n            mat[0] = covar\n            mat[1] = x\n            mat[2] = y\n            cov = np.corrcoef(mat[:, valid])\n\n            # Calculate explained variance\n            EV = (cov[1, 2] - cov[0, 1] * cov[0, 2]) / (\n                np.sqrt((1 - cov[0, 1] ** 2) * (1 - cov[0, 2] ** 2)) + 1e-10\n            )\n\n            # Calculate reverse explained variance\n            rEV = (cov[0, 1] - cov[1, 2] * cov[0, 2]) / (\n                np.sqrt((1 - cov[1, 2] ** 2) * (1 - cov[0, 2] ** 2)) + 1e-10\n            )\n\n            return EV, rEV\n\n        n_matching = len(matching_paircorr)\n        n_control = len(control_paircorr)\n        partial_corr = np.zeros((n_control, n_matching))\n        rev_partial_corr = np.zeros((n_control, n_matching))\n\n        for m_i, m_pairs in enumerate(matching_paircorr):\n            for c_i, c_pairs in enumerate(control_paircorr):\n                partial_corr[c_i, m_i], rev_partial_corr[c_i, m_i] = (\n                    __explained_variance(template_corr, m_pairs, c_pairs)\n                )\n        return partial_corr, rev_partial_corr\n\n    def __calculate_statistics(self, partial_corr, rev_partial_corr):\n        \"\"\"Calculate explained variance statistics.\"\"\"\n        self.ev = np.nanmean(partial_corr**2, axis=0)\n        self.rev = np.nanmean(rev_partial_corr**2, axis=0)\n        self.ev_std = np.nanstd(partial_corr**2, axis=0)\n        self.rev_std = np.nanstd(rev_partial_corr**2, axis=0)\n        self.partial_corr = partial_corr**2\n        self.rev_partial_corr = rev_partial_corr**2\n        self.n_pairs = len(self.template_corr)\n        self.matching_time = np.mean(self.matching_windows, axis=1)\n        self.control_time = np.mean(self.control_windows, axis=1)\n\n    @property\n    def ev_signal(self):\n        \"\"\"Return explained variance signal.\"\"\"\n        return AnalogSignalArray(\n            data=self.ev,\n            timestamps=self.matching_time,\n            fs=1 / np.diff(self.matching_time)[0],\n            support=EpochArray(data=[self.matching.start, self.matching.stop]),\n        )\n\n    @property\n    def rev_signal(self):\n        \"\"\"Return reverse explained variance signal.\"\"\"\n        return AnalogSignalArray(\n            data=self.rev,\n            timestamps=self.matching_time,\n            fs=1 / np.diff(self.matching_time)[0],\n            support=EpochArray(data=[self.matching.start, self.matching.stop]),\n        )\n\n    def pvalue(self, n_shuffles=1000):\n        \"\"\"\n        Calculate p-value for explained variance by shuffling the template correlations.\n        \"\"\"\n        from copy import deepcopy\n\n        def shuffle_template(self):\n            template_corr = deepcopy(self.template_corr)\n            np.random.shuffle(template_corr)\n\n            partial_corr, _ = self.__calculate_partial_correlations_(\n                self.matching_paircorr, self.control_paircorr, template_corr\n            )\n            ev = np.nanmean(partial_corr**2, axis=0)\n            return ev.flatten()\n\n        if len(self.ev) &gt; 1:\n            print(\"Multiple time points, p-values are not supported\")\n            return\n\n        ev_shuffle = [shuffle_template(self) for _ in range(n_shuffles)]\n\n        ev_shuffle = np.array(ev_shuffle)\n\n        n = len(ev_shuffle)\n        r = np.sum(ev_shuffle &gt; self.ev)\n        pvalues = (r + 1) / (n + 1)\n        return pvalues\n\n    def plot(self):\n        \"\"\"Plot explained variance.\"\"\"\n        if self.matching_time.size == 1:\n            print(\"Only single time point, cannot plot\")\n            return\n        import matplotlib.pyplot as plt\n\n        fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n        ax.plot(self.matching_time, self.ev, label=\"EV\")\n        ax.fill_between(\n            self.matching_time,\n            self.ev - self.ev_std,\n            self.ev + self.ev_std,\n            alpha=0.5,\n        )\n        ax.plot(self.matching_time, self.rev, label=\"rEV\", color=\"grey\")\n        ax.fill_between(\n            self.matching_time,\n            self.rev - self.rev_std,\n            self.rev + self.rev_std,\n            alpha=0.5,\n            color=\"grey\",\n        )\n        # check if matching time overlaps with control time and plot control time\n        if np.any(\n            (self.control_time &gt;= self.matching_time[0])\n            &amp; (self.control_time &lt;= self.matching_time[-1])\n        ):\n            ax.axvspan(\n                self.control.start,\n                self.control.stop,\n                color=\"green\",\n                alpha=0.3,\n                label=\"Control\",\n                zorder=-10,\n            )\n        # check if matching time overlaps with template time and plot template time\n        if np.any(\n            (self.template.start &gt;= self.matching_time[0])\n            &amp; (self.template.stop &lt;= self.matching_time[-1])\n        ):\n            ax.axvspan(\n                self.template.start,\n                self.template.stop,\n                color=\"purple\",\n                alpha=0.4,\n                label=\"Template\",\n                zorder=-10,\n            )\n        # remove axis spines\n        ax.spines[\"right\"].set_visible(False)\n        ax.spines[\"top\"].set_visible(False)\n\n        ax.legend(frameon=False)\n        ax.set_xlabel(\"Time (s)\")\n        ax.set_ylabel(\"Explained Variance\")\n        ax.set_title(\"Explained Variance\")\n        plt.show()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/explained_variance/#neuro_py.ensemble.explained_variance.ExplainedVariance.ev_signal","title":"<code>ev_signal</code>  <code>property</code>","text":"<p>Return explained variance signal.</p>"},{"location":"reference/neuro_py/ensemble/explained_variance/#neuro_py.ensemble.explained_variance.ExplainedVariance.rev_signal","title":"<code>rev_signal</code>  <code>property</code>","text":"<p>Return reverse explained variance signal.</p>"},{"location":"reference/neuro_py/ensemble/explained_variance/#neuro_py.ensemble.explained_variance.ExplainedVariance.plot","title":"<code>plot()</code>","text":"<p>Plot explained variance.</p> Source code in <code>neuro_py/ensemble/explained_variance.py</code> <pre><code>def plot(self):\n    \"\"\"Plot explained variance.\"\"\"\n    if self.matching_time.size == 1:\n        print(\"Only single time point, cannot plot\")\n        return\n    import matplotlib.pyplot as plt\n\n    fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n    ax.plot(self.matching_time, self.ev, label=\"EV\")\n    ax.fill_between(\n        self.matching_time,\n        self.ev - self.ev_std,\n        self.ev + self.ev_std,\n        alpha=0.5,\n    )\n    ax.plot(self.matching_time, self.rev, label=\"rEV\", color=\"grey\")\n    ax.fill_between(\n        self.matching_time,\n        self.rev - self.rev_std,\n        self.rev + self.rev_std,\n        alpha=0.5,\n        color=\"grey\",\n    )\n    # check if matching time overlaps with control time and plot control time\n    if np.any(\n        (self.control_time &gt;= self.matching_time[0])\n        &amp; (self.control_time &lt;= self.matching_time[-1])\n    ):\n        ax.axvspan(\n            self.control.start,\n            self.control.stop,\n            color=\"green\",\n            alpha=0.3,\n            label=\"Control\",\n            zorder=-10,\n        )\n    # check if matching time overlaps with template time and plot template time\n    if np.any(\n        (self.template.start &gt;= self.matching_time[0])\n        &amp; (self.template.stop &lt;= self.matching_time[-1])\n    ):\n        ax.axvspan(\n            self.template.start,\n            self.template.stop,\n            color=\"purple\",\n            alpha=0.4,\n            label=\"Template\",\n            zorder=-10,\n        )\n    # remove axis spines\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n\n    ax.legend(frameon=False)\n    ax.set_xlabel(\"Time (s)\")\n    ax.set_ylabel(\"Explained Variance\")\n    ax.set_title(\"Explained Variance\")\n    plt.show()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/explained_variance/#neuro_py.ensemble.explained_variance.ExplainedVariance.pvalue","title":"<code>pvalue(n_shuffles=1000)</code>","text":"<p>Calculate p-value for explained variance by shuffling the template correlations.</p> Source code in <code>neuro_py/ensemble/explained_variance.py</code> <pre><code>def pvalue(self, n_shuffles=1000):\n    \"\"\"\n    Calculate p-value for explained variance by shuffling the template correlations.\n    \"\"\"\n    from copy import deepcopy\n\n    def shuffle_template(self):\n        template_corr = deepcopy(self.template_corr)\n        np.random.shuffle(template_corr)\n\n        partial_corr, _ = self.__calculate_partial_correlations_(\n            self.matching_paircorr, self.control_paircorr, template_corr\n        )\n        ev = np.nanmean(partial_corr**2, axis=0)\n        return ev.flatten()\n\n    if len(self.ev) &gt; 1:\n        print(\"Multiple time points, p-values are not supported\")\n        return\n\n    ev_shuffle = [shuffle_template(self) for _ in range(n_shuffles)]\n\n    ev_shuffle = np.array(ev_shuffle)\n\n    n = len(ev_shuffle)\n    r = np.sum(ev_shuffle &gt; self.ev)\n    pvalues = (r + 1) / (n + 1)\n    return pvalues\n</code></pre>"},{"location":"reference/neuro_py/ensemble/geometry/","title":"neuro_py.ensemble.geometry","text":""},{"location":"reference/neuro_py/ensemble/geometry/#neuro_py.ensemble.geometry.proximity","title":"<code>proximity(pv1, pv2)</code>","text":"<p>Proximity between two firing rate vector trajectories.</p> <p>Parameters:</p> Name Type Description Default <code>pv1</code> <code>ndarray</code> <p>Firing rate vector trajectory in one context. Shape: (num_bins, num_neurons)</p> required <code>pv2</code> <code>ndarray</code> <p>Firing rate vector trajectory in another context. Shape: (num_bins, num_neurons)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Proximity between the two contexts.</p> References <p>.. [1] Guidera, J. A., Gramling, D. P., Comrie, A. E., Joshi, A.,     Denovellis, E. L., Lee, K. H., Zhou, J., Thompson, P., Hernandez, J.,     Yorita, A., Haque, R., Kirst, C., &amp; Frank, L. M. (2024). Regional     specialization manifests in the reliability of neural population codes.     bioRxiv : the preprint server for biology, 2024.01.25.576941.     https://doi.org/10.1101/2024.01.25.576941</p> Source code in <code>neuro_py/ensemble/geometry.py</code> <pre><code>def proximity(pv1: np.ndarray, pv2: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Proximity between two firing rate vector trajectories.\n\n    Parameters\n    ----------\n    pv1 : numpy.ndarray\n        Firing rate vector trajectory in one context.\n        Shape: (num_bins, num_neurons)\n\n    pv2 : numpy.ndarray\n        Firing rate vector trajectory in another context.\n        Shape: (num_bins, num_neurons)\n\n    Returns\n    -------\n    numpy.ndarray\n        Proximity between the two contexts.\n\n    References\n    ----------\n    .. [1] Guidera, J. A., Gramling, D. P., Comrie, A. E., Joshi, A.,\n        Denovellis, E. L., Lee, K. H., Zhou, J., Thompson, P., Hernandez, J.,\n        Yorita, A., Haque, R., Kirst, C., &amp; Frank, L. M. (2024). Regional\n        specialization manifests in the reliability of neural population codes.\n        bioRxiv : the preprint server for biology, 2024.01.25.576941.\n        https://doi.org/10.1101/2024.01.25.576941\n    \"\"\"\n    # Calculate the norms\n    norm_diff = np.linalg.norm(pv1 - pv2, axis=1)\n\n    norm_diff_mean = np.apply_along_axis(\n        lambda e: np.mean(np.linalg.norm(e - pv2, axis=1)),\n        arr=pv1,\n        axis=1\n    )\n\n    # Calculate proximity\n    prox = 1 - (norm_diff / norm_diff_mean)\n\n    return prox\n</code></pre>"},{"location":"reference/neuro_py/ensemble/pairwise_bias_correlation/","title":"neuro_py.ensemble.pairwise_bias_correlation","text":""},{"location":"reference/neuro_py/ensemble/pairwise_bias_correlation/#neuro_py.ensemble.pairwise_bias_correlation.cosine_similarity_matrices","title":"<code>cosine_similarity_matrices(matrix1, matrix2)</code>","text":"<p>Compute the cosine similarity between two flattened matrices</p> <p>Parameters:</p> Name Type Description Default <code>matrix1</code> <code>ndarray</code> <p>A normalized bias matrix</p> required <code>matrix2</code> <code>ndarray</code> <p>Another normalized bias matrix</p> required <p>Returns:</p> Type Description <code>float</code> <p>The cosine similarity between the two matrices.</p> Source code in <code>neuro_py/ensemble/pairwise_bias_correlation.py</code> <pre><code>def cosine_similarity_matrices(\n    matrix1: np.ndarray,\n    matrix2: np.ndarray\n) -&gt; float:\n    \"\"\"\n    Compute the cosine similarity between two flattened matrices\n\n    Parameters\n    ----------\n    matrix1 : numpy.ndarray\n        A normalized bias matrix\n    matrix2 : numpy.ndarray\n        Another normalized bias matrix\n\n    Returns\n    -------\n    float\n        The cosine similarity between the two matrices.\n    \"\"\"\n    # Flatten matrices\n    x = matrix1.flatten().reshape(1, -1)\n    y = matrix2.flatten().reshape(1, -1)\n\n    if np.all(np.isnan(x)) or np.all(np.isnan(y)):\n        return np.nan\n\n    # handle nan values\n    x = np.nan_to_num(x)\n    y = np.nan_to_num(y)\n\n    cossim = sklearn.metrics.pairwise.cosine_similarity(x, y)\n\n    # Compute cosine similarity\n    return cossim.item()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/pairwise_bias_correlation/#neuro_py.ensemble.pairwise_bias_correlation.observed_and_shuffled_correlation","title":"<code>observed_and_shuffled_correlation(post_spikes, post_neurons, total_neurons, task_normalized, post_intervals, interval_i, num_shuffles=100)</code>","text":"<p>Calculate observed and shuffled correlations between task and post-task neural activity.</p> <p>This function computes the correlation between normalized task bias matrix and post-task bias matrix, as well as correlations with shuffled post-task data.</p> <p>Parameters:</p> Name Type Description Default <code>post_spikes</code> <code>ndarray</code> <p>Array of post-task spike times.</p> required <code>post_neurons</code> <code>ndarray</code> <p>Array of neuron IDs corresponding to post_spikes.</p> required <code>total_neurons</code> <code>int</code> <p>Total number of neurons in the dataset.</p> required <code>task_normalized</code> <code>ndarray</code> <p>Normalized bias matrix from task period.</p> required <code>post_intervals</code> <code>ndarray</code> <p>Array of post-task intervals, shape (n_intervals, 2).</p> required <code>interval_i</code> <code>int</code> <p>Index of the current interval to analyze.</p> required <code>num_shuffles</code> <code>int</code> <p>Number of times to shuffle post-task data for null distribution, by default 100.</p> <code>100</code> <p>Returns:</p> Type Description <code>Tuple[float, List[float]]</code> <p>A tuple containing: - observed_correlation: float     Cosine similarity between task and post-task bias matrices. - shuffled_correlation: List[float]     List of cosine similarities between task and shuffled post-task bias matrices.</p> Source code in <code>neuro_py/ensemble/pairwise_bias_correlation.py</code> <pre><code>def observed_and_shuffled_correlation(\n    post_spikes: np.ndarray,\n    post_neurons: np.ndarray,\n    total_neurons: int,\n    task_normalized: np.ndarray,\n    post_intervals: np.ndarray,\n    interval_i: int,\n    num_shuffles: int = 100,\n) -&gt; Tuple[float, List[float]]:\n    \"\"\"\n    Calculate observed and shuffled correlations between task and post-task neural activity.\n\n    This function computes the correlation between normalized task bias matrix and\n    post-task bias matrix, as well as correlations with shuffled post-task data.\n\n    Parameters\n    ----------\n    post_spikes : np.ndarray\n        Array of post-task spike times.\n    post_neurons : np.ndarray\n        Array of neuron IDs corresponding to post_spikes.\n    total_neurons : int\n        Total number of neurons in the dataset.\n    task_normalized : np.ndarray\n        Normalized bias matrix from task period.\n    post_intervals : np.ndarray\n        Array of post-task intervals, shape (n_intervals, 2).\n    interval_i : int\n        Index of the current interval to analyze.\n    num_shuffles : int, optional\n        Number of times to shuffle post-task data for null distribution, by default 100.\n\n    Returns\n    -------\n    Tuple[float, List[float]]\n        A tuple containing:\n        - observed_correlation: float\n            Cosine similarity between task and post-task bias matrices.\n        - shuffled_correlation: List[float]\n            List of cosine similarities between task and shuffled post-task bias matrices.\n    \"\"\"\n    # for i_interval in range(post_intervals.shape[0]):\n    idx = (post_spikes &gt; post_intervals[interval_i][0]) &amp; (\n        post_spikes &lt; post_intervals[interval_i][1]\n    )\n\n    post_bias_matrix = skew_bias_matrix(\n        post_spikes[idx], post_neurons[idx], total_neurons\n    )\n\n    # Compute cosine similarity between task and post-task bias matrices\n    observed_correlation = cosine_similarity_matrices(task_normalized, post_bias_matrix)\n\n    # Shuffle post-task spikes and compute bias matrix\n    shuffled_correlation = [\n        cosine_similarity_matrices(\n            task_normalized,\n            skew_bias_matrix(\n                post_spikes[idx],\n                np.random.permutation(post_neurons[idx]),\n                total_neurons,\n            ),\n        )\n        for _ in range(num_shuffles)\n    ]\n\n    return observed_correlation, shuffled_correlation\n</code></pre>"},{"location":"reference/neuro_py/ensemble/pairwise_bias_correlation/#neuro_py.ensemble.pairwise_bias_correlation.shuffled_significance","title":"<code>shuffled_significance(task_spikes, task_neurons, post_spikes, post_neurons, total_neurons, post_intervals=np.array([[-np.inf, np.inf]]), num_shuffles=100, n_jobs=-1)</code>","text":"<p>Computes the significance of the task-post correlation by comparing against shuffled distributions.</p> <p>Parameters:</p> Name Type Description Default <code>task_spikes</code> <code>ndarray</code> <p>Spike timestamps during the task. Shape is (n_spikes_task,)</p> required <code>task_neurons</code> <code>ndarray</code> <p>Neuron identifiers corresponding to each of <code>task_spikes</code>. Shape is (n_spikes_task,)</p> required <code>post_spikes</code> <code>ndarray</code> <p>Spike timestamps during post-task (e.g., sleep). Shape is (n_spikes_post,)</p> required <code>post_neurons</code> <code>ndarray</code> <p>Neuron identifiers corresponding to <code>post_spikes</code>. Shape is (n_spikes_post,)</p> required <code>total_neurons</code> <code>int</code> <p>Total number of neurons being considered</p> required <code>post_intervals</code> <code>ndarray</code> <p>Intervals for post-task epochs, with shape (n_intervals, 2). Each row defines the start and end of an interval. May correspond to specific sleep states. Default is <code>np.array([[-np.inf, np.inf]])</code>, representing the entire range of post-task epochs</p> <code>array([[-inf, inf]])</code> <code>num_shuffles</code> <code>int</code> <p>Number of shuffles to compute the significance. Default is 100</p> <code>100</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to use for shuffling. Default is -1 (use all available cores).</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>z_score</code> <code>ndarray</code> <p>Z-scores of the observed correlations compared to the shuffled distributions.  Shape is (n_intervals,).</p> <code>p_value</code> <code>ndarray</code> <p>P-values indicating the significance of the observed correlation.  Shape is (n_intervals,).</p> Notes <p>The function uses parallel processing to compute observed and shuffled  correlations for each post-task interval. The z-score is calculated as:</p> <pre><code>z_score = (observed_correlation - mean(shuffled_correlations)) / std(shuffled_correlations)\n</code></pre> <p>The p-value is computed as the proportion of shuffled correlations greater than  the observed correlation, with a small constant added for numerical stability.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; task_spikes = np.array([1.2, 3.4, 5.6])\n&gt;&gt;&gt; task_neurons = np.array([0, 1, 0])\n&gt;&gt;&gt; post_spikes = np.array([2.3, 4.5, 6.7])\n&gt;&gt;&gt; post_neurons = np.array([1, 0, 1])\n&gt;&gt;&gt; total_neurons = 2\n&gt;&gt;&gt; post_intervals = np.array([[0, 10]])\n&gt;&gt;&gt; z_score, p_value = shuffled_significance(task_spikes, task_neurons, post_spikes, post_neurons, total_neurons, post_intervals)\n&gt;&gt;&gt; z_score\narray([1.23])\n&gt;&gt;&gt; p_value\narray([0.04])\n</code></pre> Source code in <code>neuro_py/ensemble/pairwise_bias_correlation.py</code> <pre><code>def shuffled_significance(\n    task_spikes: np.ndarray,\n    task_neurons: np.ndarray,\n    post_spikes: np.ndarray,\n    post_neurons: np.ndarray,\n    total_neurons: int,\n    post_intervals: np.ndarray = np.array([[-np.inf, np.inf]]),\n    num_shuffles: int = 100,\n    n_jobs: int = -1,\n):\n    \"\"\"\n    Computes the significance of the task-post correlation by comparing against shuffled distributions.\n\n    Parameters\n    ----------\n    task_spikes : np.ndarray\n        Spike timestamps during the task. Shape is (n_spikes_task,)\n    task_neurons : np.ndarray\n        Neuron identifiers corresponding to each of `task_spikes`. Shape is\n        (n_spikes_task,)\n    post_spikes : np.ndarray\n        Spike timestamps during post-task (e.g., sleep). Shape is\n        (n_spikes_post,)\n    post_neurons : np.ndarray\n        Neuron identifiers corresponding to `post_spikes`. Shape is\n        (n_spikes_post,)\n    total_neurons : int\n        Total number of neurons being considered\n    post_intervals : np.ndarray, optional\n        Intervals for post-task epochs, with shape (n_intervals, 2).\n        Each row defines the start and end of an interval. May correspond to\n        specific sleep states. Default is `np.array([[-np.inf, np.inf]])`,\n        representing the entire range of post-task epochs\n    num_shuffles : int, optional\n        Number of shuffles to compute the significance. Default is 100\n    n_jobs : int, optional\n        Number of parallel jobs to use for shuffling. Default is -1 (use all\n        available cores).\n\n    Returns\n    -------\n    z_score : np.ndarray\n        Z-scores of the observed correlations compared to the shuffled distributions. \n        Shape is (n_intervals,).\n    p_value : np.ndarray\n        P-values indicating the significance of the observed correlation. \n        Shape is (n_intervals,).\n\n    Notes\n    -----\n    The function uses parallel processing to compute observed and shuffled \n    correlations for each post-task interval. The z-score is calculated as:\n\n        z_score = (observed_correlation - mean(shuffled_correlations)) / std(shuffled_correlations)\n\n    The p-value is computed as the proportion of shuffled correlations greater than \n    the observed correlation, with a small constant added for numerical stability.\n\n    Examples\n    --------\n    &gt;&gt;&gt; task_spikes = np.array([1.2, 3.4, 5.6])\n    &gt;&gt;&gt; task_neurons = np.array([0, 1, 0])\n    &gt;&gt;&gt; post_spikes = np.array([2.3, 4.5, 6.7])\n    &gt;&gt;&gt; post_neurons = np.array([1, 0, 1])\n    &gt;&gt;&gt; total_neurons = 2\n    &gt;&gt;&gt; post_intervals = np.array([[0, 10]])\n    &gt;&gt;&gt; z_score, p_value = shuffled_significance(task_spikes, task_neurons, post_spikes, post_neurons, total_neurons, post_intervals)\n    &gt;&gt;&gt; z_score\n    array([1.23])\n    &gt;&gt;&gt; p_value\n    array([0.04])\n    \"\"\"\n    # set random seed for reproducibility\n    np.random.seed(0)\n\n    # Compute bias matrices for task epochs\n    task_bias_matrix = skew_bias_matrix(\n        task_spikes, task_neurons, total_neurons\n    )\n\n    # Get shuffled and observed correlations using parallel processing\n    observed_correlation, shuffled_correlations = zip(\n        *Parallel(n_jobs=n_jobs)(\n            delayed(observed_and_shuffled_correlation)(\n                post_spikes,\n                post_neurons,\n                total_neurons,\n                task_bias_matrix,\n                post_intervals,\n                interval_i,\n                num_shuffles,\n            )\n            for interval_i in range(post_intervals.shape[0])\n        )\n    )\n    observed_correlation, shuffled_correlations = np.array(\n        observed_correlation\n    ), np.array(shuffled_correlations)\n    # Compute z-score\n    shuffled_mean = np.mean(shuffled_correlations, axis=1)\n    shuffled_std = np.std(shuffled_correlations, axis=1)\n    z_score = (observed_correlation - shuffled_mean) / shuffled_std\n\n    # significance test between the observed correlation and the shuffled distribution\n    p_value = (np.sum(shuffled_correlations.T &gt; observed_correlation, axis=0) + 1) / (\n        num_shuffles + 1\n    )\n\n    return z_score, p_value\n</code></pre>"},{"location":"reference/neuro_py/ensemble/pairwise_bias_correlation/#neuro_py.ensemble.pairwise_bias_correlation.skew_bias_matrix","title":"<code>skew_bias_matrix(spike_times, neuron_ids, total_neurons, fillneutral=0)</code>","text":"<p>Compute the pairwise skew-bias matrix for a given sequence of spikes.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>ndarray</code> <p>Spike times for the sequence, assumed to be sorted.</p> required <code>neuron_ids</code> <code>ndarray</code> <p>Neuron identifiers corresponding to <code>spike_times</code>. Values should be integers between 0 and <code>total_neurons - 1</code>.</p> required <code>total_neurons</code> <code>int</code> <p>Total number of neurons being considered.</p> required <code>fillneutral</code> <code>float</code> <p>Value to fill for neutral bias, by default 0</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Skew-bias matrix of size <code>(total_neurons, total_neurons)</code> where each entry represents the normalized bias between neuron pairs.</p> Notes <p>The probability-bias ( B_{ij} ) for neurons ( i ) and ( j ) is computed as: [ B_{ij} = \\frac{nspikes_{ij}}{nspikes_i \\cdot nspikes_j} ] where ( nspikes_{ij} ) is the count of spikes from neuron ( i ) occurring before spikes from neuron ( j ). If there are no spikes for either neuron, the bias is set to 0.5 (neutral bias).</p> <p>The skew-bias matrix is computed as: [ S_{ij} = 2 \\cdot B_{ij} - 1 ] where ( B_{ij} ) is the probability-bias matrix.</p> <p>The skew-bias matrix is a skew-symmetric matrix as ( S_{ij} = -S_{ji} ). The values are normalized between -1 and 1. A value of 1 indicates that neuron ( i ) spikes before neuron ( j ) in all cases, while -1 indicates the opposite. A value of 0 indicates that the order of spikes is random.</p> References <p>.. [1] Roth, Z. (2016). Analysis of neuronal sequences using pairwise     biases. arXiv, 11-16. https://arxiv.org/abs/1603.02916</p> Source code in <code>neuro_py/ensemble/pairwise_bias_correlation.py</code> <pre><code>@njit\ndef skew_bias_matrix(\n    spike_times: np.ndarray,\n    neuron_ids: np.ndarray,\n    total_neurons: int,\n    fillneutral: float = 0\n) -&gt; np.ndarray:\n    r\"\"\"\n    Compute the pairwise skew-bias matrix for a given sequence of spikes.\n\n    Parameters\n    ----------\n    spike_times : numpy.ndarray\n        Spike times for the sequence, assumed to be sorted.\n    neuron_ids : numpy.ndarray\n        Neuron identifiers corresponding to `spike_times`.\n        Values should be integers between 0 and `total_neurons - 1`.\n    total_neurons : int\n        Total number of neurons being considered.\n    fillneutral : float, optional\n        Value to fill for neutral bias, by default 0\n\n    Returns\n    -------\n    numpy.ndarray\n        Skew-bias matrix of size `(total_neurons, total_neurons)` where\n        each entry represents the normalized bias between neuron pairs.\n\n    Notes\n    -----\n    The probability-bias \\( B_{ij} \\) for neurons \\( i \\) and \\( j \\) is\n    computed as:\n    \\[\n    B_{ij} = \\frac{nspikes_{ij}}{nspikes_i \\cdot nspikes_j}\n    \\]\n    where \\( nspikes_{ij} \\) is the count of spikes from neuron \\( i \\)\n    occurring before spikes from neuron \\( j \\). If there are no spikes for\n    either neuron, the bias is set to 0.5 (neutral bias).\n\n    The skew-bias matrix is computed as:\n    \\[\n    S_{ij} = 2 \\cdot B_{ij} - 1\n    \\]\n    where \\( B_{ij} \\) is the probability-bias matrix.\n\n    The skew-bias matrix is a skew-symmetric matrix as \\( S_{ij} = -S_{ji} \\).\n    The values are normalized between -1 and 1. A value of 1 indicates that\n    neuron \\( i \\) spikes before neuron \\( j \\) in all cases, while -1 indicates\n    the opposite. A value of 0 indicates that the order of spikes is random.\n\n    References\n    ----------\n    .. [1] Roth, Z. (2016). Analysis of neuronal sequences using pairwise\n        biases. arXiv, 11-16. https://arxiv.org/abs/1603.02916\n    \"\"\"\n    bias = np.empty((total_neurons, total_neurons))\n    nrn_spk_rindices = np.empty(total_neurons+1, dtype=np.int64)\n    nrn_spk_rindices[0] = 0\n\n    nrns_st = numba.typed.List()\n    for _ in range(total_neurons):\n        nrns_st.append(numba.typed.List.empty_list(np.float64))\n    for i, nrn_id in enumerate(neuron_ids):\n        nrns_st[nrn_id].append(spike_times[i])\n    for nnrn in range(total_neurons):\n        nrn_spk_rindices[nnrn+1] = nrn_spk_rindices[nnrn] + len(nrns_st[nnrn])\n\n    nrns_st_all = np.empty(nrn_spk_rindices[-1], dtype=np.float64)\n    for nnrn in range(total_neurons):\n        nrns_st_all[nrn_spk_rindices[nnrn]:nrn_spk_rindices[nnrn+1]] = np.asarray(nrns_st[nnrn])\n\n    # Build bias matrix\n    for i in range(total_neurons):\n        spikes_i = nrns_st_all[nrn_spk_rindices[i]:nrn_spk_rindices[i+1]]\n        nspikes_i = len(spikes_i)\n\n        for j in range(i + 1, total_neurons):\n            nspikes_j = len(nrns_st[j])\n            if (nspikes_i == 0) or (nspikes_j == 0):\n                bias[i, j] = bias[j, i] = fillneutral\n            else:\n                spikes_j = nrns_st_all[\n                    nrn_spk_rindices[j]:nrn_spk_rindices[j+1]]\n\n                nspikes_ij = np.searchsorted(\n                    spikes_i, spikes_j, side='right').sum()\n                bias[i, j] = 2 * (nspikes_ij / (nspikes_i * nspikes_j)) - 1\n                bias[j, i] = -bias[i, j]\n\n    # set diagonal to fillneutral\n    for i in range(total_neurons):\n        bias[i, i] = fillneutral\n\n    return bias\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/","title":"neuro_py.ensemble.replay","text":""},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.PairwiseBias","title":"<code>PairwiseBias</code>","text":"<p>               Bases: <code>object</code></p> <p>Pairwise bias analysis for comparing task and post-task spike sequences.</p> <p>Parameters:</p> Name Type Description Default <code>num_shuffles</code> <code>int</code> <p>Number of shuffles to perform for significance testing. Default is 300.</p> <code>300</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to run for computing correlations. Default is 10.</p> <code>10</code> <p>Attributes:</p> Name Type Description <code>total_neurons</code> <code>int, or None</code> <p>Total number of neurons in the dataset.</p> <code>task_skew_bias</code> <code>np.ndarray, or None</code> <p>Normalized skew-bias matrix for the task data.</p> <code>observed_correlation_</code> <code>np.ndarray, or None</code> <p>Observed cosine similarity between task and post-task bias matrices.</p> <code>shuffled_correlations_</code> <code>np.ndarray, or None</code> <p>Shuffled cosine similarities for significance testing.</p> <code>z_score_</code> <code>np.ndarray, or None</code> <p>Z-score of the observed correlation compared to the shuffled distribution.</p> <code>p_value_</code> <code>np.ndarray, or None</code> <p>p-value for significance test.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Fit the model using the task spike data.</p> <code>transform</code> <p>Transform the post-task data to compute z-scores and p-values.</p> <code>fit_transform</code> <p>Fit the model with task data and transform the post-task data.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>class PairwiseBias(object):\n    \"\"\"\n    Pairwise bias analysis for comparing task and post-task spike sequences.\n\n    Parameters\n    ----------\n    num_shuffles : int, optional\n        Number of shuffles to perform for significance testing. Default is 300.\n    n_jobs : int, optional\n        Number of parallel jobs to run for computing correlations. Default is 10.\n\n    Attributes\n    ----------\n    total_neurons : int, or None\n        Total number of neurons in the dataset.\n    task_skew_bias : np.ndarray, or None\n        Normalized skew-bias matrix for the task data.\n    observed_correlation_ : np.ndarray, or None\n        Observed cosine similarity between task and post-task bias matrices.\n    shuffled_correlations_ : np.ndarray, or None\n        Shuffled cosine similarities for significance testing.\n    z_score_ : np.ndarray, or None\n        Z-score of the observed correlation compared to the shuffled distribution.\n    p_value_ : np.ndarray, or None\n        p-value for significance test.\n\n    Methods\n    -------\n    fit(task_spikes: Union[List[float], np.ndarray], task_neurons: Union[List[int], np.ndarray]) -&gt; 'PairwiseBias'\n        Fit the model using the task spike data.\n    transform(post_spikes: Union[List[float], np.ndarray], post_neurons: Union[List[int], np.ndarray], post_intervals: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]\n        Transform the post-task data to compute z-scores and p-values.\n    fit_transform(task_spikes: Union[List[float], np.ndarray], task_neurons: Union[List[int], np.ndarray], post_spikes: Union[List[float], np.ndarray], post_neurons: Union[List[int], np.ndarray], post_intervals: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]\n        Fit the model with task data and transform the post-task data.\n    \"\"\"\n\n    def __init__(\n        self, num_shuffles: int = 300, n_jobs: int = 10, fillneutral: float = np.nan\n    ):\n        self.num_shuffles = num_shuffles\n        self.n_jobs = n_jobs\n        self.fillneutral = fillneutral\n        self.total_neurons = None\n        self.task_skew_bias = None\n        self.observed_correlation_ = None\n        self.shuffled_correlations_ = None\n        self.z_score_ = None\n        self.p_value_ = None\n\n    @staticmethod\n    def bias_matrix(\n        spike_times: np.ndarray,\n        neuron_ids: np.ndarray,\n        total_neurons: int,\n        fillneutral: float = np.nan,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Optimized computation of the bias matrix B_k for a given sequence of spikes using vectorized operations.\n\n        Parameters\n        ----------\n        spike_times : np.ndarray\n            Spike times for the sequence.\n        neuron_ids : np.ndarray\n            Neuron identifiers corresponding to spike_times.\n        total_neurons : int\n            Total number of neurons being considered.\n        fillneutral : float, optional\n            Value to fill the diagonal of the bias matrix and other empty\n            combinations, by default np.nan.\n\n        Returns\n        -------\n        np.ndarray\n            A matrix of size (total_neurons, total_neurons) representing the bias.\n        \"\"\"\n        return skew_bias_matrix(spike_times, neuron_ids, total_neurons, fillneutral)\n\n    @staticmethod\n    def cosine_similarity_matrices(matrix1: np.ndarray, matrix2: np.ndarray) -&gt; float:\n        \"\"\"\n        Computes the cosine similarity between two flattened bias matrices.\n\n        Parameters\n        ----------\n        matrix1 : np.ndarray\n            A normalized bias matrix.\n        matrix2 : np.ndarray\n            Another normalized bias matrix.\n\n        Returns\n        -------\n        float\n            The cosine similarity between the two matrices.\n        \"\"\"\n        return cosine_similarity_matrices(matrix1, matrix2)\n\n    def observed_and_shuffled_correlation(\n        self,\n        post_spikes: np.ndarray,\n        post_neurons: np.ndarray,\n        task_skew_bias: np.ndarray,\n        post_intervals: np.ndarray,\n        interval_i: int,\n    ) -&gt; Tuple[float, List[float]]:\n        \"\"\"\n        Compute observed and shuffled correlation for a given post-task interval.\n\n        Parameters\n        ----------\n        post_spikes : np.ndarray\n            Spike times during post-task (e.g., sleep).\n        post_neurons : np.ndarray\n            Neuron identifiers for post-task spikes.\n        task_normalized : np.ndarray\n            Normalized task bias matrix.\n        post_intervals : np.ndarray\n            Intervals for post-task epochs.\n        interval_i : int\n            Index of the current post-task interval.\n\n        Returns\n        -------\n        Tuple[float, List[float]]\n            The observed correlation and a list of shuffled correlations.\n        \"\"\"\n        post_neurons = np.asarray(post_neurons, dtype=int)\n\n        start, end = post_intervals[interval_i]\n        start_idx = np.searchsorted(post_spikes, start, side=\"left\")\n        end_idx = np.searchsorted(post_spikes, end, side=\"right\")\n\n        filtered_spikes = post_spikes[start_idx:end_idx]\n        filtered_neurons = post_neurons[start_idx:end_idx]\n\n        post_skew_bias = self.bias_matrix(\n            filtered_spikes,\n            filtered_neurons,\n            self.total_neurons,\n            fillneutral=self.fillneutral,\n        )\n\n        observed_correlation = self.cosine_similarity_matrices(\n            task_skew_bias, post_skew_bias\n        )\n\n        shuffled_correlation = []\n        for _ in range(self.num_shuffles):\n            shuffled_neurons = np.random.permutation(filtered_neurons)\n            shuffled_skew_bias = self.bias_matrix(\n                filtered_spikes,\n                shuffled_neurons,\n                self.total_neurons,\n                fillneutral=self.fillneutral,\n            )\n            shuffled_correlation.append(\n                self.cosine_similarity_matrices(task_skew_bias, shuffled_skew_bias)\n            )\n\n        return observed_correlation, shuffled_correlation\n\n    def fit(\n        self,\n        task_spikes: np.ndarray,\n        task_neurons: np.ndarray,\n        task_intervals: np.ndarray = None,\n    ) -&gt; \"PairwiseBias\":\n        \"\"\"\n        Fit the model using the task spike data.\n\n        Parameters\n        ----------\n        task_spikes : np.ndarray\n            Spike times during the task.\n        task_neurons : np.ndarray\n            Neuron identifiers for task spikes.\n        task_intervals : np.ndarray, optional\n            Intervals for task epochs, by default None. If None, the entire task\n            data is used. Otherwise, the average bias matrix is computed across\n            all task intervals. Shape: (n_intervals, 2).\n\n        Returns\n        -------\n        PairwiseBias\n            Returns the instance itself.\n        \"\"\"\n        # Convert task_neurons to numpy array of integers\n        task_neurons = np.asarray(task_neurons, dtype=int)\n\n        # Calculate the total number of neurons based on unique entries in task_neurons\n        self.total_neurons = len(np.unique(task_neurons))\n\n        if task_intervals is None:\n            # Compute bias matrix for task data and normalize\n            task_skew_bias = self.bias_matrix(\n                task_spikes,\n                task_neurons,\n                self.total_neurons,\n                fillneutral=self.fillneutral,\n            )\n            self.task_skew_bias = task_skew_bias\n        else:\n            # Compute bias matrices for each task interval\n            task_skew_biases = []\n\n            for interval in task_intervals:\n                # find the indices of spikes within the interval\n                start_idx = np.searchsorted(task_spikes, interval[0], side=\"left\")\n                end_idx = np.searchsorted(task_spikes, interval[1], side=\"right\")\n\n                # Extract spikes and neurons within the interval\n                interval_spikes = task_spikes[start_idx:end_idx]\n                interval_neurons = task_neurons[start_idx:end_idx]\n\n                # Compute the bias matrix for the interval\n                interval_skew_bias = self.bias_matrix(\n                    interval_spikes,\n                    interval_neurons,\n                    self.total_neurons,\n                    fillneutral=self.fillneutral,\n                )\n                task_skew_biases.append(interval_skew_bias)\n\n            # Average the normalized bias matrices\n            # I expect to see RuntimeWarnings in this block\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n                self.task_skew_bias = np.nanmean(task_skew_biases, axis=0)\n        return self\n\n    def transform(\n        self,\n        post_spikes: np.ndarray,\n        post_neurons: np.ndarray,\n        post_intervals: np.ndarray,\n        allow_reverse_replay: bool = False,\n        parallel: bool = True,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Transform the post-task data to compute z-scores and p-values.\n\n        Parameters\n        ----------\n        post_spikes : np.ndarray\n            Spike times during post-task (e.g., sleep).\n        post_neurons : np.ndarray\n            Neuron identifiers for post-task spikes.\n        post_intervals : np.ndarray\n            Intervals for post-task epochs. Shape: (n_intervals, 2).\n        allow_reverse_replay : bool, optional\n            Whether to allow reverse sequences, by default False.\n        parallel : bool, optional\n            Whether to run in parallel, by default True.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray, np.ndarray]\n            z_score: The z-score of the observed correlation compared to the shuffled distribution.\n            p_value: p-value for significance test.\n            observed_correlation_: The observed correlation for each interval.\n        \"\"\"\n        # Check if the number of jobs is less than the number of intervals\n        if post_intervals.shape[0] &lt; self.n_jobs:\n            self.n_jobs = post_intervals.shape[0]\n\n        if parallel:\n            observed_correlation, shuffled_correlations = zip(\n                *Parallel(n_jobs=self.n_jobs)(\n                    delayed(self.observed_and_shuffled_correlation)(\n                        post_spikes,\n                        post_neurons,\n                        self.task_skew_bias,\n                        post_intervals,\n                        interval_i,\n                    )\n                    for interval_i in range(post_intervals.shape[0])\n                )\n            )\n        else:  # Run in serial for debugging\n            observed_correlation, shuffled_correlations = zip(\n                *[\n                    self.observed_and_shuffled_correlation(\n                        post_spikes,\n                        post_neurons,\n                        self.task_skew_bias,\n                        post_intervals,\n                        interval_i,\n                    )\n                    for interval_i in range(post_intervals.shape[0])\n                ]\n            )\n\n        self.observed_correlation_ = np.array(\n            observed_correlation\n        )  # Shape: (n_intervals,)\n        self.shuffled_correlations_ = np.array(\n            shuffled_correlations\n        )  # Shape: (n_intervals, n_shuffles)\n\n        shuffled_mean = np.mean(self.shuffled_correlations_, axis=1)\n        shuffled_std = np.std(self.shuffled_correlations_, axis=1)\n        self.z_score_ = (self.observed_correlation_ - shuffled_mean) / shuffled_std\n\n        observed_correlation = self.observed_correlation_\n        shuffled_correlations = self.shuffled_correlations_\n        if allow_reverse_replay:\n            observed_correlation = np.abs(observed_correlation)\n            shuffled_correlations = np.abs(shuffled_correlations)\n\n        self.p_value_ = (\n            np.sum(\n                shuffled_correlations.T &gt; observed_correlation,\n                axis=0,\n            )\n            + 1\n        ) / (self.num_shuffles + 1)\n\n        return self.z_score_, self.p_value_, self.observed_correlation_\n\n    def fit_transform(\n        self,\n        task_spikes: np.ndarray,\n        task_neurons: np.ndarray,\n        task_intervals: np.ndarray,\n        post_spikes: np.ndarray,\n        post_neurons: np.ndarray,\n        post_intervals: np.ndarray,\n        allow_reverse_replay: bool = False,\n        parallel: bool = True,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Fit the model with task data and transform the post-task data.\n\n        Parameters\n        ----------\n        task_spikes : np.ndarray\n            Spike times during the task.\n        task_neurons : np.ndarray\n            Neuron identifiers for task spikes.\n        task_intervals : np.ndarray\n            Intervals for task epochs. Shape: (n_intervals, 2).\n        post_spikes : np.ndarray\n            Spike times during post-task (e.g., sleep).\n        post_neurons : np.ndarray\n            Neuron identifiers for post-task spikes.\n        post_intervals : np.ndarray\n            Intervals for post-task epochs. Shape: (n_intervals, 2).\n        allow_reverse_replay : bool, optional\n            Whether to allow reverse sequences, by default False.\n        parallel : bool, optional\n            Whether to run in parallel, by default True.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray, np.ndarray]\n            z_score: The z-score of the observed correlation compared to the shuffled distribution.\n            p_value: p-value for significance test.\n            observed_correlation_: The observed correlation for each interval.\n        \"\"\"\n        self.fit(task_spikes, task_neurons, task_intervals)\n        return self.transform(\n            post_spikes, post_neurons, post_intervals, allow_reverse_replay, parallel\n        )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.PairwiseBias.bias_matrix","title":"<code>bias_matrix(spike_times, neuron_ids, total_neurons, fillneutral=np.nan)</code>  <code>staticmethod</code>","text":"<p>Optimized computation of the bias matrix B_k for a given sequence of spikes using vectorized operations.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>ndarray</code> <p>Spike times for the sequence.</p> required <code>neuron_ids</code> <code>ndarray</code> <p>Neuron identifiers corresponding to spike_times.</p> required <code>total_neurons</code> <code>int</code> <p>Total number of neurons being considered.</p> required <code>fillneutral</code> <code>float</code> <p>Value to fill the diagonal of the bias matrix and other empty combinations, by default np.nan.</p> <code>nan</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A matrix of size (total_neurons, total_neurons) representing the bias.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>@staticmethod\ndef bias_matrix(\n    spike_times: np.ndarray,\n    neuron_ids: np.ndarray,\n    total_neurons: int,\n    fillneutral: float = np.nan,\n) -&gt; np.ndarray:\n    \"\"\"\n    Optimized computation of the bias matrix B_k for a given sequence of spikes using vectorized operations.\n\n    Parameters\n    ----------\n    spike_times : np.ndarray\n        Spike times for the sequence.\n    neuron_ids : np.ndarray\n        Neuron identifiers corresponding to spike_times.\n    total_neurons : int\n        Total number of neurons being considered.\n    fillneutral : float, optional\n        Value to fill the diagonal of the bias matrix and other empty\n        combinations, by default np.nan.\n\n    Returns\n    -------\n    np.ndarray\n        A matrix of size (total_neurons, total_neurons) representing the bias.\n    \"\"\"\n    return skew_bias_matrix(spike_times, neuron_ids, total_neurons, fillneutral)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.PairwiseBias.cosine_similarity_matrices","title":"<code>cosine_similarity_matrices(matrix1, matrix2)</code>  <code>staticmethod</code>","text":"<p>Computes the cosine similarity between two flattened bias matrices.</p> <p>Parameters:</p> Name Type Description Default <code>matrix1</code> <code>ndarray</code> <p>A normalized bias matrix.</p> required <code>matrix2</code> <code>ndarray</code> <p>Another normalized bias matrix.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The cosine similarity between the two matrices.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>@staticmethod\ndef cosine_similarity_matrices(matrix1: np.ndarray, matrix2: np.ndarray) -&gt; float:\n    \"\"\"\n    Computes the cosine similarity between two flattened bias matrices.\n\n    Parameters\n    ----------\n    matrix1 : np.ndarray\n        A normalized bias matrix.\n    matrix2 : np.ndarray\n        Another normalized bias matrix.\n\n    Returns\n    -------\n    float\n        The cosine similarity between the two matrices.\n    \"\"\"\n    return cosine_similarity_matrices(matrix1, matrix2)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.PairwiseBias.fit","title":"<code>fit(task_spikes, task_neurons, task_intervals=None)</code>","text":"<p>Fit the model using the task spike data.</p> <p>Parameters:</p> Name Type Description Default <code>task_spikes</code> <code>ndarray</code> <p>Spike times during the task.</p> required <code>task_neurons</code> <code>ndarray</code> <p>Neuron identifiers for task spikes.</p> required <code>task_intervals</code> <code>ndarray</code> <p>Intervals for task epochs, by default None. If None, the entire task data is used. Otherwise, the average bias matrix is computed across all task intervals. Shape: (n_intervals, 2).</p> <code>None</code> <p>Returns:</p> Type Description <code>PairwiseBias</code> <p>Returns the instance itself.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def fit(\n    self,\n    task_spikes: np.ndarray,\n    task_neurons: np.ndarray,\n    task_intervals: np.ndarray = None,\n) -&gt; \"PairwiseBias\":\n    \"\"\"\n    Fit the model using the task spike data.\n\n    Parameters\n    ----------\n    task_spikes : np.ndarray\n        Spike times during the task.\n    task_neurons : np.ndarray\n        Neuron identifiers for task spikes.\n    task_intervals : np.ndarray, optional\n        Intervals for task epochs, by default None. If None, the entire task\n        data is used. Otherwise, the average bias matrix is computed across\n        all task intervals. Shape: (n_intervals, 2).\n\n    Returns\n    -------\n    PairwiseBias\n        Returns the instance itself.\n    \"\"\"\n    # Convert task_neurons to numpy array of integers\n    task_neurons = np.asarray(task_neurons, dtype=int)\n\n    # Calculate the total number of neurons based on unique entries in task_neurons\n    self.total_neurons = len(np.unique(task_neurons))\n\n    if task_intervals is None:\n        # Compute bias matrix for task data and normalize\n        task_skew_bias = self.bias_matrix(\n            task_spikes,\n            task_neurons,\n            self.total_neurons,\n            fillneutral=self.fillneutral,\n        )\n        self.task_skew_bias = task_skew_bias\n    else:\n        # Compute bias matrices for each task interval\n        task_skew_biases = []\n\n        for interval in task_intervals:\n            # find the indices of spikes within the interval\n            start_idx = np.searchsorted(task_spikes, interval[0], side=\"left\")\n            end_idx = np.searchsorted(task_spikes, interval[1], side=\"right\")\n\n            # Extract spikes and neurons within the interval\n            interval_spikes = task_spikes[start_idx:end_idx]\n            interval_neurons = task_neurons[start_idx:end_idx]\n\n            # Compute the bias matrix for the interval\n            interval_skew_bias = self.bias_matrix(\n                interval_spikes,\n                interval_neurons,\n                self.total_neurons,\n                fillneutral=self.fillneutral,\n            )\n            task_skew_biases.append(interval_skew_bias)\n\n        # Average the normalized bias matrices\n        # I expect to see RuntimeWarnings in this block\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n            self.task_skew_bias = np.nanmean(task_skew_biases, axis=0)\n    return self\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.PairwiseBias.fit_transform","title":"<code>fit_transform(task_spikes, task_neurons, task_intervals, post_spikes, post_neurons, post_intervals, allow_reverse_replay=False, parallel=True)</code>","text":"<p>Fit the model with task data and transform the post-task data.</p> <p>Parameters:</p> Name Type Description Default <code>task_spikes</code> <code>ndarray</code> <p>Spike times during the task.</p> required <code>task_neurons</code> <code>ndarray</code> <p>Neuron identifiers for task spikes.</p> required <code>task_intervals</code> <code>ndarray</code> <p>Intervals for task epochs. Shape: (n_intervals, 2).</p> required <code>post_spikes</code> <code>ndarray</code> <p>Spike times during post-task (e.g., sleep).</p> required <code>post_neurons</code> <code>ndarray</code> <p>Neuron identifiers for post-task spikes.</p> required <code>post_intervals</code> <code>ndarray</code> <p>Intervals for post-task epochs. Shape: (n_intervals, 2).</p> required <code>allow_reverse_replay</code> <code>bool</code> <p>Whether to allow reverse sequences, by default False.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to run in parallel, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>z_score: The z-score of the observed correlation compared to the shuffled distribution. p_value: p-value for significance test. observed_correlation_: The observed correlation for each interval.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def fit_transform(\n    self,\n    task_spikes: np.ndarray,\n    task_neurons: np.ndarray,\n    task_intervals: np.ndarray,\n    post_spikes: np.ndarray,\n    post_neurons: np.ndarray,\n    post_intervals: np.ndarray,\n    allow_reverse_replay: bool = False,\n    parallel: bool = True,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Fit the model with task data and transform the post-task data.\n\n    Parameters\n    ----------\n    task_spikes : np.ndarray\n        Spike times during the task.\n    task_neurons : np.ndarray\n        Neuron identifiers for task spikes.\n    task_intervals : np.ndarray\n        Intervals for task epochs. Shape: (n_intervals, 2).\n    post_spikes : np.ndarray\n        Spike times during post-task (e.g., sleep).\n    post_neurons : np.ndarray\n        Neuron identifiers for post-task spikes.\n    post_intervals : np.ndarray\n        Intervals for post-task epochs. Shape: (n_intervals, 2).\n    allow_reverse_replay : bool, optional\n        Whether to allow reverse sequences, by default False.\n    parallel : bool, optional\n        Whether to run in parallel, by default True.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, np.ndarray]\n        z_score: The z-score of the observed correlation compared to the shuffled distribution.\n        p_value: p-value for significance test.\n        observed_correlation_: The observed correlation for each interval.\n    \"\"\"\n    self.fit(task_spikes, task_neurons, task_intervals)\n    return self.transform(\n        post_spikes, post_neurons, post_intervals, allow_reverse_replay, parallel\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.PairwiseBias.observed_and_shuffled_correlation","title":"<code>observed_and_shuffled_correlation(post_spikes, post_neurons, task_skew_bias, post_intervals, interval_i)</code>","text":"<p>Compute observed and shuffled correlation for a given post-task interval.</p> <p>Parameters:</p> Name Type Description Default <code>post_spikes</code> <code>ndarray</code> <p>Spike times during post-task (e.g., sleep).</p> required <code>post_neurons</code> <code>ndarray</code> <p>Neuron identifiers for post-task spikes.</p> required <code>task_normalized</code> <code>ndarray</code> <p>Normalized task bias matrix.</p> required <code>post_intervals</code> <code>ndarray</code> <p>Intervals for post-task epochs.</p> required <code>interval_i</code> <code>int</code> <p>Index of the current post-task interval.</p> required <p>Returns:</p> Type Description <code>Tuple[float, List[float]]</code> <p>The observed correlation and a list of shuffled correlations.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def observed_and_shuffled_correlation(\n    self,\n    post_spikes: np.ndarray,\n    post_neurons: np.ndarray,\n    task_skew_bias: np.ndarray,\n    post_intervals: np.ndarray,\n    interval_i: int,\n) -&gt; Tuple[float, List[float]]:\n    \"\"\"\n    Compute observed and shuffled correlation for a given post-task interval.\n\n    Parameters\n    ----------\n    post_spikes : np.ndarray\n        Spike times during post-task (e.g., sleep).\n    post_neurons : np.ndarray\n        Neuron identifiers for post-task spikes.\n    task_normalized : np.ndarray\n        Normalized task bias matrix.\n    post_intervals : np.ndarray\n        Intervals for post-task epochs.\n    interval_i : int\n        Index of the current post-task interval.\n\n    Returns\n    -------\n    Tuple[float, List[float]]\n        The observed correlation and a list of shuffled correlations.\n    \"\"\"\n    post_neurons = np.asarray(post_neurons, dtype=int)\n\n    start, end = post_intervals[interval_i]\n    start_idx = np.searchsorted(post_spikes, start, side=\"left\")\n    end_idx = np.searchsorted(post_spikes, end, side=\"right\")\n\n    filtered_spikes = post_spikes[start_idx:end_idx]\n    filtered_neurons = post_neurons[start_idx:end_idx]\n\n    post_skew_bias = self.bias_matrix(\n        filtered_spikes,\n        filtered_neurons,\n        self.total_neurons,\n        fillneutral=self.fillneutral,\n    )\n\n    observed_correlation = self.cosine_similarity_matrices(\n        task_skew_bias, post_skew_bias\n    )\n\n    shuffled_correlation = []\n    for _ in range(self.num_shuffles):\n        shuffled_neurons = np.random.permutation(filtered_neurons)\n        shuffled_skew_bias = self.bias_matrix(\n            filtered_spikes,\n            shuffled_neurons,\n            self.total_neurons,\n            fillneutral=self.fillneutral,\n        )\n        shuffled_correlation.append(\n            self.cosine_similarity_matrices(task_skew_bias, shuffled_skew_bias)\n        )\n\n    return observed_correlation, shuffled_correlation\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.PairwiseBias.transform","title":"<code>transform(post_spikes, post_neurons, post_intervals, allow_reverse_replay=False, parallel=True)</code>","text":"<p>Transform the post-task data to compute z-scores and p-values.</p> <p>Parameters:</p> Name Type Description Default <code>post_spikes</code> <code>ndarray</code> <p>Spike times during post-task (e.g., sleep).</p> required <code>post_neurons</code> <code>ndarray</code> <p>Neuron identifiers for post-task spikes.</p> required <code>post_intervals</code> <code>ndarray</code> <p>Intervals for post-task epochs. Shape: (n_intervals, 2).</p> required <code>allow_reverse_replay</code> <code>bool</code> <p>Whether to allow reverse sequences, by default False.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to run in parallel, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>z_score: The z-score of the observed correlation compared to the shuffled distribution. p_value: p-value for significance test. observed_correlation_: The observed correlation for each interval.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def transform(\n    self,\n    post_spikes: np.ndarray,\n    post_neurons: np.ndarray,\n    post_intervals: np.ndarray,\n    allow_reverse_replay: bool = False,\n    parallel: bool = True,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Transform the post-task data to compute z-scores and p-values.\n\n    Parameters\n    ----------\n    post_spikes : np.ndarray\n        Spike times during post-task (e.g., sleep).\n    post_neurons : np.ndarray\n        Neuron identifiers for post-task spikes.\n    post_intervals : np.ndarray\n        Intervals for post-task epochs. Shape: (n_intervals, 2).\n    allow_reverse_replay : bool, optional\n        Whether to allow reverse sequences, by default False.\n    parallel : bool, optional\n        Whether to run in parallel, by default True.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, np.ndarray]\n        z_score: The z-score of the observed correlation compared to the shuffled distribution.\n        p_value: p-value for significance test.\n        observed_correlation_: The observed correlation for each interval.\n    \"\"\"\n    # Check if the number of jobs is less than the number of intervals\n    if post_intervals.shape[0] &lt; self.n_jobs:\n        self.n_jobs = post_intervals.shape[0]\n\n    if parallel:\n        observed_correlation, shuffled_correlations = zip(\n            *Parallel(n_jobs=self.n_jobs)(\n                delayed(self.observed_and_shuffled_correlation)(\n                    post_spikes,\n                    post_neurons,\n                    self.task_skew_bias,\n                    post_intervals,\n                    interval_i,\n                )\n                for interval_i in range(post_intervals.shape[0])\n            )\n        )\n    else:  # Run in serial for debugging\n        observed_correlation, shuffled_correlations = zip(\n            *[\n                self.observed_and_shuffled_correlation(\n                    post_spikes,\n                    post_neurons,\n                    self.task_skew_bias,\n                    post_intervals,\n                    interval_i,\n                )\n                for interval_i in range(post_intervals.shape[0])\n            ]\n        )\n\n    self.observed_correlation_ = np.array(\n        observed_correlation\n    )  # Shape: (n_intervals,)\n    self.shuffled_correlations_ = np.array(\n        shuffled_correlations\n    )  # Shape: (n_intervals, n_shuffles)\n\n    shuffled_mean = np.mean(self.shuffled_correlations_, axis=1)\n    shuffled_std = np.std(self.shuffled_correlations_, axis=1)\n    self.z_score_ = (self.observed_correlation_ - shuffled_mean) / shuffled_std\n\n    observed_correlation = self.observed_correlation_\n    shuffled_correlations = self.shuffled_correlations_\n    if allow_reverse_replay:\n        observed_correlation = np.abs(observed_correlation)\n        shuffled_correlations = np.abs(shuffled_correlations)\n\n    self.p_value_ = (\n        np.sum(\n            shuffled_correlations.T &gt; observed_correlation,\n            axis=0,\n        )\n        + 1\n    ) / (self.num_shuffles + 1)\n\n    return self.z_score_, self.p_value_, self.observed_correlation_\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.WeightedCorr","title":"<code>WeightedCorr(weights, x=None, y=None)</code>","text":"<p>Calculate the weighted correlation between the X and Y dimensions of the matrix.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>ndarray</code> <p>A matrix of weights.</p> required <code>x</code> <code>Optional[ndarray]</code> <p>X-values for each column and row, by default None.</p> <code>None</code> <code>y</code> <code>Optional[ndarray]</code> <p>Y-values for each column and row, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The weighted correlation coefficient.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def WeightedCorr(\n    weights: np.ndarray, x: Optional[np.ndarray] = None, y: Optional[np.ndarray] = None\n) -&gt; float:\n    \"\"\"\n    Calculate the weighted correlation between the X and Y dimensions of the matrix.\n\n    Parameters\n    ----------\n    weights : np.ndarray\n        A matrix of weights.\n    x : Optional[np.ndarray], optional\n        X-values for each column and row, by default None.\n    y : Optional[np.ndarray], optional\n        Y-values for each column and row, by default None.\n\n    Returns\n    -------\n    float\n        The weighted correlation coefficient.\n    \"\"\"\n    weights[np.isnan(weights)] = 0.0\n\n    if x is not None and x.size &gt; 0:\n        if np.ndim(x) == 1:\n            x = np.tile(x, (weights.shape[0], 1))\n    else:\n        x, _ = np.meshgrid(\n            np.arange(1, weights.shape[1] + 1), np.arange(1, weights.shape[0] + 1)\n        )\n\n    if y is not None and y.size &gt; 0:\n        if np.ndim(y) == 1:\n            y = np.tile(y, (weights.shape[0], 1))\n    else:\n        _, y = np.meshgrid(\n            np.arange(1, weights.shape[1] + 1), np.arange(1, weights.shape[0] + 1)\n        )\n\n    x = x.flatten()\n    y = y.flatten()\n    w = weights.flatten()\n\n    mX = np.nansum(w * x) / np.nansum(w)\n    mY = np.nansum(w * y) / np.nansum(w)\n\n    covXY = np.nansum(w * (x - mX) * (y - mY)) / np.nansum(w)\n    covXX = np.nansum(w * (x - mX) ** 2) / np.nansum(w)\n    covYY = np.nansum(w * (y - mY) ** 2) / np.nansum(w)\n\n    c = covXY / np.sqrt(covXX * covYY)\n\n    return c\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.WeightedCorrCirc","title":"<code>WeightedCorrCirc(weights, x=None, alpha=None)</code>","text":"<p>Compute the correlation between x and y dimensions of a matrix with angular (circular) values.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>ndarray</code> <p>A 2D numpy array of weights.</p> required <code>x</code> <code>Optional[ndarray]</code> <p>A 2D numpy array of x-values, by default None.</p> <code>None</code> <code>alpha</code> <code>Optional[ndarray]</code> <p>A 2D numpy array of angular (circular) y-values, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The correlation between x and y dimensions.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def WeightedCorrCirc(\n    weights: np.ndarray,\n    x: Optional[np.ndarray] = None,\n    alpha: Optional[np.ndarray] = None,\n) -&gt; float:\n    \"\"\"\n    Compute the correlation between x and y dimensions of a matrix with angular (circular) values.\n\n    Parameters\n    ----------\n    weights : np.ndarray\n        A 2D numpy array of weights.\n    x : Optional[np.ndarray], optional\n        A 2D numpy array of x-values, by default None.\n    alpha : Optional[np.ndarray], optional\n        A 2D numpy array of angular (circular) y-values, by default None.\n\n    Returns\n    -------\n    float\n        The correlation between x and y dimensions.\n    \"\"\"\n    weights[np.isnan(weights)] = 0.0\n\n    if x is not None and x.size &gt; 0:\n        if np.ndim(x) == 1:\n            x = np.tile(x, (weights.shape[0], 1))\n    else:\n        x, _ = np.meshgrid(\n            np.arange(1, weights.shape[1] + 1), np.arange(1, weights.shape[0] + 1)\n        )\n    if alpha is None:\n        alpha = np.tile(\n            np.linspace(0, 2 * np.pi, weights.shape[0], endpoint=False),\n            (weights.shape[1], 1),\n        ).T\n\n    rxs = WeightedCorr(weights, x, np.sin(alpha))\n    rxc = WeightedCorr(weights, x, np.cos(alpha))\n    rcs = WeightedCorr(weights, np.sin(alpha), np.cos(alpha))\n\n    # Compute angular-linear correlation\n    rho = np.sqrt((rxc**2 + rxs**2 - 2 * rxc * rxs * rcs) / (1 - rcs**2))\n    return rho\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay._shuffle_and_score","title":"<code>_shuffle_and_score(posterior_array, tuningcurve, w, normalize, ds, dp, n_shuffles)</code>","text":"<p>Shuffle the posterior array and compute scores and weighted correlations.</p> <p>Parameters:</p> Name Type Description Default <code>posterior_array</code> <code>ndarray</code> <p>The posterior probability array.</p> required <code>tuningcurve</code> <code>ndarray</code> <p>The tuning curve array.</p> required <code>w</code> <code>ndarray</code> <p>Weights array.</p> required <code>normalize</code> <code>bool</code> <p>Whether to normalize the scores.</p> required <code>ds</code> <code>float</code> <p>Delta space.</p> required <code>dp</code> <code>float</code> <p>Delta probability.</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, float, List[ndarray], List[ndarray], List[float], List[float]]</code> <p>Scores and weighted correlations for original, time-swapped, and column-cycled arrays.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def _shuffle_and_score(\n    posterior_array: np.ndarray,\n    tuningcurve: np.ndarray,\n    w: np.ndarray,\n    normalize: bool,\n    ds: float,\n    dp: float,\n    n_shuffles: int,\n) -&gt; Tuple[\n    np.ndarray, float, List[np.ndarray], List[np.ndarray], List[float], List[float]\n]:\n    \"\"\"\n    Shuffle the posterior array and compute scores and weighted correlations.\n\n    Parameters\n    ----------\n    posterior_array : np.ndarray\n        The posterior probability array.\n    tuningcurve : np.ndarray\n        The tuning curve array.\n    w : np.ndarray\n        Weights array.\n    normalize : bool\n        Whether to normalize the scores.\n    ds : float\n        Delta space.\n    dp : float\n        Delta probability.\n    n_shuffles : int\n        Number of shuffles.\n\n    Returns\n    -------\n    Tuple[np.ndarray, float, List[np.ndarray], List[np.ndarray], List[float], List[float]]\n        Scores and weighted correlations for original, time-swapped, and column-cycled arrays.\n    \"\"\"\n    weighted_corr = weighted_correlation(posterior_array)\n    scores = replay.trajectory_score_array(\n        posterior=posterior_array, w=w, normalize=normalize\n    )\n\n    (\n        scores_time_swap,\n        scores_col_cycle,\n        weighted_corr_time_swap,\n        weighted_corr_col_cycle,\n    ) = zip(\n        *[\n            shuffle_and_score(posterior_array, w, normalize, tuningcurve, ds, dp)\n            for _ in range(n_shuffles)\n        ]\n    )\n    return (\n        scores,\n        weighted_corr,\n        scores_time_swap,\n        scores_col_cycle,\n        weighted_corr_time_swap,\n        weighted_corr_col_cycle,\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.compute_bias_matrix_optimized_","title":"<code>compute_bias_matrix_optimized_(spike_times, neuron_ids, total_neurons)</code>","text":"<p>Optimized computation of the bias matrix B_k for a given sequence of spikes using vectorized operations.</p> <p>Parameters: - spike_times: list or array of spike times for the sequence. - neuron_ids: list or array of neuron identifiers corresponding to spike_times. - total_neurons: total number of neurons being considered.</p> <p>Returns: - bias_matrix: A matrix of size (total_neurons, total_neurons) representing the bias.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>@jit(nopython=True)\ndef compute_bias_matrix_optimized_(spike_times, neuron_ids, total_neurons):\n    \"\"\"\n    Optimized computation of the bias matrix B_k for a given sequence of spikes using vectorized operations.\n\n    Parameters:\n    - spike_times: list or array of spike times for the sequence.\n    - neuron_ids: list or array of neuron identifiers corresponding to spike_times.\n    - total_neurons: total number of neurons being considered.\n\n    Returns:\n    - bias_matrix: A matrix of size (total_neurons, total_neurons) representing the bias.\n    \"\"\"\n\n    # Create an empty bias matrix\n    bias_matrix = np.full((total_neurons, total_neurons), 0.5)\n\n    # Create boolean masks for all neurons in advance\n    masks = [neuron_ids == i for i in range(total_neurons)]\n\n    # Iterate over each pair of neurons\n    for i in range(total_neurons):\n        spikes_i = spike_times[masks[i]]\n        size_i = spikes_i.size\n\n        if size_i == 0:\n            continue  # Skip if neuron i has no spikes\n\n        for j in range(total_neurons):\n            if i == j:\n                continue  # Skip self-correlation\n\n            spikes_j = spike_times[masks[j]]\n            size_j = spikes_j.size\n\n            if size_j == 0:\n                continue  # Skip if neuron j has no spikes\n\n            crosscorr = crossCorr(spikes_i, spikes_j, 0.001, 100)\n\n            # Count how many times neuron i spikes before neuron j\n            bias_matrix[i, j] = np.divide(crosscorr[:50].sum(), crosscorr[51:].sum())\n\n    return bias_matrix\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.shuffle_and_score","title":"<code>shuffle_and_score(posterior_array, w, normalize, tc, ds, dp)</code>","text":"<p>Shuffle the posterior array and compute scores and weighted correlations.</p> <p>Parameters:</p> Name Type Description Default <code>posterior_array</code> <code>ndarray</code> <p>The posterior probability array.</p> required <code>w</code> <code>ndarray</code> <p>Weights array.</p> required <code>normalize</code> <code>bool</code> <p>Whether to normalize the scores.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, float, float]</code> <p>Scores and weighted correlations for time-swapped and column-cycled arrays.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def shuffle_and_score(\n    posterior_array: np.ndarray,\n    w: np.ndarray,\n    normalize: bool,\n    tc: float,\n    ds: float,\n    dp: float,\n) -&gt; Tuple[np.ndarray, np.ndarray, float, float]:\n    \"\"\"\n    Shuffle the posterior array and compute scores and weighted correlations.\n\n    Parameters\n    ----------\n    posterior_array : np.ndarray\n        The posterior probability array.\n    w : np.ndarray\n        Weights array.\n    normalize : bool\n        Whether to normalize the scores.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, float, float]\n        Scores and weighted correlations for time-swapped and column-cycled arrays.\n    \"\"\"\n\n    posterior_ts = replay.time_swap_array(posterior_array)\n    posterior_cs = replay.column_cycle_array(posterior_array)\n\n    scores_time_swap = replay.trajectory_score_array(\n        posterior=posterior_ts, w=w, normalize=normalize\n    )\n    scores_col_cycle = replay.trajectory_score_array(\n        posterior=posterior_cs, w=w, normalize=normalize\n    )\n\n    weighted_corr_time_swap = weighted_correlation(posterior_ts)\n    weighted_corr_col_cycle = weighted_correlation(posterior_cs)\n\n    return (\n        scores_time_swap,\n        scores_col_cycle,\n        weighted_corr_time_swap,\n        weighted_corr_col_cycle,\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.trajectory_score_bst","title":"<code>trajectory_score_bst(bst, tuningcurve, w=None, n_shuffles=1000, weights=None, normalize=False, parallel=True)</code>","text":"<p>Calculate trajectory scores and weighted correlations for Bayesian spike train decoding.</p> <p>Parameters:</p> Name Type Description Default <code>bst</code> <code>BinnedSpikeTrainArray</code> <p>Binned spike train object.</p> required <code>tuningcurve</code> <code>TuningCurve1D</code> <p>Tuning curve object.</p> required <code>w</code> <code>Optional[int]</code> <p>Window size, by default None.</p> <code>None</code> <code>n_shuffles</code> <code>int</code> <p>Number of shuffles, by default 1000.</p> <code>1000</code> <code>weights</code> <code>Optional[ndarray]</code> <p>Weights array, by default None.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the scores, by default False.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>Whether to run in parallel, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray],</p> <code>]</code> <p>Scores and weighted correlations for original, time-swapped, and column-cycled arrays.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def trajectory_score_bst(\n    bst: BinnedSpikeTrainArray,\n    tuningcurve: TuningCurve1D,\n    w: Optional[int] = None,\n    n_shuffles: int = 1000,\n    weights: Optional[np.ndarray] = None,\n    normalize: bool = False,\n    parallel: bool = True,\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n    Tuple[np.ndarray, np.ndarray],\n]:\n    \"\"\"\n    Calculate trajectory scores and weighted correlations for Bayesian spike train decoding.\n\n    Parameters\n    ----------\n    bst : BinnedSpikeTrainArray\n        Binned spike train object.\n    tuningcurve : TuningCurve1D\n        Tuning curve object.\n    w : Optional[int], optional\n        Window size, by default None.\n    n_shuffles : int, optional\n        Number of shuffles, by default 1000.\n    weights : Optional[np.ndarray], optional\n        Weights array, by default None.\n    normalize : bool, optional\n        Whether to normalize the scores, by default False.\n    parallel : bool, optional\n        Whether to run in parallel, by default True.\n\n    Returns\n    -------\n    Union[\n        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n        Tuple[np.ndarray, np.ndarray],\n    ]\n        Scores and weighted correlations for original, time-swapped, and column-cycled arrays.\n    \"\"\"\n\n    if w is None:\n        w = 0\n    if not float(w).is_integer:\n        raise ValueError(\"w has to be an integer!\")\n\n    if float(n_shuffles).is_integer:\n        n_shuffles = int(n_shuffles)\n    else:\n        raise ValueError(\"n_shuffles must be an integer!\")\n\n    posterior, bdries, _, _ = decode(bst=bst, ratemap=tuningcurve)\n\n    num_cores = 1\n\n    if parallel:\n        # all but one core\n        num_cores = multiprocessing.cpu_count() - 1\n\n    ds, dp = bst.ds, np.diff(tuningcurve.bins)[0]\n\n    (\n        scores,\n        weighted_corr,\n        scores_time_swap,\n        scores_col_cycle,\n        weighted_corr_time_swap,\n        weighted_corr_col_cycle,\n    ) = zip(\n        *Parallel(n_jobs=num_cores)(\n            delayed(_shuffle_and_score)(\n                posterior[:, bdries[idx] : bdries[idx + 1]],\n                tuningcurve,\n                w,\n                normalize,\n                ds,\n                dp,\n                n_shuffles,\n            )\n            for idx in range(bst.n_epochs)\n        )\n    )\n\n    if n_shuffles &gt; 0:\n        return (\n            np.array(scores),\n            np.array(weighted_corr),\n            np.array(scores_time_swap).T,\n            np.array(scores_col_cycle).T,\n            np.array(weighted_corr_time_swap).T,\n            np.array(weighted_corr_col_cycle).T,\n        )\n    return scores, weighted_corr\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.weighted_corr_2d","title":"<code>weighted_corr_2d(weights, x_coords=None, y_coords=None, time_coords=None)</code>","text":"<p>Calculate the weighted correlation between the X and Y dimensions of the matrix.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>ndarray</code> <p>A matrix of weights.</p> required <code>x_coords</code> <code>Optional[ndarray]</code> <p>X-values for each column and row, by default None.</p> <code>None</code> <code>y_coords</code> <code>Optional[ndarray]</code> <p>Y-values for each column and row, by default None.</p> <code>None</code> <code>time_coords</code> <code>Optional[ndarray]</code> <p>Time-values for each column and row, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[float, ndarray, ndarray, float, float, float, float]</code> <p>The weighted correlation coefficient, x trajectory, y trajectory, slope_x, slope_y, mean_x, mean_y.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; weights = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n&gt;&gt;&gt; x_coords = np.array([0, 1])\n&gt;&gt;&gt; y_coords = np.array([0, 1])\n&gt;&gt;&gt; time_coords = np.array([0, 1, 2])\n&gt;&gt;&gt; weighted_corr_2d(weights, x_coords, y_coords, time_coords)\n</code></pre> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def weighted_corr_2d(\n    weights: np.ndarray,\n    x_coords: Optional[np.ndarray] = None,\n    y_coords: Optional[np.ndarray] = None,\n    time_coords: Optional[np.ndarray] = None,\n) -&gt; Tuple[float, np.ndarray, np.ndarray, float, float, float, float]:\n    \"\"\"\n    Calculate the weighted correlation between the X and Y dimensions of the matrix.\n\n    Parameters\n    ----------\n    weights : np.ndarray\n        A matrix of weights.\n    x_coords : Optional[np.ndarray], optional\n        X-values for each column and row, by default None.\n    y_coords : Optional[np.ndarray], optional\n        Y-values for each column and row, by default None.\n    time_coords : Optional[np.ndarray], optional\n        Time-values for each column and row, by default None.\n\n    Returns\n    -------\n    Tuple[float, np.ndarray, np.ndarray, float, float, float, float]\n        The weighted correlation coefficient, x trajectory, y trajectory,\n        slope_x, slope_y, mean_x, mean_y.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; weights = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n    &gt;&gt;&gt; x_coords = np.array([0, 1])\n    &gt;&gt;&gt; y_coords = np.array([0, 1])\n    &gt;&gt;&gt; time_coords = np.array([0, 1, 2])\n    &gt;&gt;&gt; weighted_corr_2d(weights, x_coords, y_coords, time_coords)\n\n    \"\"\"\n    x_dim, y_dim, t_dim = weights.shape\n    if x_coords is None:\n        x_coords = np.arange(x_dim)\n    if y_coords is None:\n        y_coords = np.arange(y_dim)\n    if time_coords is None:\n        time_coords = np.arange(t_dim)\n    return __weighted_corr_2d_jit(weights, x_coords, y_coords, time_coords)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/replay/#neuro_py.ensemble.replay.weighted_correlation","title":"<code>weighted_correlation(posterior, time=None, place_bin_centers=None)</code>","text":"<p>Calculate the weighted correlation between time and place bin centers using a posterior probability matrix.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>ndarray</code> <p>A 2D numpy array representing the posterior probability matrix.</p> required <code>time</code> <code>Optional[ndarray]</code> <p>A 1D numpy array representing the time bins, by default None.</p> <code>None</code> <code>place_bin_centers</code> <code>Optional[ndarray]</code> <p>A 1D numpy array representing the place bin centers, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The weighted correlation coefficient.</p> Source code in <code>neuro_py/ensemble/replay.py</code> <pre><code>def weighted_correlation(\n    posterior: np.ndarray,\n    time: Optional[np.ndarray] = None,\n    place_bin_centers: Optional[np.ndarray] = None,\n) -&gt; float:\n    \"\"\"\n    Calculate the weighted correlation between time and place bin centers using a posterior probability matrix.\n\n    Parameters\n    ----------\n    posterior : np.ndarray\n        A 2D numpy array representing the posterior probability matrix.\n    time : Optional[np.ndarray], optional\n        A 1D numpy array representing the time bins, by default None.\n    place_bin_centers : Optional[np.ndarray], optional\n        A 1D numpy array representing the place bin centers, by default None.\n\n    Returns\n    -------\n    float\n        The weighted correlation coefficient.\n    \"\"\"\n\n    def _m(x, w) -&gt; float:\n        \"\"\"Weighted Mean\"\"\"\n        return np.sum(x * w) / np.sum(w)\n\n    def _cov(x, y, w) -&gt; float:\n        \"\"\"Weighted Covariance\"\"\"\n        return np.sum(w * (x - _m(x, w)) * (y - _m(y, w))) / np.sum(w)\n\n    def _corr(x, y, w) -&gt; float:\n        \"\"\"Weighted Correlation\"\"\"\n        return _cov(x, y, w) / np.sqrt(_cov(x, x, w) * _cov(y, y, w))\n\n    if time is None:\n        time = np.arange(posterior.shape[1])\n    if place_bin_centers is None:\n        place_bin_centers = np.arange(posterior.shape[0])\n\n    place_bin_centers = place_bin_centers.squeeze()\n    posterior[np.isnan(posterior)] = 0.0\n\n    return _corr(time[:, np.newaxis], place_bin_centers[np.newaxis, :], posterior.T)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/similarity_index/","title":"neuro_py.ensemble.similarity_index","text":""},{"location":"reference/neuro_py/ensemble/similarity_index/#neuro_py.ensemble.similarity_index.similarity_index","title":"<code>similarity_index(patterns, n_shuffles=1000, parallel=True, groups=None, adjust_pvalue=True)</code>","text":"<p>Calculate the similarity index of a set of patterns.</p> <p>To use a quantitative criterion to compare assembly composition, a Similarity Index (SI) was defined as the absolute value of the inner product between the assembly patterns (unitary vectors) of two given assemblies, varying from 0 to 1. Thus, if two assemblies attribute large weights to the same neurons, SI will be large; if assemblies are orthogonal, SI will be zero.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>ndarray</code> <p>List of patterns (n patterns x n neurons).</p> required <code>n_shuffles</code> <code>int</code> <p>Number of shuffles to calculate the similarity index, by default 1000.</p> <code>1000</code> <code>parallel</code> <code>bool</code> <p>Whether to run in parallel, by default True.</p> <code>True</code> <code>groups</code> <code>ndarray</code> <p>List of groups for each pattern (n patterns, ), will return cross-group comparisons by default None.</p> <code>None</code> <code>adjust_pvalue</code> <code>bool</code> <p>Where to adjust p-values to control the false discovery rate.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <p>si: similarity index (n_combinations,) combos: list of all possible combinations of patterns (n_combinations, 2) pvalues: list of p-values for each pattern combination (n_combinations,)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; patterns = np.random.random_sample((60,20))\n&gt;&gt;&gt; si, combos, pvalues = similarity_index(patterns)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/similarity_index/#neuro_py.ensemble.similarity_index.similarity_index--with-groups","title":"with groups","text":"<pre><code>&gt;&gt;&gt; patterns = np.random.random_sample((60,20))\n&gt;&gt;&gt; groups = np.hstack([np.ones(20), np.ones(40)+1])\n&gt;&gt;&gt; si, combos, pvalues = similarity_index(patterns, groups=groups)\n</code></pre> References <p>Based on Almeida-Filho et al., 2014 to detect similar assemblies.</p> Source code in <code>neuro_py/ensemble/similarity_index.py</code> <pre><code>def similarity_index(\n    patterns: np.ndarray,\n    n_shuffles: int = 1000,\n    parallel: bool = True,\n    groups: np.ndarray = None,\n    adjust_pvalue: bool = True,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate the similarity index of a set of patterns.\n\n    To use a quantitative criterion to compare assembly composition,\n    a Similarity Index (SI) was defined as the absolute value of the\n    inner product between the assembly patterns (unitary vectors) of\n    two given assemblies, varying from 0 to 1. Thus, if two assemblies\n    attribute large weights to the same neurons, SI will be large;\n    if assemblies are orthogonal, SI will be zero.\n\n    Parameters\n    ----------\n    patterns : np.ndarray\n        List of patterns (n patterns x n neurons).\n    n_shuffles : int, optional\n        Number of shuffles to calculate the similarity index, by default 1000.\n    parallel : bool, optional\n        Whether to run in parallel, by default True.\n    groups : np.ndarray, optional\n        List of groups for each pattern (n patterns, ), will return cross-group comparisons by default None.\n    adjust_pvalue : bool, optional\n        Where to adjust p-values to control the false discovery rate.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, np.ndarray]\n        si: similarity index (n_combinations,)\n        combos: list of all possible combinations of patterns (n_combinations, 2)\n        pvalues: list of p-values for each pattern combination (n_combinations,)\n\n    Examples\n    --------\n    &gt;&gt;&gt; patterns = np.random.random_sample((60,20))\n    &gt;&gt;&gt; si, combos, pvalues = similarity_index(patterns)\n\n    # with groups\n    &gt;&gt;&gt; patterns = np.random.random_sample((60,20))\n    &gt;&gt;&gt; groups = np.hstack([np.ones(20), np.ones(40)+1])\n    &gt;&gt;&gt; si, combos, pvalues = similarity_index(patterns, groups=groups)\n\n    References\n    ----------\n    Based on Almeida-Filho et al., 2014 to detect similar assemblies.\n\n    \"\"\"\n    # check to see if patterns are numpy arrays\n    if not isinstance(patterns, np.ndarray):\n        patterns = np.array(patterns)\n\n    if patterns.shape[0] &lt; 2:\n        raise ValueError(\"At least 2 patterns are required to compute similarity.\")\n\n    # set seed to ensure exact results between runs\n    np.random.seed(42)\n\n    # maximum number of n_shuffles based on number of neurons\n    n_shuffles = min(n_shuffles, int(math.factorial(patterns.shape[1])))\n\n    # Normalize patterns\n    patterns = patterns / np.linalg.norm(patterns, axis=1, keepdims=True)\n\n    # shuffle patterns over neurons\n    def shuffle_patterns(patterns):\n        return np.array([np.random.permutation(pattern) for pattern in patterns])\n\n    # Calculate absolute inner product between patterns\n    def get_si(patterns):\n        si = np.array(\n            [np.abs(np.inner(patterns[i], patterns[j])) for i, j in COMBINATIONS]\n        )\n        return si\n\n    # get all possible combinations of patterns\n    COMBINATIONS = np.array(list(itertools.combinations(range(patterns.shape[0]), 2)))\n\n    # calculate observed si\n    si = get_si(patterns)\n\n    # shuffle patterns and calculate si\n    if parallel:\n        num_cores = multiprocessing.cpu_count()\n        si_shuffles = Parallel(n_jobs=num_cores)(\n            delayed(get_si)(shuffle_patterns(patterns)) for _ in range(n_shuffles)\n        )\n    else:\n        si_shuffles = [get_si(shuffle_patterns(patterns)) for _ in range(n_shuffles)]\n\n    # calculate p-values for each pattern combination\n    _, pvalues, _ = get_significant_events(si, np.array(si_shuffles))\n\n    # Filter outputs to only include cross-group comparisons\n    if groups is not None:\n        # Ensure groups is a numpy array\n        groups = np.asarray(groups)\n\n        # Identify cross-group comparisons\n        cross_group_mask = groups[COMBINATIONS[:, 0]] != groups[COMBINATIONS[:, 1]]\n        si = si[cross_group_mask]\n        COMBINATIONS = COMBINATIONS[cross_group_mask]\n        pvalues = pvalues[cross_group_mask]\n\n    if adjust_pvalue:\n        pvalues = stats.false_discovery_control(pvalues)\n\n    return si, COMBINATIONS, pvalues\n</code></pre>"},{"location":"reference/neuro_py/ensemble/similaritymat/","title":"neuro_py.ensemble.similaritymat","text":""},{"location":"reference/neuro_py/ensemble/similaritymat/#neuro_py.ensemble.similaritymat.similaritymat","title":"<code>similaritymat(patternsX, patternsY=None, method='cosine', findpairs=False)</code>","text":"<p>Calculate the similarity matrix of co-activation patterns (assemblies).</p> <p>Parameters:</p> Name Type Description Default <code>patternsX</code> <code>ndarray</code> <p>Co-activation patterns (assemblies) - numpy array (assemblies, neurons).</p> required <code>patternsY</code> <code>Optional[ndarray]</code> <p>Co-activation patterns (assemblies) - numpy array (assemblies, neurons). If None, will compute similarity of patternsX to itself, by default None.</p> <code>None</code> <code>method</code> <code>str</code> <p>Defines similarity measure method, by default 'cosine'. 'cosine' - cosine similarity.</p> <code>'cosine'</code> <code>findpairs</code> <code>bool</code> <p>Maximizes main diagonal of the similarity matrix to define pairs from patterns X and Y. Returns rowind, colind which can be used to reorder patterns X and Y to maximize the diagonal, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, ndarray, ndarray]]</code> <p>Similarity matrix (assemblies from X, assemblies from Y). If findpairs is True, also returns rowind and colind.</p> Source code in <code>neuro_py/ensemble/similaritymat.py</code> <pre><code>def similaritymat(\n    patternsX: np.ndarray,\n    patternsY: Optional[np.ndarray] = None,\n    method: str = \"cosine\",\n    findpairs: bool = False,\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"\n    Calculate the similarity matrix of co-activation patterns (assemblies).\n\n    Parameters\n    ----------\n    patternsX : np.ndarray\n        Co-activation patterns (assemblies) - numpy array (assemblies, neurons).\n    patternsY : Optional[np.ndarray], optional\n        Co-activation patterns (assemblies) - numpy array (assemblies, neurons).\n        If None, will compute similarity of patternsX to itself, by default None.\n    method : str, optional\n        Defines similarity measure method, by default 'cosine'.\n        'cosine' - cosine similarity.\n    findpairs : bool, optional\n        Maximizes main diagonal of the similarity matrix to define pairs\n        from patterns X and Y. Returns rowind, colind which can be used to reorder\n        patterns X and Y to maximize the diagonal, by default False.\n\n    Returns\n    -------\n    Union[np.ndarray, Tuple[np.ndarray, np.ndarray, np.ndarray]]\n        Similarity matrix (assemblies from X, assemblies from Y).\n        If findpairs is True, also returns rowind and colind.\n    \"\"\"\n\n    if method != \"cosine\":\n        print(method + \" for similarity has not been implemented yet.\")\n        return\n\n    inputs = {\"X\": patternsX, \"Y\": patternsY}\n    simmat = getsim(**inputs)\n\n    if findpairs:\n\n        def fillmissingidxs(ind, n):\n            missing = list(set(np.arange(n)) - set(ind))\n            ind = np.array(list(ind) + missing)\n            return ind\n\n        rowind, colind = optimize.linear_sum_assignment(-simmat)\n\n        rowind = fillmissingidxs(rowind, np.size(simmat, 0))\n        colind = fillmissingidxs(colind, np.size(simmat, 1))\n\n        return simmat, rowind, colind\n    else:\n        return simmat\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/","title":"neuro_py.ensemble.decoding","text":""},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.LSTM","title":"<code>LSTM</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Long Short-Term Memory (LSTM) model.</p> <p>This class implements an LSTM model using PyTorch Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Dimensionality of input data, by default 100</p> <code>100</code> <code>out_dim</code> <code>int</code> <p>Dimensionality of output data, by default 2</p> <code>2</code> <code>hidden_dims</code> <code>Tuple[int, int, float]</code> <p>Architectural parameters of the model (hidden_size, num_layers, dropout), by default (400, 1, 0.0)</p> <code>(400, 1, 0.0)</code> <code>use_bias</code> <code>bool</code> <p>Whether to use bias or not in the final linear layer, by default True</p> <code>True</code> <code>args</code> <code>Dict</code> <p>Additional arguments for model configuration, by default {}</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>lstm</code> <code>LSTM</code> <p>LSTM layer</p> <code>fc</code> <code>Linear</code> <p>Fully connected layer</p> <code>hidden_state</code> <code>Optional[Tensor]</code> <p>Hidden state of the LSTM</p> <code>cell_state</code> <code>Optional[Tensor]</code> <p>Cell state of the LSTM</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>class LSTM(L.LightningModule):\n    \"\"\"\n    Long Short-Term Memory (LSTM) model.\n\n    This class implements an LSTM model using PyTorch Lightning.\n\n    Parameters\n    ----------\n    in_dim : int, optional\n        Dimensionality of input data, by default 100\n    out_dim : int, optional\n        Dimensionality of output data, by default 2\n    hidden_dims : Tuple[int, int, float], optional\n        Architectural parameters of the model (hidden_size, num_layers, dropout),\n        by default (400, 1, 0.0)\n    use_bias : bool, optional\n        Whether to use bias or not in the final linear layer, by default True\n    args : Dict, optional\n        Additional arguments for model configuration, by default {}\n\n    Attributes\n    ----------\n    lstm : nn.LSTM\n        LSTM layer\n    fc : nn.Linear\n        Fully connected layer\n    hidden_state : Optional[torch.Tensor]\n        Hidden state of the LSTM\n    cell_state : Optional[torch.Tensor]\n        Cell state of the LSTM\n    \"\"\"\n    def __init__(self, in_dim: int = 100, out_dim: int = 2,\n                 hidden_dims: Tuple[int, int, float] = (400, 1, 0.0),\n                 use_bias: bool = True, args: Dict = {}):\n        super().__init__()\n        self.save_hyperparameters()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        if len(hidden_dims) != 3:\n            raise ValueError('`hidden_dims` should be of size 3')\n        self.hidden_size, self.nlayers, self.dropout = hidden_dims\n        self.args = args\n\n        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=self.hidden_size,\n                            num_layers=self.nlayers, batch_first=True, \n                            dropout=self.dropout, bidirectional=True)\n        self.fc = nn.Linear(in_features=2*self.hidden_size, out_features=out_dim, bias=use_bias)\n        self.hidden_state: Optional[torch.Tensor] = None\n        self.cell_state: Optional[torch.Tensor] = None\n\n        self._init_params()\n\n    def _init_params(self) -&gt; None:\n        \"\"\"Initialize model parameters.\"\"\"\n        def init_params(m: nn.Module) -&gt; None:\n            if isinstance(m, nn.Linear):\n                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n                if m.bias is not None:\n                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                    bound = 1 / torch.math.sqrt(fan_in)\n                    nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n        init_params(self.fc)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the LSTM model.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape (batch_size, sequence_length, input_dim)\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor of shape (batch_size, output_dim)\n        \"\"\"\n        lstm_out, (self.hidden_state, self.cell_state) = \\\n            self.lstm(x, (self.hidden_state, self.cell_state))\n        lstm_out = lstm_out[:, -1, :].contiguous()\n        out = self.fc(lstm_out)\n        if self.args.get('clf', False):\n            out = F.log_softmax(out, dim=1)\n        return out\n\n    def init_hidden(self, batch_size: int) -&gt; None:\n        \"\"\"\n        Initialize hidden state and cell state.\n\n        Parameters\n        ----------\n        batch_size : int\n            Batch size for initialization\n        \"\"\"\n        self.batch_size = batch_size\n        h0 = torch.zeros(\n            (2*self.nlayers, batch_size, self.hidden_size),\n            requires_grad=False\n        )\n        c0 = torch.zeros(\n            (2*self.nlayers, batch_size, self.hidden_size),\n            requires_grad=False\n        )\n        self.hidden_state = h0\n        self.cell_state = c0\n\n    def predict(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Make predictions using the LSTM model.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor\n\n        Returns\n        -------\n        torch.Tensor\n            Predicted output\n        \"\"\"\n        self.hidden_state = self.hidden_state.to(x.device)\n        self.cell_state = self.cell_state.to(x.device)\n        preds = []\n        batch_size = self.batch_size\n        for i in range(batch_size, x.shape[0]+batch_size, batch_size):\n            iptensor = x[i-batch_size:i]\n            if i &gt; x.shape[0]:\n                iptensor = F.pad(iptensor, (0,0,0,0,0,i-x.shape[0]))\n            pred_loc = self.forward(iptensor)\n            if i &gt; x.shape[0]:\n                pred_loc = pred_loc[:batch_size-(i-x.shape[0])]\n            preds.extend(pred_loc)\n        out = torch.stack(preds)\n        if self.args.get('clf', False):\n            out = F.log_softmax(out, dim=1)\n        return out\n\n    def _step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Perform a single step (forward pass + loss calculation).\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        xs, ys = batch\n        outs = self(xs)\n        loss = self.args['criterion'](outs, ys)\n        return loss\n\n    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for training step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('train_loss', loss)\n        return loss\n\n    def on_after_backward(self) -&gt; None:\n        \"\"\"Lightning method called after backpropagation.\"\"\"\n        self.hidden_state.detach_()\n        self.cell_state.detach_()\n\n    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for validation step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('val_loss', loss)\n        return loss\n\n    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for test step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('test_loss', loss)\n        return loss\n\n    def configure_optimizers(self) -&gt; Tuple[List[torch.optim.Optimizer], List[Dict]]:\n        \"\"\"\n        Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        Tuple[List[torch.optim.Optimizer], List[Dict]]\n            Tuple containing a list of optimizers and a list of scheduler configurations\n        \"\"\"\n        optimizer = torch.optim.AdamW(\n            self.parameters(), weight_decay=self.args['weight_decay'])\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer, max_lr=self.args['lr'],\n            epochs=self.args['epochs'],\n            steps_per_epoch=len(\n                self.trainer._data_connector._train_dataloader_source.dataloader()\n            )\n        )\n        lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n        return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.LSTM._init_params","title":"<code>_init_params()</code>","text":"<p>Initialize model parameters.</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def _init_params(self) -&gt; None:\n    \"\"\"Initialize model parameters.\"\"\"\n    def init_params(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n            if m.bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                bound = 1 / torch.math.sqrt(fan_in)\n                nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n    init_params(self.fc)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.LSTM._step","title":"<code>_step(batch, batch_idx)</code>","text":"<p>Perform a single step (forward pass + loss calculation).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def _step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Perform a single step (forward pass + loss calculation).\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    xs, ys = batch\n    outs = self(xs)\n    loss = self.args['criterion'](outs, ys)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.LSTM.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>Tuple[List[Optimizer], List[Dict]]</code> <p>Tuple containing a list of optimizers and a list of scheduler configurations</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def configure_optimizers(self) -&gt; Tuple[List[torch.optim.Optimizer], List[Dict]]:\n    \"\"\"\n    Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    Tuple[List[torch.optim.Optimizer], List[Dict]]\n        Tuple containing a list of optimizers and a list of scheduler configurations\n    \"\"\"\n    optimizer = torch.optim.AdamW(\n        self.parameters(), weight_decay=self.args['weight_decay'])\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=self.args['lr'],\n        epochs=self.args['epochs'],\n        steps_per_epoch=len(\n            self.trainer._data_connector._train_dataloader_source.dataloader()\n        )\n    )\n    lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n    return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.LSTM.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the LSTM model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length, input_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, output_dim)</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the LSTM model.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (batch_size, sequence_length, input_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (batch_size, output_dim)\n    \"\"\"\n    lstm_out, (self.hidden_state, self.cell_state) = \\\n        self.lstm(x, (self.hidden_state, self.cell_state))\n    lstm_out = lstm_out[:, -1, :].contiguous()\n    out = self.fc(lstm_out)\n    if self.args.get('clf', False):\n        out = F.log_softmax(out, dim=1)\n    return out\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.LSTM.init_hidden","title":"<code>init_hidden(batch_size)</code>","text":"<p>Initialize hidden state and cell state.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for initialization</p> required Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def init_hidden(self, batch_size: int) -&gt; None:\n    \"\"\"\n    Initialize hidden state and cell state.\n\n    Parameters\n    ----------\n    batch_size : int\n        Batch size for initialization\n    \"\"\"\n    self.batch_size = batch_size\n    h0 = torch.zeros(\n        (2*self.nlayers, batch_size, self.hidden_size),\n        requires_grad=False\n    )\n    c0 = torch.zeros(\n        (2*self.nlayers, batch_size, self.hidden_size),\n        requires_grad=False\n    )\n    self.hidden_state = h0\n    self.cell_state = c0\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.LSTM.on_after_backward","title":"<code>on_after_backward()</code>","text":"<p>Lightning method called after backpropagation.</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def on_after_backward(self) -&gt; None:\n    \"\"\"Lightning method called after backpropagation.\"\"\"\n    self.hidden_state.detach_()\n    self.cell_state.detach_()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.LSTM.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the LSTM model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Predicted output</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def predict(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Make predictions using the LSTM model.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor\n\n    Returns\n    -------\n    torch.Tensor\n        Predicted output\n    \"\"\"\n    self.hidden_state = self.hidden_state.to(x.device)\n    self.cell_state = self.cell_state.to(x.device)\n    preds = []\n    batch_size = self.batch_size\n    for i in range(batch_size, x.shape[0]+batch_size, batch_size):\n        iptensor = x[i-batch_size:i]\n        if i &gt; x.shape[0]:\n            iptensor = F.pad(iptensor, (0,0,0,0,0,i-x.shape[0]))\n        pred_loc = self.forward(iptensor)\n        if i &gt; x.shape[0]:\n            pred_loc = pred_loc[:batch_size-(i-x.shape[0])]\n        preds.extend(pred_loc)\n    out = torch.stack(preds)\n    if self.args.get('clf', False):\n        out = F.log_softmax(out, dim=1)\n    return out\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.LSTM.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Lightning method for test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for test step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('test_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.LSTM.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Lightning method for training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for training step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('train_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.LSTM.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Lightning method for validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for validation step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('val_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.M2MLSTM","title":"<code>M2MLSTM</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Many-to-Many Long Short-Term Memory (LSTM) model.</p> <p>This class implements a Many-to-Many LSTM model using PyTorch Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Dimensionality of input data, by default 100</p> <code>100</code> <code>out_dim</code> <code>int</code> <p>Number of output columns, by default 2</p> <code>2</code> <code>hidden_dims</code> <code>Tuple[int, int, float]</code> <p>Architectural parameters of the model (hidden_size, num_layers, dropout), by default (400, 1, 0.0)</p> <code>(400, 1, 0.0)</code> <code>use_bias</code> <code>bool</code> <p>Whether to use bias or not in the final linear layer, by default True</p> <code>True</code> <code>args</code> <code>Dict</code> <p>Additional arguments for model configuration, by default {}</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>lstm</code> <code>LSTM</code> <p>LSTM layer</p> <code>fc</code> <code>Linear</code> <p>Fully connected layer</p> <code>hidden_state</code> <code>Optional[Tensor]</code> <p>Hidden state of the LSTM</p> <code>cell_state</code> <code>Optional[Tensor]</code> <p>Cell state of the LSTM</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>class M2MLSTM(L.LightningModule):\n    \"\"\"\n    Many-to-Many Long Short-Term Memory (LSTM) model.\n\n    This class implements a Many-to-Many LSTM model using PyTorch Lightning.\n\n    Parameters\n    ----------\n    in_dim : int, optional\n        Dimensionality of input data, by default 100\n    out_dim : int, optional\n        Number of output columns, by default 2\n    hidden_dims : Tuple[int, int, float], optional\n        Architectural parameters of the model (hidden_size, num_layers, dropout),\n        by default (400, 1, 0.0)\n    use_bias : bool, optional\n        Whether to use bias or not in the final linear layer, by default True\n    args : Dict, optional\n        Additional arguments for model configuration, by default {}\n\n    Attributes\n    ----------\n    lstm : nn.LSTM\n        LSTM layer\n    fc : nn.Linear\n        Fully connected layer\n    hidden_state : Optional[torch.Tensor]\n        Hidden state of the LSTM\n    cell_state : Optional[torch.Tensor]\n        Cell state of the LSTM\n    \"\"\"\n    def __init__(self, in_dim: int = 100, out_dim: int = 2,\n                 hidden_dims: Tuple[int, int, float] = (400, 1, 0.0),\n                 use_bias: bool = True, args: Dict = {}):\n        super().__init__()\n        self.save_hyperparameters()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        if len(hidden_dims) != 3:\n            raise ValueError('`hidden_dims` should be of size 3')\n        self.hidden_size, self.nlayers, self.dropout = hidden_dims\n        self.args = args\n\n        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=self.hidden_size,\n                            num_layers=self.nlayers, batch_first=True, \n                            dropout=self.dropout, bidirectional=False)\n        self.fc = nn.Linear(in_features=self.hidden_size, out_features=out_dim, bias=use_bias)\n        self.hidden_state: Optional[torch.Tensor] = None\n        self.cell_state: Optional[torch.Tensor] = None\n\n        self._init_params()\n\n    def _init_params(self) -&gt; None:\n        \"\"\"Initialize model parameters.\"\"\"\n        def init_params(m: nn.Module) -&gt; None:\n            if isinstance(m, nn.Linear):\n                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n                if m.bias is not None:\n                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                    bound = 1 / np.sqrt(fan_in)\n                    nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n        init_params(self.fc)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the LSTM model.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape (batch_size, sequence_length, input_dim)\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor of shape (batch_size, sequence_length, output_dim)\n        \"\"\"\n        B, L, N = x.shape\n        self.hidden_state = self.hidden_state.to(x.device)\n        self.cell_state = self.cell_state.to(x.device)\n        self.hidden_state.data.fill_(0.0)\n        self.cell_state.data.fill_(0.0)\n        lstm_outs = []\n        for i in range(L):\n            lstm_out, (self.hidden_state, self.cell_state) = \\\n                self.lstm(x[:, i].unsqueeze(1), (self.hidden_state, self.cell_state))\n            lstm_outs.append(lstm_out)\n\n        lstm_outs = torch.stack(lstm_outs, dim=1)  # B, L, N\n        out = self.fc(lstm_outs)\n        out = out.view(B, L, self.out_dim)\n        if self.args.get('clf', False):\n            out = F.log_softmax(out, dim=-1)\n\n        return out\n\n    def init_hidden(self, batch_size: int) -&gt; None:\n        \"\"\"\n        Initialize hidden state and cell state.\n\n        Parameters\n        ----------\n        batch_size : int\n            Batch size for initialization\n        \"\"\"\n        self.batch_size = batch_size\n        self.hidden_state = torch.zeros((self.nlayers, batch_size, self.hidden_size), requires_grad=False)\n        self.cell_state = torch.zeros((self.nlayers, batch_size, self.hidden_size), requires_grad=False)\n\n    def _step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Perform a single step (forward pass + loss calculation).\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        xs, ys = batch\n        outs = self(xs)\n        loss = self.args['criterion'](outs, ys)\n        return loss\n\n    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for training step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('train_loss', loss)\n        return loss\n\n    def on_after_backward(self) -&gt; None:\n        \"\"\"Lightning method called after backpropagation.\"\"\"\n        self.hidden_state = self.hidden_state.detach()\n        self.cell_state = self.cell_state.detach()\n\n    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for validation step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('val_loss', loss)\n        return loss\n\n    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for test step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('test_loss', loss)\n        return loss\n\n    def configure_optimizers(self) -&gt; Tuple[List[torch.optim.Optimizer], List[Dict]]:\n        \"\"\"\n        Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        Tuple[List[torch.optim.Optimizer], List[Dict]]\n            Tuple containing a list of optimizers and a list of scheduler configurations\n        \"\"\"\n        optimizer = torch.optim.AdamW(\n            self.parameters(), weight_decay=self.args['weight_decay'])\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer, max_lr=self.args['lr'],\n            epochs=self.args['epochs'],\n            total_steps=self.trainer.estimated_stepping_batches\n        )\n        lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n        return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.M2MLSTM._init_params","title":"<code>_init_params()</code>","text":"<p>Initialize model parameters.</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def _init_params(self) -&gt; None:\n    \"\"\"Initialize model parameters.\"\"\"\n    def init_params(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n            if m.bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                bound = 1 / np.sqrt(fan_in)\n                nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n    init_params(self.fc)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.M2MLSTM._step","title":"<code>_step(batch, batch_idx)</code>","text":"<p>Perform a single step (forward pass + loss calculation).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def _step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Perform a single step (forward pass + loss calculation).\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    xs, ys = batch\n    outs = self(xs)\n    loss = self.args['criterion'](outs, ys)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.M2MLSTM.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>Tuple[List[Optimizer], List[Dict]]</code> <p>Tuple containing a list of optimizers and a list of scheduler configurations</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def configure_optimizers(self) -&gt; Tuple[List[torch.optim.Optimizer], List[Dict]]:\n    \"\"\"\n    Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    Tuple[List[torch.optim.Optimizer], List[Dict]]\n        Tuple containing a list of optimizers and a list of scheduler configurations\n    \"\"\"\n    optimizer = torch.optim.AdamW(\n        self.parameters(), weight_decay=self.args['weight_decay'])\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=self.args['lr'],\n        epochs=self.args['epochs'],\n        total_steps=self.trainer.estimated_stepping_batches\n    )\n    lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n    return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.M2MLSTM.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the LSTM model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length, input_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, sequence_length, output_dim)</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the LSTM model.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (batch_size, sequence_length, input_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (batch_size, sequence_length, output_dim)\n    \"\"\"\n    B, L, N = x.shape\n    self.hidden_state = self.hidden_state.to(x.device)\n    self.cell_state = self.cell_state.to(x.device)\n    self.hidden_state.data.fill_(0.0)\n    self.cell_state.data.fill_(0.0)\n    lstm_outs = []\n    for i in range(L):\n        lstm_out, (self.hidden_state, self.cell_state) = \\\n            self.lstm(x[:, i].unsqueeze(1), (self.hidden_state, self.cell_state))\n        lstm_outs.append(lstm_out)\n\n    lstm_outs = torch.stack(lstm_outs, dim=1)  # B, L, N\n    out = self.fc(lstm_outs)\n    out = out.view(B, L, self.out_dim)\n    if self.args.get('clf', False):\n        out = F.log_softmax(out, dim=-1)\n\n    return out\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.M2MLSTM.init_hidden","title":"<code>init_hidden(batch_size)</code>","text":"<p>Initialize hidden state and cell state.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for initialization</p> required Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def init_hidden(self, batch_size: int) -&gt; None:\n    \"\"\"\n    Initialize hidden state and cell state.\n\n    Parameters\n    ----------\n    batch_size : int\n        Batch size for initialization\n    \"\"\"\n    self.batch_size = batch_size\n    self.hidden_state = torch.zeros((self.nlayers, batch_size, self.hidden_size), requires_grad=False)\n    self.cell_state = torch.zeros((self.nlayers, batch_size, self.hidden_size), requires_grad=False)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.M2MLSTM.on_after_backward","title":"<code>on_after_backward()</code>","text":"<p>Lightning method called after backpropagation.</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def on_after_backward(self) -&gt; None:\n    \"\"\"Lightning method called after backpropagation.\"\"\"\n    self.hidden_state = self.hidden_state.detach()\n    self.cell_state = self.cell_state.detach()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.M2MLSTM.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Lightning method for test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for test step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('test_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.M2MLSTM.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Lightning method for training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for training step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('train_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.M2MLSTM.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Lightning method for validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for validation step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('val_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Multi-Layer Perceptron (MLP) in PyTorch with an arbitrary number of hidden layers.</p> <p>This class implements an MLP model using PyTorch Lightning, allowing for flexible architecture with varying hidden layer sizes and dropout probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Dimensionality of input data, by default 100</p> <code>100</code> <code>out_dim</code> <code>int</code> <p>Dimensionality of output data, by default 2</p> <code>2</code> <code>hidden_dims</code> <code>List[Union[int, float]]</code> <p>List containing architectural parameters of the model. If an element is an int, it represents a hidden layer of that size. If an element is a float, it represents a dropout layer with that probability. By default ()</p> <code>()</code> <code>use_bias</code> <code>bool</code> <p>Whether to use bias in all linear layers, by default True</p> <code>True</code> <code>args</code> <code>Optional[Dict]</code> <p>Dictionary containing the hyperparameters of the model, by default None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>main</code> <code>Sequential</code> <p>The main sequential container of the MLP layers</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>class MLP(L.LightningModule):\n    \"\"\"\n    Multi-Layer Perceptron (MLP) in PyTorch with an arbitrary number of hidden layers.\n\n    This class implements an MLP model using PyTorch Lightning, allowing for flexible\n    architecture with varying hidden layer sizes and dropout probabilities.\n\n    Parameters\n    ----------\n    in_dim : int, optional\n        Dimensionality of input data, by default 100\n    out_dim : int, optional\n        Dimensionality of output data, by default 2\n    hidden_dims : List[Union[int, float]], optional\n        List containing architectural parameters of the model. If an element is\n        an int, it represents a hidden layer of that size. If an element is a float,\n        it represents a dropout layer with that probability. By default ()\n    use_bias : bool, optional\n        Whether to use bias in all linear layers, by default True\n    args : Optional[Dict], optional\n        Dictionary containing the hyperparameters of the model, by default None\n\n    Attributes\n    ----------\n    main : nn.Sequential\n        The main sequential container of the MLP layers\n    \"\"\"\n    def __init__(self, in_dim: int = 100, out_dim: int = 2,\n                 hidden_dims: List[Union[int, float]] = (),\n                 use_bias: bool = True, args: Optional[Dict] = None):\n        super().__init__()\n        self.save_hyperparameters()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.args = args if args is not None else {}\n        activations = nn.CELU if self.args.get('activations') is None else self.args['activations']\n\n        layers = self._build_layers(in_dim, out_dim, hidden_dims, use_bias, activations)\n        self.main = nn.Sequential(*layers)\n        self._init_params()\n\n    def _build_layers(\n            self,\n            in_dim: int,\n            out_dim: int,\n            hidden_dims: List[Union[int, float]],\n            use_bias: bool,\n            activations: nn.Module\n        ) -&gt; List[nn.Module]:\n        \"\"\"\n        Build the layers of the MLP.\n\n        Parameters\n        ----------\n        in_dim : int\n            Dimensionality of input data\n        out_dim : int\n            Dimensionality of output data\n        hidden_dims : List[Union[int, float]]\n            List of hidden layer sizes and dropout probabilities\n        use_bias : bool\n            Whether to use bias in linear layers\n        activations : nn.Module\n            Activation function to use\n\n        Returns\n        -------\n        List[nn.Module]\n            List of layers for the MLP\n        \"\"\"\n        if len(hidden_dims) == 0:\n            return [nn.Linear(in_dim, out_dim, bias=use_bias)]\n\n        layers = []\n        hidden_dims = [in_dim] + hidden_dims\n\n        for i, hidden_dim in enumerate(hidden_dims[:-1]):\n            if isinstance(hidden_dim, float):\n                continue\n            if isinstance(hidden_dims[i+1], float):\n                layers.extend([\n                    nn.Linear(hidden_dim, hidden_dims[i + 2], bias=use_bias),\n                    nn.Dropout(p=hidden_dims[i+1]),\n                    activations() if i &lt; len(hidden_dims)-1 else nn.Tanh()\n                ])\n            else:\n                layers.extend([\n                    nn.Linear(hidden_dim, hidden_dims[i + 1], bias=use_bias),\n                    activations() if i &lt; len(hidden_dims)-1 else nn.Tanh()\n                ])\n\n        layers.append(nn.Linear(hidden_dims[-1], out_dim, bias=use_bias))\n        if self.args.get('clf', False):\n            layers.append(nn.LogSoftmax(dim=1))\n\n        return layers\n\n    def _init_params(self) -&gt; None:\n        \"\"\"Initialize the parameters of the model.\"\"\"\n        def init_params(m: nn.Module) -&gt; None:\n            if isinstance(m, nn.Linear):\n                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n                if m.bias is not None:\n                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                    bound = 1 / torch.math.sqrt(fan_in)\n                    nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n        self.main.apply(init_params)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Defines the network structure and flow from input to output.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input data\n\n        Returns\n        -------\n        torch.Tensor\n            Output data\n        \"\"\"\n        return self.main(x)\n\n    def _step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Perform a single step (forward pass + loss calculation).\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        xs, ys = batch\n        outs = self(xs)\n        loss = self.args['criterion'](outs, ys)\n        return loss\n\n    def training_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for training step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for validation step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('val_loss', loss)\n        return loss\n\n    def test_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for test step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('test_loss', loss)\n        return loss\n\n    def configure_optimizers(self):\n        \"\"\"\n        Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        tuple\n            Tuple containing a list of optimizers and a list of scheduler configurations\n        \"\"\"\n        optimizer = torch.optim.AdamW(\n            self.parameters(),\n            weight_decay=self.args['weight_decay'],\n            betas=(0.9, 0.999),\n            amsgrad=True\n        )\n        scheduler = torch.optim.lr_scheduler.CyclicLR(\n            optimizer, \n            base_lr=self.args['base_lr'],\n            max_lr=self.args['lr'],\n            step_size_up=self.args['scheduler_step_size_multiplier'] * self.args['num_training_batches'],\n            cycle_momentum=False,\n            mode='triangular2',\n            gamma=0.99994,\n            last_epoch=-1,\n        )\n        lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n        return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.MLP._build_layers","title":"<code>_build_layers(in_dim, out_dim, hidden_dims, use_bias, activations)</code>","text":"<p>Build the layers of the MLP.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Dimensionality of input data</p> required <code>out_dim</code> <code>int</code> <p>Dimensionality of output data</p> required <code>hidden_dims</code> <code>List[Union[int, float]]</code> <p>List of hidden layer sizes and dropout probabilities</p> required <code>use_bias</code> <code>bool</code> <p>Whether to use bias in linear layers</p> required <code>activations</code> <code>Module</code> <p>Activation function to use</p> required <p>Returns:</p> Type Description <code>List[Module]</code> <p>List of layers for the MLP</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def _build_layers(\n        self,\n        in_dim: int,\n        out_dim: int,\n        hidden_dims: List[Union[int, float]],\n        use_bias: bool,\n        activations: nn.Module\n    ) -&gt; List[nn.Module]:\n    \"\"\"\n    Build the layers of the MLP.\n\n    Parameters\n    ----------\n    in_dim : int\n        Dimensionality of input data\n    out_dim : int\n        Dimensionality of output data\n    hidden_dims : List[Union[int, float]]\n        List of hidden layer sizes and dropout probabilities\n    use_bias : bool\n        Whether to use bias in linear layers\n    activations : nn.Module\n        Activation function to use\n\n    Returns\n    -------\n    List[nn.Module]\n        List of layers for the MLP\n    \"\"\"\n    if len(hidden_dims) == 0:\n        return [nn.Linear(in_dim, out_dim, bias=use_bias)]\n\n    layers = []\n    hidden_dims = [in_dim] + hidden_dims\n\n    for i, hidden_dim in enumerate(hidden_dims[:-1]):\n        if isinstance(hidden_dim, float):\n            continue\n        if isinstance(hidden_dims[i+1], float):\n            layers.extend([\n                nn.Linear(hidden_dim, hidden_dims[i + 2], bias=use_bias),\n                nn.Dropout(p=hidden_dims[i+1]),\n                activations() if i &lt; len(hidden_dims)-1 else nn.Tanh()\n            ])\n        else:\n            layers.extend([\n                nn.Linear(hidden_dim, hidden_dims[i + 1], bias=use_bias),\n                activations() if i &lt; len(hidden_dims)-1 else nn.Tanh()\n            ])\n\n    layers.append(nn.Linear(hidden_dims[-1], out_dim, bias=use_bias))\n    if self.args.get('clf', False):\n        layers.append(nn.LogSoftmax(dim=1))\n\n    return layers\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.MLP._init_params","title":"<code>_init_params()</code>","text":"<p>Initialize the parameters of the model.</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def _init_params(self) -&gt; None:\n    \"\"\"Initialize the parameters of the model.\"\"\"\n    def init_params(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n            if m.bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                bound = 1 / torch.math.sqrt(fan_in)\n                nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n    self.main.apply(init_params)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.MLP._step","title":"<code>_step(batch, batch_idx)</code>","text":"<p>Perform a single step (forward pass + loss calculation).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def _step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Perform a single step (forward pass + loss calculation).\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    xs, ys = batch\n    outs = self(xs)\n    loss = self.args['criterion'](outs, ys)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.MLP.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing a list of optimizers and a list of scheduler configurations</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"\n    Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    tuple\n        Tuple containing a list of optimizers and a list of scheduler configurations\n    \"\"\"\n    optimizer = torch.optim.AdamW(\n        self.parameters(),\n        weight_decay=self.args['weight_decay'],\n        betas=(0.9, 0.999),\n        amsgrad=True\n    )\n    scheduler = torch.optim.lr_scheduler.CyclicLR(\n        optimizer, \n        base_lr=self.args['base_lr'],\n        max_lr=self.args['lr'],\n        step_size_up=self.args['scheduler_step_size_multiplier'] * self.args['num_training_batches'],\n        cycle_momentum=False,\n        mode='triangular2',\n        gamma=0.99994,\n        last_epoch=-1,\n    )\n    lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n    return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.MLP.forward","title":"<code>forward(x)</code>","text":"<p>Defines the network structure and flow from input to output.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output data</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Defines the network structure and flow from input to output.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input data\n\n    Returns\n    -------\n    torch.Tensor\n        Output data\n    \"\"\"\n    return self.main(x)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.MLP.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Lightning method for test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for test step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('test_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.MLP.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Lightning method for training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def training_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for training step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('train_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.MLP.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Lightning method for validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for validation step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('val_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.NDT","title":"<code>NDT</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Transformer encoder-based dynamical systems decoder.</p> <p>This class implements a Transformer-based decoder trained on MLM loss. It returns loss and predicted rates.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Dimensionality of input data, by default 100</p> <code>100</code> <code>out_dim</code> <code>int</code> <p>Number of output columns, by default 2</p> <code>2</code> <code>hidden_dims</code> <code>Tuple[int]</code> <p>Architectural parameters of the model (dim_feedforward, num_layers, nhead, dropout, rate_dropout),  by default [400, 1, 1, 0.0, 0.0]</p> <code>(400, 1, 1, 0.0, 0.0)</code> <code>max_context_len</code> <code>int</code> <p>Maximum context length, by default 2</p> <code>2</code> <code>args</code> <code>Optional[Dict]</code> <p>Dictionary containing the hyperparameters of the model, by default None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>pos_encoder</code> <code>PositionalEncoding</code> <p>Positional encoding module</p> <code>transformer_encoder</code> <code>TransformerEncoder</code> <p>Transformer encoder module</p> <code>rate_dropout</code> <code>Dropout</code> <p>Dropout layer for rates</p> <code>decoder</code> <code>Sequential</code> <p>Decoder network</p> <code>src_mask</code> <code>Dict[str, Tensor]</code> <p>Dictionary to store source masks for different devices</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>class NDT(L.LightningModule):\n    \"\"\"\n    Transformer encoder-based dynamical systems decoder.\n\n    This class implements a Transformer-based decoder trained on MLM loss.\n    It returns loss and predicted rates.\n\n    Parameters\n    ----------\n    in_dim : int, optional\n        Dimensionality of input data, by default 100\n    out_dim : int, optional\n        Number of output columns, by default 2\n    hidden_dims : Tuple[int], optional\n        Architectural parameters of the model\n        (dim_feedforward, num_layers, nhead, dropout, rate_dropout), \n        by default [400, 1, 1, 0.0, 0.0]\n    max_context_len : int, optional\n        Maximum context length, by default 2\n    args : Optional[Dict], optional\n        Dictionary containing the hyperparameters of the model, by default None\n\n    Attributes\n    ----------\n    pos_encoder : PositionalEncoding\n        Positional encoding module\n    transformer_encoder : nn.TransformerEncoder\n        Transformer encoder module\n    rate_dropout : nn.Dropout\n        Dropout layer for rates\n    decoder : nn.Sequential\n        Decoder network\n    src_mask : Dict[str, torch.Tensor]\n        Dictionary to store source masks for different devices\n    \"\"\"\n    def __init__(self, in_dim: int = 100, out_dim: int = 2,\n                 hidden_dims: Tuple[int] = (400, 1, 1, 0.0, 0.0),\n                 max_context_len: int = 2, args: Optional[Dict] = None):\n        super().__init__()\n        self.save_hyperparameters()\n        self.max_context_len = max_context_len\n        self.in_dim = in_dim\n        self.args = args if args is not None else {}\n        activations = nn.CELU if self.args.get('activations') is None else self.args['activations']\n\n        self.src_mask: Dict[str, torch.Tensor] = {}\n\n        self.pos_encoder = PositionalEncoding(in_dim, max_context_len, self.args)\n\n        encoder_lyr = nn.TransformerEncoderLayer(\n            in_dim,\n            nhead=hidden_dims[2],\n            dim_feedforward=hidden_dims[0],\n            dropout=hidden_dims[3],\n            activation=nn.functional.relu\n        )\n\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_lyr, hidden_dims[1], nn.LayerNorm(in_dim))\n\n        self.rate_dropout = nn.Dropout(hidden_dims[4])\n\n        self.decoder = nn.Sequential(\n            nn.Linear(in_dim, 16), activations(), nn.Linear(16, out_dim)\n        )\n\n        self._init_params()\n\n    def _init_params(self) -&gt; None:\n        \"\"\"Initialize the parameters of the decoder.\"\"\"\n        def init_params(m: nn.Module) -&gt; None:\n            if isinstance(m, nn.Linear):\n                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n                if m.bias is not None:\n                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                    bound = 1 / np.sqrt(fan_in)\n                    nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n        self.decoder.apply(init_params)\n\n    def forward(self, x: torch.Tensor, mask_labels: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input data of shape (batch_size, seq_len, in_dim)\n        mask_labels : Optional[torch.Tensor], optional\n            Masking labels for the input data, by default None\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor of shape (batch_size, seq_len, out_dim)\n        \"\"\"\n        x = x.permute(1, 0, 2)  # LxBxN\n        x = self.pos_encoder(x)\n        x_mask = self._get_or_generate_context_mask(x)\n        z = self.transformer_encoder(x, x_mask)\n        z = self.rate_dropout(z)\n        out = self.decoder(z).permute(1, 0, 2)  # B x L x out_dim\n        if self.args.get('clf', False):\n            out = F.log_softmax(out, dim=-1)\n        return out\n\n    def _get_or_generate_context_mask(self, src: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Get or generate the context mask for the input tensor.\n\n        Parameters\n        ----------\n        src : torch.Tensor\n            Input tensor\n\n        Returns\n        -------\n        torch.Tensor\n            Context mask for the input tensor\n        \"\"\"\n        context_forward = 4\n        size = src.size(0)  # T\n        mask = (torch.triu(torch.ones(size, size, device=src.device), diagonal=-context_forward) == 1).transpose(0, 1)\n        mask = mask.float()\n        self.src_mask[str(src.device)] = mask\n        return self.src_mask[str(src.device)]\n\n    def _step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Perform a single step (forward pass + loss calculation).\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        xs, ys = batch\n        outs = self(xs)\n        loss = self.args['criterion'](outs, ys)\n        return loss\n\n    def training_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for training step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for validation step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('val_loss', loss)\n        return loss\n\n    def test_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for test step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('test_loss', loss)\n        return loss\n\n    def configure_optimizers(self) -&gt; tuple:\n        \"\"\"\n        Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        tuple\n            List of optimizers and a list of scheduler configurations\n        \"\"\"\n        optimizer = torch.optim.AdamW(\n            self.parameters(), weight_decay=self.args['weight_decay'])\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer, max_lr=self.args['lr'],\n            epochs=self.args['epochs'],\n            total_steps=self.trainer.estimated_stepping_batches\n        )\n        lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n        return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.NDT._get_or_generate_context_mask","title":"<code>_get_or_generate_context_mask(src)</code>","text":"<p>Get or generate the context mask for the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Context mask for the input tensor</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def _get_or_generate_context_mask(self, src: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Get or generate the context mask for the input tensor.\n\n    Parameters\n    ----------\n    src : torch.Tensor\n        Input tensor\n\n    Returns\n    -------\n    torch.Tensor\n        Context mask for the input tensor\n    \"\"\"\n    context_forward = 4\n    size = src.size(0)  # T\n    mask = (torch.triu(torch.ones(size, size, device=src.device), diagonal=-context_forward) == 1).transpose(0, 1)\n    mask = mask.float()\n    self.src_mask[str(src.device)] = mask\n    return self.src_mask[str(src.device)]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.NDT._init_params","title":"<code>_init_params()</code>","text":"<p>Initialize the parameters of the decoder.</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def _init_params(self) -&gt; None:\n    \"\"\"Initialize the parameters of the decoder.\"\"\"\n    def init_params(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n            if m.bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                bound = 1 / np.sqrt(fan_in)\n                nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n    self.decoder.apply(init_params)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.NDT._step","title":"<code>_step(batch, batch_idx)</code>","text":"<p>Perform a single step (forward pass + loss calculation).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def _step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Perform a single step (forward pass + loss calculation).\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    xs, ys = batch\n    outs = self(xs)\n    loss = self.args['criterion'](outs, ys)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.NDT.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>List of optimizers and a list of scheduler configurations</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def configure_optimizers(self) -&gt; tuple:\n    \"\"\"\n    Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    tuple\n        List of optimizers and a list of scheduler configurations\n    \"\"\"\n    optimizer = torch.optim.AdamW(\n        self.parameters(), weight_decay=self.args['weight_decay'])\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=self.args['lr'],\n        epochs=self.args['epochs'],\n        total_steps=self.trainer.estimated_stepping_batches\n    )\n    lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n    return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.NDT.forward","title":"<code>forward(x, mask_labels=None)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data of shape (batch_size, seq_len, in_dim)</p> required <code>mask_labels</code> <code>Optional[Tensor]</code> <p>Masking labels for the input data, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, seq_len, out_dim)</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def forward(self, x: torch.Tensor, mask_labels: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input data of shape (batch_size, seq_len, in_dim)\n    mask_labels : Optional[torch.Tensor], optional\n        Masking labels for the input data, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (batch_size, seq_len, out_dim)\n    \"\"\"\n    x = x.permute(1, 0, 2)  # LxBxN\n    x = self.pos_encoder(x)\n    x_mask = self._get_or_generate_context_mask(x)\n    z = self.transformer_encoder(x, x_mask)\n    z = self.rate_dropout(z)\n    out = self.decoder(z).permute(1, 0, 2)  # B x L x out_dim\n    if self.args.get('clf', False):\n        out = F.log_softmax(out, dim=-1)\n    return out\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.NDT.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Lightning method for test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for test step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('test_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.NDT.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Lightning method for training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def training_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for training step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('train_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.NDT.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Lightning method for validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for validation step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('val_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.create_model","title":"<code>create_model(hyperparams)</code>","text":"<p>Create a model based on the given hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparams</code> <code>Dict[str, Any]</code> <p>Dictionary containing model hyperparameters.</p> required <p>Returns:</p> Type Description <code>Tuple[Any, LightningModule]</code> <p>The decoder class and instantiated model.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def create_model(hyperparams: Dict[str, Any]) -&gt; Tuple[Any, pl.LightningModule]:\n    \"\"\"\n    Create a model based on the given hyperparameters.\n\n    Parameters\n    ----------\n    hyperparams : Dict[str, Any]\n        Dictionary containing model hyperparameters.\n\n    Returns\n    -------\n    Tuple[Any, pl.LightningModule]\n        The decoder class and instantiated model.\n    \"\"\"\n    decoder = eval(f\"{hyperparams['model']}\")\n    model = decoder(**hyperparams['model_args'])\n\n    if 'LSTM' in hyperparams['model']:\n        model.init_hidden(hyperparams['batch_size'])\n        model.hidden_state = model.hidden_state.to(hyperparams['device'])\n        model.cell_state = model.cell_state.to(hyperparams['device'])\n\n    return decoder, model\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.decode","title":"<code>decode(ct, tc, occupancy, bin_size_s, uniform_prior=False)</code>","text":"<p>Decode position from spike counts in an N-dimensional spatial environment</p> <p>Parameters:</p> Name Type Description Default <code>ct</code> <code>ndarray</code> <p>2D array, spike counts matrix with shape (n_bins, n_cells)</p> required <code>tc</code> <code>ndarray</code> <p>ND array, ratemap matrix with shape (n_xbins, n_ybins, ..., n_cells)</p> required <code>occupancy</code> <code>ndarray</code> <p>(N-1)D array, occupancy matrix with shape (n_xbins, n_ybins, ...)</p> required <code>bin_size_s</code> <code>float</code> <p>float, width of each time bin in seconds</p> required <code>uniform_prior</code> <code>bool</code> <p>bool, whether to use uniform prior, by default False</p> <code>False</code> <p>Returns:</p> Name Type Description <code>p</code> <code>ndarray</code> <p>(N+1)D array, decoded position probabilities matrix with shape (n_bins, n_xbins, n_ybins, ...)</p> <p>Examples:</p>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.decode--1d-example","title":"1D example","text":"<pre><code>&gt;&gt;&gt; ct = np.random.rand(10, 5)\n&gt;&gt;&gt; tc = np.random.rand(3, 5)\n&gt;&gt;&gt; occupancy = np.random.rand(3)\n&gt;&gt;&gt; bin_size_s = 0.1\n&gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.decode--2d-example","title":"2D example","text":"<pre><code>&gt;&gt;&gt; ct = np.random.rand(10, 5)\n&gt;&gt;&gt; tc = np.random.rand(3, 3, 5)\n&gt;&gt;&gt; occupancy = np.random.rand(3, 3)\n&gt;&gt;&gt; bin_size_s = 0.1\n&gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.decode--3d-example","title":"3D example","text":"<pre><code>&gt;&gt;&gt; ct = np.random.rand(10, 5)\n&gt;&gt;&gt; tc = np.random.rand(3, 3, 3, 5)\n&gt;&gt;&gt; occupancy = np.random.rand(3, 3, 3)\n&gt;&gt;&gt; bin_size_s = 0.1\n&gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n</code></pre> Source code in <code>neuro_py/ensemble/decoding/bayesian.py</code> <pre><code>@njit(parallel=True, fastmath=True)\ndef decode(\n    ct: np.ndarray,\n    tc: np.ndarray,\n    occupancy: np.ndarray,\n    bin_size_s: float,\n    uniform_prior: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Decode position from spike counts in an N-dimensional spatial environment\n\n    Parameters\n    ----------\n    ct : ndarray\n        2D array, spike counts matrix with shape (n_bins, n_cells)\n    tc : ndarray\n        ND array, ratemap matrix with shape (n_xbins, n_ybins, ..., n_cells)\n    occupancy : ndarray\n        (N-1)D array, occupancy matrix with shape (n_xbins, n_ybins, ...)\n    bin_size_s : float\n        float, width of each time bin in seconds\n    uniform_prior : bool, optional\n        bool, whether to use uniform prior, by default False\n\n    Returns\n    ----------\n    p : ndarray\n        (N+1)D array, decoded position probabilities matrix with shape (n_bins, n_xbins, n_ybins, ...)\n\n    Examples\n    ----------\n    # 1D example\n    &gt;&gt;&gt; ct = np.random.rand(10, 5)\n    &gt;&gt;&gt; tc = np.random.rand(3, 5)\n    &gt;&gt;&gt; occupancy = np.random.rand(3)\n    &gt;&gt;&gt; bin_size_s = 0.1\n    &gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n\n    # 2D example\n    &gt;&gt;&gt; ct = np.random.rand(10, 5)\n    &gt;&gt;&gt; tc = np.random.rand(3, 3, 5)\n    &gt;&gt;&gt; occupancy = np.random.rand(3, 3)\n    &gt;&gt;&gt; bin_size_s = 0.1\n    &gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n\n    # 3D example\n    &gt;&gt;&gt; ct = np.random.rand(10, 5)\n    &gt;&gt;&gt; tc = np.random.rand(3, 3, 3, 5)\n    &gt;&gt;&gt; occupancy = np.random.rand(3, 3, 3)\n    &gt;&gt;&gt; bin_size_s = 0.1\n    &gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n    \"\"\"\n\n    # Ensure input arrays are contiguous for vectorization\n    ct = np.ascontiguousarray(ct)\n    tc = np.ascontiguousarray(tc)\n    occupancy = np.ascontiguousarray(occupancy)\n\n    # Validate input shapes\n    assert ct.ndim == 2, \"ct must be a 2D array with shape (n_bins, n_cells)\"\n    assert tc.ndim &gt;= 2, (\n        \"tc must be at least a 2D array with shape (n_xbins, ..., n_cells)\"\n    )\n    assert occupancy.ndim == tc.ndim - 1, (\n        \"occupancy must have one fewer dimension than tc\"\n    )\n    assert ct.shape[1] == tc.shape[-1], \"Number of cells in ct and tc must match\"\n\n    # Flatten spatial dimensions\n    n_cells = tc.shape[-1]\n    spatial_shape = tc.shape[:-1]  # Shape of spatial dimensions\n\n    # Calculate the total number of spatial bins\n    n_spatial_bins = 1\n    for dim in spatial_shape:\n        n_spatial_bins *= dim\n\n    tc_flat = tc.reshape(n_spatial_bins, n_cells)\n    occupancy_flat = occupancy.flatten()\n\n    if uniform_prior:\n        # Use uniform prior\n        occupancy_flat = np.ones_like(occupancy_flat)\n\n    # Precompute log values\n    log_tc_flat = np.log(tc_flat + 1e-10)  # add small value to avoid log(0)\n    log_p1 = -tc_flat.sum(axis=1) * bin_size_s\n    log_p2 = np.log(occupancy_flat / occupancy_flat.sum())\n\n    # Initialize the probability matrix\n    n_bins = ct.shape[0]\n    p = np.zeros((n_bins, n_spatial_bins))\n\n    # Vectorized calculation of log probabilities\n    for i in prange(n_bins):  # prange for parallel loop\n        log_likelihood = log_p1 + log_p2 + np.sum(log_tc_flat * ct[i, :], axis=1)\n        p[i, :] = np.exp(\n            log_likelihood - np.max(log_likelihood)\n        )  # Subtract max for numerical stability\n\n    # Normalize the probabilities along the spatial axis\n    p_sum = p.sum(axis=1)  # Sum over spatial bins\n    p = p / p_sum.reshape(-1, 1)  # Reshape p_sum to (n_bins, 1) for broadcasting\n\n    # Reshape the probabilities to the original spatial dimensions\n    p = p.reshape((n_bins,) + spatial_shape)\n\n    return p\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.evaluate_model","title":"<code>evaluate_model(hyperparams, ohe, predictor, X_test, y_test)</code>","text":"<p>Evaluate the model on test data.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparams</code> <code>Dict[str, Any]</code> <p>Dictionary containing hyperparameters.</p> required <code>ohe</code> <code>OneHotEncoder</code> <p>One-hot encoder for categorical variables.</p> required <code>predictor</code> <code>Module</code> <p>The trained model.</p> required <code>X_test</code> <code>NDArray</code> <p>Test features.</p> required <code>y_test</code> <code>NDArray</code> <p>Test labels.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, float], NDArray]</code> <p>Evaluation metrics and model predictions.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def evaluate_model(\n        hyperparams: Dict[str, Any],\n        ohe: sklearn.preprocessing.OneHotEncoder,\n        predictor: torch.nn.Module,\n        X_test: NDArray,\n        y_test: NDArray\n    ) -&gt; Tuple[Dict[str, float], NDArray]:\n    \"\"\"\n    Evaluate the model on test data.\n\n    Parameters\n    ----------\n    hyperparams : Dict[str, Any]\n        Dictionary containing hyperparameters.\n    ohe : OneHotEncoder\n        One-hot encoder for categorical variables.\n    predictor : torch.nn.Module\n        The trained model.\n    X_test : NDArray\n        Test features.\n    y_test : NDArray\n        Test labels.\n\n    Returns\n    -------\n    Tuple[Dict[str, float], NDArray]\n        Evaluation metrics and model predictions.\n    \"\"\"\n    if hyperparams['model'] in ('M2MLSTM', 'NDT'):\n        out_dim = hyperparams['model_args']['out_dim']\n        with torch.no_grad():\n            bv_preds_fold = [\n                predictor(\n                    torch.from_numpy(X.reshape(1, *X.shape)).type(torch.float32)\n                )\n                for X in X_test\n            ]\n        bv_preds_fold = np.vstack([\n            bv.squeeze().detach().cpu().numpy().reshape(-1, out_dim)\n            for bv in bv_preds_fold\n        ])\n    else:\n        bv_preds_fold = predictor(\n            torch.from_numpy(X_test).type(torch.float32)\n        )\n        bv_preds_fold = bv_preds_fold.detach().cpu().numpy()\n\n    bv_preds_fold = copy.deepcopy(bv_preds_fold)\n\n    logits = bv_preds_fold\n    labels = np.vstack(y_test)\n    if hyperparams['model_args']['args']['clf']:\n        logits = ohe.inverse_transform(logits)\n        labels = ohe.inverse_transform(labels)\n        accuracy = sklearn.metrics.accuracy_score(labels, logits)\n        metrics = dict(accuracy=accuracy)\n        bv_preds_fold = logits\n    else:\n        coeff_determination = sklearn.metrics.r2_score(\n            labels,\n            logits,\n            multioutput='variance_weighted'\n        )\n        rmse = sklearn.metrics.root_mean_squared_error(labels, logits)\n        metrics = dict(coeff_determination=coeff_determination, rmse=rmse)\n    return metrics, bv_preds_fold\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.format_trial_segs_nsv","title":"<code>format_trial_segs_nsv(nsv_train_normed, nsv_rest_normed, bv_train, bv_rest, predict_bv, bins_before=0, bins_current=1, bins_after=0)</code>","text":"<p>Format trial segments for neural state vectors.</p> <p>Parameters:</p> Name Type Description Default <code>nsv_train_normed</code> <code>List[NDArray]</code> <p>Normalized neural state vectors for training.</p> required <code>nsv_rest_normed</code> <code>List[NDArray]</code> <p>Normalized neural state vectors for rest.</p> required <code>bv_train</code> <code>NDArray</code> <p>Behavioral state vectors for training.</p> required <code>bv_rest</code> <code>List[NDArray]</code> <p>Behavioral state vectors for rest.</p> required <code>predict_bv</code> <code>List[int]</code> <p>Indices of behavioral state vectors to predict.</p> required <code>bins_before</code> <code>int</code> <p>Number of bins before the current bin, by default 0.</p> <code>0</code> <code>bins_current</code> <code>int</code> <p>Number of current bins, by default 1.</p> <code>1</code> <code>bins_after</code> <code>int</code> <p>Number of bins after the current bin, by default 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[NDArray, List[NDArray], NDArray, List[NDArray], NDArray, List[NDArray]]</code> <p>Formatted trial segments for neural state vectors.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def format_trial_segs_nsv(\n    nsv_train_normed: List[NDArray],\n    nsv_rest_normed: List[NDArray],\n    bv_train: NDArray,\n    bv_rest: List[NDArray],\n    predict_bv: List[int],\n    bins_before: int = 0,\n    bins_current: int = 1,\n    bins_after: int = 0\n) -&gt; Tuple[NDArray, List[NDArray], NDArray, List[NDArray], NDArray, List[NDArray]]:\n    \"\"\"\n    Format trial segments for neural state vectors.\n\n    Parameters\n    ----------\n    nsv_train_normed : List[NDArray]\n        Normalized neural state vectors for training.\n    nsv_rest_normed : List[NDArray]\n        Normalized neural state vectors for rest.\n    bv_train : NDArray\n        Behavioral state vectors for training.\n    bv_rest : List[NDArray]\n        Behavioral state vectors for rest.\n    predict_bv : List[int]\n        Indices of behavioral state vectors to predict.\n    bins_before : int, optional\n        Number of bins before the current bin, by default 0.\n    bins_current : int, optional\n        Number of current bins, by default 1.\n    bins_after : int, optional\n        Number of bins after the current bin, by default 0.\n\n    Returns\n    -------\n    Tuple[NDArray, List[NDArray], NDArray, List[NDArray], NDArray, List[NDArray]]\n        Formatted trial segments for neural state vectors.\n    \"\"\"\n    is_2D = nsv_train_normed[0].ndim == 1\n    # Format for RNNs: covariate matrix including spike history from previous bins\n    X_train = np.concatenate(_get_trial_spikes_with_no_overlap_history(\n        nsv_train_normed, bins_before, bins_after, bins_current))\n    X_rest = []\n    for nsv_feats in nsv_rest_normed:\n        X_feats = np.concatenate(_get_trial_spikes_with_no_overlap_history(\n            nsv_feats, bins_before, bins_after, bins_current))\n        X_rest.append(X_feats)\n\n    # each \"neuron / time\" is a single feature\n    X_flat_train = X_train.reshape(\n        X_train.shape[0], (X_train.shape[1] * X_train.shape[2]))\n    X_flat_rest = []\n    for X_feat in X_rest:\n        X_flat_feat = X_feat.reshape(\n            X_feat.shape[0], (X_feat.shape[1] * X_feat.shape[2]))\n        X_flat_rest.append(X_flat_feat)\n\n    bv_train = bv_train if not is_2D else [bv_train]\n    y_train = np.concatenate(bv_train)\n    y_train = y_train[:, predict_bv]\n    y_rest = []\n    for bv_y in bv_rest:\n        bv_y = bv_y if not is_2D else [bv_y]\n        y = np.concatenate(bv_y)\n        y = y[:, predict_bv]\n        y_rest.append(y)\n\n    return X_train, X_rest, X_flat_train, X_flat_rest, y_train, y_rest\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.get_spikes_with_history","title":"<code>get_spikes_with_history(neural_data, bins_before, bins_after, bins_current=1)</code>","text":"<p>Create the covariate matrix of neural activity.</p> <p>Parameters:</p> Name Type Description Default <code>neural_data</code> <code>ndarray</code> <p>A matrix of size \"number of time bins\" x \"number of neurons\", representing the number of spikes in each time bin for each neuron.</p> required <code>bins_before</code> <code>int</code> <p>How many bins of neural data prior to the output are used for decoding.</p> required <code>bins_after</code> <code>int</code> <p>How many bins of neural data after the output are used for decoding.</p> required <code>bins_current</code> <code>int</code> <p>Whether to use the concurrent time bin of neural data for decoding, by default 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A matrix of size \"number of total time bins\" x \"number of surrounding time bins used for prediction\" x \"number of neurons\".</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def get_spikes_with_history(neural_data: np.ndarray, bins_before: int, bins_after: int, bins_current: int = 1) -&gt; np.ndarray:\n    \"\"\"\n    Create the covariate matrix of neural activity.\n\n    Parameters\n    ----------\n    neural_data : np.ndarray\n        A matrix of size \"number of time bins\" x \"number of neurons\",\n        representing the number of spikes in each time bin for each neuron.\n    bins_before : int\n        How many bins of neural data prior to the output are used for decoding.\n    bins_after : int\n        How many bins of neural data after the output are used for decoding.\n    bins_current : int, optional\n        Whether to use the concurrent time bin of neural data for decoding, by\n        default 1.\n\n    Returns\n    -------\n    np.ndarray\n        A matrix of size \"number of total time bins\" x \"number of surrounding\n        time bins used for prediction\" x \"number of neurons\".\n    \"\"\"\n    num_examples, num_neurons = neural_data.shape\n    surrounding_bins = bins_before + bins_after + bins_current\n    X = np.zeros([num_examples, surrounding_bins, num_neurons])\n\n    for i in range(num_examples - bins_before - bins_after):\n        start_idx = i\n        end_idx = start_idx + surrounding_bins\n        X[i + bins_before] = neural_data[start_idx:end_idx]\n\n    return X\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.minibatchify","title":"<code>minibatchify(Xtrain, ytrain, Xval, yval, Xtest, ytest, seed=0, batch_size=128, num_workers=5, modeltype='MLP')</code>","text":"<p>Create minibatches for training, validation, and testing.</p> <p>Parameters:</p> Name Type Description Default <code>Xtrain</code> <code>NDArray</code> <p>Training features.</p> required <code>ytrain</code> <code>NDArray</code> <p>Training labels.</p> required <code>Xval</code> <code>NDArray</code> <p>Validation features.</p> required <code>yval</code> <code>NDArray</code> <p>Validation labels.</p> required <code>Xtest</code> <code>NDArray</code> <p>Test features.</p> required <code>ytest</code> <code>NDArray</code> <p>Test labels.</p> required <code>seed</code> <code>int</code> <p>Random seed, by default 0.</p> <code>0</code> <code>batch_size</code> <code>int</code> <p>Batch size, by default 128.</p> <code>128</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading, by default 5.</p> <code>5</code> <code>modeltype</code> <code>str</code> <p>Type of model, by default 'MLP'.</p> <code>'MLP'</code> <p>Returns:</p> Type Description <code>Tuple[DataLoader, DataLoader, DataLoader]</code> <p>DataLoaders for training, validation, and testing.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def minibatchify(\n        Xtrain: NDArray,\n        ytrain: NDArray,\n        Xval: NDArray,\n        yval: NDArray,\n        Xtest: NDArray,\n        ytest: NDArray,\n        seed: int = 0,\n        batch_size: int = 128,\n        num_workers: int = 5,\n        modeltype: str = 'MLP'\n    ) -&gt; Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n    \"\"\"\n    Create minibatches for training, validation, and testing.\n\n    Parameters\n    ----------\n    Xtrain : NDArray\n        Training features.\n    ytrain : NDArray\n        Training labels.\n    Xval : NDArray\n        Validation features.\n    yval : NDArray\n        Validation labels.\n    Xtest : NDArray\n        Test features.\n    ytest : NDArray\n        Test labels.\n    seed : int, optional\n        Random seed, by default 0.\n    batch_size : int, optional\n        Batch size, by default 128.\n    num_workers : int, optional\n        Number of workers for data loading, by default 5.\n    modeltype : str, optional\n        Type of model, by default 'MLP'.\n\n    Returns\n    -------\n    Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader]\n        DataLoaders for training, validation, and testing.\n    \"\"\"\n    g_seed = torch.Generator()\n    g_seed.manual_seed(seed)\n    if Xtrain.ndim == 2:  # handle object arrays\n        Xtrain = Xtrain.astype(np.float32)\n        Xval = Xval.astype(np.float32)\n        Xtest = Xtest.astype(np.float32)\n        ytrain = ytrain.astype(np.float32)\n        yval = yval.astype(np.float32)\n        ytest = ytest.astype(np.float32)\n    train = torch.utils.data.TensorDataset(\n        torch.from_numpy(Xtrain).type(torch.float32),\n        torch.from_numpy(ytrain).type(torch.float32))\n    val = torch.utils.data.TensorDataset(\n        torch.from_numpy(Xval).type(torch.float32), \n        torch.from_numpy(yval).type(torch.float32))\n    test = torch.utils.data.TensorDataset(\n        torch.from_numpy(Xtest).type(torch.float32),\n        torch.from_numpy(ytest).type(torch.float32))\n\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n        shuffle=True, num_workers=num_workers, pin_memory=True,\n        drop_last=(modeltype=='LSTM'), worker_init_fn=seed_worker,\n        generator=g_seed)\n\n    val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size,\n        shuffle=False, num_workers=num_workers, pin_memory=True,\n        drop_last=(modeltype=='LSTM'), worker_init_fn=seed_worker,\n        generator=g_seed)\n\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size,\n        shuffle=False, num_workers=num_workers, pin_memory=True,\n        drop_last=(modeltype=='LSTM'), worker_init_fn=seed_worker,\n        generator=g_seed)\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.normalize_format_trial_segs","title":"<code>normalize_format_trial_segs(nsv_train, nsv_rest, bv_train, bv_rest, predict_bv=[4, 5], bins_before=0, bins_current=1, bins_after=0, normparams=None)</code>","text":"<p>Normalize and format trial segments.</p> <p>Parameters:</p> Name Type Description Default <code>nsv_train</code> <code>NDArray</code> <p>Neural state vectors for training.</p> required <code>nsv_rest</code> <code>List[NDArray]</code> <p>Neural state vectors for rest.</p> required <code>bv_train</code> <code>NDArray</code> <p>Behavioral state vectors for training.</p> required <code>bv_rest</code> <code>List[NDArray]</code> <p>Behavioral state vectors for rest.</p> required <code>predict_bv</code> <code>List[int]</code> <p>Indices of behavioral state vectors to predict, by default [4, 5].</p> <code>[4, 5]</code> <code>bins_before</code> <code>int</code> <p>Number of bins before the current bin, by default 0.</p> <code>0</code> <code>bins_current</code> <code>int</code> <p>Number of current bins, by default 1.</p> <code>1</code> <code>bins_after</code> <code>int</code> <p>Number of bins after the current bin, by default 0.</p> <code>0</code> <code>normparams</code> <code>Optional[Dict[str, Any]]</code> <p>Normalization parameters, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[NDArray, NDArray, NDArray, List[Tuple[NDArray, NDArray, NDArray]], Dict[str, Any]]</code> <p>Normalized and formatted trial segments.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def normalize_format_trial_segs(\n    nsv_train: NDArray,\n    nsv_rest: List[NDArray],\n    bv_train: NDArray,\n    bv_rest: List[NDArray],\n    predict_bv: List[int] = [4, 5],\n    bins_before: int = 0,\n    bins_current: int = 1,\n    bins_after: int = 0,\n    normparams: Optional[Dict[str, Any]] = None\n) -&gt; Tuple[NDArray, NDArray, NDArray, List[Tuple[NDArray, NDArray, NDArray]], Dict[str, Any]]:\n    \"\"\"\n    Normalize and format trial segments.\n\n    Parameters\n    ----------\n    nsv_train : NDArray\n        Neural state vectors for training.\n    nsv_rest : List[NDArray]\n        Neural state vectors for rest.\n    bv_train : NDArray\n        Behavioral state vectors for training.\n    bv_rest : List[NDArray]\n        Behavioral state vectors for rest.\n    predict_bv : List[int], optional\n        Indices of behavioral state vectors to predict, by default [4, 5].\n    bins_before : int, optional\n        Number of bins before the current bin, by default 0.\n    bins_current : int, optional\n        Number of current bins, by default 1.\n    bins_after : int, optional\n        Number of bins after the current bin, by default 0.\n    normparams : Optional[Dict[str, Any]], optional\n        Normalization parameters, by default None.\n\n    Returns\n    -------\n    Tuple[NDArray, NDArray, NDArray, List[Tuple[NDArray, NDArray, NDArray]], Dict[str, Any]]\n        Normalized and formatted trial segments.\n    \"\"\"\n    nsv_train_normed, nsv_rest_normed, norm_params = zscore_trial_segs(nsv_train, nsv_rest, normparams)\n\n    (X_train, X_rest, X_flat_train, X_flat_rest, y_train, y_rest\n    ) = format_trial_segs_nsv(\n        nsv_train_normed, nsv_rest_normed, bv_train, bv_rest, predict_bv,\n        bins_before, bins_current, bins_after)\n\n    #Zero-center outputs\n    y_train_mean = normparams['y_train_mean'] if normparams is not None else np.mean(y_train, axis=0)\n    y_train = y_train - y_train_mean\n    y_centered_rest = []\n    for y in y_rest:\n        y_centered_rest.append(y - y_train_mean)\n\n    norm_params['y_train_mean'] = y_train_mean\n\n    return X_train, X_flat_train, y_train, tuple(zip(X_rest, X_flat_rest, y_centered_rest)), norm_params\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.normalize_labels","title":"<code>normalize_labels(y_train, y_val, y_test)</code>","text":"<p>Normalize labels to integers in [0, n_classes).</p> <p>Parameters:</p> Name Type Description Default <code>y_train</code> <code>NDArray</code> <p>Training labels.</p> required <code>y_val</code> <code>NDArray</code> <p>Validation labels.</p> required <code>y_test</code> <code>NDArray</code> <p>Test labels.</p> required <p>Returns:</p> Type Description <code>Tuple[Tuple[NDArray, NDArray, NDArray], int]</code> <p>Normalized labels and number of classes.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def normalize_labels(\n        y_train: NDArray,\n        y_val: NDArray,\n        y_test: NDArray\n    ) -&gt; Tuple[Tuple[NDArray, NDArray, NDArray], int]:\n    \"\"\"\n    Normalize labels to integers in [0, n_classes).\n\n    Parameters\n    ----------\n    y_train : NDArray\n        Training labels.\n    y_val : NDArray\n        Validation labels.\n    y_test : NDArray\n        Test labels.\n\n    Returns\n    -------\n    Tuple[Tuple[NDArray, NDArray, NDArray], int]\n        Normalized labels and number of classes.\n    \"\"\"\n    # map labels to integers in [0, n_classes)\n    uniq_labels = np.unique(np.concatenate((y_train, y_val, y_test)))\n    n_classes = len(uniq_labels)\n    uniq_labels_idx_map = dict(zip(uniq_labels, range(n_classes)))\n    y_train = np.vectorize(lambda v: uniq_labels_idx_map[v])(y_train)\n    y_val = np.vectorize(lambda v: uniq_labels_idx_map[v])(y_val)\n    y_test = np.vectorize(lambda v: uniq_labels_idx_map[v])(y_test)\n    return (y_train, y_val, y_test), n_classes\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.partition_indices","title":"<code>partition_indices(folds)</code>","text":"<p>Partition indices into train, validation, and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>folds</code> <code>List[ndarray]</code> <p>Indices for each fold.</p> required <p>Returns:</p> Type Description <code>List[Tuple[ndarray, ndarray, ndarray]]</code> <p>Train, validation, and test indices.</p> Source code in <code>neuro_py/ensemble/decoding/preprocess.py</code> <pre><code>def partition_indices(folds: List[np.ndarray]) -&gt; List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"\n    Partition indices into train, validation, and test sets.\n\n    Parameters\n    ----------\n    folds : List[np.ndarray]\n        Indices for each fold.\n\n    Returns\n    -------\n    List[Tuple[np.ndarray, np.ndarray, np.ndarray]]\n        Train, validation, and test indices.\n    \"\"\"\n    partition_mask = np.zeros(len(folds), dtype=int)\n    partition_mask[0:2] = (2, 1)\n    folds_arr = np.asarray(folds, dtype=object)\n\n    partitions_indices = []\n    for i in range(len(folds)):\n        curr_pmask = np.roll(partition_mask, i)\n        train_indices = np.concatenate(folds_arr[curr_pmask == 0]).tolist()\n        val_indices = np.concatenate(folds_arr[curr_pmask == 1]).tolist()\n        test_indices = np.concatenate(folds_arr[curr_pmask == 2]).tolist()\n\n        partitions_indices.append((train_indices, val_indices, test_indices))\n    return partitions_indices\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.partition_sets","title":"<code>partition_sets(partitions_indices, nsv_trial_segs, bv_trial_segs)</code>","text":"<p>Partition neural state vectors and behavioral variables into train, validation, and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>partitions_indices</code> <code>List[Tuple[ndarray, ndarray, ndarray]]</code> <p>List of tuples containing indices of divided trials into train, validation, and test sets.</p> required <code>nsv_trial_segs</code> <code>Union[ndarray, DataFrame]</code> <p>Neural state vectors for each trial. Shape: [n_trials, n_timepoints, n_neurons] or [n_timepoints, n_neurons]</p> required <code>bv_trial_segs</code> <code>Union[ndarray, DataFrame]</code> <p>Behavioral variables for each trial. Shape: [n_trials, n_timepoints, n_bvars] or [n_timepoints, n_bvars]</p> required <p>Returns:</p> Type Description <code>List[Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray]]</code> <p>List of tuples containing train, validation, and test sets for neural state vectors and behavioral variables.</p> Source code in <code>neuro_py/ensemble/decoding/preprocess.py</code> <pre><code>def partition_sets(\n    partitions_indices: List[Tuple[np.ndarray, np.ndarray, np.ndarray]],\n    nsv_trial_segs: Union[np.ndarray, pd.DataFrame],\n    bv_trial_segs: Union[np.ndarray, pd.DataFrame]\n) -&gt; List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"\n    Partition neural state vectors and behavioral variables into train,\n    validation, and test sets.\n\n    Parameters\n    ----------\n    partitions_indices : List[Tuple[np.ndarray, np.ndarray, np.ndarray]]\n        List of tuples containing indices of divided trials into train,\n        validation, and test sets.\n    nsv_trial_segs : Union[np.ndarray, pd.DataFrame]\n        Neural state vectors for each trial.\n        Shape: [n_trials, n_timepoints, n_neurons] or [n_timepoints, n_neurons]\n    bv_trial_segs : Union[np.ndarray, pd.DataFrame]\n        Behavioral variables for each trial.\n        Shape: [n_trials, n_timepoints, n_bvars] or [n_timepoints, n_bvars]\n\n    Returns\n    -------\n    List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]\n        List of tuples containing train, validation, and test sets for neural\n        state vectors and behavioral variables.\n    \"\"\"\n    partitions = []\n    is_2D = nsv_trial_segs[0].ndim == 1\n    for (train_indices, val_indices, test_indices) in partitions_indices:\n        if is_2D:\n            if isinstance(nsv_trial_segs, pd.DataFrame):\n                nsv_trial_segs = nsv_trial_segs.loc\n                bv_trial_segs = bv_trial_segs.loc\n            train = nsv_trial_segs[train_indices]\n            val = nsv_trial_segs[val_indices]\n            test = nsv_trial_segs[test_indices]\n            train_bv = bv_trial_segs[train_indices]\n            val_bv = bv_trial_segs[val_indices]\n            test_bv = bv_trial_segs[test_indices]\n        else:\n            train = np.take(nsv_trial_segs, train_indices, axis=0)\n            val = np.take(nsv_trial_segs, val_indices, axis=0)\n            test = np.take(nsv_trial_segs, test_indices, axis=0)\n            train_bv = np.take(bv_trial_segs, train_indices, axis=0)\n            val_bv = np.take(bv_trial_segs, val_indices, axis=0)\n            test_bv = np.take(bv_trial_segs, test_indices, axis=0)\n\n        partitions.append((train, train_bv, val, val_bv, test, test_bv))\n    return partitions\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.predict_models_folds","title":"<code>predict_models_folds(partitions, hyperparams, bv_models_folds, foldnormparams)</code>","text":"<p>Predict and evaluate models across multiple folds.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <code>List[Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray]]</code> <p>List of data partitions for each fold. Each partition contains: (nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test)</p> required <code>hyperparams</code> <code>Dict[str, Any]</code> <p>Dictionary of hyperparameters for the models.</p> required <code>bv_models_folds</code> <code>List[Any]</code> <p>List of trained models for each fold.</p> required <code>foldnormparams</code> <code>List[Dict[str, Any]]</code> <p>List of normalization parameters for each fold.</p> required <p>Returns:</p> Type Description <code>Tuple[List[NDArray], Dict[str, List[float]]]</code> <p>A tuple containing: - List of predictions for each fold - Dictionary of evaluation metrics for each fold</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def predict_models_folds(\n        partitions: List[Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray]],\n        hyperparams: Dict[str, Any],\n        bv_models_folds: List[Any],\n        foldnormparams: List[Dict[str, Any]]\n    ) -&gt; Tuple[List[NDArray], Dict[str, List[float]]]:\n    \"\"\"\n    Predict and evaluate models across multiple folds.\n\n    Parameters\n    ----------\n    partitions : List[Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray]]\n        List of data partitions for each fold. Each partition contains:\n        (nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test)\n    hyperparams : Dict[str, Any]\n        Dictionary of hyperparameters for the models.\n    bv_models_folds : List[Any]\n        List of trained models for each fold.\n    foldnormparams : List[Dict[str, Any]]\n        List of normalization parameters for each fold.\n\n    Returns\n    -------\n    Tuple[List[NDArray], Dict[str, List[float]]]\n        A tuple containing:\n        - List of predictions for each fold\n        - Dictionary of evaluation metrics for each fold\n    \"\"\"\n    ohe = sklearn.preprocessing.OneHotEncoder()\n    bv_preds_folds = []\n    metrics_folds = dict()\n    for i, (nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test) in enumerate(partitions):\n        preprocessed_data = preprocess_data(hyperparams, ohe, nsv_train, nsv_val, nsv_test, bv_train, bv_val, bv_test, foldnormparams[i])\n        (X_train, y_train, X_val, y_val, X_test, y_test), (train_loader, val_loader, test_loader), fold_norm_params = preprocessed_data\n        model = bv_models_folds[i]\n\n        model.eval()\n        predictor = model if hyperparams['model'] != 'LSTM' else model.predict\n        metrics, bv_preds_fold = evaluate_model(hyperparams, ohe, predictor, X_test, y_test)\n        bv_preds_folds.append(bv_preds_fold)\n        if hyperparams['model_args']['args']['clf']:\n            if 'accuracy' not in metrics_folds:\n                metrics_folds['accuracy'] = []\n            metrics_folds['accuracy'].append(metrics['accuracy'])\n        else:\n            coeff_determination = metrics['coeff_determination']\n            rmse = metrics['rmse']\n            if 'coeff_determination' not in metrics_folds:\n                metrics_folds['coeff_determination'] = []\n                metrics_folds['rmse'] = []\n            metrics_folds['coeff_determination'].append(coeff_determination)\n            metrics_folds['rmse'].append(rmse)\n\n    return bv_preds_folds, metrics_folds\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.preprocess_data","title":"<code>preprocess_data(hyperparams, ohe, nsv_train, nsv_val, nsv_test, bv_train, bv_val, bv_test, foldnormparams=None)</code>","text":"<p>Preprocess the data for model training and evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparams</code> <code>Dict[str, Any]</code> <p>Dictionary containing hyperparameters.</p> required <code>ohe</code> <code>OneHotEncoder</code> <p>One-hot encoder for categorical variables.</p> required <code>nsv_train</code> <code>NDArray</code> <p>Neural state vectors for training.</p> required <code>nsv_val</code> <code>NDArray</code> <p>Neural state vectors for validation.</p> required <code>nsv_test</code> <code>NDArray</code> <p>Neural state vectors for testing.</p> required <code>bv_train</code> <code>NDArray</code> <p>Behavioral state vectors for training.</p> required <code>bv_val</code> <code>NDArray</code> <p>Behavioral state vectors for validation.</p> required <code>bv_test</code> <code>NDArray</code> <p>Behavioral state vectors for testing.</p> required <code>foldnormparams</code> <code>Optional[Dict[str, Any]]</code> <p>Normalization parameters for the current fold, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray], Tuple[DataLoader, DataLoader, DataLoader], Dict[str, Any]]</code> <p>Preprocessed data, data loaders, and normalization parameters.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def preprocess_data(\n        hyperparams: Dict[str, Any],\n        ohe: sklearn.preprocessing.OneHotEncoder,\n        nsv_train: NDArray,\n        nsv_val: NDArray,\n        nsv_test: NDArray,\n        bv_train: NDArray,\n        bv_val: NDArray,\n        bv_test: NDArray,\n        foldnormparams: Optional[Dict[str, Any]] = None\n    ) -&gt; Tuple[\n        Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray],\n        Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader],\n        Dict[str, Any]\n    ]:\n    \"\"\"\n    Preprocess the data for model training and evaluation.\n\n    Parameters\n    ----------\n    hyperparams : Dict[str, Any]\n        Dictionary containing hyperparameters.\n    ohe : OneHotEncoder\n        One-hot encoder for categorical variables.\n    nsv_train : NDArray\n        Neural state vectors for training.\n    nsv_val : NDArray\n        Neural state vectors for validation.\n    nsv_test : NDArray\n        Neural state vectors for testing.\n    bv_train : NDArray\n        Behavioral state vectors for training.\n    bv_val : NDArray\n        Behavioral state vectors for validation.\n    bv_test : NDArray\n        Behavioral state vectors for testing.\n    foldnormparams : Optional[Dict[str, Any]], optional\n        Normalization parameters for the current fold, by default None.\n\n    Returns\n    -------\n    Tuple[Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray], Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader], Dict[str, Any]]\n        Preprocessed data, data loaders, and normalization parameters.\n    \"\"\"\n    bins_before = hyperparams['bins_before']\n    bins_current = hyperparams['bins_current']\n    bins_after = hyperparams['bins_after']\n    is_2D = nsv_train[0].ndim == 1\n    if hyperparams['model'] not in ('M2MLSTM', 'NDT'):\n        (\n            X_cov_train, X_flat_train, y_train,\n            ((X_cov_val, X_flat_val, y_val), (X_cov_test, X_flat_test, y_test)),\n            fold_norm_params\n        ) = normalize_format_trial_segs(\n            nsv_train, (nsv_val, nsv_test),\n            bv_train, (bv_val, bv_test), predict_bv=hyperparams['behaviors'],\n            bins_before=bins_before, bins_current=bins_current,\n            bins_after=bins_after, normparams=foldnormparams)\n        X_train = X_cov_train if hyperparams['model'] == 'LSTM' else X_flat_train\n        X_val = X_cov_val if hyperparams['model'] == 'LSTM' else X_flat_val\n        X_test = X_cov_test if hyperparams['model'] == 'LSTM' else X_flat_test\n\n        if hyperparams['model_args']['args']['clf']:\n            (y_train, y_val, y_test), n_classes = normalize_labels(y_train, y_val, y_test)\n            y_train = ohe.fit_transform(y_train).toarray()\n            y_val = ohe.transform(y_val).toarray()\n            y_test = ohe.transform(y_test).toarray()\n            hyperparams['model_args']['out_dim'] = n_classes\n            fold_norm_params['ohe'] = ohe\n\n        train_loader, val_loader, test_loader = minibatchify(\n            X_train, y_train, X_val, y_val, X_test, y_test,\n            seed=hyperparams['seed'], batch_size=hyperparams['batch_size'],\n            num_workers=hyperparams['num_workers'], modeltype=hyperparams['model'])\n        hyperparams['model_args']['in_dim'] = X_train.shape[-1]\n    else:\n        if is_2D:\n            nsv_train, bv_train = [nsv_train], [bv_train]\n            nsv_val, bv_val = [nsv_val], [bv_val]\n            nsv_test, bv_test = [nsv_test], [bv_test]\n        if type(bv_train[0]) is pd.DataFrame:\n            y_train = [y.values[:, hyperparams['behaviors']] for y in bv_train]\n        else:\n            y_train = [y[:, hyperparams['behaviors']] for y in bv_train]\n        nbins_per_tseg = [len(y) for y in y_train]  # number of time bins in each trial\n        tseg_bounds_train = np.cumsum([0] + nbins_per_tseg)\n        if type(bv_val[0]) is pd.DataFrame:\n            y_val = [y.values[:, hyperparams['behaviors']] for y in bv_val]\n        else:\n            y_val = [y[:, hyperparams['behaviors']] for y in bv_val]\n        nbins_per_tseg = [len(y) for y in y_val]\n        tseg_bounds_val = np.cumsum([0] + nbins_per_tseg)\n        if type(bv_test[0]) is pd.DataFrame:\n            y_test = [y.values[:, hyperparams['behaviors']] for y in bv_test]\n        else:\n            y_test = [y[:, hyperparams['behaviors']] for y in bv_test]\n        nbins_per_tseg = [len(y) for y in y_test]\n        tseg_bounds_test = np.cumsum([0] + nbins_per_tseg)\n\n        (\n            _, X_flat_train, y_train,\n            ((_, X_flat_val, y_val), (_, X_flat_test, y_test)),\n            fold_norm_params\n        ) = normalize_format_trial_segs(\n            nsv_train, (nsv_val, nsv_test),\n            bv_train, (bv_val, bv_test, bv_test), predict_bv=hyperparams['behaviors'],\n            bins_before=bins_before, bins_current=bins_current,\n            bins_after=bins_after, normparams=foldnormparams)\n        X_train = X_flat_train\n        X_val = X_flat_val\n        X_test = X_flat_test\n\n        if hyperparams['model_args']['args']['clf']:\n            (y_train, y_val, y_test), n_classes = normalize_labels(y_train, y_val, y_test)\n            y_train = ohe.fit_transform(y_train).toarray()\n            y_val = ohe.transform(y_val).toarray()\n            y_test = ohe.transform(y_test).toarray()\n            hyperparams['model_args']['out_dim'] = n_classes\n            fold_norm_params['ohe'] = ohe\n\n        X_train_tsegs, y_train_tsegs = [], []\n        X_val_tsegs, y_val_tsegs = [], []\n        X_test_tsegs, y_test_tsegs = [], []\n        for i in range(1, len(tseg_bounds_train)):\n            X_train_tsegs.append(X_train[tseg_bounds_train[i-1]:tseg_bounds_train[i]])\n            y_train_tsegs.append(y_train[tseg_bounds_train[i-1]:tseg_bounds_train[i]])\n        for i in range(1, len(tseg_bounds_val)):\n            X_val_tsegs.append(X_val[tseg_bounds_val[i-1]:tseg_bounds_val[i]])\n            y_val_tsegs.append(y_val[tseg_bounds_val[i-1]:tseg_bounds_val[i]])\n        for i in range(1, len(tseg_bounds_test)):\n            X_test_tsegs.append(X_test[tseg_bounds_test[i-1]:tseg_bounds_test[i]])\n            y_test_tsegs.append(y_test[tseg_bounds_test[i-1]:tseg_bounds_test[i]])\n\n        X_train, y_train = X_train_tsegs, y_train_tsegs\n        X_val, y_val = X_val_tsegs, y_val_tsegs\n        X_test, y_test = X_test_tsegs, y_test_tsegs\n\n        train_dataset = NSVDataset(X_train, y_train)\n        val_dataset = NSVDataset(X_val, y_val)\n        test_dataset = NSVDataset(X_test, y_test)\n\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset, shuffle=True, num_workers=hyperparams['num_workers'],\n            batch_size=1\n        )\n        val_loader = torch.utils.data.DataLoader(\n            val_dataset, shuffle=False, num_workers=hyperparams['num_workers'],\n            batch_size=1\n        )\n        test_loader = torch.utils.data.DataLoader(\n            test_dataset, shuffle=False, num_workers=hyperparams['num_workers'],\n            batch_size=1\n        )\n        hyperparams['model_args']['in_dim'] = X_train[0].shape[-1]\n\n    return (X_train, y_train, X_val, y_val, X_test, y_test), (train_loader, val_loader, test_loader), fold_norm_params\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.seed_worker","title":"<code>seed_worker(worker_id)</code>","text":"<p>Seed a worker with the given ID for reproducibility in data loading.</p> <p>Parameters:</p> Name Type Description Default <code>worker_id</code> <code>int</code> <p>The ID of the worker to be seeded.</p> required Notes <p>This function is used to ensure reproducibility when using multi-process data loading.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def seed_worker(worker_id: int) -&gt; None:\n    \"\"\"\n    Seed a worker with the given ID for reproducibility in data loading.\n\n    Parameters\n    ----------\n    worker_id : int\n        The ID of the worker to be seeded.\n\n    Notes\n    -----\n    This function is used to ensure reproducibility when using multi-process data loading.\n    \"\"\"\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.shuffle_nsv_intrialsegs","title":"<code>shuffle_nsv_intrialsegs(nsv_trialsegs)</code>","text":"<p>Shuffle neural state variables within trial segments.</p> <p>Parameters:</p> Name Type Description Default <code>nsv_trialsegs</code> <code>List[DataFrame]</code> <p>List of neural state variable trial segments.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Shuffled neural state variables.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def shuffle_nsv_intrialsegs(nsv_trialsegs: List[pd.DataFrame]) -&gt; NDArray:\n    \"\"\"\n    Shuffle neural state variables within trial segments.\n\n    Parameters\n    ----------\n    nsv_trialsegs : List[pd.DataFrame]\n        List of neural state variable trial segments.\n\n    Returns\n    -------\n    NDArray\n        Shuffled neural state variables.\n    \"\"\"\n    nsv_shuffled_intrialsegs = []\n    for nsv_tseg in nsv_trialsegs:\n        # shuffle the data\n        nsv_shuffled_intrialsegs.append(\n            nsv_tseg.sample(frac=1).reset_index(drop=True)\n        )\n    return np.asarray(nsv_shuffled_intrialsegs, dtype=object)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.split_data","title":"<code>split_data(trial_nsvs, splitby, trainsize=0.8, seed=0)</code>","text":"<p>Split data into stratified folds.</p> <p>Parameters:</p> Name Type Description Default <code>trial_nsvs</code> <code>ndarray</code> <p>Neural state vectors for trials.</p> required <code>splitby</code> <code>ndarray</code> <p>Labels for stratification.</p> required <code>trainsize</code> <code>float</code> <p>Proportion of data to use for training, by default 0.8</p> <code>0.8</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility, by default 0</p> <code>0</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List of indices for each fold.</p> Source code in <code>neuro_py/ensemble/decoding/preprocess.py</code> <pre><code>def split_data(trial_nsvs: np.ndarray, splitby: np.ndarray, trainsize: float = 0.8, seed: int = 0) -&gt; List[np.ndarray]:\n    \"\"\"\n    Split data into stratified folds.\n\n    Parameters\n    ----------\n    trial_nsvs : np.ndarray\n        Neural state vectors for trials.\n    splitby : np.ndarray\n        Labels for stratification.\n    trainsize : float, optional\n        Proportion of data to use for training, by default 0.8\n    seed : int, optional\n        Random seed for reproducibility, by default 0\n\n    Returns\n    -------\n    List[np.ndarray]\n        List of indices for each fold.\n    \"\"\"\n    n_splits = int(np.round(1 / ((1 - trainsize) / 2)))\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    folds = [fold_indices for _, fold_indices in skf.split(trial_nsvs, splitby)]\n    return folds\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.train_model","title":"<code>train_model(partitions, hyperparams, resultspath=None, stop_partition=None)</code>","text":"<p>Train a DNN model on the given data partitions with in-built caching &amp; checkpointing.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <code>List[Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray]]</code> <p>K-fold partitions of the data with the following format: [(nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test), ...] Each element of the list is a tuple of numpy arrays containing the with pairs of neural state vectors and behavioral variables for the training, validation, and test sets. Each array has the shape (ntrials, nbins, nfeats) where nfeats is the number of neurons for the neural state vectors and number of behavioral features to be predicted for the behavioral variables.</p> required <code>hyperparams</code> <code>Dict[str, Any]</code> <p>Dictionary containing the hyperparameters for the model training.</p> required <code>resultspath</code> <code>Optional[str]</code> <p>Path to the directory where the trained models and logs will be saved.</p> <code>None</code> <code>stop_partition</code> <code>Optional[int]</code> <p>Index of the partition to stop training at. Only useful for debugging, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing the predicted behavioral variables for each fold, the trained models for each fold, the normalization parameters for each fold, and the evaluation metrics for each fold.</p> Notes <p>The hyperparameters dictionary should contain the following keys: - <code>model</code>: str, the type of the model to be trained. Multi-layer     Perceptron (MLP), Long Short-Term Memory (LSTM), many-to-many LSTM     (M2MLSTM), Transformer (NDT). - <code>model_args</code>: dict, the arguments to be passed to the model constructor.     The arguments should be in the format expected by the model constructor.     - <code>in_dim</code>: The number of input features.     - <code>out_dim</code>: The number of output features.     - <code>hidden_dims</code>: The number of hidden units each hidden layer of the         model. Can also take float values to specify the dropout rate.         - For LSTM and M2MLSTM, it should be a tuple of the hidden size,             the number of layers, and the dropout rate.             If the model is an MLP, it should be a list of hidden layer             sizes which can also take float values to specify the dropout             rate.         - If the model is an LSTM or M2MLSTM, it should be a list of the         hidden layer size, the number of layers, and the dropout rate.         - If the model is an NDT, it should be a list of the hidden layer             size, the number of layers, the number of attention heads, the             dropout rate for the encoder layer, and the dropout rate applied             before the decoder layer.     - <code>max_context_len</code>: The maximum context length for the transformer         model. Only used if the model is an NDT.     - <code>args</code>:         - <code>clf</code>: If True, the model is a classifier; otherwise, it is a             regressor.         - <code>activations</code>: The activation functions for each layer.         - <code>criterion</code>: The loss function to optimize.         - <code>epochs</code>: The number of complete passes through the training             dataset.         - <code>lr</code>: Controls how much to change the model in response to the             estimated error each time the model weights are updated. A             smaller value ensures stable convergence but may slow down             training, while a larger value speeds up training but risks             overshooting.         - <code>base_lr</code>: The initial learning rate for the learning rate             scheduler.         - <code>max_grad_norm</code>: The maximum norm of the gradients.         - <code>iters_to_accumulate</code>: The number of iterations to accumulate             gradients.         - <code>weight_decay</code>: The L2 regularization strength.         - <code>num_training_batches</code>: The number of training batches. If             None, the number of batches is calculated based on the batch             size and the length of the training data.         - <code>scheduler_step_size_multiplier</code>: The multiplier for the             learning rate scheduler step size. Higher values lead to             faster learning rate decay. - <code>bins_before</code>: int, the number of bins before the current bin to     include in the input data. - <code>bins_current</code>: int, the number of bins in the current time bin to     include in the input data. - <code>bins_after</code>: int, the number of bins after the current bin to include     in the input data. - <code>behaviors</code>: list, the indices of the columns of behavioral features     to be predicted. Selected behavioral variable must have homogenous     data types across all features (continuous for regression and     categorical for classification) - <code>batch_size</code>: int, the number of training examples utilized in one     iteration. Larger batch sizes offer stable gradient estimates but     require more memory, while smaller batches introduce noise that can     help escape local minima.     - When using M2MLSTM or NDT and input trials are of inconsistents         lengths, the batch size should be set to 1.     - M2MLSTM does not support batch_size != 1. - <code>num_workers</code>: int, The number of parallel processes to use for data     loading. Increasing the number of workers can speed up data loading     but may lead to memory issues. Too many workers can also slow down     the training process due to contention for resources. - <code>device</code>: str, the device to use for training. Should be 'cuda' or     'cpu'. - <code>seed</code>: int, the random seed for reproducibility.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def train_model(\n    partitions: List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]],\n    hyperparams: Dict[str, Any],\n    resultspath: Optional[str] = None,\n    stop_partition: Optional[int] = None\n) -&gt; Tuple[List[np.ndarray], List[Any], List[Dict[str, Any]], Dict[str, List[float]]]:\n    \"\"\"\n    Train a DNN model on the given data partitions with in-built caching &amp; checkpointing.\n\n    Parameters\n    ----------\n    partitions : List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]\n        K-fold partitions of the data with the following format:\n        [(nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test), ...]\n        Each element of the list is a tuple of numpy arrays containing the with\n        pairs of neural state vectors and behavioral variables for the training,\n        validation, and test sets. Each array has the shape\n        (ntrials, nbins, nfeats) where nfeats is the number of neurons for the\n        neural state vectors and number of behavioral features to be predicted\n        for the behavioral variables.\n    hyperparams : Dict[str, Any]\n        Dictionary containing the hyperparameters for the model training.\n    resultspath : Optional[str], default=None\n        Path to the directory where the trained models and logs will be saved.\n    stop_partition : Optional[int], default=None\n        Index of the partition to stop training at. Only useful for debugging,\n        by default None\n\n    Returns\n    -------\n    tuple\n        Tuple containing the predicted behavioral variables for each fold,\n        the trained models for each fold, the normalization parameters for each\n        fold, and the evaluation metrics for each fold.\n\n    Notes\n    -----\n    The hyperparameters dictionary should contain the following keys:\n    - `model`: str, the type of the model to be trained. Multi-layer\n        Perceptron (MLP), Long Short-Term Memory (LSTM), many-to-many LSTM\n        (M2MLSTM), Transformer (NDT).\n    - `model_args`: dict, the arguments to be passed to the model constructor.\n        The arguments should be in the format expected by the model constructor.\n        - `in_dim`: The number of input features.\n        - `out_dim`: The number of output features.\n        - `hidden_dims`: The number of hidden units each hidden layer of the\n            model. Can also take float values to specify the dropout rate.\n            - For LSTM and M2MLSTM, it should be a tuple of the hidden size,\n                the number of layers, and the dropout rate.\n                If the model is an MLP, it should be a list of hidden layer\n                sizes which can also take float values to specify the dropout\n                rate.\n            - If the model is an LSTM or M2MLSTM, it should be a list of the\n            hidden layer size, the number of layers, and the dropout rate.\n            - If the model is an NDT, it should be a list of the hidden layer\n                size, the number of layers, the number of attention heads, the\n                dropout rate for the encoder layer, and the dropout rate applied\n                before the decoder layer.\n        - `max_context_len`: The maximum context length for the transformer\n            model. Only used if the model is an NDT.\n        - `args`:\n            - `clf`: If True, the model is a classifier; otherwise, it is a\n                regressor.\n            - `activations`: The activation functions for each layer.\n            - `criterion`: The loss function to optimize.\n            - `epochs`: The number of complete passes through the training\n                dataset.\n            - `lr`: Controls how much to change the model in response to the\n                estimated error each time the model weights are updated. A\n                smaller value ensures stable convergence but may slow down\n                training, while a larger value speeds up training but risks\n                overshooting.\n            - `base_lr`: The initial learning rate for the learning rate\n                scheduler.\n            - `max_grad_norm`: The maximum norm of the gradients.\n            - `iters_to_accumulate`: The number of iterations to accumulate\n                gradients.\n            - `weight_decay`: The L2 regularization strength.\n            - `num_training_batches`: The number of training batches. If\n                None, the number of batches is calculated based on the batch\n                size and the length of the training data.\n            - `scheduler_step_size_multiplier`: The multiplier for the\n                learning rate scheduler step size. Higher values lead to\n                faster learning rate decay.\n    - `bins_before`: int, the number of bins before the current bin to\n        include in the input data.\n    - `bins_current`: int, the number of bins in the current time bin to\n        include in the input data.\n    - `bins_after`: int, the number of bins after the current bin to include\n        in the input data.\n    - `behaviors`: list, the indices of the columns of behavioral features\n        to be predicted. Selected behavioral variable must have homogenous\n        data types across all features (continuous for regression and\n        categorical for classification)\n    - `batch_size`: int, the number of training examples utilized in one\n        iteration. Larger batch sizes offer stable gradient estimates but\n        require more memory, while smaller batches introduce noise that can\n        help escape local minima.\n        - When using M2MLSTM or NDT and input trials are of inconsistents\n            lengths, the batch size should be set to 1.\n        - M2MLSTM does not support batch_size != 1.\n    - `num_workers`: int, The number of parallel processes to use for data\n        loading. Increasing the number of workers can speed up data loading\n        but may lead to memory issues. Too many workers can also slow down\n        the training process due to contention for resources.\n    - `device`: str, the device to use for training. Should be 'cuda' or\n        'cpu'.\n    - `seed`: int, the random seed for reproducibility.\n    \"\"\"\n    ohe = sklearn.preprocessing.OneHotEncoder()\n    bv_preds_folds = []\n    bv_models_folds = []\n    norm_params_folds = []\n    metrics_folds = dict() # dict with keys 'accuracy', 'coeff_determination', 'rmse' and values of length number of folds\n    for i, (nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test) in enumerate(partitions):\n        # shuffle nsv bins in between tsegs to generate baseline distribution for vector dev plots\n        preprocessed_data = preprocess_data(hyperparams, ohe, nsv_train, nsv_val, nsv_test, bv_train, bv_val, bv_test)\n        (X_train, y_train, X_val, y_val, X_test, y_test), (train_loader, val_loader, test_loader), fold_norm_params = preprocessed_data\n        hyperparams['model_args']['args']['num_training_batches'] = len(train_loader)\n\n        decoder, model = create_model(hyperparams)\n\n        hyperparams_cp = copy.deepcopy(hyperparams)\n        del hyperparams_cp['model_args']['args']['epochs']\n        del hyperparams_cp['model_args']['args']['num_training_batches']\n        model_cache_name = zlib.crc32(str(hyperparams_cp).encode('utf-8'))\n        best_ckpt_path = None\n        if resultspath is not None:\n            model_cache_path = os.path.join(resultspath, 'models', str(model_cache_name))\n            best_ckpt_name_file = os.path.join(model_cache_path, f'{i}-best_model.txt')\n            if os.path.exists(best_ckpt_name_file):\n                with open(best_ckpt_name_file, 'r') as f:\n                    best_ckpt_path = f.read()\n\n        lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n        callbacks = [lr_monitor]\n        if resultspath is not None:\n            checkpoint_callback = pl.callbacks.ModelCheckpoint(\n                save_top_k=1,\n                monitor='val_loss',\n                dirpath=model_cache_path,\n                filename=f'{i}' + '-{epoch:02d}-{val_loss:.2f}',\n            )\n            callbacks.append(checkpoint_callback)\n        logger = pl.loggers.TensorBoardLogger(\n            save_dir='logs',\n            name=f'{model_cache_name}-{i}',\n        )\n        pl.seed_everything(hyperparams['seed'], workers=True)\n        trainer = pl.Trainer(\n            accelerator=hyperparams['device'], devices=1,\n            max_epochs=hyperparams['model_args']['args']['epochs'],\n            gradient_clip_val=hyperparams['model_args']['args']['max_grad_norm'],\n            accumulate_grad_batches=hyperparams['model_args']['args']['iters_to_accumulate'],\n            logger=logger,\n            callbacks=callbacks,\n            enable_progress_bar=False,\n            log_every_n_steps=5,\n            reload_dataloaders_every_n_epochs=1\n        )\n        trainer.fit(\n            model, train_loader, val_loader,\n            ckpt_path=best_ckpt_path\n        )\n        if resultspath is not None:\n            with open(best_ckpt_name_file, 'w') as f:\n                f.write(checkpoint_callback.best_model_path)\n        model.eval()\n        trainer.test(model, test_loader)\n        predictor = model if hyperparams['model'] != 'LSTM' else model.predict\n\n        metrics, bv_preds_fold = evaluate_model(hyperparams, ohe, predictor, X_test, y_test)\n        bv_preds_folds.append(bv_preds_fold)\n        bv_models_folds.append(model)\n        norm_params_folds.append(copy.deepcopy(fold_norm_params))\n        if hyperparams['model_args']['args']['clf']:\n            print('Accuracy:', metrics['accuracy'])\n            if 'accuracy' not in metrics_folds:\n                metrics_folds['accuracy'] = []\n            metrics_folds['accuracy'].append(metrics['accuracy'])\n        else:\n            coeff_determination = metrics['coeff_determination']\n            rmse = metrics['rmse']\n            print('Variance weighed avg. coefficient of determination:',\n                coeff_determination)\n            print('RMSE:', rmse)\n\n            if 'coeff_determination' not in metrics_folds:\n                metrics_folds['coeff_determination'] = []\n                metrics_folds['rmse'] = []\n            metrics_folds['coeff_determination'].append(coeff_determination)\n            metrics_folds['rmse'].append(rmse)\n\n        if stop_partition is not None and i == stop_partition:\n            break\n    return bv_preds_folds, bv_models_folds, norm_params_folds, metrics_folds\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/#neuro_py.ensemble.decoding.zscore_trial_segs","title":"<code>zscore_trial_segs(train, rest_feats=None, normparams=None)</code>","text":"<p>Z-score trial segments.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>NDArray</code> <p>Training data.</p> required <code>rest_feats</code> <code>Optional[List[NDArray]]</code> <p>Rest features, by default None.</p> <code>None</code> <code>normparams</code> <code>Optional[Dict[str, Any]]</code> <p>Normalization parameters, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[NDArray, List[NDArray], Dict[str, Any]]</code> <p>Normalized train data, normalized rest features, and normalization parameters.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def zscore_trial_segs(\n    train: NDArray,\n    rest_feats: Optional[List[NDArray]] = None,\n    normparams: Optional[Dict[str, Any]] = None\n) -&gt; Tuple[NDArray, List[NDArray], Dict[str, Any]]:\n    \"\"\"\n    Z-score trial segments.\n\n    Parameters\n    ----------\n    train : NDArray\n        Training data.\n    rest_feats : Optional[List[NDArray]], optional\n        Rest features, by default None.\n    normparams : Optional[Dict[str, Any]], optional\n        Normalization parameters, by default None.\n\n    Returns\n    -------\n    Tuple[NDArray, List[NDArray], Dict[str, Any]]\n        Normalized train data, normalized rest features, and normalization parameters.\n    \"\"\"\n    is_2D = train[0].ndim == 1\n    concat_train = train if is_2D else np.concatenate(train).astype(float)\n    train_mean = (\n        normparams['X_train_mean'] if normparams is not None\n        else bn.nanmean(concat_train, axis=0)\n    )\n    train_std = normparams['X_train_std'] if normparams is not None else bn.nanstd(concat_train, axis=0)\n\n    train_notnan_cols = train_std != 0\n    train_nan_cols = ~train_notnan_cols\n    if is_2D:\n        normed_train = np.divide(train-train_mean, train_std, where=train_notnan_cols)\n        # if train is not jagged, it gets converted completely to object\n        # np.ndarray. Hence, cannot exclusively use normed_train.loc\n        if isinstance(normed_train, pd.DataFrame):\n            normed_train.loc[:, train_nan_cols] = 0\n        else:\n            normed_train[:, train_nan_cols] = 0\n    else:\n        normed_train = np.empty_like(train)\n        for i, nsvstseg in enumerate(train):\n            zscored = np.divide(nsvstseg-train_mean, train_std, where=train_notnan_cols)\n            if isinstance(zscored, pd.DataFrame):\n                zscored.loc[:, train_nan_cols] = 0\n            else:\n                zscored[:, train_nan_cols] = 0\n            normed_train[i] = zscored\n\n    normed_rest_feats = []\n    if rest_feats is not None:\n        for feats in rest_feats:\n            if is_2D:\n                normed_feats = np.divide(feats-train_mean, train_std, where=train_notnan_cols)\n                if isinstance(normed_feats, pd.DataFrame):\n                    normed_feats.loc[:, train_nan_cols] = 0\n                else:\n                    normed_feats[:, train_nan_cols] = 0\n                normed_rest_feats.append(normed_feats)\n            else:\n                normed_feats = np.empty_like(feats)\n                for i, trialSegROI in enumerate(feats):\n                    zscored = np.divide(feats[i]-train_mean, train_std, where=train_notnan_cols)\n                    if isinstance(zscored, pd.DataFrame):\n                        zscored.loc[:, train_nan_cols] = 0\n                    else:\n                        zscored[:, train_nan_cols] = 0\n                    normed_feats[i] = zscored\n                normed_rest_feats.append(normed_feats)\n\n    return normed_train, normed_rest_feats, dict(\n        X_train_mean=train_mean, X_train_std=train_std,\n        X_train_notnan_mask=train_notnan_cols,\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/bayesian/","title":"neuro_py.ensemble.decoding.bayesian","text":""},{"location":"reference/neuro_py/ensemble/decoding/bayesian/#neuro_py.ensemble.decoding.bayesian.decode","title":"<code>decode(ct, tc, occupancy, bin_size_s, uniform_prior=False)</code>","text":"<p>Decode position from spike counts in an N-dimensional spatial environment</p> <p>Parameters:</p> Name Type Description Default <code>ct</code> <code>ndarray</code> <p>2D array, spike counts matrix with shape (n_bins, n_cells)</p> required <code>tc</code> <code>ndarray</code> <p>ND array, ratemap matrix with shape (n_xbins, n_ybins, ..., n_cells)</p> required <code>occupancy</code> <code>ndarray</code> <p>(N-1)D array, occupancy matrix with shape (n_xbins, n_ybins, ...)</p> required <code>bin_size_s</code> <code>float</code> <p>float, width of each time bin in seconds</p> required <code>uniform_prior</code> <code>bool</code> <p>bool, whether to use uniform prior, by default False</p> <code>False</code> <p>Returns:</p> Name Type Description <code>p</code> <code>ndarray</code> <p>(N+1)D array, decoded position probabilities matrix with shape (n_bins, n_xbins, n_ybins, ...)</p> <p>Examples:</p>"},{"location":"reference/neuro_py/ensemble/decoding/bayesian/#neuro_py.ensemble.decoding.bayesian.decode--1d-example","title":"1D example","text":"<pre><code>&gt;&gt;&gt; ct = np.random.rand(10, 5)\n&gt;&gt;&gt; tc = np.random.rand(3, 5)\n&gt;&gt;&gt; occupancy = np.random.rand(3)\n&gt;&gt;&gt; bin_size_s = 0.1\n&gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/bayesian/#neuro_py.ensemble.decoding.bayesian.decode--2d-example","title":"2D example","text":"<pre><code>&gt;&gt;&gt; ct = np.random.rand(10, 5)\n&gt;&gt;&gt; tc = np.random.rand(3, 3, 5)\n&gt;&gt;&gt; occupancy = np.random.rand(3, 3)\n&gt;&gt;&gt; bin_size_s = 0.1\n&gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/bayesian/#neuro_py.ensemble.decoding.bayesian.decode--3d-example","title":"3D example","text":"<pre><code>&gt;&gt;&gt; ct = np.random.rand(10, 5)\n&gt;&gt;&gt; tc = np.random.rand(3, 3, 3, 5)\n&gt;&gt;&gt; occupancy = np.random.rand(3, 3, 3)\n&gt;&gt;&gt; bin_size_s = 0.1\n&gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n</code></pre> Source code in <code>neuro_py/ensemble/decoding/bayesian.py</code> <pre><code>@njit(parallel=True, fastmath=True)\ndef decode(\n    ct: np.ndarray,\n    tc: np.ndarray,\n    occupancy: np.ndarray,\n    bin_size_s: float,\n    uniform_prior: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Decode position from spike counts in an N-dimensional spatial environment\n\n    Parameters\n    ----------\n    ct : ndarray\n        2D array, spike counts matrix with shape (n_bins, n_cells)\n    tc : ndarray\n        ND array, ratemap matrix with shape (n_xbins, n_ybins, ..., n_cells)\n    occupancy : ndarray\n        (N-1)D array, occupancy matrix with shape (n_xbins, n_ybins, ...)\n    bin_size_s : float\n        float, width of each time bin in seconds\n    uniform_prior : bool, optional\n        bool, whether to use uniform prior, by default False\n\n    Returns\n    ----------\n    p : ndarray\n        (N+1)D array, decoded position probabilities matrix with shape (n_bins, n_xbins, n_ybins, ...)\n\n    Examples\n    ----------\n    # 1D example\n    &gt;&gt;&gt; ct = np.random.rand(10, 5)\n    &gt;&gt;&gt; tc = np.random.rand(3, 5)\n    &gt;&gt;&gt; occupancy = np.random.rand(3)\n    &gt;&gt;&gt; bin_size_s = 0.1\n    &gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n\n    # 2D example\n    &gt;&gt;&gt; ct = np.random.rand(10, 5)\n    &gt;&gt;&gt; tc = np.random.rand(3, 3, 5)\n    &gt;&gt;&gt; occupancy = np.random.rand(3, 3)\n    &gt;&gt;&gt; bin_size_s = 0.1\n    &gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n\n    # 3D example\n    &gt;&gt;&gt; ct = np.random.rand(10, 5)\n    &gt;&gt;&gt; tc = np.random.rand(3, 3, 3, 5)\n    &gt;&gt;&gt; occupancy = np.random.rand(3, 3, 3)\n    &gt;&gt;&gt; bin_size_s = 0.1\n    &gt;&gt;&gt; p = decode(ct, tc, occupancy, bin_size_s)\n    \"\"\"\n\n    # Ensure input arrays are contiguous for vectorization\n    ct = np.ascontiguousarray(ct)\n    tc = np.ascontiguousarray(tc)\n    occupancy = np.ascontiguousarray(occupancy)\n\n    # Validate input shapes\n    assert ct.ndim == 2, \"ct must be a 2D array with shape (n_bins, n_cells)\"\n    assert tc.ndim &gt;= 2, (\n        \"tc must be at least a 2D array with shape (n_xbins, ..., n_cells)\"\n    )\n    assert occupancy.ndim == tc.ndim - 1, (\n        \"occupancy must have one fewer dimension than tc\"\n    )\n    assert ct.shape[1] == tc.shape[-1], \"Number of cells in ct and tc must match\"\n\n    # Flatten spatial dimensions\n    n_cells = tc.shape[-1]\n    spatial_shape = tc.shape[:-1]  # Shape of spatial dimensions\n\n    # Calculate the total number of spatial bins\n    n_spatial_bins = 1\n    for dim in spatial_shape:\n        n_spatial_bins *= dim\n\n    tc_flat = tc.reshape(n_spatial_bins, n_cells)\n    occupancy_flat = occupancy.flatten()\n\n    if uniform_prior:\n        # Use uniform prior\n        occupancy_flat = np.ones_like(occupancy_flat)\n\n    # Precompute log values\n    log_tc_flat = np.log(tc_flat + 1e-10)  # add small value to avoid log(0)\n    log_p1 = -tc_flat.sum(axis=1) * bin_size_s\n    log_p2 = np.log(occupancy_flat / occupancy_flat.sum())\n\n    # Initialize the probability matrix\n    n_bins = ct.shape[0]\n    p = np.zeros((n_bins, n_spatial_bins))\n\n    # Vectorized calculation of log probabilities\n    for i in prange(n_bins):  # prange for parallel loop\n        log_likelihood = log_p1 + log_p2 + np.sum(log_tc_flat * ct[i, :], axis=1)\n        p[i, :] = np.exp(\n            log_likelihood - np.max(log_likelihood)\n        )  # Subtract max for numerical stability\n\n    # Normalize the probabilities along the spatial axis\n    p_sum = p.sum(axis=1)  # Sum over spatial bins\n    p = p / p_sum.reshape(-1, 1)  # Reshape p_sum to (n_bins, 1) for broadcasting\n\n    # Reshape the probabilities to the original spatial dimensions\n    p = p.reshape((n_bins,) + spatial_shape)\n\n    return p\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/lstm/","title":"neuro_py.ensemble.decoding.lstm","text":""},{"location":"reference/neuro_py/ensemble/decoding/lstm/#neuro_py.ensemble.decoding.lstm.LSTM","title":"<code>LSTM</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Long Short-Term Memory (LSTM) model.</p> <p>This class implements an LSTM model using PyTorch Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Dimensionality of input data, by default 100</p> <code>100</code> <code>out_dim</code> <code>int</code> <p>Dimensionality of output data, by default 2</p> <code>2</code> <code>hidden_dims</code> <code>Tuple[int, int, float]</code> <p>Architectural parameters of the model (hidden_size, num_layers, dropout), by default (400, 1, 0.0)</p> <code>(400, 1, 0.0)</code> <code>use_bias</code> <code>bool</code> <p>Whether to use bias or not in the final linear layer, by default True</p> <code>True</code> <code>args</code> <code>Dict</code> <p>Additional arguments for model configuration, by default {}</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>lstm</code> <code>LSTM</code> <p>LSTM layer</p> <code>fc</code> <code>Linear</code> <p>Fully connected layer</p> <code>hidden_state</code> <code>Optional[Tensor]</code> <p>Hidden state of the LSTM</p> <code>cell_state</code> <code>Optional[Tensor]</code> <p>Cell state of the LSTM</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>class LSTM(L.LightningModule):\n    \"\"\"\n    Long Short-Term Memory (LSTM) model.\n\n    This class implements an LSTM model using PyTorch Lightning.\n\n    Parameters\n    ----------\n    in_dim : int, optional\n        Dimensionality of input data, by default 100\n    out_dim : int, optional\n        Dimensionality of output data, by default 2\n    hidden_dims : Tuple[int, int, float], optional\n        Architectural parameters of the model (hidden_size, num_layers, dropout),\n        by default (400, 1, 0.0)\n    use_bias : bool, optional\n        Whether to use bias or not in the final linear layer, by default True\n    args : Dict, optional\n        Additional arguments for model configuration, by default {}\n\n    Attributes\n    ----------\n    lstm : nn.LSTM\n        LSTM layer\n    fc : nn.Linear\n        Fully connected layer\n    hidden_state : Optional[torch.Tensor]\n        Hidden state of the LSTM\n    cell_state : Optional[torch.Tensor]\n        Cell state of the LSTM\n    \"\"\"\n    def __init__(self, in_dim: int = 100, out_dim: int = 2,\n                 hidden_dims: Tuple[int, int, float] = (400, 1, 0.0),\n                 use_bias: bool = True, args: Dict = {}):\n        super().__init__()\n        self.save_hyperparameters()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        if len(hidden_dims) != 3:\n            raise ValueError('`hidden_dims` should be of size 3')\n        self.hidden_size, self.nlayers, self.dropout = hidden_dims\n        self.args = args\n\n        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=self.hidden_size,\n                            num_layers=self.nlayers, batch_first=True, \n                            dropout=self.dropout, bidirectional=True)\n        self.fc = nn.Linear(in_features=2*self.hidden_size, out_features=out_dim, bias=use_bias)\n        self.hidden_state: Optional[torch.Tensor] = None\n        self.cell_state: Optional[torch.Tensor] = None\n\n        self._init_params()\n\n    def _init_params(self) -&gt; None:\n        \"\"\"Initialize model parameters.\"\"\"\n        def init_params(m: nn.Module) -&gt; None:\n            if isinstance(m, nn.Linear):\n                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n                if m.bias is not None:\n                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                    bound = 1 / torch.math.sqrt(fan_in)\n                    nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n        init_params(self.fc)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the LSTM model.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape (batch_size, sequence_length, input_dim)\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor of shape (batch_size, output_dim)\n        \"\"\"\n        lstm_out, (self.hidden_state, self.cell_state) = \\\n            self.lstm(x, (self.hidden_state, self.cell_state))\n        lstm_out = lstm_out[:, -1, :].contiguous()\n        out = self.fc(lstm_out)\n        if self.args.get('clf', False):\n            out = F.log_softmax(out, dim=1)\n        return out\n\n    def init_hidden(self, batch_size: int) -&gt; None:\n        \"\"\"\n        Initialize hidden state and cell state.\n\n        Parameters\n        ----------\n        batch_size : int\n            Batch size for initialization\n        \"\"\"\n        self.batch_size = batch_size\n        h0 = torch.zeros(\n            (2*self.nlayers, batch_size, self.hidden_size),\n            requires_grad=False\n        )\n        c0 = torch.zeros(\n            (2*self.nlayers, batch_size, self.hidden_size),\n            requires_grad=False\n        )\n        self.hidden_state = h0\n        self.cell_state = c0\n\n    def predict(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Make predictions using the LSTM model.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor\n\n        Returns\n        -------\n        torch.Tensor\n            Predicted output\n        \"\"\"\n        self.hidden_state = self.hidden_state.to(x.device)\n        self.cell_state = self.cell_state.to(x.device)\n        preds = []\n        batch_size = self.batch_size\n        for i in range(batch_size, x.shape[0]+batch_size, batch_size):\n            iptensor = x[i-batch_size:i]\n            if i &gt; x.shape[0]:\n                iptensor = F.pad(iptensor, (0,0,0,0,0,i-x.shape[0]))\n            pred_loc = self.forward(iptensor)\n            if i &gt; x.shape[0]:\n                pred_loc = pred_loc[:batch_size-(i-x.shape[0])]\n            preds.extend(pred_loc)\n        out = torch.stack(preds)\n        if self.args.get('clf', False):\n            out = F.log_softmax(out, dim=1)\n        return out\n\n    def _step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Perform a single step (forward pass + loss calculation).\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        xs, ys = batch\n        outs = self(xs)\n        loss = self.args['criterion'](outs, ys)\n        return loss\n\n    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for training step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('train_loss', loss)\n        return loss\n\n    def on_after_backward(self) -&gt; None:\n        \"\"\"Lightning method called after backpropagation.\"\"\"\n        self.hidden_state.detach_()\n        self.cell_state.detach_()\n\n    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for validation step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('val_loss', loss)\n        return loss\n\n    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for test step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('test_loss', loss)\n        return loss\n\n    def configure_optimizers(self) -&gt; Tuple[List[torch.optim.Optimizer], List[Dict]]:\n        \"\"\"\n        Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        Tuple[List[torch.optim.Optimizer], List[Dict]]\n            Tuple containing a list of optimizers and a list of scheduler configurations\n        \"\"\"\n        optimizer = torch.optim.AdamW(\n            self.parameters(), weight_decay=self.args['weight_decay'])\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer, max_lr=self.args['lr'],\n            epochs=self.args['epochs'],\n            steps_per_epoch=len(\n                self.trainer._data_connector._train_dataloader_source.dataloader()\n            )\n        )\n        lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n        return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/lstm/#neuro_py.ensemble.decoding.lstm.LSTM._init_params","title":"<code>_init_params()</code>","text":"<p>Initialize model parameters.</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def _init_params(self) -&gt; None:\n    \"\"\"Initialize model parameters.\"\"\"\n    def init_params(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n            if m.bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                bound = 1 / torch.math.sqrt(fan_in)\n                nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n    init_params(self.fc)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/lstm/#neuro_py.ensemble.decoding.lstm.LSTM._step","title":"<code>_step(batch, batch_idx)</code>","text":"<p>Perform a single step (forward pass + loss calculation).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def _step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Perform a single step (forward pass + loss calculation).\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    xs, ys = batch\n    outs = self(xs)\n    loss = self.args['criterion'](outs, ys)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/lstm/#neuro_py.ensemble.decoding.lstm.LSTM.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>Tuple[List[Optimizer], List[Dict]]</code> <p>Tuple containing a list of optimizers and a list of scheduler configurations</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def configure_optimizers(self) -&gt; Tuple[List[torch.optim.Optimizer], List[Dict]]:\n    \"\"\"\n    Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    Tuple[List[torch.optim.Optimizer], List[Dict]]\n        Tuple containing a list of optimizers and a list of scheduler configurations\n    \"\"\"\n    optimizer = torch.optim.AdamW(\n        self.parameters(), weight_decay=self.args['weight_decay'])\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=self.args['lr'],\n        epochs=self.args['epochs'],\n        steps_per_epoch=len(\n            self.trainer._data_connector._train_dataloader_source.dataloader()\n        )\n    )\n    lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n    return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/lstm/#neuro_py.ensemble.decoding.lstm.LSTM.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the LSTM model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length, input_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, output_dim)</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the LSTM model.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (batch_size, sequence_length, input_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (batch_size, output_dim)\n    \"\"\"\n    lstm_out, (self.hidden_state, self.cell_state) = \\\n        self.lstm(x, (self.hidden_state, self.cell_state))\n    lstm_out = lstm_out[:, -1, :].contiguous()\n    out = self.fc(lstm_out)\n    if self.args.get('clf', False):\n        out = F.log_softmax(out, dim=1)\n    return out\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/lstm/#neuro_py.ensemble.decoding.lstm.LSTM.init_hidden","title":"<code>init_hidden(batch_size)</code>","text":"<p>Initialize hidden state and cell state.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for initialization</p> required Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def init_hidden(self, batch_size: int) -&gt; None:\n    \"\"\"\n    Initialize hidden state and cell state.\n\n    Parameters\n    ----------\n    batch_size : int\n        Batch size for initialization\n    \"\"\"\n    self.batch_size = batch_size\n    h0 = torch.zeros(\n        (2*self.nlayers, batch_size, self.hidden_size),\n        requires_grad=False\n    )\n    c0 = torch.zeros(\n        (2*self.nlayers, batch_size, self.hidden_size),\n        requires_grad=False\n    )\n    self.hidden_state = h0\n    self.cell_state = c0\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/lstm/#neuro_py.ensemble.decoding.lstm.LSTM.on_after_backward","title":"<code>on_after_backward()</code>","text":"<p>Lightning method called after backpropagation.</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def on_after_backward(self) -&gt; None:\n    \"\"\"Lightning method called after backpropagation.\"\"\"\n    self.hidden_state.detach_()\n    self.cell_state.detach_()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/lstm/#neuro_py.ensemble.decoding.lstm.LSTM.predict","title":"<code>predict(x)</code>","text":"<p>Make predictions using the LSTM model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Predicted output</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def predict(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Make predictions using the LSTM model.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor\n\n    Returns\n    -------\n    torch.Tensor\n        Predicted output\n    \"\"\"\n    self.hidden_state = self.hidden_state.to(x.device)\n    self.cell_state = self.cell_state.to(x.device)\n    preds = []\n    batch_size = self.batch_size\n    for i in range(batch_size, x.shape[0]+batch_size, batch_size):\n        iptensor = x[i-batch_size:i]\n        if i &gt; x.shape[0]:\n            iptensor = F.pad(iptensor, (0,0,0,0,0,i-x.shape[0]))\n        pred_loc = self.forward(iptensor)\n        if i &gt; x.shape[0]:\n            pred_loc = pred_loc[:batch_size-(i-x.shape[0])]\n        preds.extend(pred_loc)\n    out = torch.stack(preds)\n    if self.args.get('clf', False):\n        out = F.log_softmax(out, dim=1)\n    return out\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/lstm/#neuro_py.ensemble.decoding.lstm.LSTM.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Lightning method for test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for test step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('test_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/lstm/#neuro_py.ensemble.decoding.lstm.LSTM.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Lightning method for training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for training step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('train_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/lstm/#neuro_py.ensemble.decoding.lstm.LSTM.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Lightning method for validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/lstm.py</code> <pre><code>def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for validation step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('val_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/","title":"neuro_py.ensemble.decoding.m2mlstm","text":""},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/#neuro_py.ensemble.decoding.m2mlstm.M2MLSTM","title":"<code>M2MLSTM</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Many-to-Many Long Short-Term Memory (LSTM) model.</p> <p>This class implements a Many-to-Many LSTM model using PyTorch Lightning.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Dimensionality of input data, by default 100</p> <code>100</code> <code>out_dim</code> <code>int</code> <p>Number of output columns, by default 2</p> <code>2</code> <code>hidden_dims</code> <code>Tuple[int, int, float]</code> <p>Architectural parameters of the model (hidden_size, num_layers, dropout), by default (400, 1, 0.0)</p> <code>(400, 1, 0.0)</code> <code>use_bias</code> <code>bool</code> <p>Whether to use bias or not in the final linear layer, by default True</p> <code>True</code> <code>args</code> <code>Dict</code> <p>Additional arguments for model configuration, by default {}</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>lstm</code> <code>LSTM</code> <p>LSTM layer</p> <code>fc</code> <code>Linear</code> <p>Fully connected layer</p> <code>hidden_state</code> <code>Optional[Tensor]</code> <p>Hidden state of the LSTM</p> <code>cell_state</code> <code>Optional[Tensor]</code> <p>Cell state of the LSTM</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>class M2MLSTM(L.LightningModule):\n    \"\"\"\n    Many-to-Many Long Short-Term Memory (LSTM) model.\n\n    This class implements a Many-to-Many LSTM model using PyTorch Lightning.\n\n    Parameters\n    ----------\n    in_dim : int, optional\n        Dimensionality of input data, by default 100\n    out_dim : int, optional\n        Number of output columns, by default 2\n    hidden_dims : Tuple[int, int, float], optional\n        Architectural parameters of the model (hidden_size, num_layers, dropout),\n        by default (400, 1, 0.0)\n    use_bias : bool, optional\n        Whether to use bias or not in the final linear layer, by default True\n    args : Dict, optional\n        Additional arguments for model configuration, by default {}\n\n    Attributes\n    ----------\n    lstm : nn.LSTM\n        LSTM layer\n    fc : nn.Linear\n        Fully connected layer\n    hidden_state : Optional[torch.Tensor]\n        Hidden state of the LSTM\n    cell_state : Optional[torch.Tensor]\n        Cell state of the LSTM\n    \"\"\"\n    def __init__(self, in_dim: int = 100, out_dim: int = 2,\n                 hidden_dims: Tuple[int, int, float] = (400, 1, 0.0),\n                 use_bias: bool = True, args: Dict = {}):\n        super().__init__()\n        self.save_hyperparameters()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        if len(hidden_dims) != 3:\n            raise ValueError('`hidden_dims` should be of size 3')\n        self.hidden_size, self.nlayers, self.dropout = hidden_dims\n        self.args = args\n\n        self.lstm = nn.LSTM(input_size=in_dim, hidden_size=self.hidden_size,\n                            num_layers=self.nlayers, batch_first=True, \n                            dropout=self.dropout, bidirectional=False)\n        self.fc = nn.Linear(in_features=self.hidden_size, out_features=out_dim, bias=use_bias)\n        self.hidden_state: Optional[torch.Tensor] = None\n        self.cell_state: Optional[torch.Tensor] = None\n\n        self._init_params()\n\n    def _init_params(self) -&gt; None:\n        \"\"\"Initialize model parameters.\"\"\"\n        def init_params(m: nn.Module) -&gt; None:\n            if isinstance(m, nn.Linear):\n                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n                if m.bias is not None:\n                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                    bound = 1 / np.sqrt(fan_in)\n                    nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n        init_params(self.fc)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the LSTM model.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape (batch_size, sequence_length, input_dim)\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor of shape (batch_size, sequence_length, output_dim)\n        \"\"\"\n        B, L, N = x.shape\n        self.hidden_state = self.hidden_state.to(x.device)\n        self.cell_state = self.cell_state.to(x.device)\n        self.hidden_state.data.fill_(0.0)\n        self.cell_state.data.fill_(0.0)\n        lstm_outs = []\n        for i in range(L):\n            lstm_out, (self.hidden_state, self.cell_state) = \\\n                self.lstm(x[:, i].unsqueeze(1), (self.hidden_state, self.cell_state))\n            lstm_outs.append(lstm_out)\n\n        lstm_outs = torch.stack(lstm_outs, dim=1)  # B, L, N\n        out = self.fc(lstm_outs)\n        out = out.view(B, L, self.out_dim)\n        if self.args.get('clf', False):\n            out = F.log_softmax(out, dim=-1)\n\n        return out\n\n    def init_hidden(self, batch_size: int) -&gt; None:\n        \"\"\"\n        Initialize hidden state and cell state.\n\n        Parameters\n        ----------\n        batch_size : int\n            Batch size for initialization\n        \"\"\"\n        self.batch_size = batch_size\n        self.hidden_state = torch.zeros((self.nlayers, batch_size, self.hidden_size), requires_grad=False)\n        self.cell_state = torch.zeros((self.nlayers, batch_size, self.hidden_size), requires_grad=False)\n\n    def _step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Perform a single step (forward pass + loss calculation).\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        xs, ys = batch\n        outs = self(xs)\n        loss = self.args['criterion'](outs, ys)\n        return loss\n\n    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for training step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('train_loss', loss)\n        return loss\n\n    def on_after_backward(self) -&gt; None:\n        \"\"\"Lightning method called after backpropagation.\"\"\"\n        self.hidden_state = self.hidden_state.detach()\n        self.cell_state = self.cell_state.detach()\n\n    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for validation step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('val_loss', loss)\n        return loss\n\n    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for test step.\n\n        Parameters\n        ----------\n        batch : Tuple[torch.Tensor, torch.Tensor]\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('test_loss', loss)\n        return loss\n\n    def configure_optimizers(self) -&gt; Tuple[List[torch.optim.Optimizer], List[Dict]]:\n        \"\"\"\n        Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        Tuple[List[torch.optim.Optimizer], List[Dict]]\n            Tuple containing a list of optimizers and a list of scheduler configurations\n        \"\"\"\n        optimizer = torch.optim.AdamW(\n            self.parameters(), weight_decay=self.args['weight_decay'])\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer, max_lr=self.args['lr'],\n            epochs=self.args['epochs'],\n            total_steps=self.trainer.estimated_stepping_batches\n        )\n        lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n        return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/#neuro_py.ensemble.decoding.m2mlstm.M2MLSTM._init_params","title":"<code>_init_params()</code>","text":"<p>Initialize model parameters.</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def _init_params(self) -&gt; None:\n    \"\"\"Initialize model parameters.\"\"\"\n    def init_params(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n            if m.bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                bound = 1 / np.sqrt(fan_in)\n                nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n    init_params(self.fc)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/#neuro_py.ensemble.decoding.m2mlstm.M2MLSTM._step","title":"<code>_step(batch, batch_idx)</code>","text":"<p>Perform a single step (forward pass + loss calculation).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def _step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Perform a single step (forward pass + loss calculation).\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    xs, ys = batch\n    outs = self(xs)\n    loss = self.args['criterion'](outs, ys)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/#neuro_py.ensemble.decoding.m2mlstm.M2MLSTM.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>Tuple[List[Optimizer], List[Dict]]</code> <p>Tuple containing a list of optimizers and a list of scheduler configurations</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def configure_optimizers(self) -&gt; Tuple[List[torch.optim.Optimizer], List[Dict]]:\n    \"\"\"\n    Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    Tuple[List[torch.optim.Optimizer], List[Dict]]\n        Tuple containing a list of optimizers and a list of scheduler configurations\n    \"\"\"\n    optimizer = torch.optim.AdamW(\n        self.parameters(), weight_decay=self.args['weight_decay'])\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=self.args['lr'],\n        epochs=self.args['epochs'],\n        total_steps=self.trainer.estimated_stepping_batches\n    )\n    lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n    return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/#neuro_py.ensemble.decoding.m2mlstm.M2MLSTM.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the LSTM model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length, input_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, sequence_length, output_dim)</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the LSTM model.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (batch_size, sequence_length, input_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (batch_size, sequence_length, output_dim)\n    \"\"\"\n    B, L, N = x.shape\n    self.hidden_state = self.hidden_state.to(x.device)\n    self.cell_state = self.cell_state.to(x.device)\n    self.hidden_state.data.fill_(0.0)\n    self.cell_state.data.fill_(0.0)\n    lstm_outs = []\n    for i in range(L):\n        lstm_out, (self.hidden_state, self.cell_state) = \\\n            self.lstm(x[:, i].unsqueeze(1), (self.hidden_state, self.cell_state))\n        lstm_outs.append(lstm_out)\n\n    lstm_outs = torch.stack(lstm_outs, dim=1)  # B, L, N\n    out = self.fc(lstm_outs)\n    out = out.view(B, L, self.out_dim)\n    if self.args.get('clf', False):\n        out = F.log_softmax(out, dim=-1)\n\n    return out\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/#neuro_py.ensemble.decoding.m2mlstm.M2MLSTM.init_hidden","title":"<code>init_hidden(batch_size)</code>","text":"<p>Initialize hidden state and cell state.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for initialization</p> required Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def init_hidden(self, batch_size: int) -&gt; None:\n    \"\"\"\n    Initialize hidden state and cell state.\n\n    Parameters\n    ----------\n    batch_size : int\n        Batch size for initialization\n    \"\"\"\n    self.batch_size = batch_size\n    self.hidden_state = torch.zeros((self.nlayers, batch_size, self.hidden_size), requires_grad=False)\n    self.cell_state = torch.zeros((self.nlayers, batch_size, self.hidden_size), requires_grad=False)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/#neuro_py.ensemble.decoding.m2mlstm.M2MLSTM.on_after_backward","title":"<code>on_after_backward()</code>","text":"<p>Lightning method called after backpropagation.</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def on_after_backward(self) -&gt; None:\n    \"\"\"Lightning method called after backpropagation.\"\"\"\n    self.hidden_state = self.hidden_state.detach()\n    self.cell_state = self.cell_state.detach()\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/#neuro_py.ensemble.decoding.m2mlstm.M2MLSTM.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Lightning method for test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for test step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('test_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/#neuro_py.ensemble.decoding.m2mlstm.M2MLSTM.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Lightning method for training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for training step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('train_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/#neuro_py.ensemble.decoding.m2mlstm.M2MLSTM.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Lightning method for validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for validation step.\n\n    Parameters\n    ----------\n    batch : Tuple[torch.Tensor, torch.Tensor]\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('val_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/m2mlstm/#neuro_py.ensemble.decoding.m2mlstm.NSVDataset","title":"<code>NSVDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Custom Dataset for neural state vector (binned spike train) data.</p> <p>Parameters:</p> Name Type Description Default <code>nsv</code> <code>List[ndarray]</code> <p>List of trial-segmented neural state vector arrays</p> required <code>dv</code> <code>List[ndarray]</code> <p>List of trial-segmented behavioral state vector arrays</p> required <p>Attributes:</p> Name Type Description <code>nsv</code> <code>List[ndarray]</code> <p>List of trial-segmented neural state vector arrays as float32</p> <code>dv</code> <code>List[ndarray]</code> <p>List of trial-segmented behavioral state vector arrays as float32</p> Source code in <code>neuro_py/ensemble/decoding/m2mlstm.py</code> <pre><code>class NSVDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Custom Dataset for neural state vector (binned spike train) data.\n\n    Parameters\n    ----------\n    nsv : List[np.ndarray]\n        List of trial-segmented neural state vector arrays\n    dv : List[np.ndarray]\n        List of trial-segmented behavioral state vector arrays\n\n    Attributes\n    ----------\n    nsv : List[np.ndarray]\n        List of trial-segmented neural state vector arrays as float32\n    dv : List[np.ndarray]\n        List of trial-segmented behavioral state vector arrays as float32\n    \"\"\"\n    def __init__(self, nsv: List[np.ndarray], dv: List[np.ndarray]):\n        self.nsv = [i.astype(np.float32) for i in nsv]\n        self.dv = [i.astype(np.float32) for i in dv]\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Get the length of the dataset.\n\n        Returns\n        -------\n        int\n            Number of samples in the dataset\n        \"\"\"\n        return len(self.nsv)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Get a sample from the dataset.\n\n        Parameters\n        ----------\n        idx : int\n            Index of the sample\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray]\n            Tuple containing NSV and DV arrays\n        \"\"\"\n        nsv, dv = self.nsv[idx], self.dv[idx]\n        return nsv, dv\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/mlp/","title":"neuro_py.ensemble.decoding.mlp","text":""},{"location":"reference/neuro_py/ensemble/decoding/mlp/#neuro_py.ensemble.decoding.mlp.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Multi-Layer Perceptron (MLP) in PyTorch with an arbitrary number of hidden layers.</p> <p>This class implements an MLP model using PyTorch Lightning, allowing for flexible architecture with varying hidden layer sizes and dropout probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Dimensionality of input data, by default 100</p> <code>100</code> <code>out_dim</code> <code>int</code> <p>Dimensionality of output data, by default 2</p> <code>2</code> <code>hidden_dims</code> <code>List[Union[int, float]]</code> <p>List containing architectural parameters of the model. If an element is an int, it represents a hidden layer of that size. If an element is a float, it represents a dropout layer with that probability. By default ()</p> <code>()</code> <code>use_bias</code> <code>bool</code> <p>Whether to use bias in all linear layers, by default True</p> <code>True</code> <code>args</code> <code>Optional[Dict]</code> <p>Dictionary containing the hyperparameters of the model, by default None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>main</code> <code>Sequential</code> <p>The main sequential container of the MLP layers</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>class MLP(L.LightningModule):\n    \"\"\"\n    Multi-Layer Perceptron (MLP) in PyTorch with an arbitrary number of hidden layers.\n\n    This class implements an MLP model using PyTorch Lightning, allowing for flexible\n    architecture with varying hidden layer sizes and dropout probabilities.\n\n    Parameters\n    ----------\n    in_dim : int, optional\n        Dimensionality of input data, by default 100\n    out_dim : int, optional\n        Dimensionality of output data, by default 2\n    hidden_dims : List[Union[int, float]], optional\n        List containing architectural parameters of the model. If an element is\n        an int, it represents a hidden layer of that size. If an element is a float,\n        it represents a dropout layer with that probability. By default ()\n    use_bias : bool, optional\n        Whether to use bias in all linear layers, by default True\n    args : Optional[Dict], optional\n        Dictionary containing the hyperparameters of the model, by default None\n\n    Attributes\n    ----------\n    main : nn.Sequential\n        The main sequential container of the MLP layers\n    \"\"\"\n    def __init__(self, in_dim: int = 100, out_dim: int = 2,\n                 hidden_dims: List[Union[int, float]] = (),\n                 use_bias: bool = True, args: Optional[Dict] = None):\n        super().__init__()\n        self.save_hyperparameters()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.args = args if args is not None else {}\n        activations = nn.CELU if self.args.get('activations') is None else self.args['activations']\n\n        layers = self._build_layers(in_dim, out_dim, hidden_dims, use_bias, activations)\n        self.main = nn.Sequential(*layers)\n        self._init_params()\n\n    def _build_layers(\n            self,\n            in_dim: int,\n            out_dim: int,\n            hidden_dims: List[Union[int, float]],\n            use_bias: bool,\n            activations: nn.Module\n        ) -&gt; List[nn.Module]:\n        \"\"\"\n        Build the layers of the MLP.\n\n        Parameters\n        ----------\n        in_dim : int\n            Dimensionality of input data\n        out_dim : int\n            Dimensionality of output data\n        hidden_dims : List[Union[int, float]]\n            List of hidden layer sizes and dropout probabilities\n        use_bias : bool\n            Whether to use bias in linear layers\n        activations : nn.Module\n            Activation function to use\n\n        Returns\n        -------\n        List[nn.Module]\n            List of layers for the MLP\n        \"\"\"\n        if len(hidden_dims) == 0:\n            return [nn.Linear(in_dim, out_dim, bias=use_bias)]\n\n        layers = []\n        hidden_dims = [in_dim] + hidden_dims\n\n        for i, hidden_dim in enumerate(hidden_dims[:-1]):\n            if isinstance(hidden_dim, float):\n                continue\n            if isinstance(hidden_dims[i+1], float):\n                layers.extend([\n                    nn.Linear(hidden_dim, hidden_dims[i + 2], bias=use_bias),\n                    nn.Dropout(p=hidden_dims[i+1]),\n                    activations() if i &lt; len(hidden_dims)-1 else nn.Tanh()\n                ])\n            else:\n                layers.extend([\n                    nn.Linear(hidden_dim, hidden_dims[i + 1], bias=use_bias),\n                    activations() if i &lt; len(hidden_dims)-1 else nn.Tanh()\n                ])\n\n        layers.append(nn.Linear(hidden_dims[-1], out_dim, bias=use_bias))\n        if self.args.get('clf', False):\n            layers.append(nn.LogSoftmax(dim=1))\n\n        return layers\n\n    def _init_params(self) -&gt; None:\n        \"\"\"Initialize the parameters of the model.\"\"\"\n        def init_params(m: nn.Module) -&gt; None:\n            if isinstance(m, nn.Linear):\n                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n                if m.bias is not None:\n                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                    bound = 1 / torch.math.sqrt(fan_in)\n                    nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n        self.main.apply(init_params)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Defines the network structure and flow from input to output.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input data\n\n        Returns\n        -------\n        torch.Tensor\n            Output data\n        \"\"\"\n        return self.main(x)\n\n    def _step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Perform a single step (forward pass + loss calculation).\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        xs, ys = batch\n        outs = self(xs)\n        loss = self.args['criterion'](outs, ys)\n        return loss\n\n    def training_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for training step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for validation step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('val_loss', loss)\n        return loss\n\n    def test_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for test step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('test_loss', loss)\n        return loss\n\n    def configure_optimizers(self):\n        \"\"\"\n        Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        tuple\n            Tuple containing a list of optimizers and a list of scheduler configurations\n        \"\"\"\n        optimizer = torch.optim.AdamW(\n            self.parameters(),\n            weight_decay=self.args['weight_decay'],\n            betas=(0.9, 0.999),\n            amsgrad=True\n        )\n        scheduler = torch.optim.lr_scheduler.CyclicLR(\n            optimizer, \n            base_lr=self.args['base_lr'],\n            max_lr=self.args['lr'],\n            step_size_up=self.args['scheduler_step_size_multiplier'] * self.args['num_training_batches'],\n            cycle_momentum=False,\n            mode='triangular2',\n            gamma=0.99994,\n            last_epoch=-1,\n        )\n        lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n        return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/mlp/#neuro_py.ensemble.decoding.mlp.MLP._build_layers","title":"<code>_build_layers(in_dim, out_dim, hidden_dims, use_bias, activations)</code>","text":"<p>Build the layers of the MLP.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Dimensionality of input data</p> required <code>out_dim</code> <code>int</code> <p>Dimensionality of output data</p> required <code>hidden_dims</code> <code>List[Union[int, float]]</code> <p>List of hidden layer sizes and dropout probabilities</p> required <code>use_bias</code> <code>bool</code> <p>Whether to use bias in linear layers</p> required <code>activations</code> <code>Module</code> <p>Activation function to use</p> required <p>Returns:</p> Type Description <code>List[Module]</code> <p>List of layers for the MLP</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def _build_layers(\n        self,\n        in_dim: int,\n        out_dim: int,\n        hidden_dims: List[Union[int, float]],\n        use_bias: bool,\n        activations: nn.Module\n    ) -&gt; List[nn.Module]:\n    \"\"\"\n    Build the layers of the MLP.\n\n    Parameters\n    ----------\n    in_dim : int\n        Dimensionality of input data\n    out_dim : int\n        Dimensionality of output data\n    hidden_dims : List[Union[int, float]]\n        List of hidden layer sizes and dropout probabilities\n    use_bias : bool\n        Whether to use bias in linear layers\n    activations : nn.Module\n        Activation function to use\n\n    Returns\n    -------\n    List[nn.Module]\n        List of layers for the MLP\n    \"\"\"\n    if len(hidden_dims) == 0:\n        return [nn.Linear(in_dim, out_dim, bias=use_bias)]\n\n    layers = []\n    hidden_dims = [in_dim] + hidden_dims\n\n    for i, hidden_dim in enumerate(hidden_dims[:-1]):\n        if isinstance(hidden_dim, float):\n            continue\n        if isinstance(hidden_dims[i+1], float):\n            layers.extend([\n                nn.Linear(hidden_dim, hidden_dims[i + 2], bias=use_bias),\n                nn.Dropout(p=hidden_dims[i+1]),\n                activations() if i &lt; len(hidden_dims)-1 else nn.Tanh()\n            ])\n        else:\n            layers.extend([\n                nn.Linear(hidden_dim, hidden_dims[i + 1], bias=use_bias),\n                activations() if i &lt; len(hidden_dims)-1 else nn.Tanh()\n            ])\n\n    layers.append(nn.Linear(hidden_dims[-1], out_dim, bias=use_bias))\n    if self.args.get('clf', False):\n        layers.append(nn.LogSoftmax(dim=1))\n\n    return layers\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/mlp/#neuro_py.ensemble.decoding.mlp.MLP._init_params","title":"<code>_init_params()</code>","text":"<p>Initialize the parameters of the model.</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def _init_params(self) -&gt; None:\n    \"\"\"Initialize the parameters of the model.\"\"\"\n    def init_params(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n            if m.bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                bound = 1 / torch.math.sqrt(fan_in)\n                nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n    self.main.apply(init_params)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/mlp/#neuro_py.ensemble.decoding.mlp.MLP._step","title":"<code>_step(batch, batch_idx)</code>","text":"<p>Perform a single step (forward pass + loss calculation).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def _step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Perform a single step (forward pass + loss calculation).\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    xs, ys = batch\n    outs = self(xs)\n    loss = self.args['criterion'](outs, ys)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/mlp/#neuro_py.ensemble.decoding.mlp.MLP.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing a list of optimizers and a list of scheduler configurations</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"\n    Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    tuple\n        Tuple containing a list of optimizers and a list of scheduler configurations\n    \"\"\"\n    optimizer = torch.optim.AdamW(\n        self.parameters(),\n        weight_decay=self.args['weight_decay'],\n        betas=(0.9, 0.999),\n        amsgrad=True\n    )\n    scheduler = torch.optim.lr_scheduler.CyclicLR(\n        optimizer, \n        base_lr=self.args['base_lr'],\n        max_lr=self.args['lr'],\n        step_size_up=self.args['scheduler_step_size_multiplier'] * self.args['num_training_batches'],\n        cycle_momentum=False,\n        mode='triangular2',\n        gamma=0.99994,\n        last_epoch=-1,\n    )\n    lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n    return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/mlp/#neuro_py.ensemble.decoding.mlp.MLP.forward","title":"<code>forward(x)</code>","text":"<p>Defines the network structure and flow from input to output.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output data</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Defines the network structure and flow from input to output.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input data\n\n    Returns\n    -------\n    torch.Tensor\n        Output data\n    \"\"\"\n    return self.main(x)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/mlp/#neuro_py.ensemble.decoding.mlp.MLP.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Lightning method for test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for test step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('test_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/mlp/#neuro_py.ensemble.decoding.mlp.MLP.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Lightning method for training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def training_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for training step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('train_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/mlp/#neuro_py.ensemble.decoding.mlp.MLP.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Lightning method for validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/mlp.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for validation step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('val_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/","title":"neuro_py.ensemble.decoding.pipeline","text":""},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline._get_trial_spikes_with_no_overlap_history","title":"<code>_get_trial_spikes_with_no_overlap_history(X, bins_before, bins_after, bins_current)</code>","text":"<p>Get trial spikes with no overlap history.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>NDArray</code> <p>Input binned spike data.</p> required <code>bins_before</code> <code>int</code> <p>Number of bins before the current bin.</p> required <code>bins_after</code> <code>int</code> <p>Number of bins after the current bin.</p> required <code>bins_current</code> <code>int</code> <p>Number of current bins.</p> required <p>Returns:</p> Type Description <code>List[NDArray]</code> <p>List of trial covariates with no overlap history.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def _get_trial_spikes_with_no_overlap_history(\n        X: NDArray,\n        bins_before: int,\n        bins_after: int,\n        bins_current: int\n    ) -&gt; List[NDArray]:\n    \"\"\"\n    Get trial spikes with no overlap history.\n\n    Parameters\n    ----------\n    X : NDArray\n        Input binned spike data.\n    bins_before : int\n        Number of bins before the current bin.\n    bins_after : int\n        Number of bins after the current bin.\n    bins_current : int\n        Number of current bins.\n\n    Returns\n    -------\n    List[NDArray]\n        List of trial covariates with no overlap history.\n    \"\"\"\n    nonoverlap_trial_covariates = []\n    if X.ndim == 2:\n        X_cov = get_spikes_with_history(\n            X, bins_before, bins_after, bins_current)\n        nonoverlap_trial_covariates.append(X_cov)\n    else:\n        for X_trial in X:\n            X_cov = get_spikes_with_history(\n                np.asarray(X_trial), bins_before, bins_after, bins_current\n            )\n            nonoverlap_trial_covariates.append(X_cov)\n    return nonoverlap_trial_covariates\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.create_model","title":"<code>create_model(hyperparams)</code>","text":"<p>Create a model based on the given hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparams</code> <code>Dict[str, Any]</code> <p>Dictionary containing model hyperparameters.</p> required <p>Returns:</p> Type Description <code>Tuple[Any, LightningModule]</code> <p>The decoder class and instantiated model.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def create_model(hyperparams: Dict[str, Any]) -&gt; Tuple[Any, pl.LightningModule]:\n    \"\"\"\n    Create a model based on the given hyperparameters.\n\n    Parameters\n    ----------\n    hyperparams : Dict[str, Any]\n        Dictionary containing model hyperparameters.\n\n    Returns\n    -------\n    Tuple[Any, pl.LightningModule]\n        The decoder class and instantiated model.\n    \"\"\"\n    decoder = eval(f\"{hyperparams['model']}\")\n    model = decoder(**hyperparams['model_args'])\n\n    if 'LSTM' in hyperparams['model']:\n        model.init_hidden(hyperparams['batch_size'])\n        model.hidden_state = model.hidden_state.to(hyperparams['device'])\n        model.cell_state = model.cell_state.to(hyperparams['device'])\n\n    return decoder, model\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.evaluate_model","title":"<code>evaluate_model(hyperparams, ohe, predictor, X_test, y_test)</code>","text":"<p>Evaluate the model on test data.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparams</code> <code>Dict[str, Any]</code> <p>Dictionary containing hyperparameters.</p> required <code>ohe</code> <code>OneHotEncoder</code> <p>One-hot encoder for categorical variables.</p> required <code>predictor</code> <code>Module</code> <p>The trained model.</p> required <code>X_test</code> <code>NDArray</code> <p>Test features.</p> required <code>y_test</code> <code>NDArray</code> <p>Test labels.</p> required <p>Returns:</p> Type Description <code>Tuple[Dict[str, float], NDArray]</code> <p>Evaluation metrics and model predictions.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def evaluate_model(\n        hyperparams: Dict[str, Any],\n        ohe: sklearn.preprocessing.OneHotEncoder,\n        predictor: torch.nn.Module,\n        X_test: NDArray,\n        y_test: NDArray\n    ) -&gt; Tuple[Dict[str, float], NDArray]:\n    \"\"\"\n    Evaluate the model on test data.\n\n    Parameters\n    ----------\n    hyperparams : Dict[str, Any]\n        Dictionary containing hyperparameters.\n    ohe : OneHotEncoder\n        One-hot encoder for categorical variables.\n    predictor : torch.nn.Module\n        The trained model.\n    X_test : NDArray\n        Test features.\n    y_test : NDArray\n        Test labels.\n\n    Returns\n    -------\n    Tuple[Dict[str, float], NDArray]\n        Evaluation metrics and model predictions.\n    \"\"\"\n    if hyperparams['model'] in ('M2MLSTM', 'NDT'):\n        out_dim = hyperparams['model_args']['out_dim']\n        with torch.no_grad():\n            bv_preds_fold = [\n                predictor(\n                    torch.from_numpy(X.reshape(1, *X.shape)).type(torch.float32)\n                )\n                for X in X_test\n            ]\n        bv_preds_fold = np.vstack([\n            bv.squeeze().detach().cpu().numpy().reshape(-1, out_dim)\n            for bv in bv_preds_fold\n        ])\n    else:\n        bv_preds_fold = predictor(\n            torch.from_numpy(X_test).type(torch.float32)\n        )\n        bv_preds_fold = bv_preds_fold.detach().cpu().numpy()\n\n    bv_preds_fold = copy.deepcopy(bv_preds_fold)\n\n    logits = bv_preds_fold\n    labels = np.vstack(y_test)\n    if hyperparams['model_args']['args']['clf']:\n        logits = ohe.inverse_transform(logits)\n        labels = ohe.inverse_transform(labels)\n        accuracy = sklearn.metrics.accuracy_score(labels, logits)\n        metrics = dict(accuracy=accuracy)\n        bv_preds_fold = logits\n    else:\n        coeff_determination = sklearn.metrics.r2_score(\n            labels,\n            logits,\n            multioutput='variance_weighted'\n        )\n        rmse = sklearn.metrics.root_mean_squared_error(labels, logits)\n        metrics = dict(coeff_determination=coeff_determination, rmse=rmse)\n    return metrics, bv_preds_fold\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.format_trial_segs_nsv","title":"<code>format_trial_segs_nsv(nsv_train_normed, nsv_rest_normed, bv_train, bv_rest, predict_bv, bins_before=0, bins_current=1, bins_after=0)</code>","text":"<p>Format trial segments for neural state vectors.</p> <p>Parameters:</p> Name Type Description Default <code>nsv_train_normed</code> <code>List[NDArray]</code> <p>Normalized neural state vectors for training.</p> required <code>nsv_rest_normed</code> <code>List[NDArray]</code> <p>Normalized neural state vectors for rest.</p> required <code>bv_train</code> <code>NDArray</code> <p>Behavioral state vectors for training.</p> required <code>bv_rest</code> <code>List[NDArray]</code> <p>Behavioral state vectors for rest.</p> required <code>predict_bv</code> <code>List[int]</code> <p>Indices of behavioral state vectors to predict.</p> required <code>bins_before</code> <code>int</code> <p>Number of bins before the current bin, by default 0.</p> <code>0</code> <code>bins_current</code> <code>int</code> <p>Number of current bins, by default 1.</p> <code>1</code> <code>bins_after</code> <code>int</code> <p>Number of bins after the current bin, by default 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[NDArray, List[NDArray], NDArray, List[NDArray], NDArray, List[NDArray]]</code> <p>Formatted trial segments for neural state vectors.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def format_trial_segs_nsv(\n    nsv_train_normed: List[NDArray],\n    nsv_rest_normed: List[NDArray],\n    bv_train: NDArray,\n    bv_rest: List[NDArray],\n    predict_bv: List[int],\n    bins_before: int = 0,\n    bins_current: int = 1,\n    bins_after: int = 0\n) -&gt; Tuple[NDArray, List[NDArray], NDArray, List[NDArray], NDArray, List[NDArray]]:\n    \"\"\"\n    Format trial segments for neural state vectors.\n\n    Parameters\n    ----------\n    nsv_train_normed : List[NDArray]\n        Normalized neural state vectors for training.\n    nsv_rest_normed : List[NDArray]\n        Normalized neural state vectors for rest.\n    bv_train : NDArray\n        Behavioral state vectors for training.\n    bv_rest : List[NDArray]\n        Behavioral state vectors for rest.\n    predict_bv : List[int]\n        Indices of behavioral state vectors to predict.\n    bins_before : int, optional\n        Number of bins before the current bin, by default 0.\n    bins_current : int, optional\n        Number of current bins, by default 1.\n    bins_after : int, optional\n        Number of bins after the current bin, by default 0.\n\n    Returns\n    -------\n    Tuple[NDArray, List[NDArray], NDArray, List[NDArray], NDArray, List[NDArray]]\n        Formatted trial segments for neural state vectors.\n    \"\"\"\n    is_2D = nsv_train_normed[0].ndim == 1\n    # Format for RNNs: covariate matrix including spike history from previous bins\n    X_train = np.concatenate(_get_trial_spikes_with_no_overlap_history(\n        nsv_train_normed, bins_before, bins_after, bins_current))\n    X_rest = []\n    for nsv_feats in nsv_rest_normed:\n        X_feats = np.concatenate(_get_trial_spikes_with_no_overlap_history(\n            nsv_feats, bins_before, bins_after, bins_current))\n        X_rest.append(X_feats)\n\n    # each \"neuron / time\" is a single feature\n    X_flat_train = X_train.reshape(\n        X_train.shape[0], (X_train.shape[1] * X_train.shape[2]))\n    X_flat_rest = []\n    for X_feat in X_rest:\n        X_flat_feat = X_feat.reshape(\n            X_feat.shape[0], (X_feat.shape[1] * X_feat.shape[2]))\n        X_flat_rest.append(X_flat_feat)\n\n    bv_train = bv_train if not is_2D else [bv_train]\n    y_train = np.concatenate(bv_train)\n    y_train = y_train[:, predict_bv]\n    y_rest = []\n    for bv_y in bv_rest:\n        bv_y = bv_y if not is_2D else [bv_y]\n        y = np.concatenate(bv_y)\n        y = y[:, predict_bv]\n        y_rest.append(y)\n\n    return X_train, X_rest, X_flat_train, X_flat_rest, y_train, y_rest\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.get_spikes_with_history","title":"<code>get_spikes_with_history(neural_data, bins_before, bins_after, bins_current=1)</code>","text":"<p>Create the covariate matrix of neural activity.</p> <p>Parameters:</p> Name Type Description Default <code>neural_data</code> <code>ndarray</code> <p>A matrix of size \"number of time bins\" x \"number of neurons\", representing the number of spikes in each time bin for each neuron.</p> required <code>bins_before</code> <code>int</code> <p>How many bins of neural data prior to the output are used for decoding.</p> required <code>bins_after</code> <code>int</code> <p>How many bins of neural data after the output are used for decoding.</p> required <code>bins_current</code> <code>int</code> <p>Whether to use the concurrent time bin of neural data for decoding, by default 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A matrix of size \"number of total time bins\" x \"number of surrounding time bins used for prediction\" x \"number of neurons\".</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def get_spikes_with_history(neural_data: np.ndarray, bins_before: int, bins_after: int, bins_current: int = 1) -&gt; np.ndarray:\n    \"\"\"\n    Create the covariate matrix of neural activity.\n\n    Parameters\n    ----------\n    neural_data : np.ndarray\n        A matrix of size \"number of time bins\" x \"number of neurons\",\n        representing the number of spikes in each time bin for each neuron.\n    bins_before : int\n        How many bins of neural data prior to the output are used for decoding.\n    bins_after : int\n        How many bins of neural data after the output are used for decoding.\n    bins_current : int, optional\n        Whether to use the concurrent time bin of neural data for decoding, by\n        default 1.\n\n    Returns\n    -------\n    np.ndarray\n        A matrix of size \"number of total time bins\" x \"number of surrounding\n        time bins used for prediction\" x \"number of neurons\".\n    \"\"\"\n    num_examples, num_neurons = neural_data.shape\n    surrounding_bins = bins_before + bins_after + bins_current\n    X = np.zeros([num_examples, surrounding_bins, num_neurons])\n\n    for i in range(num_examples - bins_before - bins_after):\n        start_idx = i\n        end_idx = start_idx + surrounding_bins\n        X[i + bins_before] = neural_data[start_idx:end_idx]\n\n    return X\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.minibatchify","title":"<code>minibatchify(Xtrain, ytrain, Xval, yval, Xtest, ytest, seed=0, batch_size=128, num_workers=5, modeltype='MLP')</code>","text":"<p>Create minibatches for training, validation, and testing.</p> <p>Parameters:</p> Name Type Description Default <code>Xtrain</code> <code>NDArray</code> <p>Training features.</p> required <code>ytrain</code> <code>NDArray</code> <p>Training labels.</p> required <code>Xval</code> <code>NDArray</code> <p>Validation features.</p> required <code>yval</code> <code>NDArray</code> <p>Validation labels.</p> required <code>Xtest</code> <code>NDArray</code> <p>Test features.</p> required <code>ytest</code> <code>NDArray</code> <p>Test labels.</p> required <code>seed</code> <code>int</code> <p>Random seed, by default 0.</p> <code>0</code> <code>batch_size</code> <code>int</code> <p>Batch size, by default 128.</p> <code>128</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading, by default 5.</p> <code>5</code> <code>modeltype</code> <code>str</code> <p>Type of model, by default 'MLP'.</p> <code>'MLP'</code> <p>Returns:</p> Type Description <code>Tuple[DataLoader, DataLoader, DataLoader]</code> <p>DataLoaders for training, validation, and testing.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def minibatchify(\n        Xtrain: NDArray,\n        ytrain: NDArray,\n        Xval: NDArray,\n        yval: NDArray,\n        Xtest: NDArray,\n        ytest: NDArray,\n        seed: int = 0,\n        batch_size: int = 128,\n        num_workers: int = 5,\n        modeltype: str = 'MLP'\n    ) -&gt; Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n    \"\"\"\n    Create minibatches for training, validation, and testing.\n\n    Parameters\n    ----------\n    Xtrain : NDArray\n        Training features.\n    ytrain : NDArray\n        Training labels.\n    Xval : NDArray\n        Validation features.\n    yval : NDArray\n        Validation labels.\n    Xtest : NDArray\n        Test features.\n    ytest : NDArray\n        Test labels.\n    seed : int, optional\n        Random seed, by default 0.\n    batch_size : int, optional\n        Batch size, by default 128.\n    num_workers : int, optional\n        Number of workers for data loading, by default 5.\n    modeltype : str, optional\n        Type of model, by default 'MLP'.\n\n    Returns\n    -------\n    Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader]\n        DataLoaders for training, validation, and testing.\n    \"\"\"\n    g_seed = torch.Generator()\n    g_seed.manual_seed(seed)\n    if Xtrain.ndim == 2:  # handle object arrays\n        Xtrain = Xtrain.astype(np.float32)\n        Xval = Xval.astype(np.float32)\n        Xtest = Xtest.astype(np.float32)\n        ytrain = ytrain.astype(np.float32)\n        yval = yval.astype(np.float32)\n        ytest = ytest.astype(np.float32)\n    train = torch.utils.data.TensorDataset(\n        torch.from_numpy(Xtrain).type(torch.float32),\n        torch.from_numpy(ytrain).type(torch.float32))\n    val = torch.utils.data.TensorDataset(\n        torch.from_numpy(Xval).type(torch.float32), \n        torch.from_numpy(yval).type(torch.float32))\n    test = torch.utils.data.TensorDataset(\n        torch.from_numpy(Xtest).type(torch.float32),\n        torch.from_numpy(ytest).type(torch.float32))\n\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n        shuffle=True, num_workers=num_workers, pin_memory=True,\n        drop_last=(modeltype=='LSTM'), worker_init_fn=seed_worker,\n        generator=g_seed)\n\n    val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size,\n        shuffle=False, num_workers=num_workers, pin_memory=True,\n        drop_last=(modeltype=='LSTM'), worker_init_fn=seed_worker,\n        generator=g_seed)\n\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size,\n        shuffle=False, num_workers=num_workers, pin_memory=True,\n        drop_last=(modeltype=='LSTM'), worker_init_fn=seed_worker,\n        generator=g_seed)\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.normalize_format_trial_segs","title":"<code>normalize_format_trial_segs(nsv_train, nsv_rest, bv_train, bv_rest, predict_bv=[4, 5], bins_before=0, bins_current=1, bins_after=0, normparams=None)</code>","text":"<p>Normalize and format trial segments.</p> <p>Parameters:</p> Name Type Description Default <code>nsv_train</code> <code>NDArray</code> <p>Neural state vectors for training.</p> required <code>nsv_rest</code> <code>List[NDArray]</code> <p>Neural state vectors for rest.</p> required <code>bv_train</code> <code>NDArray</code> <p>Behavioral state vectors for training.</p> required <code>bv_rest</code> <code>List[NDArray]</code> <p>Behavioral state vectors for rest.</p> required <code>predict_bv</code> <code>List[int]</code> <p>Indices of behavioral state vectors to predict, by default [4, 5].</p> <code>[4, 5]</code> <code>bins_before</code> <code>int</code> <p>Number of bins before the current bin, by default 0.</p> <code>0</code> <code>bins_current</code> <code>int</code> <p>Number of current bins, by default 1.</p> <code>1</code> <code>bins_after</code> <code>int</code> <p>Number of bins after the current bin, by default 0.</p> <code>0</code> <code>normparams</code> <code>Optional[Dict[str, Any]]</code> <p>Normalization parameters, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[NDArray, NDArray, NDArray, List[Tuple[NDArray, NDArray, NDArray]], Dict[str, Any]]</code> <p>Normalized and formatted trial segments.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def normalize_format_trial_segs(\n    nsv_train: NDArray,\n    nsv_rest: List[NDArray],\n    bv_train: NDArray,\n    bv_rest: List[NDArray],\n    predict_bv: List[int] = [4, 5],\n    bins_before: int = 0,\n    bins_current: int = 1,\n    bins_after: int = 0,\n    normparams: Optional[Dict[str, Any]] = None\n) -&gt; Tuple[NDArray, NDArray, NDArray, List[Tuple[NDArray, NDArray, NDArray]], Dict[str, Any]]:\n    \"\"\"\n    Normalize and format trial segments.\n\n    Parameters\n    ----------\n    nsv_train : NDArray\n        Neural state vectors for training.\n    nsv_rest : List[NDArray]\n        Neural state vectors for rest.\n    bv_train : NDArray\n        Behavioral state vectors for training.\n    bv_rest : List[NDArray]\n        Behavioral state vectors for rest.\n    predict_bv : List[int], optional\n        Indices of behavioral state vectors to predict, by default [4, 5].\n    bins_before : int, optional\n        Number of bins before the current bin, by default 0.\n    bins_current : int, optional\n        Number of current bins, by default 1.\n    bins_after : int, optional\n        Number of bins after the current bin, by default 0.\n    normparams : Optional[Dict[str, Any]], optional\n        Normalization parameters, by default None.\n\n    Returns\n    -------\n    Tuple[NDArray, NDArray, NDArray, List[Tuple[NDArray, NDArray, NDArray]], Dict[str, Any]]\n        Normalized and formatted trial segments.\n    \"\"\"\n    nsv_train_normed, nsv_rest_normed, norm_params = zscore_trial_segs(nsv_train, nsv_rest, normparams)\n\n    (X_train, X_rest, X_flat_train, X_flat_rest, y_train, y_rest\n    ) = format_trial_segs_nsv(\n        nsv_train_normed, nsv_rest_normed, bv_train, bv_rest, predict_bv,\n        bins_before, bins_current, bins_after)\n\n    #Zero-center outputs\n    y_train_mean = normparams['y_train_mean'] if normparams is not None else np.mean(y_train, axis=0)\n    y_train = y_train - y_train_mean\n    y_centered_rest = []\n    for y in y_rest:\n        y_centered_rest.append(y - y_train_mean)\n\n    norm_params['y_train_mean'] = y_train_mean\n\n    return X_train, X_flat_train, y_train, tuple(zip(X_rest, X_flat_rest, y_centered_rest)), norm_params\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.normalize_labels","title":"<code>normalize_labels(y_train, y_val, y_test)</code>","text":"<p>Normalize labels to integers in [0, n_classes).</p> <p>Parameters:</p> Name Type Description Default <code>y_train</code> <code>NDArray</code> <p>Training labels.</p> required <code>y_val</code> <code>NDArray</code> <p>Validation labels.</p> required <code>y_test</code> <code>NDArray</code> <p>Test labels.</p> required <p>Returns:</p> Type Description <code>Tuple[Tuple[NDArray, NDArray, NDArray], int]</code> <p>Normalized labels and number of classes.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def normalize_labels(\n        y_train: NDArray,\n        y_val: NDArray,\n        y_test: NDArray\n    ) -&gt; Tuple[Tuple[NDArray, NDArray, NDArray], int]:\n    \"\"\"\n    Normalize labels to integers in [0, n_classes).\n\n    Parameters\n    ----------\n    y_train : NDArray\n        Training labels.\n    y_val : NDArray\n        Validation labels.\n    y_test : NDArray\n        Test labels.\n\n    Returns\n    -------\n    Tuple[Tuple[NDArray, NDArray, NDArray], int]\n        Normalized labels and number of classes.\n    \"\"\"\n    # map labels to integers in [0, n_classes)\n    uniq_labels = np.unique(np.concatenate((y_train, y_val, y_test)))\n    n_classes = len(uniq_labels)\n    uniq_labels_idx_map = dict(zip(uniq_labels, range(n_classes)))\n    y_train = np.vectorize(lambda v: uniq_labels_idx_map[v])(y_train)\n    y_val = np.vectorize(lambda v: uniq_labels_idx_map[v])(y_val)\n    y_test = np.vectorize(lambda v: uniq_labels_idx_map[v])(y_test)\n    return (y_train, y_val, y_test), n_classes\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.predict_models_folds","title":"<code>predict_models_folds(partitions, hyperparams, bv_models_folds, foldnormparams)</code>","text":"<p>Predict and evaluate models across multiple folds.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <code>List[Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray]]</code> <p>List of data partitions for each fold. Each partition contains: (nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test)</p> required <code>hyperparams</code> <code>Dict[str, Any]</code> <p>Dictionary of hyperparameters for the models.</p> required <code>bv_models_folds</code> <code>List[Any]</code> <p>List of trained models for each fold.</p> required <code>foldnormparams</code> <code>List[Dict[str, Any]]</code> <p>List of normalization parameters for each fold.</p> required <p>Returns:</p> Type Description <code>Tuple[List[NDArray], Dict[str, List[float]]]</code> <p>A tuple containing: - List of predictions for each fold - Dictionary of evaluation metrics for each fold</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def predict_models_folds(\n        partitions: List[Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray]],\n        hyperparams: Dict[str, Any],\n        bv_models_folds: List[Any],\n        foldnormparams: List[Dict[str, Any]]\n    ) -&gt; Tuple[List[NDArray], Dict[str, List[float]]]:\n    \"\"\"\n    Predict and evaluate models across multiple folds.\n\n    Parameters\n    ----------\n    partitions : List[Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray]]\n        List of data partitions for each fold. Each partition contains:\n        (nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test)\n    hyperparams : Dict[str, Any]\n        Dictionary of hyperparameters for the models.\n    bv_models_folds : List[Any]\n        List of trained models for each fold.\n    foldnormparams : List[Dict[str, Any]]\n        List of normalization parameters for each fold.\n\n    Returns\n    -------\n    Tuple[List[NDArray], Dict[str, List[float]]]\n        A tuple containing:\n        - List of predictions for each fold\n        - Dictionary of evaluation metrics for each fold\n    \"\"\"\n    ohe = sklearn.preprocessing.OneHotEncoder()\n    bv_preds_folds = []\n    metrics_folds = dict()\n    for i, (nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test) in enumerate(partitions):\n        preprocessed_data = preprocess_data(hyperparams, ohe, nsv_train, nsv_val, nsv_test, bv_train, bv_val, bv_test, foldnormparams[i])\n        (X_train, y_train, X_val, y_val, X_test, y_test), (train_loader, val_loader, test_loader), fold_norm_params = preprocessed_data\n        model = bv_models_folds[i]\n\n        model.eval()\n        predictor = model if hyperparams['model'] != 'LSTM' else model.predict\n        metrics, bv_preds_fold = evaluate_model(hyperparams, ohe, predictor, X_test, y_test)\n        bv_preds_folds.append(bv_preds_fold)\n        if hyperparams['model_args']['args']['clf']:\n            if 'accuracy' not in metrics_folds:\n                metrics_folds['accuracy'] = []\n            metrics_folds['accuracy'].append(metrics['accuracy'])\n        else:\n            coeff_determination = metrics['coeff_determination']\n            rmse = metrics['rmse']\n            if 'coeff_determination' not in metrics_folds:\n                metrics_folds['coeff_determination'] = []\n                metrics_folds['rmse'] = []\n            metrics_folds['coeff_determination'].append(coeff_determination)\n            metrics_folds['rmse'].append(rmse)\n\n    return bv_preds_folds, metrics_folds\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.preprocess_data","title":"<code>preprocess_data(hyperparams, ohe, nsv_train, nsv_val, nsv_test, bv_train, bv_val, bv_test, foldnormparams=None)</code>","text":"<p>Preprocess the data for model training and evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>hyperparams</code> <code>Dict[str, Any]</code> <p>Dictionary containing hyperparameters.</p> required <code>ohe</code> <code>OneHotEncoder</code> <p>One-hot encoder for categorical variables.</p> required <code>nsv_train</code> <code>NDArray</code> <p>Neural state vectors for training.</p> required <code>nsv_val</code> <code>NDArray</code> <p>Neural state vectors for validation.</p> required <code>nsv_test</code> <code>NDArray</code> <p>Neural state vectors for testing.</p> required <code>bv_train</code> <code>NDArray</code> <p>Behavioral state vectors for training.</p> required <code>bv_val</code> <code>NDArray</code> <p>Behavioral state vectors for validation.</p> required <code>bv_test</code> <code>NDArray</code> <p>Behavioral state vectors for testing.</p> required <code>foldnormparams</code> <code>Optional[Dict[str, Any]]</code> <p>Normalization parameters for the current fold, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray], Tuple[DataLoader, DataLoader, DataLoader], Dict[str, Any]]</code> <p>Preprocessed data, data loaders, and normalization parameters.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def preprocess_data(\n        hyperparams: Dict[str, Any],\n        ohe: sklearn.preprocessing.OneHotEncoder,\n        nsv_train: NDArray,\n        nsv_val: NDArray,\n        nsv_test: NDArray,\n        bv_train: NDArray,\n        bv_val: NDArray,\n        bv_test: NDArray,\n        foldnormparams: Optional[Dict[str, Any]] = None\n    ) -&gt; Tuple[\n        Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray],\n        Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader],\n        Dict[str, Any]\n    ]:\n    \"\"\"\n    Preprocess the data for model training and evaluation.\n\n    Parameters\n    ----------\n    hyperparams : Dict[str, Any]\n        Dictionary containing hyperparameters.\n    ohe : OneHotEncoder\n        One-hot encoder for categorical variables.\n    nsv_train : NDArray\n        Neural state vectors for training.\n    nsv_val : NDArray\n        Neural state vectors for validation.\n    nsv_test : NDArray\n        Neural state vectors for testing.\n    bv_train : NDArray\n        Behavioral state vectors for training.\n    bv_val : NDArray\n        Behavioral state vectors for validation.\n    bv_test : NDArray\n        Behavioral state vectors for testing.\n    foldnormparams : Optional[Dict[str, Any]], optional\n        Normalization parameters for the current fold, by default None.\n\n    Returns\n    -------\n    Tuple[Tuple[NDArray, NDArray, NDArray, NDArray, NDArray, NDArray], Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader, torch.utils.data.DataLoader], Dict[str, Any]]\n        Preprocessed data, data loaders, and normalization parameters.\n    \"\"\"\n    bins_before = hyperparams['bins_before']\n    bins_current = hyperparams['bins_current']\n    bins_after = hyperparams['bins_after']\n    is_2D = nsv_train[0].ndim == 1\n    if hyperparams['model'] not in ('M2MLSTM', 'NDT'):\n        (\n            X_cov_train, X_flat_train, y_train,\n            ((X_cov_val, X_flat_val, y_val), (X_cov_test, X_flat_test, y_test)),\n            fold_norm_params\n        ) = normalize_format_trial_segs(\n            nsv_train, (nsv_val, nsv_test),\n            bv_train, (bv_val, bv_test), predict_bv=hyperparams['behaviors'],\n            bins_before=bins_before, bins_current=bins_current,\n            bins_after=bins_after, normparams=foldnormparams)\n        X_train = X_cov_train if hyperparams['model'] == 'LSTM' else X_flat_train\n        X_val = X_cov_val if hyperparams['model'] == 'LSTM' else X_flat_val\n        X_test = X_cov_test if hyperparams['model'] == 'LSTM' else X_flat_test\n\n        if hyperparams['model_args']['args']['clf']:\n            (y_train, y_val, y_test), n_classes = normalize_labels(y_train, y_val, y_test)\n            y_train = ohe.fit_transform(y_train).toarray()\n            y_val = ohe.transform(y_val).toarray()\n            y_test = ohe.transform(y_test).toarray()\n            hyperparams['model_args']['out_dim'] = n_classes\n            fold_norm_params['ohe'] = ohe\n\n        train_loader, val_loader, test_loader = minibatchify(\n            X_train, y_train, X_val, y_val, X_test, y_test,\n            seed=hyperparams['seed'], batch_size=hyperparams['batch_size'],\n            num_workers=hyperparams['num_workers'], modeltype=hyperparams['model'])\n        hyperparams['model_args']['in_dim'] = X_train.shape[-1]\n    else:\n        if is_2D:\n            nsv_train, bv_train = [nsv_train], [bv_train]\n            nsv_val, bv_val = [nsv_val], [bv_val]\n            nsv_test, bv_test = [nsv_test], [bv_test]\n        if type(bv_train[0]) is pd.DataFrame:\n            y_train = [y.values[:, hyperparams['behaviors']] for y in bv_train]\n        else:\n            y_train = [y[:, hyperparams['behaviors']] for y in bv_train]\n        nbins_per_tseg = [len(y) for y in y_train]  # number of time bins in each trial\n        tseg_bounds_train = np.cumsum([0] + nbins_per_tseg)\n        if type(bv_val[0]) is pd.DataFrame:\n            y_val = [y.values[:, hyperparams['behaviors']] for y in bv_val]\n        else:\n            y_val = [y[:, hyperparams['behaviors']] for y in bv_val]\n        nbins_per_tseg = [len(y) for y in y_val]\n        tseg_bounds_val = np.cumsum([0] + nbins_per_tseg)\n        if type(bv_test[0]) is pd.DataFrame:\n            y_test = [y.values[:, hyperparams['behaviors']] for y in bv_test]\n        else:\n            y_test = [y[:, hyperparams['behaviors']] for y in bv_test]\n        nbins_per_tseg = [len(y) for y in y_test]\n        tseg_bounds_test = np.cumsum([0] + nbins_per_tseg)\n\n        (\n            _, X_flat_train, y_train,\n            ((_, X_flat_val, y_val), (_, X_flat_test, y_test)),\n            fold_norm_params\n        ) = normalize_format_trial_segs(\n            nsv_train, (nsv_val, nsv_test),\n            bv_train, (bv_val, bv_test, bv_test), predict_bv=hyperparams['behaviors'],\n            bins_before=bins_before, bins_current=bins_current,\n            bins_after=bins_after, normparams=foldnormparams)\n        X_train = X_flat_train\n        X_val = X_flat_val\n        X_test = X_flat_test\n\n        if hyperparams['model_args']['args']['clf']:\n            (y_train, y_val, y_test), n_classes = normalize_labels(y_train, y_val, y_test)\n            y_train = ohe.fit_transform(y_train).toarray()\n            y_val = ohe.transform(y_val).toarray()\n            y_test = ohe.transform(y_test).toarray()\n            hyperparams['model_args']['out_dim'] = n_classes\n            fold_norm_params['ohe'] = ohe\n\n        X_train_tsegs, y_train_tsegs = [], []\n        X_val_tsegs, y_val_tsegs = [], []\n        X_test_tsegs, y_test_tsegs = [], []\n        for i in range(1, len(tseg_bounds_train)):\n            X_train_tsegs.append(X_train[tseg_bounds_train[i-1]:tseg_bounds_train[i]])\n            y_train_tsegs.append(y_train[tseg_bounds_train[i-1]:tseg_bounds_train[i]])\n        for i in range(1, len(tseg_bounds_val)):\n            X_val_tsegs.append(X_val[tseg_bounds_val[i-1]:tseg_bounds_val[i]])\n            y_val_tsegs.append(y_val[tseg_bounds_val[i-1]:tseg_bounds_val[i]])\n        for i in range(1, len(tseg_bounds_test)):\n            X_test_tsegs.append(X_test[tseg_bounds_test[i-1]:tseg_bounds_test[i]])\n            y_test_tsegs.append(y_test[tseg_bounds_test[i-1]:tseg_bounds_test[i]])\n\n        X_train, y_train = X_train_tsegs, y_train_tsegs\n        X_val, y_val = X_val_tsegs, y_val_tsegs\n        X_test, y_test = X_test_tsegs, y_test_tsegs\n\n        train_dataset = NSVDataset(X_train, y_train)\n        val_dataset = NSVDataset(X_val, y_val)\n        test_dataset = NSVDataset(X_test, y_test)\n\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset, shuffle=True, num_workers=hyperparams['num_workers'],\n            batch_size=1\n        )\n        val_loader = torch.utils.data.DataLoader(\n            val_dataset, shuffle=False, num_workers=hyperparams['num_workers'],\n            batch_size=1\n        )\n        test_loader = torch.utils.data.DataLoader(\n            test_dataset, shuffle=False, num_workers=hyperparams['num_workers'],\n            batch_size=1\n        )\n        hyperparams['model_args']['in_dim'] = X_train[0].shape[-1]\n\n    return (X_train, y_train, X_val, y_val, X_test, y_test), (train_loader, val_loader, test_loader), fold_norm_params\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.seed_worker","title":"<code>seed_worker(worker_id)</code>","text":"<p>Seed a worker with the given ID for reproducibility in data loading.</p> <p>Parameters:</p> Name Type Description Default <code>worker_id</code> <code>int</code> <p>The ID of the worker to be seeded.</p> required Notes <p>This function is used to ensure reproducibility when using multi-process data loading.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def seed_worker(worker_id: int) -&gt; None:\n    \"\"\"\n    Seed a worker with the given ID for reproducibility in data loading.\n\n    Parameters\n    ----------\n    worker_id : int\n        The ID of the worker to be seeded.\n\n    Notes\n    -----\n    This function is used to ensure reproducibility when using multi-process data loading.\n    \"\"\"\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.shuffle_nsv_intrialsegs","title":"<code>shuffle_nsv_intrialsegs(nsv_trialsegs)</code>","text":"<p>Shuffle neural state variables within trial segments.</p> <p>Parameters:</p> Name Type Description Default <code>nsv_trialsegs</code> <code>List[DataFrame]</code> <p>List of neural state variable trial segments.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Shuffled neural state variables.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def shuffle_nsv_intrialsegs(nsv_trialsegs: List[pd.DataFrame]) -&gt; NDArray:\n    \"\"\"\n    Shuffle neural state variables within trial segments.\n\n    Parameters\n    ----------\n    nsv_trialsegs : List[pd.DataFrame]\n        List of neural state variable trial segments.\n\n    Returns\n    -------\n    NDArray\n        Shuffled neural state variables.\n    \"\"\"\n    nsv_shuffled_intrialsegs = []\n    for nsv_tseg in nsv_trialsegs:\n        # shuffle the data\n        nsv_shuffled_intrialsegs.append(\n            nsv_tseg.sample(frac=1).reset_index(drop=True)\n        )\n    return np.asarray(nsv_shuffled_intrialsegs, dtype=object)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.train_model","title":"<code>train_model(partitions, hyperparams, resultspath=None, stop_partition=None)</code>","text":"<p>Train a DNN model on the given data partitions with in-built caching &amp; checkpointing.</p> <p>Parameters:</p> Name Type Description Default <code>partitions</code> <code>List[Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray]]</code> <p>K-fold partitions of the data with the following format: [(nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test), ...] Each element of the list is a tuple of numpy arrays containing the with pairs of neural state vectors and behavioral variables for the training, validation, and test sets. Each array has the shape (ntrials, nbins, nfeats) where nfeats is the number of neurons for the neural state vectors and number of behavioral features to be predicted for the behavioral variables.</p> required <code>hyperparams</code> <code>Dict[str, Any]</code> <p>Dictionary containing the hyperparameters for the model training.</p> required <code>resultspath</code> <code>Optional[str]</code> <p>Path to the directory where the trained models and logs will be saved.</p> <code>None</code> <code>stop_partition</code> <code>Optional[int]</code> <p>Index of the partition to stop training at. Only useful for debugging, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing the predicted behavioral variables for each fold, the trained models for each fold, the normalization parameters for each fold, and the evaluation metrics for each fold.</p> Notes <p>The hyperparameters dictionary should contain the following keys: - <code>model</code>: str, the type of the model to be trained. Multi-layer     Perceptron (MLP), Long Short-Term Memory (LSTM), many-to-many LSTM     (M2MLSTM), Transformer (NDT). - <code>model_args</code>: dict, the arguments to be passed to the model constructor.     The arguments should be in the format expected by the model constructor.     - <code>in_dim</code>: The number of input features.     - <code>out_dim</code>: The number of output features.     - <code>hidden_dims</code>: The number of hidden units each hidden layer of the         model. Can also take float values to specify the dropout rate.         - For LSTM and M2MLSTM, it should be a tuple of the hidden size,             the number of layers, and the dropout rate.             If the model is an MLP, it should be a list of hidden layer             sizes which can also take float values to specify the dropout             rate.         - If the model is an LSTM or M2MLSTM, it should be a list of the         hidden layer size, the number of layers, and the dropout rate.         - If the model is an NDT, it should be a list of the hidden layer             size, the number of layers, the number of attention heads, the             dropout rate for the encoder layer, and the dropout rate applied             before the decoder layer.     - <code>max_context_len</code>: The maximum context length for the transformer         model. Only used if the model is an NDT.     - <code>args</code>:         - <code>clf</code>: If True, the model is a classifier; otherwise, it is a             regressor.         - <code>activations</code>: The activation functions for each layer.         - <code>criterion</code>: The loss function to optimize.         - <code>epochs</code>: The number of complete passes through the training             dataset.         - <code>lr</code>: Controls how much to change the model in response to the             estimated error each time the model weights are updated. A             smaller value ensures stable convergence but may slow down             training, while a larger value speeds up training but risks             overshooting.         - <code>base_lr</code>: The initial learning rate for the learning rate             scheduler.         - <code>max_grad_norm</code>: The maximum norm of the gradients.         - <code>iters_to_accumulate</code>: The number of iterations to accumulate             gradients.         - <code>weight_decay</code>: The L2 regularization strength.         - <code>num_training_batches</code>: The number of training batches. If             None, the number of batches is calculated based on the batch             size and the length of the training data.         - <code>scheduler_step_size_multiplier</code>: The multiplier for the             learning rate scheduler step size. Higher values lead to             faster learning rate decay. - <code>bins_before</code>: int, the number of bins before the current bin to     include in the input data. - <code>bins_current</code>: int, the number of bins in the current time bin to     include in the input data. - <code>bins_after</code>: int, the number of bins after the current bin to include     in the input data. - <code>behaviors</code>: list, the indices of the columns of behavioral features     to be predicted. Selected behavioral variable must have homogenous     data types across all features (continuous for regression and     categorical for classification) - <code>batch_size</code>: int, the number of training examples utilized in one     iteration. Larger batch sizes offer stable gradient estimates but     require more memory, while smaller batches introduce noise that can     help escape local minima.     - When using M2MLSTM or NDT and input trials are of inconsistents         lengths, the batch size should be set to 1.     - M2MLSTM does not support batch_size != 1. - <code>num_workers</code>: int, The number of parallel processes to use for data     loading. Increasing the number of workers can speed up data loading     but may lead to memory issues. Too many workers can also slow down     the training process due to contention for resources. - <code>device</code>: str, the device to use for training. Should be 'cuda' or     'cpu'. - <code>seed</code>: int, the random seed for reproducibility.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def train_model(\n    partitions: List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]],\n    hyperparams: Dict[str, Any],\n    resultspath: Optional[str] = None,\n    stop_partition: Optional[int] = None\n) -&gt; Tuple[List[np.ndarray], List[Any], List[Dict[str, Any]], Dict[str, List[float]]]:\n    \"\"\"\n    Train a DNN model on the given data partitions with in-built caching &amp; checkpointing.\n\n    Parameters\n    ----------\n    partitions : List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]\n        K-fold partitions of the data with the following format:\n        [(nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test), ...]\n        Each element of the list is a tuple of numpy arrays containing the with\n        pairs of neural state vectors and behavioral variables for the training,\n        validation, and test sets. Each array has the shape\n        (ntrials, nbins, nfeats) where nfeats is the number of neurons for the\n        neural state vectors and number of behavioral features to be predicted\n        for the behavioral variables.\n    hyperparams : Dict[str, Any]\n        Dictionary containing the hyperparameters for the model training.\n    resultspath : Optional[str], default=None\n        Path to the directory where the trained models and logs will be saved.\n    stop_partition : Optional[int], default=None\n        Index of the partition to stop training at. Only useful for debugging,\n        by default None\n\n    Returns\n    -------\n    tuple\n        Tuple containing the predicted behavioral variables for each fold,\n        the trained models for each fold, the normalization parameters for each\n        fold, and the evaluation metrics for each fold.\n\n    Notes\n    -----\n    The hyperparameters dictionary should contain the following keys:\n    - `model`: str, the type of the model to be trained. Multi-layer\n        Perceptron (MLP), Long Short-Term Memory (LSTM), many-to-many LSTM\n        (M2MLSTM), Transformer (NDT).\n    - `model_args`: dict, the arguments to be passed to the model constructor.\n        The arguments should be in the format expected by the model constructor.\n        - `in_dim`: The number of input features.\n        - `out_dim`: The number of output features.\n        - `hidden_dims`: The number of hidden units each hidden layer of the\n            model. Can also take float values to specify the dropout rate.\n            - For LSTM and M2MLSTM, it should be a tuple of the hidden size,\n                the number of layers, and the dropout rate.\n                If the model is an MLP, it should be a list of hidden layer\n                sizes which can also take float values to specify the dropout\n                rate.\n            - If the model is an LSTM or M2MLSTM, it should be a list of the\n            hidden layer size, the number of layers, and the dropout rate.\n            - If the model is an NDT, it should be a list of the hidden layer\n                size, the number of layers, the number of attention heads, the\n                dropout rate for the encoder layer, and the dropout rate applied\n                before the decoder layer.\n        - `max_context_len`: The maximum context length for the transformer\n            model. Only used if the model is an NDT.\n        - `args`:\n            - `clf`: If True, the model is a classifier; otherwise, it is a\n                regressor.\n            - `activations`: The activation functions for each layer.\n            - `criterion`: The loss function to optimize.\n            - `epochs`: The number of complete passes through the training\n                dataset.\n            - `lr`: Controls how much to change the model in response to the\n                estimated error each time the model weights are updated. A\n                smaller value ensures stable convergence but may slow down\n                training, while a larger value speeds up training but risks\n                overshooting.\n            - `base_lr`: The initial learning rate for the learning rate\n                scheduler.\n            - `max_grad_norm`: The maximum norm of the gradients.\n            - `iters_to_accumulate`: The number of iterations to accumulate\n                gradients.\n            - `weight_decay`: The L2 regularization strength.\n            - `num_training_batches`: The number of training batches. If\n                None, the number of batches is calculated based on the batch\n                size and the length of the training data.\n            - `scheduler_step_size_multiplier`: The multiplier for the\n                learning rate scheduler step size. Higher values lead to\n                faster learning rate decay.\n    - `bins_before`: int, the number of bins before the current bin to\n        include in the input data.\n    - `bins_current`: int, the number of bins in the current time bin to\n        include in the input data.\n    - `bins_after`: int, the number of bins after the current bin to include\n        in the input data.\n    - `behaviors`: list, the indices of the columns of behavioral features\n        to be predicted. Selected behavioral variable must have homogenous\n        data types across all features (continuous for regression and\n        categorical for classification)\n    - `batch_size`: int, the number of training examples utilized in one\n        iteration. Larger batch sizes offer stable gradient estimates but\n        require more memory, while smaller batches introduce noise that can\n        help escape local minima.\n        - When using M2MLSTM or NDT and input trials are of inconsistents\n            lengths, the batch size should be set to 1.\n        - M2MLSTM does not support batch_size != 1.\n    - `num_workers`: int, The number of parallel processes to use for data\n        loading. Increasing the number of workers can speed up data loading\n        but may lead to memory issues. Too many workers can also slow down\n        the training process due to contention for resources.\n    - `device`: str, the device to use for training. Should be 'cuda' or\n        'cpu'.\n    - `seed`: int, the random seed for reproducibility.\n    \"\"\"\n    ohe = sklearn.preprocessing.OneHotEncoder()\n    bv_preds_folds = []\n    bv_models_folds = []\n    norm_params_folds = []\n    metrics_folds = dict() # dict with keys 'accuracy', 'coeff_determination', 'rmse' and values of length number of folds\n    for i, (nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test) in enumerate(partitions):\n        # shuffle nsv bins in between tsegs to generate baseline distribution for vector dev plots\n        preprocessed_data = preprocess_data(hyperparams, ohe, nsv_train, nsv_val, nsv_test, bv_train, bv_val, bv_test)\n        (X_train, y_train, X_val, y_val, X_test, y_test), (train_loader, val_loader, test_loader), fold_norm_params = preprocessed_data\n        hyperparams['model_args']['args']['num_training_batches'] = len(train_loader)\n\n        decoder, model = create_model(hyperparams)\n\n        hyperparams_cp = copy.deepcopy(hyperparams)\n        del hyperparams_cp['model_args']['args']['epochs']\n        del hyperparams_cp['model_args']['args']['num_training_batches']\n        model_cache_name = zlib.crc32(str(hyperparams_cp).encode('utf-8'))\n        best_ckpt_path = None\n        if resultspath is not None:\n            model_cache_path = os.path.join(resultspath, 'models', str(model_cache_name))\n            best_ckpt_name_file = os.path.join(model_cache_path, f'{i}-best_model.txt')\n            if os.path.exists(best_ckpt_name_file):\n                with open(best_ckpt_name_file, 'r') as f:\n                    best_ckpt_path = f.read()\n\n        lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n        callbacks = [lr_monitor]\n        if resultspath is not None:\n            checkpoint_callback = pl.callbacks.ModelCheckpoint(\n                save_top_k=1,\n                monitor='val_loss',\n                dirpath=model_cache_path,\n                filename=f'{i}' + '-{epoch:02d}-{val_loss:.2f}',\n            )\n            callbacks.append(checkpoint_callback)\n        logger = pl.loggers.TensorBoardLogger(\n            save_dir='logs',\n            name=f'{model_cache_name}-{i}',\n        )\n        pl.seed_everything(hyperparams['seed'], workers=True)\n        trainer = pl.Trainer(\n            accelerator=hyperparams['device'], devices=1,\n            max_epochs=hyperparams['model_args']['args']['epochs'],\n            gradient_clip_val=hyperparams['model_args']['args']['max_grad_norm'],\n            accumulate_grad_batches=hyperparams['model_args']['args']['iters_to_accumulate'],\n            logger=logger,\n            callbacks=callbacks,\n            enable_progress_bar=False,\n            log_every_n_steps=5,\n            reload_dataloaders_every_n_epochs=1\n        )\n        trainer.fit(\n            model, train_loader, val_loader,\n            ckpt_path=best_ckpt_path\n        )\n        if resultspath is not None:\n            with open(best_ckpt_name_file, 'w') as f:\n                f.write(checkpoint_callback.best_model_path)\n        model.eval()\n        trainer.test(model, test_loader)\n        predictor = model if hyperparams['model'] != 'LSTM' else model.predict\n\n        metrics, bv_preds_fold = evaluate_model(hyperparams, ohe, predictor, X_test, y_test)\n        bv_preds_folds.append(bv_preds_fold)\n        bv_models_folds.append(model)\n        norm_params_folds.append(copy.deepcopy(fold_norm_params))\n        if hyperparams['model_args']['args']['clf']:\n            print('Accuracy:', metrics['accuracy'])\n            if 'accuracy' not in metrics_folds:\n                metrics_folds['accuracy'] = []\n            metrics_folds['accuracy'].append(metrics['accuracy'])\n        else:\n            coeff_determination = metrics['coeff_determination']\n            rmse = metrics['rmse']\n            print('Variance weighed avg. coefficient of determination:',\n                coeff_determination)\n            print('RMSE:', rmse)\n\n            if 'coeff_determination' not in metrics_folds:\n                metrics_folds['coeff_determination'] = []\n                metrics_folds['rmse'] = []\n            metrics_folds['coeff_determination'].append(coeff_determination)\n            metrics_folds['rmse'].append(rmse)\n\n        if stop_partition is not None and i == stop_partition:\n            break\n    return bv_preds_folds, bv_models_folds, norm_params_folds, metrics_folds\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/pipeline/#neuro_py.ensemble.decoding.pipeline.zscore_trial_segs","title":"<code>zscore_trial_segs(train, rest_feats=None, normparams=None)</code>","text":"<p>Z-score trial segments.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>NDArray</code> <p>Training data.</p> required <code>rest_feats</code> <code>Optional[List[NDArray]]</code> <p>Rest features, by default None.</p> <code>None</code> <code>normparams</code> <code>Optional[Dict[str, Any]]</code> <p>Normalization parameters, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[NDArray, List[NDArray], Dict[str, Any]]</code> <p>Normalized train data, normalized rest features, and normalization parameters.</p> Source code in <code>neuro_py/ensemble/decoding/pipeline.py</code> <pre><code>def zscore_trial_segs(\n    train: NDArray,\n    rest_feats: Optional[List[NDArray]] = None,\n    normparams: Optional[Dict[str, Any]] = None\n) -&gt; Tuple[NDArray, List[NDArray], Dict[str, Any]]:\n    \"\"\"\n    Z-score trial segments.\n\n    Parameters\n    ----------\n    train : NDArray\n        Training data.\n    rest_feats : Optional[List[NDArray]], optional\n        Rest features, by default None.\n    normparams : Optional[Dict[str, Any]], optional\n        Normalization parameters, by default None.\n\n    Returns\n    -------\n    Tuple[NDArray, List[NDArray], Dict[str, Any]]\n        Normalized train data, normalized rest features, and normalization parameters.\n    \"\"\"\n    is_2D = train[0].ndim == 1\n    concat_train = train if is_2D else np.concatenate(train).astype(float)\n    train_mean = (\n        normparams['X_train_mean'] if normparams is not None\n        else bn.nanmean(concat_train, axis=0)\n    )\n    train_std = normparams['X_train_std'] if normparams is not None else bn.nanstd(concat_train, axis=0)\n\n    train_notnan_cols = train_std != 0\n    train_nan_cols = ~train_notnan_cols\n    if is_2D:\n        normed_train = np.divide(train-train_mean, train_std, where=train_notnan_cols)\n        # if train is not jagged, it gets converted completely to object\n        # np.ndarray. Hence, cannot exclusively use normed_train.loc\n        if isinstance(normed_train, pd.DataFrame):\n            normed_train.loc[:, train_nan_cols] = 0\n        else:\n            normed_train[:, train_nan_cols] = 0\n    else:\n        normed_train = np.empty_like(train)\n        for i, nsvstseg in enumerate(train):\n            zscored = np.divide(nsvstseg-train_mean, train_std, where=train_notnan_cols)\n            if isinstance(zscored, pd.DataFrame):\n                zscored.loc[:, train_nan_cols] = 0\n            else:\n                zscored[:, train_nan_cols] = 0\n            normed_train[i] = zscored\n\n    normed_rest_feats = []\n    if rest_feats is not None:\n        for feats in rest_feats:\n            if is_2D:\n                normed_feats = np.divide(feats-train_mean, train_std, where=train_notnan_cols)\n                if isinstance(normed_feats, pd.DataFrame):\n                    normed_feats.loc[:, train_nan_cols] = 0\n                else:\n                    normed_feats[:, train_nan_cols] = 0\n                normed_rest_feats.append(normed_feats)\n            else:\n                normed_feats = np.empty_like(feats)\n                for i, trialSegROI in enumerate(feats):\n                    zscored = np.divide(feats[i]-train_mean, train_std, where=train_notnan_cols)\n                    if isinstance(zscored, pd.DataFrame):\n                        zscored.loc[:, train_nan_cols] = 0\n                    else:\n                        zscored[:, train_nan_cols] = 0\n                    normed_feats[i] = zscored\n                normed_rest_feats.append(normed_feats)\n\n    return normed_train, normed_rest_feats, dict(\n        X_train_mean=train_mean, X_train_std=train_std,\n        X_train_notnan_mask=train_notnan_cols,\n    )\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/preprocess/","title":"neuro_py.ensemble.decoding.preprocess","text":""},{"location":"reference/neuro_py/ensemble/decoding/preprocess/#neuro_py.ensemble.decoding.preprocess.partition_indices","title":"<code>partition_indices(folds)</code>","text":"<p>Partition indices into train, validation, and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>folds</code> <code>List[ndarray]</code> <p>Indices for each fold.</p> required <p>Returns:</p> Type Description <code>List[Tuple[ndarray, ndarray, ndarray]]</code> <p>Train, validation, and test indices.</p> Source code in <code>neuro_py/ensemble/decoding/preprocess.py</code> <pre><code>def partition_indices(folds: List[np.ndarray]) -&gt; List[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"\n    Partition indices into train, validation, and test sets.\n\n    Parameters\n    ----------\n    folds : List[np.ndarray]\n        Indices for each fold.\n\n    Returns\n    -------\n    List[Tuple[np.ndarray, np.ndarray, np.ndarray]]\n        Train, validation, and test indices.\n    \"\"\"\n    partition_mask = np.zeros(len(folds), dtype=int)\n    partition_mask[0:2] = (2, 1)\n    folds_arr = np.asarray(folds, dtype=object)\n\n    partitions_indices = []\n    for i in range(len(folds)):\n        curr_pmask = np.roll(partition_mask, i)\n        train_indices = np.concatenate(folds_arr[curr_pmask == 0]).tolist()\n        val_indices = np.concatenate(folds_arr[curr_pmask == 1]).tolist()\n        test_indices = np.concatenate(folds_arr[curr_pmask == 2]).tolist()\n\n        partitions_indices.append((train_indices, val_indices, test_indices))\n    return partitions_indices\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/preprocess/#neuro_py.ensemble.decoding.preprocess.partition_sets","title":"<code>partition_sets(partitions_indices, nsv_trial_segs, bv_trial_segs)</code>","text":"<p>Partition neural state vectors and behavioral variables into train, validation, and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>partitions_indices</code> <code>List[Tuple[ndarray, ndarray, ndarray]]</code> <p>List of tuples containing indices of divided trials into train, validation, and test sets.</p> required <code>nsv_trial_segs</code> <code>Union[ndarray, DataFrame]</code> <p>Neural state vectors for each trial. Shape: [n_trials, n_timepoints, n_neurons] or [n_timepoints, n_neurons]</p> required <code>bv_trial_segs</code> <code>Union[ndarray, DataFrame]</code> <p>Behavioral variables for each trial. Shape: [n_trials, n_timepoints, n_bvars] or [n_timepoints, n_bvars]</p> required <p>Returns:</p> Type Description <code>List[Tuple[ndarray, ndarray, ndarray, ndarray, ndarray, ndarray]]</code> <p>List of tuples containing train, validation, and test sets for neural state vectors and behavioral variables.</p> Source code in <code>neuro_py/ensemble/decoding/preprocess.py</code> <pre><code>def partition_sets(\n    partitions_indices: List[Tuple[np.ndarray, np.ndarray, np.ndarray]],\n    nsv_trial_segs: Union[np.ndarray, pd.DataFrame],\n    bv_trial_segs: Union[np.ndarray, pd.DataFrame]\n) -&gt; List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"\n    Partition neural state vectors and behavioral variables into train,\n    validation, and test sets.\n\n    Parameters\n    ----------\n    partitions_indices : List[Tuple[np.ndarray, np.ndarray, np.ndarray]]\n        List of tuples containing indices of divided trials into train,\n        validation, and test sets.\n    nsv_trial_segs : Union[np.ndarray, pd.DataFrame]\n        Neural state vectors for each trial.\n        Shape: [n_trials, n_timepoints, n_neurons] or [n_timepoints, n_neurons]\n    bv_trial_segs : Union[np.ndarray, pd.DataFrame]\n        Behavioral variables for each trial.\n        Shape: [n_trials, n_timepoints, n_bvars] or [n_timepoints, n_bvars]\n\n    Returns\n    -------\n    List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]\n        List of tuples containing train, validation, and test sets for neural\n        state vectors and behavioral variables.\n    \"\"\"\n    partitions = []\n    is_2D = nsv_trial_segs[0].ndim == 1\n    for (train_indices, val_indices, test_indices) in partitions_indices:\n        if is_2D:\n            if isinstance(nsv_trial_segs, pd.DataFrame):\n                nsv_trial_segs = nsv_trial_segs.loc\n                bv_trial_segs = bv_trial_segs.loc\n            train = nsv_trial_segs[train_indices]\n            val = nsv_trial_segs[val_indices]\n            test = nsv_trial_segs[test_indices]\n            train_bv = bv_trial_segs[train_indices]\n            val_bv = bv_trial_segs[val_indices]\n            test_bv = bv_trial_segs[test_indices]\n        else:\n            train = np.take(nsv_trial_segs, train_indices, axis=0)\n            val = np.take(nsv_trial_segs, val_indices, axis=0)\n            test = np.take(nsv_trial_segs, test_indices, axis=0)\n            train_bv = np.take(bv_trial_segs, train_indices, axis=0)\n            val_bv = np.take(bv_trial_segs, val_indices, axis=0)\n            test_bv = np.take(bv_trial_segs, test_indices, axis=0)\n\n        partitions.append((train, train_bv, val, val_bv, test, test_bv))\n    return partitions\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/preprocess/#neuro_py.ensemble.decoding.preprocess.split_data","title":"<code>split_data(trial_nsvs, splitby, trainsize=0.8, seed=0)</code>","text":"<p>Split data into stratified folds.</p> <p>Parameters:</p> Name Type Description Default <code>trial_nsvs</code> <code>ndarray</code> <p>Neural state vectors for trials.</p> required <code>splitby</code> <code>ndarray</code> <p>Labels for stratification.</p> required <code>trainsize</code> <code>float</code> <p>Proportion of data to use for training, by default 0.8</p> <code>0.8</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility, by default 0</p> <code>0</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List of indices for each fold.</p> Source code in <code>neuro_py/ensemble/decoding/preprocess.py</code> <pre><code>def split_data(trial_nsvs: np.ndarray, splitby: np.ndarray, trainsize: float = 0.8, seed: int = 0) -&gt; List[np.ndarray]:\n    \"\"\"\n    Split data into stratified folds.\n\n    Parameters\n    ----------\n    trial_nsvs : np.ndarray\n        Neural state vectors for trials.\n    splitby : np.ndarray\n        Labels for stratification.\n    trainsize : float, optional\n        Proportion of data to use for training, by default 0.8\n    seed : int, optional\n        Random seed for reproducibility, by default 0\n\n    Returns\n    -------\n    List[np.ndarray]\n        List of indices for each fold.\n    \"\"\"\n    n_splits = int(np.round(1 / ((1 - trainsize) / 2)))\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    folds = [fold_indices for _, fold_indices in skf.split(trial_nsvs, splitby)]\n    return folds\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/transformer/","title":"neuro_py.ensemble.decoding.transformer","text":""},{"location":"reference/neuro_py/ensemble/decoding/transformer/#neuro_py.ensemble.decoding.transformer.NDT","title":"<code>NDT</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Transformer encoder-based dynamical systems decoder.</p> <p>This class implements a Transformer-based decoder trained on MLM loss. It returns loss and predicted rates.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Dimensionality of input data, by default 100</p> <code>100</code> <code>out_dim</code> <code>int</code> <p>Number of output columns, by default 2</p> <code>2</code> <code>hidden_dims</code> <code>Tuple[int]</code> <p>Architectural parameters of the model (dim_feedforward, num_layers, nhead, dropout, rate_dropout),  by default [400, 1, 1, 0.0, 0.0]</p> <code>(400, 1, 1, 0.0, 0.0)</code> <code>max_context_len</code> <code>int</code> <p>Maximum context length, by default 2</p> <code>2</code> <code>args</code> <code>Optional[Dict]</code> <p>Dictionary containing the hyperparameters of the model, by default None</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>pos_encoder</code> <code>PositionalEncoding</code> <p>Positional encoding module</p> <code>transformer_encoder</code> <code>TransformerEncoder</code> <p>Transformer encoder module</p> <code>rate_dropout</code> <code>Dropout</code> <p>Dropout layer for rates</p> <code>decoder</code> <code>Sequential</code> <p>Decoder network</p> <code>src_mask</code> <code>Dict[str, Tensor]</code> <p>Dictionary to store source masks for different devices</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>class NDT(L.LightningModule):\n    \"\"\"\n    Transformer encoder-based dynamical systems decoder.\n\n    This class implements a Transformer-based decoder trained on MLM loss.\n    It returns loss and predicted rates.\n\n    Parameters\n    ----------\n    in_dim : int, optional\n        Dimensionality of input data, by default 100\n    out_dim : int, optional\n        Number of output columns, by default 2\n    hidden_dims : Tuple[int], optional\n        Architectural parameters of the model\n        (dim_feedforward, num_layers, nhead, dropout, rate_dropout), \n        by default [400, 1, 1, 0.0, 0.0]\n    max_context_len : int, optional\n        Maximum context length, by default 2\n    args : Optional[Dict], optional\n        Dictionary containing the hyperparameters of the model, by default None\n\n    Attributes\n    ----------\n    pos_encoder : PositionalEncoding\n        Positional encoding module\n    transformer_encoder : nn.TransformerEncoder\n        Transformer encoder module\n    rate_dropout : nn.Dropout\n        Dropout layer for rates\n    decoder : nn.Sequential\n        Decoder network\n    src_mask : Dict[str, torch.Tensor]\n        Dictionary to store source masks for different devices\n    \"\"\"\n    def __init__(self, in_dim: int = 100, out_dim: int = 2,\n                 hidden_dims: Tuple[int] = (400, 1, 1, 0.0, 0.0),\n                 max_context_len: int = 2, args: Optional[Dict] = None):\n        super().__init__()\n        self.save_hyperparameters()\n        self.max_context_len = max_context_len\n        self.in_dim = in_dim\n        self.args = args if args is not None else {}\n        activations = nn.CELU if self.args.get('activations') is None else self.args['activations']\n\n        self.src_mask: Dict[str, torch.Tensor] = {}\n\n        self.pos_encoder = PositionalEncoding(in_dim, max_context_len, self.args)\n\n        encoder_lyr = nn.TransformerEncoderLayer(\n            in_dim,\n            nhead=hidden_dims[2],\n            dim_feedforward=hidden_dims[0],\n            dropout=hidden_dims[3],\n            activation=nn.functional.relu\n        )\n\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_lyr, hidden_dims[1], nn.LayerNorm(in_dim))\n\n        self.rate_dropout = nn.Dropout(hidden_dims[4])\n\n        self.decoder = nn.Sequential(\n            nn.Linear(in_dim, 16), activations(), nn.Linear(16, out_dim)\n        )\n\n        self._init_params()\n\n    def _init_params(self) -&gt; None:\n        \"\"\"Initialize the parameters of the decoder.\"\"\"\n        def init_params(m: nn.Module) -&gt; None:\n            if isinstance(m, nn.Linear):\n                torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n                if m.bias is not None:\n                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                    bound = 1 / np.sqrt(fan_in)\n                    nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n        self.decoder.apply(init_params)\n\n    def forward(self, x: torch.Tensor, mask_labels: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input data of shape (batch_size, seq_len, in_dim)\n        mask_labels : Optional[torch.Tensor], optional\n            Masking labels for the input data, by default None\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor of shape (batch_size, seq_len, out_dim)\n        \"\"\"\n        x = x.permute(1, 0, 2)  # LxBxN\n        x = self.pos_encoder(x)\n        x_mask = self._get_or_generate_context_mask(x)\n        z = self.transformer_encoder(x, x_mask)\n        z = self.rate_dropout(z)\n        out = self.decoder(z).permute(1, 0, 2)  # B x L x out_dim\n        if self.args.get('clf', False):\n            out = F.log_softmax(out, dim=-1)\n        return out\n\n    def _get_or_generate_context_mask(self, src: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Get or generate the context mask for the input tensor.\n\n        Parameters\n        ----------\n        src : torch.Tensor\n            Input tensor\n\n        Returns\n        -------\n        torch.Tensor\n            Context mask for the input tensor\n        \"\"\"\n        context_forward = 4\n        size = src.size(0)  # T\n        mask = (torch.triu(torch.ones(size, size, device=src.device), diagonal=-context_forward) == 1).transpose(0, 1)\n        mask = mask.float()\n        self.src_mask[str(src.device)] = mask\n        return self.src_mask[str(src.device)]\n\n    def _step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Perform a single step (forward pass + loss calculation).\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        xs, ys = batch\n        outs = self(xs)\n        loss = self.args['criterion'](outs, ys)\n        return loss\n\n    def training_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for training step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for validation step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('val_loss', loss)\n        return loss\n\n    def test_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Lightning method for test step.\n\n        Parameters\n        ----------\n        batch : tuple\n            Batch of input data and labels\n        batch_idx : int\n            Index of the current batch\n\n        Returns\n        -------\n        torch.Tensor\n            Computed loss\n        \"\"\"\n        loss = self._step(batch, batch_idx)\n        self.log('test_loss', loss)\n        return loss\n\n    def configure_optimizers(self) -&gt; tuple:\n        \"\"\"\n        Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        tuple\n            List of optimizers and a list of scheduler configurations\n        \"\"\"\n        optimizer = torch.optim.AdamW(\n            self.parameters(), weight_decay=self.args['weight_decay'])\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer, max_lr=self.args['lr'],\n            epochs=self.args['epochs'],\n            total_steps=self.trainer.estimated_stepping_batches\n        )\n        lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n        return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/transformer/#neuro_py.ensemble.decoding.transformer.NDT._get_or_generate_context_mask","title":"<code>_get_or_generate_context_mask(src)</code>","text":"<p>Get or generate the context mask for the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Context mask for the input tensor</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def _get_or_generate_context_mask(self, src: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Get or generate the context mask for the input tensor.\n\n    Parameters\n    ----------\n    src : torch.Tensor\n        Input tensor\n\n    Returns\n    -------\n    torch.Tensor\n        Context mask for the input tensor\n    \"\"\"\n    context_forward = 4\n    size = src.size(0)  # T\n    mask = (torch.triu(torch.ones(size, size, device=src.device), diagonal=-context_forward) == 1).transpose(0, 1)\n    mask = mask.float()\n    self.src_mask[str(src.device)] = mask\n    return self.src_mask[str(src.device)]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/transformer/#neuro_py.ensemble.decoding.transformer.NDT._init_params","title":"<code>_init_params()</code>","text":"<p>Initialize the parameters of the decoder.</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def _init_params(self) -&gt; None:\n    \"\"\"Initialize the parameters of the decoder.\"\"\"\n    def init_params(m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n            if m.bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n                bound = 1 / np.sqrt(fan_in)\n                nn.init.uniform_(m.bias, -bound, bound)  # LeCunn init\n    self.decoder.apply(init_params)\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/transformer/#neuro_py.ensemble.decoding.transformer.NDT._step","title":"<code>_step(batch, batch_idx)</code>","text":"<p>Perform a single step (forward pass + loss calculation).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def _step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Perform a single step (forward pass + loss calculation).\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    xs, ys = batch\n    outs = self(xs)\n    loss = self.args['criterion'](outs, ys)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/transformer/#neuro_py.ensemble.decoding.transformer.NDT.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>tuple</code> <p>List of optimizers and a list of scheduler configurations</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def configure_optimizers(self) -&gt; tuple:\n    \"\"\"\n    Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    tuple\n        List of optimizers and a list of scheduler configurations\n    \"\"\"\n    optimizer = torch.optim.AdamW(\n        self.parameters(), weight_decay=self.args['weight_decay'])\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=self.args['lr'],\n        epochs=self.args['epochs'],\n        total_steps=self.trainer.estimated_stepping_batches\n    )\n    lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n    return [optimizer], [lr_scheduler]\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/transformer/#neuro_py.ensemble.decoding.transformer.NDT.forward","title":"<code>forward(x, mask_labels=None)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data of shape (batch_size, seq_len, in_dim)</p> required <code>mask_labels</code> <code>Optional[Tensor]</code> <p>Masking labels for the input data, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor of shape (batch_size, seq_len, out_dim)</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def forward(self, x: torch.Tensor, mask_labels: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input data of shape (batch_size, seq_len, in_dim)\n    mask_labels : Optional[torch.Tensor], optional\n        Masking labels for the input data, by default None\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (batch_size, seq_len, out_dim)\n    \"\"\"\n    x = x.permute(1, 0, 2)  # LxBxN\n    x = self.pos_encoder(x)\n    x_mask = self._get_or_generate_context_mask(x)\n    z = self.transformer_encoder(x, x_mask)\n    z = self.rate_dropout(z)\n    out = self.decoder(z).permute(1, 0, 2)  # B x L x out_dim\n    if self.args.get('clf', False):\n        out = F.log_softmax(out, dim=-1)\n    return out\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/transformer/#neuro_py.ensemble.decoding.transformer.NDT.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Lightning method for test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def test_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for test step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('test_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/transformer/#neuro_py.ensemble.decoding.transformer.NDT.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Lightning method for training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def training_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for training step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('train_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/transformer/#neuro_py.ensemble.decoding.transformer.NDT.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Lightning method for validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>Batch of input data and labels</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed loss</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def validation_step(self, batch: tuple, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Lightning method for validation step.\n\n    Parameters\n    ----------\n    batch : tuple\n        Batch of input data and labels\n    batch_idx : int\n        Index of the current batch\n\n    Returns\n    -------\n    torch.Tensor\n        Computed loss\n    \"\"\"\n    loss = self._step(batch, batch_idx)\n    self.log('val_loss', loss)\n    return loss\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/transformer/#neuro_py.ensemble.decoding.transformer.PositionalEncoding","title":"<code>PositionalEncoding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Positional Encoding module for Transformer models.</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input dimension of the model</p> required <code>max_context_len</code> <code>int</code> <p>Maximum context length</p> required <code>args</code> <code>Dict</code> <p>Additional arguments (not used in this implementation)</p> required <p>Attributes:</p> Name Type Description <code>pe</code> <code>Tensor</code> <p>Positional encoding tensor</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>class PositionalEncoding(nn.Module):\n    \"\"\"\n    Positional Encoding module for Transformer models.\n\n    Parameters\n    ----------\n    in_dim : int\n        Input dimension of the model\n    max_context_len : int\n        Maximum context length\n    args : Dict\n        Additional arguments (not used in this implementation)\n\n    Attributes\n    ----------\n    pe : torch.Tensor\n        Positional encoding tensor\n    \"\"\"\n    def __init__(self, in_dim: int, max_context_len: int, args: Dict):\n        super().__init__()\n        pe = torch.zeros(max_context_len, in_dim)\n        position = torch.arange(0, max_context_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, in_dim, 2).float() * (-np.log(1e4) / in_dim))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)  # t x 1 x d\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Add positional encoding to the input tensor.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape (seq_len, batch_size, in_dim)\n\n        Returns\n        -------\n        torch.Tensor\n            Input tensor with added positional encoding\n        \"\"\"\n        self.pe = self.pe.to(x.device)\n        x = x + self.pe[:x.size(0), :]  # t x 1 x d, # t x b x d\n        return x\n</code></pre>"},{"location":"reference/neuro_py/ensemble/decoding/transformer/#neuro_py.ensemble.decoding.transformer.PositionalEncoding.forward","title":"<code>forward(x)</code>","text":"<p>Add positional encoding to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (seq_len, batch_size, in_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Input tensor with added positional encoding</p> Source code in <code>neuro_py/ensemble/decoding/transformer.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Add positional encoding to the input tensor.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (seq_len, batch_size, in_dim)\n\n    Returns\n    -------\n    torch.Tensor\n        Input tensor with added positional encoding\n    \"\"\"\n    self.pe = self.pe.to(x.device)\n    x = x + self.pe[:x.size(0), :]  # t x 1 x d, # t x b x d\n    return x\n</code></pre>"},{"location":"reference/neuro_py/io/","title":"neuro_py.io","text":""},{"location":"reference/neuro_py/io/#neuro_py.io.LFPLoader","title":"<code>LFPLoader</code>","text":"<p>               Bases: <code>object</code></p> <p>Simple class to load LFP or wideband data from a recording folder.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the recording folder.</p> required <code>channels</code> <code>Union[int, list, None]</code> <p>Channel number or list of channel numbers, by default None (load all channels memmap).</p> <code>None</code> <code>ext</code> <code>str</code> <p>File extension, by default \"lfp\".</p> <code>'lfp'</code> <code>epoch</code> <code>Union[ndarray, EpochArray, None]</code> <p>Epoch array or ndarray, by default None (load all data).</p> <code>None</code> <p>Returns:</p> Type Description <code>AnalogSignalArray</code> <p>Analog signal array of shape (n_channels, n_samples).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # load lfp file\n&gt;&gt;&gt; basepath = r\"X:/data/Barrage/NN10/day10\"\n&gt;&gt;&gt; lfp = loading.LFPLoader(basepath,ext=\"lfp\")\n&gt;&gt;&gt; lfp\n    &lt;AnalogSignalArray at 0x25ba1576640: 128 signals&gt; for a total of 5:33:58:789 hours\n</code></pre> <pre><code>&gt;&gt;&gt; # Loading dat file\n&gt;&gt;&gt; dat = loading.LFPLoader(basepath,ext=\"dat\")\n&gt;&gt;&gt; dat\n    &lt;AnalogSignalArray at 0x25ba4fedc40: 128 signals&gt; for a total of 5:33:58:790 hours\n&gt;&gt;&gt; dat.lfp.data.shape\n    (128, 400775808)\n&gt;&gt;&gt; type(dat.lfp.data)\n    numpy.memmap\n</code></pre> Source code in <code>neuro_py/io/loading.py</code> <pre><code>class LFPLoader(object):\n    \"\"\"\n    Simple class to load LFP or wideband data from a recording folder.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the recording folder.\n    channels : Union[int, list, None], optional\n        Channel number or list of channel numbers, by default None (load all channels memmap).\n    ext : str, optional\n        File extension, by default \"lfp\".\n    epoch : Union[np.ndarray, nel.EpochArray, None], optional\n        Epoch array or ndarray, by default None (load all data).\n\n    Returns\n    -------\n    nelpy.AnalogSignalArray\n        Analog signal array of shape (n_channels, n_samples).\n\n    Examples\n    --------\n    &gt;&gt;&gt; # load lfp file\n    &gt;&gt;&gt; basepath = r\"X:/data/Barrage/NN10/day10\"\n    &gt;&gt;&gt; lfp = loading.LFPLoader(basepath,ext=\"lfp\")\n    &gt;&gt;&gt; lfp\n        &lt;AnalogSignalArray at 0x25ba1576640: 128 signals&gt; for a total of 5:33:58:789 hours\n\n    &gt;&gt;&gt; # Loading dat file\n    &gt;&gt;&gt; dat = loading.LFPLoader(basepath,ext=\"dat\")\n    &gt;&gt;&gt; dat\n        &lt;AnalogSignalArray at 0x25ba4fedc40: 128 signals&gt; for a total of 5:33:58:790 hours\n    &gt;&gt;&gt; dat.lfp.data.shape\n        (128, 400775808)\n    &gt;&gt;&gt; type(dat.lfp.data)\n        numpy.memmap\n    \"\"\"\n\n    def __init__(\n        self,\n        basepath: str,\n        channels: Union[int, list, None] = None,\n        ext: str = \"lfp\",\n        epoch: Union[np.ndarray, nel.EpochArray, None] = None,\n    ) -&gt; None:\n        self.basepath = basepath  # path to the recording folder\n        self.channels = channels  # channel number or list of channel numbers\n        self.ext = ext  # lfp or dat\n        self.epoch = epoch\n\n        # get xml data\n        self.get_xml_data()\n\n        # set sampling rate based on the extension of the file (lfp or dat)\n        if self.ext == \"dat\":\n            self.fs = self.fs_dat\n\n        # load lfp\n        self.load_lfp()\n\n    def get_xml_data(self) -&gt; None:\n        nChannels, fs, fs_dat, shank_to_channel = loadXML(self.basepath)\n        self.nChannels = nChannels\n        self.fs = fs\n        self.fs_dat = fs_dat\n        self.shank_to_channel = shank_to_channel\n\n    def load_lfp(self) -&gt; None:\n        lfp, timestep = loadLFP(\n            self.basepath,\n            n_channels=self.nChannels,\n            channel=self.channels,\n            frequency=self.fs,\n            ext=self.ext,\n        )\n\n        if isinstance(self.epoch, nel.EpochArray):\n            intervals = self.epoch.data\n        elif isinstance(self.epoch, np.ndarray):\n            intervals = self.epoch\n            if intervals.ndim == 1:\n                intervals = intervals[np.newaxis, :]\n        else:\n            intervals = np.array([0, timestep.shape[0] / self.fs])[np.newaxis, :]\n\n        idx = in_intervals(timestep, intervals)\n\n        # if loading all, don't index as to preserve memmap\n        if idx.all():\n            self.lfp = nel.AnalogSignalArray(\n                data=lfp.T,\n                timestamps=timestep,\n                fs=self.fs,\n                support=nel.EpochArray(intervals),\n            )\n        else:\n            self.lfp = nel.AnalogSignalArray(\n                data=lfp[idx, None].T,\n                timestamps=timestep[idx],\n                fs=self.fs,\n                support=nel.EpochArray(\n                    np.array([min(timestep[idx]), max(timestep[idx])])\n                ),\n            )\n\n    def __repr__(self) -&gt; None:\n        return self.lfp.__repr__()\n\n    def get_phase(self, band2filter: list = [6, 12], ford: int = 3) -&gt; np.ndarray:\n        \"\"\"\n        Get the phase of the LFP signal using a bandpass filter and Hilbert transform.\n\n        Parameters\n        ----------\n        band2filter : list, optional\n            The frequency band to filter, by default [6, 12].\n        ford : int, optional\n            The order of the Butterworth filter, by default 3.\n\n        Returns\n        -------\n        np.ndarray\n            The phase of the LFP signal.\n        \"\"\"\n        band2filter = np.array(band2filter, dtype=float)\n        b, a = signal.butter(ford, band2filter / (self.fs / 2), btype=\"bandpass\")\n        filt_sig = signal.filtfilt(b, a, self.lfp.data, padtype=\"odd\")\n        return np.angle(signal.hilbert(filt_sig))\n\n    def get_freq_phase_amp(\n        self, band2filter: list = [6, 12], ford: int = 3, kernel_size: int = 13\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Get the filtered signal, phase, amplitude, and filtered amplitude of the LFP signal.\n\n        Parameters\n        ----------\n        band2filter : list, optional\n            The frequency band to filter, by default [6, 12].\n        ford : int, optional\n            The order of the Butterworth filter, by default 3.\n        kernel_size : int, optional\n            The kernel size for the median filter, by default 13.\n\n        Returns\n        -------\n        filt_sig : np.ndarray\n            The filtered signal.\n        phase : np.ndarray\n            The phase of the LFP signal.\n        amplitude : np.ndarray\n            The amplitude of the LFP signal.\n        amplitude_filtered : np.ndarray\n            The filtered amplitude of the LFP signal.\n        frequency : np.ndarray\n            The instantaneous frequency of the LFP signal.\n        \"\"\"\n\n        band2filter = np.array(band2filter, dtype=float)\n\n        b, a = signal.butter(ford, band2filter / (self.fs / 2), btype=\"bandpass\")\n\n        filt_sig = signal.filtfilt(b, a, self.lfp.data, padtype=\"odd\")\n        phase = np.angle(signal.hilbert(filt_sig))\n        amplitude = np.abs(signal.hilbert(filt_sig))\n        amplitude_filtered = signal.filtfilt(b, a, amplitude, padtype=\"odd\")\n\n        # calculate the frequency\n        # median filter to smooth the unwrapped phase (this is to avoid jumps in the frequency)\n        filtered_signal = signal.medfilt2d(\n            np.unwrap(phase), kernel_size=[1, kernel_size]\n        )\n\n        # Calculate the derivative of the unwrapped phase to get frequency\n        dt = np.diff(self.lfp.abscissa_vals)\n        if np.allclose(dt, dt[0]):  # Check if sampling is uniform\n            dt = dt[0]  # Use a single scalar for uniform sampling\n        else:\n            dt = np.hstack((dt[0], dt))  # Use an array for non-uniform sampling\n        derivative = np.gradient(filtered_signal, dt, axis=-1)\n        frequency = derivative / (2 * np.pi)\n\n        return filt_sig, phase, amplitude, amplitude_filtered, frequency\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.LFPLoader.get_freq_phase_amp","title":"<code>get_freq_phase_amp(band2filter=[6, 12], ford=3, kernel_size=13)</code>","text":"<p>Get the filtered signal, phase, amplitude, and filtered amplitude of the LFP signal.</p> <p>Parameters:</p> Name Type Description Default <code>band2filter</code> <code>list</code> <p>The frequency band to filter, by default [6, 12].</p> <code>[6, 12]</code> <code>ford</code> <code>int</code> <p>The order of the Butterworth filter, by default 3.</p> <code>3</code> <code>kernel_size</code> <code>int</code> <p>The kernel size for the median filter, by default 13.</p> <code>13</code> <p>Returns:</p> Name Type Description <code>filt_sig</code> <code>ndarray</code> <p>The filtered signal.</p> <code>phase</code> <code>ndarray</code> <p>The phase of the LFP signal.</p> <code>amplitude</code> <code>ndarray</code> <p>The amplitude of the LFP signal.</p> <code>amplitude_filtered</code> <code>ndarray</code> <p>The filtered amplitude of the LFP signal.</p> <code>frequency</code> <code>ndarray</code> <p>The instantaneous frequency of the LFP signal.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def get_freq_phase_amp(\n    self, band2filter: list = [6, 12], ford: int = 3, kernel_size: int = 13\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Get the filtered signal, phase, amplitude, and filtered amplitude of the LFP signal.\n\n    Parameters\n    ----------\n    band2filter : list, optional\n        The frequency band to filter, by default [6, 12].\n    ford : int, optional\n        The order of the Butterworth filter, by default 3.\n    kernel_size : int, optional\n        The kernel size for the median filter, by default 13.\n\n    Returns\n    -------\n    filt_sig : np.ndarray\n        The filtered signal.\n    phase : np.ndarray\n        The phase of the LFP signal.\n    amplitude : np.ndarray\n        The amplitude of the LFP signal.\n    amplitude_filtered : np.ndarray\n        The filtered amplitude of the LFP signal.\n    frequency : np.ndarray\n        The instantaneous frequency of the LFP signal.\n    \"\"\"\n\n    band2filter = np.array(band2filter, dtype=float)\n\n    b, a = signal.butter(ford, band2filter / (self.fs / 2), btype=\"bandpass\")\n\n    filt_sig = signal.filtfilt(b, a, self.lfp.data, padtype=\"odd\")\n    phase = np.angle(signal.hilbert(filt_sig))\n    amplitude = np.abs(signal.hilbert(filt_sig))\n    amplitude_filtered = signal.filtfilt(b, a, amplitude, padtype=\"odd\")\n\n    # calculate the frequency\n    # median filter to smooth the unwrapped phase (this is to avoid jumps in the frequency)\n    filtered_signal = signal.medfilt2d(\n        np.unwrap(phase), kernel_size=[1, kernel_size]\n    )\n\n    # Calculate the derivative of the unwrapped phase to get frequency\n    dt = np.diff(self.lfp.abscissa_vals)\n    if np.allclose(dt, dt[0]):  # Check if sampling is uniform\n        dt = dt[0]  # Use a single scalar for uniform sampling\n    else:\n        dt = np.hstack((dt[0], dt))  # Use an array for non-uniform sampling\n    derivative = np.gradient(filtered_signal, dt, axis=-1)\n    frequency = derivative / (2 * np.pi)\n\n    return filt_sig, phase, amplitude, amplitude_filtered, frequency\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.LFPLoader.get_phase","title":"<code>get_phase(band2filter=[6, 12], ford=3)</code>","text":"<p>Get the phase of the LFP signal using a bandpass filter and Hilbert transform.</p> <p>Parameters:</p> Name Type Description Default <code>band2filter</code> <code>list</code> <p>The frequency band to filter, by default [6, 12].</p> <code>[6, 12]</code> <code>ford</code> <code>int</code> <p>The order of the Butterworth filter, by default 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The phase of the LFP signal.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def get_phase(self, band2filter: list = [6, 12], ford: int = 3) -&gt; np.ndarray:\n    \"\"\"\n    Get the phase of the LFP signal using a bandpass filter and Hilbert transform.\n\n    Parameters\n    ----------\n    band2filter : list, optional\n        The frequency band to filter, by default [6, 12].\n    ford : int, optional\n        The order of the Butterworth filter, by default 3.\n\n    Returns\n    -------\n    np.ndarray\n        The phase of the LFP signal.\n    \"\"\"\n    band2filter = np.array(band2filter, dtype=float)\n    b, a = signal.butter(ford, band2filter / (self.fs / 2), btype=\"bandpass\")\n    filt_sig = signal.filtfilt(b, a, self.lfp.data, padtype=\"odd\")\n    return np.angle(signal.hilbert(filt_sig))\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.add_animal_id","title":"<code>add_animal_id(df)</code>","text":"<p>Add animal_id column to a dataframe based on the basepath column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe with a basepath column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with an additional animal_id column.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def add_animal_id(df: pd.core.frame.DataFrame) -&gt; pd.core.frame.DataFrame:\n    \"\"\"\n    Add animal_id column to a dataframe based on the basepath column.\n\n    Parameters\n    ----------\n    df : pd.core.frame.DataFrame\n        Dataframe with a basepath column.\n\n    Returns\n    -------\n    pd.core.frame.DataFrame\n        Dataframe with an additional animal_id column.\n    \"\"\"\n    df[\"animal_id\"] = df.basepath.map(\n        dict([(basepath, get_animal_id(basepath)) for basepath in df.basepath.unique()])\n    )\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.epoch_to_mat","title":"<code>epoch_to_mat(epoch, basepath, epoch_name, detection_name=None)</code>","text":"<p>Save an EpochArray to a .mat file in the Cell Explorer format.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>EpochArray to save.</p> required <code>basepath</code> <code>str</code> <p>Basepath to save the file to.</p> required <code>epoch_name</code> <code>str</code> <p>Name of the epoch.</p> required <code>detection_name</code> <code>Union[None, str]</code> <p>Name of the detection, by default None.</p> <code>None</code> Source code in <code>neuro_py/io/saving.py</code> <pre><code>def epoch_to_mat(\n    epoch: nel.EpochArray,\n    basepath: str,\n    epoch_name: str,\n    detection_name: Union[None, str] = None,\n) -&gt; None:\n    \"\"\"\n    Save an EpochArray to a .mat file in the Cell Explorer format.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray\n        EpochArray to save.\n    basepath : str\n        Basepath to save the file to.\n    epoch_name : str\n        Name of the epoch.\n    detection_name : Union[None, str], optional\n        Name of the detection, by default None.\n    \"\"\"\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".\" + epoch_name + \".events.mat\"\n    )\n    data = {}\n    data[epoch_name] = {}\n\n    data[epoch_name][\"timestamps\"] = epoch.data\n\n    # check if only single epoch\n    if epoch.data.ndim == 1:\n        data[epoch_name][\"peaks\"] = np.median(epoch.data, axis=0)\n    else:\n        data[epoch_name][\"peaks\"] = np.median(epoch.data, axis=1)\n\n    data[epoch_name][\"amplitudes\"] = []\n    data[epoch_name][\"amplitudeUnits\"] = []\n    data[epoch_name][\"eventID\"] = []\n    data[epoch_name][\"eventIDlabels\"] = []\n    data[epoch_name][\"eventIDbinary\"] = []\n\n    # check if only single epoch\n    if epoch.data.ndim == 1:\n        data[epoch_name][\"duration\"] = epoch.data[1] - epoch.data[0]\n    else:\n        data[epoch_name][\"duration\"] = epoch.durations\n\n    data[epoch_name][\"center\"] = data[epoch_name][\"peaks\"]\n    data[epoch_name][\"detectorinfo\"] = {}\n    if detection_name is None:\n        data[epoch_name][\"detectorinfo\"][\"detectorname\"] = []\n    else:\n        data[epoch_name][\"detectorinfo\"][\"detectorname\"] = detection_name\n    data[epoch_name][\"detectorinfo\"][\"detectionparms\"] = []\n    data[epoch_name][\"detectorinfo\"][\"detectionintervals\"] = []\n\n    savemat(filename, data, long_field_names=True)\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.get_animal_id","title":"<code>get_animal_id(basepath)</code>","text":"<p>Return animal ID from basepath using basename.session.mat.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to session folder.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Animal ID.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def get_animal_id(basepath: str) -&gt; str:\n    \"\"\"\n    Return animal ID from basepath using basename.session.mat.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to session folder.\n\n    Returns\n    -------\n    str\n        Animal ID.\n    \"\"\"\n    try:\n        filename = glob.glob(os.path.join(basepath, \"*.session.mat\"))[0]\n    except Exception:\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame()\n\n    # load file\n    data = sio.loadmat(filename)\n    return data[\"session\"][0][0][\"animal\"][0][0][\"name\"][0]\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.loadLFP","title":"<code>loadLFP(basepath, n_channels=90, channel=None, frequency=1250.0, precision='int16', ext='lfp', filename=None)</code>","text":"<p>Load LFP data from a specified file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder containing the LFP file.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels, by default 90.</p> <code>90</code> <code>channel</code> <code>Optional[Union[int, list]]</code> <p>Specific channel(s) to load, by default None.</p> <code>None</code> <code>frequency</code> <code>float</code> <p>Sampling frequency, by default 1250.0.</p> <code>1250.0</code> <code>precision</code> <code>str</code> <p>Data precision, by default \"int16\".</p> <code>'int16'</code> <code>ext</code> <code>str</code> <p>File extension, by default \"lfp\".</p> <code>'lfp'</code> <code>filename</code> <code>Optional[str]</code> <p>Name of the file to load, located in basepath, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Tuple[ndarray, ndarray]]</code> <p>Data and corresponding timestamps.</p> Notes <p>If both .lfp and .eeg files are present, .lfp file is prioritized. If neither are present, returns None.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def loadLFP(\n    basepath: str,\n    n_channels: int = 90,\n    channel: Union[int, None] = None,\n    frequency: float = 1250.0,\n    precision: str = \"int16\",\n    ext: str = \"lfp\",\n    filename: str = None,  # name of file to load, located in basepath\n):\n    \"\"\"\n    Load LFP data from a specified file.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder containing the LFP file.\n    n_channels : int, optional\n        Number of channels, by default 90.\n    channel : Optional[Union[int, list]], optional\n        Specific channel(s) to load, by default None.\n    frequency : float, optional\n        Sampling frequency, by default 1250.0.\n    precision : str, optional\n        Data precision, by default \"int16\".\n    ext : str, optional\n        File extension, by default \"lfp\".\n    filename : Optional[str], optional\n        Name of the file to load, located in basepath, by default None.\n\n    Returns\n    -------\n    Optional[Tuple[np.ndarray, np.ndarray]]\n        Data and corresponding timestamps.\n\n    Notes\n    -----\n    If both .lfp and .eeg files are present, .lfp file is prioritized.\n    If neither are present, returns None.\n    \"\"\"\n    if filename is not None:\n        path = os.path.join(basepath, filename)\n    else:\n        path = \"\"\n        if ext == \"lfp\":\n            path = os.path.join(basepath, os.path.basename(basepath) + \".lfp\")\n            if not os.path.exists(path):\n                path = os.path.join(basepath, os.path.basename(basepath) + \".eeg\")\n        if ext == \"dat\":\n            path = os.path.join(basepath, os.path.basename(basepath) + \".dat\")\n\n    # check if saved file exists\n    if not os.path.exists(path):\n        warnings.warn(\"file does not exist\")\n        return\n    if channel is None:\n        n_channels = int(n_channels)\n\n        f = open(path, \"rb\")\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        bytes_size = 2\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n        f.close()\n        data = np.memmap(path, np.int16, \"r\", shape=(n_samples, n_channels))\n        timestep = np.arange(0, n_samples) / frequency\n        return data, timestep\n\n    if type(channel) is not list:\n        f = open(path, \"rb\")\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        bytes_size = 2\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n        f.close()\n        with open(path, \"rb\") as f:\n            data = np.fromfile(f, np.int16).reshape((n_samples, n_channels))[:, channel]\n            timestep = np.arange(0, len(data)) / frequency\n            # check if lfp time stamps exist\n            lfp_ts_path = os.path.join(\n                os.path.dirname(os.path.abspath(path)), \"lfp_ts.npy\"\n            )\n            if os.path.exists(lfp_ts_path):\n                timestep = np.load(lfp_ts_path).reshape(-1)\n\n            return data, timestep\n\n    elif type(channel) is list:\n        f = open(path, \"rb\")\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        bytes_size = 2\n\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n        f.close()\n        with open(path, \"rb\") as f:\n            data = np.fromfile(f, np.int16).reshape((n_samples, n_channels))[:, channel]\n            timestep = np.arange(0, len(data)) / frequency\n            # check if lfp time stamps exist\n            lfp_ts_path = os.path.join(\n                os.path.dirname(os.path.abspath(path)), \"lfp_ts.npy\"\n            )\n            if os.path.exists(lfp_ts_path):\n                timestep = np.load(lfp_ts_path).reshape(-1)\n            return data, timestep\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.loadXML","title":"<code>loadXML(basepath)</code>","text":"<p>Load XML file and extract relevant information.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder session containing the XML file.</p> required <p>Returns:</p> Type Description <code>Union[Tuple[int, int, int, Dict[int, list]], None]</code> <p>A tuple containing: - The number of channels (int) - The sampling frequency of the dat file (int) - The sampling frequency of the eeg file (int) - The mappings shanks to channels as a dict (Dict[int, list])</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def loadXML(basepath: str) -&gt; Union[Tuple[int, int, int, Dict[int, list]], None]:\n    \"\"\"\n    Load XML file and extract relevant information.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder session containing the XML file.\n\n    Returns\n    -------\n    Union[Tuple[int, int, int, Dict[int, list]], None]\n        A tuple containing:\n        - The number of channels (int)\n        - The sampling frequency of the dat file (int)\n        - The sampling frequency of the eeg file (int)\n        - The mappings shanks to channels as a dict (Dict[int, list])\n    \"\"\"\n    # check if saved file exists\n    try:\n        basename = os.path.basename(basepath)\n        filename = glob.glob(os.path.join(basepath, basename + \".xml\"))[0]\n    except Exception:\n        warnings.warn(\"xml file does not exist\")\n        return\n\n    xmldoc = minidom.parse(filename)\n    nChannels = (\n        xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n        .getElementsByTagName(\"nChannels\")[0]\n        .firstChild.data\n    )\n    fs_dat = (\n        xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n        .getElementsByTagName(\"samplingRate\")[0]\n        .firstChild.data\n    )\n    fs = (\n        xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n        .getElementsByTagName(\"lfpSamplingRate\")[0]\n        .firstChild.data\n    )\n\n    shank_to_channel = {}\n    groups = (\n        xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n        .getElementsByTagName(\"channelGroups\")[0]\n        .getElementsByTagName(\"group\")\n    )\n    for i in range(len(groups)):\n        shank_to_channel[i] = [\n            int(child.firstChild.data)\n            for child in groups[i].getElementsByTagName(\"channel\")\n        ]\n    return int(nChannels), int(fs), int(fs_dat), shank_to_channel\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_SWRunitMetrics","title":"<code>load_SWRunitMetrics(basepath)</code>","text":"<p>Load SWRunitMetrics.mat into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder containing the SWRunitMetrics.mat file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the following fields: - particip: the probability of participation into ripples for each unit - FRall: mean firing rate during ripples - FRparticip: mean firing rate for ripples with at least 1 spike - nSpkAll: mean number of spikes in all ripples - nSpkParticip: mean number of spikes in ripples with at least 1 spike - epoch: behavioral epoch label</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_SWRunitMetrics(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Load SWRunitMetrics.mat into a pandas DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder containing the SWRunitMetrics.mat file.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with the following fields:\n        - particip: the probability of participation into ripples for each unit\n        - FRall: mean firing rate during ripples\n        - FRparticip: mean firing rate for ripples with at least 1 spike\n        - nSpkAll: mean number of spikes in all ripples\n        - nSpkParticip: mean number of spikes in ripples with at least 1 spike\n        - epoch: behavioral epoch label\n    \"\"\"\n\n    def extract_swr_epoch_data(data, epoch):\n        # get var names\n        dt = data[\"SWRunitMetrics\"][epoch][0][0].dtype\n\n        df2 = pd.DataFrame()\n\n        # get n units\n        # there might be other fields within here like the epoch timestamps\n        # skip those by returning empty df\n        try:\n            n_cells = data[\"SWRunitMetrics\"][epoch][0][0][0][\"particip\"][0].shape[0]\n        except Exception:\n            return df2\n\n        for dn in dt.names:\n            if (data[\"SWRunitMetrics\"][epoch][0][0][0][dn][0].shape[1] == 1) &amp; (\n                data[\"SWRunitMetrics\"][epoch][0][0][0][dn][0].shape[0] == n_cells\n            ):\n                df2[dn] = data[\"SWRunitMetrics\"][epoch][0][0][0][dn][0].T[0]\n        df2[\"epoch\"] = epoch\n        return df2\n\n    try:\n        filename = glob.glob(os.path.join(basepath, \"*.SWRunitMetrics.mat\"))[0]\n    except Exception:\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame()\n\n    # load file\n    data = sio.loadmat(filename)\n\n    df2 = pd.DataFrame()\n    # loop through each available epoch and pull out contents\n    for epoch in data[\"SWRunitMetrics\"].dtype.names:\n        if data[\"SWRunitMetrics\"][epoch][0][0].size &gt; 0:  # not empty\n            # call content extractor\n            df_ = extract_swr_epoch_data(data, epoch)\n\n            # append conents to overall data frame\n            if df_.size &gt; 0:\n                df2 = pd.concat([df2, df_], ignore_index=True)\n\n    return df2\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_SleepState_states","title":"<code>load_SleepState_states(basepath, return_epoch_array=False, states_list=['WAKEstate', 'NREMstate', 'REMstate', 'THETA', 'nonTHETA'])</code>","text":"<p>Loader of SleepState.states.mat.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder containing the SleepState.states.mat file.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, return an dict of EpochArrays, by default False.</p> <code>False</code> <code>states_list</code> <code>list</code> <p>List of states to load, by default [\"WAKEstate\", \"NREMstate\", \"REMstate\", \"THETA\", \"nonTHETA\"].</p> <code>['WAKEstate', 'NREMstate', 'REMstate', 'THETA', 'nonTHETA']</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the contents of the SleepState.states.mat file.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_SleepState_states(\n    basepath: str,\n    return_epoch_array: bool = False,\n    states_list: list = [\"WAKEstate\", \"NREMstate\", \"REMstate\", \"THETA\", \"nonTHETA\"],\n) -&gt; dict:\n    \"\"\"\n    Loader of SleepState.states.mat.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder containing the SleepState.states.mat file.\n    return_epoch_array : bool, optional\n        If True, return an dict of EpochArrays, by default False.\n    states_list : list, optional\n        List of states to load, by default [\"WAKEstate\", \"NREMstate\", \"REMstate\", \"THETA\", \"nonTHETA\"].\n\n    Returns\n    -------\n    dict\n        Dictionary containing the contents of the SleepState.states.mat file.\n    \"\"\"\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".SleepState.states.mat\"\n    )\n    if not os.path.exists(filename):\n        warnings.warn(\"file does not exist\")\n        return None\n\n    # load cell_metrics file\n    data = sio.loadmat(filename)\n\n    # get epoch id\n    wake_id = (\n        np.where(data[\"SleepState\"][\"idx\"][0][0][\"statenames\"][0][0][0] == \"WAKE\")[0][0]\n        + 1\n    )\n    rem_id = (\n        np.where(data[\"SleepState\"][\"idx\"][0][0][\"statenames\"][0][0][0] == \"REM\")[0][0]\n        + 1\n    )\n    nrem_id = (\n        np.where(data[\"SleepState\"][\"idx\"][0][0][\"statenames\"][0][0][0] == \"NREM\")[0][0]\n        + 1\n    )\n\n    # get states and timestamps vectors\n    states = data[\"SleepState\"][\"idx\"][0][0][\"states\"][0][0]\n    timestamps = data[\"SleepState\"][\"idx\"][0][0][\"timestamps\"][0][0]\n\n    # set up dict\n    dict_ = {\n        \"wake_id\": wake_id,\n        \"rem_id\": rem_id,\n        \"nrem_id\": nrem_id,\n        \"states\": states,\n        \"timestamps\": timestamps,\n    }\n\n    # iter through states and add to dict\n    dt = data[\"SleepState\"][\"ints\"][0][0].dtype\n    for dn in dt.names:\n        dict_[dn] = data[\"SleepState\"][\"ints\"][0][0][dn][0][0]\n\n    if not return_epoch_array:\n        return dict_\n    else:\n        epoch_df = load_epoch(basepath)\n        # get session bounds to provide support\n        session_domain = nel.EpochArray(\n            [epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]]\n        )\n        states_dict = {}\n        for state in states_list:\n            states_dict[state] = nel.EpochArray(\n                dict_.get(state, []), domain=session_domain\n            )\n        return states_dict\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_all_cell_metrics","title":"<code>load_all_cell_metrics(basepaths)</code>","text":"<p>Load cell metrics from multiple sessions.</p> <p>Parameters:</p> Name Type Description Default <code>basepaths</code> <code>List[str]</code> <p>List of basepaths, can be a pandas column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Concatenated pandas DataFrame with metrics.</p> Notes <p>To get waveforms, spike times, etc., use load_cell_metrics.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_all_cell_metrics(basepaths: List[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Load cell metrics from multiple sessions.\n\n    Parameters\n    ----------\n    basepaths : List[str]\n        List of basepaths, can be a pandas column.\n\n    Returns\n    -------\n    pd.DataFrame\n        Concatenated pandas DataFrame with metrics.\n\n    Notes\n    -----\n    To get waveforms, spike times, etc., use load_cell_metrics.\n    \"\"\"\n\n    # to speed up, use parallel\n    num_cores = multiprocessing.cpu_count()\n    cell_metrics = Parallel(n_jobs=num_cores)(\n        delayed(load_cell_metrics)(basepath, True) for basepath in basepaths\n    )\n\n    return pd.concat(cell_metrics, ignore_index=True)\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_animal_behavior","title":"<code>load_animal_behavior(basepath, alternative_file=None)</code>","text":"<p>load_animal_behavior loads basename.animal.behavior.mat files created by general_behavior_file.m The output is a pandas data frame with [time,x,y,z,linearized,speed,acceleration,trials,epochs]</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>alternative_file</code> <code>Union[str, None]</code> <p>Alternative file name to load, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the following fields: - time: timestamps - x: x-coordinate - y: y-coordinate - z: z-coordinate - linearized: linearized position - speed: speed of the animal - acceleration: acceleration of the animal - trials: trial numbers - epochs: epoch names - environment: environment names</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_animal_behavior(\n    basepath: str, alternative_file: Union[str, None] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    load_animal_behavior loads basename.animal.behavior.mat files created by general_behavior_file.m\n    The output is a pandas data frame with [time,x,y,z,linearized,speed,acceleration,trials,epochs]\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    alternative_file : Union[str, None], optional\n        Alternative file name to load, by default None.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with the following fields:\n        - time: timestamps\n        - x: x-coordinate\n        - y: y-coordinate\n        - z: z-coordinate\n        - linearized: linearized position\n        - speed: speed of the animal\n        - acceleration: acceleration of the animal\n        - trials: trial numbers\n        - epochs: epoch names\n        - environment: environment names\n    \"\"\"\n    df = pd.DataFrame()\n\n    if alternative_file is None:\n        try:\n            filename = glob.glob(os.path.join(basepath, \"*.animal.behavior.mat\"))[0]\n        except Exception:\n            warnings.warn(\"file does not exist\")\n            return df\n    else:\n        try:\n            filename = glob.glob(\n                os.path.join(basepath, \"*\" + alternative_file + \".mat\")\n            )[0]\n        except Exception:\n            warnings.warn(\"file does not exist\")\n            return df\n\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    # add timestamps first which provide the correct shape of df\n    # here, I'm naming them time, but this should be deprecated\n    df[\"time\"] = data[\"behavior\"][\"timestamps\"]\n\n    # add all other position coordinates to df (will add everything it can within position)\n    for key in data[\"behavior\"][\"position\"].keys():\n        values = data[\"behavior\"][\"position\"][key]\n        if isinstance(values, (list, np.ndarray)) and len(values) == 0:\n            continue\n        df[key] = values\n\n    # add other fields from behavior to df (acceleration,speed,states)\n    for key in data[\"behavior\"].keys():\n        values = data[\"behavior\"][key]\n        if isinstance(values, (list, np.ndarray)) and len(values) != len(df):\n            continue\n        df[key] = values\n\n    # add speed and acceleration\n    if \"speed\" not in df.columns:\n        df[\"speed\"] = get_speed(df[[\"x\", \"y\"]].values, df.time.values)\n    if \"acceleration\" not in df.columns:  # using backward difference\n        df.loc[1:, \"acceleration\"] = np.diff(df[\"speed\"]) / np.diff(df[\"time\"])\n        df.loc[0, \"acceleration\"] = 0  # assuming no acceleration at start\n\n    trials = data[\"behavior\"][\"trials\"]\n    try:\n        for t in range(trials.shape[0]):\n            idx = (df.time &gt;= trials[t, 0]) &amp; (df.time &lt;= trials[t, 1])\n            df.loc[idx, \"trials\"] = t\n    except Exception:\n        pass\n\n    epochs = load_epoch(basepath)\n    for t in range(epochs.shape[0]):\n        idx = (df.time &gt;= epochs.startTime.iloc[t]) &amp; (\n            df.time &lt;= epochs.stopTime.iloc[t]\n        )\n        df.loc[idx, \"epochs\"] = epochs.name.iloc[t]\n        df.loc[idx, \"environment\"] = epochs.environment.iloc[t]\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_barrage_events","title":"<code>load_barrage_events(basepath, return_epoch_array=False, restrict_to_nrem=True, min_duration=0.0)</code>","text":"<p>Load barrage events from the .HSEn2.events.mat file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Basepath to the session folder.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, return an EpochArray instead of a DataFrame, by default False</p> <code>False</code> <code>restrict_to_nrem</code> <code>bool</code> <p>If True, restrict to NREM sleep, by default True</p> <code>True</code> <code>min_duration</code> <code>float</code> <p>Minimum duration of a barrage, by default 0.0</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame with barrage events.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_barrage_events(\n    basepath: str,\n    return_epoch_array: bool = False,\n    restrict_to_nrem: bool = True,\n    min_duration: float = 0.0,\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Load barrage events from the .HSEn2.events.mat file.\n\n    Parameters\n    ----------\n    basepath : str\n        Basepath to the session folder.\n    return_epoch_array : bool, optional\n        If True, return an EpochArray instead of a DataFrame, by default False\n    restrict_to_nrem : bool, optional\n        If True, restrict to NREM sleep, by default True\n    min_duration : float, optional\n        Minimum duration of a barrage, by default 0.0\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame with barrage events.\n    \"\"\"\n\n    # locate barrage file\n    filename = os.path.join(basepath, os.path.basename(basepath) + \".HSEn2.events.mat\")\n\n    # check if file exists\n    if os.path.exists(filename) is False:\n        warnings.warn(\"No barrage file found for {}\".format(basepath))\n        if return_epoch_array:\n            return nel.EpochArray()\n        return pd.DataFrame()\n\n    # load data from file and extract relevant data\n    data = sio.loadmat(filename, simplify_cells=True)\n    data = data[\"HSEn2\"]\n\n    # convert to DataFrame\n    df = pd.DataFrame()\n    df[\"start\"] = data[\"timestamps\"][:, 0]\n    df[\"stop\"] = data[\"timestamps\"][:, 1]\n    df[\"peaks\"] = data[\"peaks\"]\n    df[\"duration\"] = data[\"timestamps\"][:, 1] - data[\"timestamps\"][:, 0]\n\n    # restrict to NREM sleep\n    if restrict_to_nrem:\n        state_dict = load_SleepState_states(basepath)\n        nrem_epochs = nel.EpochArray(state_dict[\"NREMstate\"]).expand(2)\n        idx = in_intervals(df[\"start\"].values, nrem_epochs.data)\n        df = df[idx].reset_index(drop=True)\n\n    # restrict to barrages with a minimum duration\n    df = df[df.duration &gt; min_duration].reset_index(drop=True)\n\n    # make sure each barrage has some ca2 activity\n    # load ca2 pyr cells\n    st, _ = load_spikes(basepath, putativeCellType=\"Pyr\", brainRegion=\"CA2\")\n    # bin spikes into barrages\n    bst = get_participation(st.data, df[\"start\"].values, df[\"stop\"].values)\n    # keep only barrages with some activity\n    df = df[np.sum(bst &gt; 0, axis=0) &gt; 0].reset_index(drop=True)\n\n    if return_epoch_array:\n        return nel.EpochArray([np.array([df.start, df.stop]).T], label=\"barrage\")\n\n    # get basename and animal\n    normalized_path = os.path.normpath(filename)\n    path_components = normalized_path.split(os.sep)\n    df[\"basepath\"] = basepath\n    df[\"basename\"] = path_components[-2]\n    df[\"animal\"] = path_components[-3]\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_basic_data","title":"<code>load_basic_data(basepath)</code>","text":"<p>Load basic data from the specified basepath.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, dict, DataFrame, float]</code> <ul> <li>cell_metrics: DataFrame containing cell metrics.</li> <li>data: Dictionary containing additional data.</li> <li>ripples: DataFrame containing ripple events.</li> <li>fs_dat: Sampling rate of the data.</li> </ul> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_basic_data(basepath: str) -&gt; Tuple[pd.DataFrame, dict, pd.DataFrame, float]:\n    \"\"\"\n    Load basic data from the specified basepath.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, dict, pd.DataFrame, float]\n        - cell_metrics: DataFrame containing cell metrics.\n        - data: Dictionary containing additional data.\n        - ripples: DataFrame containing ripple events.\n        - fs_dat: Sampling rate of the data.\n    \"\"\"\n    try:\n        nChannels, fs, fs_dat, shank_to_channel = loadXML(basepath)\n    except Exception:\n        fs_dat = load_extracellular_metadata(basepath).get(\"sr\")\n\n    ripples = load_ripples_events(basepath)\n    cell_metrics, data = load_cell_metrics(basepath)\n\n    return cell_metrics, data, ripples, fs_dat\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_brain_regions","title":"<code>load_brain_regions(basepath, out_format='dict')</code>","text":"<p>Loads brain region info from cell explorer basename.session and stores in dict (default) or DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>out_format</code> <code>str</code> <p>Output format, either 'dict' or 'DataFrame', by default 'dict'.</p> <code>'dict'</code> <p>Returns:</p> Type Description <code>Union[dict, DataFrame]</code> <p>Dictionary or DataFrame with brain region information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; brainRegions = load_brain_regions(\"Z:\\Data\\GirardeauG\\Rat09\\Rat09-20140327\")\n&gt;&gt;&gt; print(brainRegions.keys())\ndict_keys(['CA1', 'Unknown', 'blv', 'bmp', 'ven'])\n&gt;&gt;&gt; print(brainRegions['CA1'].keys())\ndict_keys(['channels', 'electrodeGroups'])\n&gt;&gt;&gt; print(brainRegions['CA1']['channels'])\n[145 146 147 148 149 153 155 157 150 151 154 159 156 152 158 160 137 140\n129 136 138 134 130 132 142 143 144 141 131 139 133 135]\n&gt;&gt;&gt; print(brainRegions['CA1']['electrodeGroups'])\n    [17 18 19 20]\n</code></pre> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_brain_regions(\n    basepath: str, out_format: str = \"dict\"\n) -&gt; Union[dict, pd.DataFrame]:\n    \"\"\"\n    Loads brain region info from cell explorer basename.session and stores in dict (default) or DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    out_format : str, optional\n        Output format, either 'dict' or 'DataFrame', by default 'dict'.\n\n    Returns\n    -------\n    Union[dict, pd.DataFrame]\n        Dictionary or DataFrame with brain region information.\n\n    Examples\n    -------\n    &gt;&gt;&gt; brainRegions = load_brain_regions(\"Z:\\\\Data\\\\GirardeauG\\\\Rat09\\\\Rat09-20140327\")\n    &gt;&gt;&gt; print(brainRegions.keys())\n    dict_keys(['CA1', 'Unknown', 'blv', 'bmp', 'ven'])\n    &gt;&gt;&gt; print(brainRegions['CA1'].keys())\n    dict_keys(['channels', 'electrodeGroups'])\n    &gt;&gt;&gt; print(brainRegions['CA1']['channels'])\n    [145 146 147 148 149 153 155 157 150 151 154 159 156 152 158 160 137 140\n    129 136 138 134 130 132 142 143 144 141 131 139 133 135]\n    &gt;&gt;&gt; print(brainRegions['CA1']['electrodeGroups'])\n        [17 18 19 20]\n    \"\"\"\n    filename = os.path.join(basepath, os.path.basename(basepath) + \".session.mat\")\n\n    if not os.path.exists(filename):\n        warnings.warn(f\"file {filename} does not exist\")\n        if out_format == \"DataFrame\":\n            return pd.DataFrame()\n        else:\n            return {}\n\n    # load file\n    data = sio.loadmat(filename, simplify_cells=True)\n    data = data[\"session\"]\n\n    if \"brainRegions\" not in data.keys():\n        warnings.warn(\"brainRegions not found in file\")\n        if out_format == \"DataFrame\":\n            return pd.DataFrame()\n        else:\n            return {}\n\n    brainRegions = {}\n    for region in data[\"brainRegions\"].keys():\n        if len(data[\"brainRegions\"][region]) == 0:\n            continue\n        channels = data[\"brainRegions\"][region][\"channels\"] - 1\n        try:\n            electrodeGroups = data[\"brainRegions\"][region][\"electrodeGroups\"]\n        except Exception:\n            electrodeGroups = np.nan\n\n        brainRegions[region] = {\n            \"channels\": channels,\n            \"electrodeGroups\": electrodeGroups,\n        }\n\n    if out_format == \"DataFrame\":  # return as DataFrame\n        # get channel order from electrodeGroups in session file\n        shank_to_channel = data[\"extracellular\"][\"electrodeGroups\"][\"channels\"] - 1\n\n        # check if nested array for multi shank\n        if is_nested(shank_to_channel) or shank_to_channel.ndim &gt; 1:\n            channels = np.hstack(shank_to_channel)\n            shanks = np.hstack(\n                [\n                    np.repeat(i, len(shank_to_channel[i]))\n                    for i in range(len(shank_to_channel))\n                ]\n            )\n        else:\n            channels = shank_to_channel\n            shanks = np.zeros(len(channels))\n\n        mapped_df = pd.DataFrame(columns=[\"channels\", \"region\"])\n        mapped_df[\"channels\"] = channels\n        mapped_df[\"region\"] = \"Unknown\"\n        mapped_df[\"shank\"] = shanks\n\n        for key in brainRegions.keys():\n            idx = np.in1d(channels, brainRegions[key][\"channels\"])\n            mapped_df.loc[idx, \"region\"] = key\n\n        # save channel as zero-indexed\n        mapped_df[\"channels\"] = mapped_df[\"channels\"]\n\n        return mapped_df.reset_index(drop=True)\n\n    elif out_format == \"dict\":\n        return brainRegions\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_cell_metrics","title":"<code>load_cell_metrics(basepath, only_metrics=False)</code>","text":"<p>Loader of cell-explorer cell_metrics.cellinfo.mat.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to folder with cell_metrics.cellinfo.mat.</p> required <code>only_metrics</code> <code>bool</code> <p>If True, only metrics are loaded, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[DataFrame, Tuple[DataFrame, dict]]</code> <p>DataFrame of single unit features and a dictionary with data that does not fit nicely into a DataFrame (waveforms, acgs, epochs, etc.).</p> Notes <p>See https://cellexplorer.org/datastructure/standard-cell-metrics/ for details.</p> <p>TODO: Extract all fields from cell_metrics.cellinfo. There are more items that can be extracted.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_cell_metrics(\n    basepath: str, only_metrics: bool = False\n) -&gt; Union[pd.DataFrame, Tuple[pd.DataFrame, dict]]:\n    \"\"\"\n    Loader of cell-explorer cell_metrics.cellinfo.mat.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to folder with cell_metrics.cellinfo.mat.\n    only_metrics : bool, optional\n        If True, only metrics are loaded, by default False.\n\n    Returns\n    -------\n    Union[pd.DataFrame, Tuple[pd.DataFrame, dict]]\n        DataFrame of single unit features and a dictionary with data that does not fit nicely into a DataFrame (waveforms, acgs, epochs, etc.).\n\n    Notes\n    -----\n    See https://cellexplorer.org/datastructure/standard-cell-metrics/ for details.\n\n    TODO: Extract all fields from cell_metrics.cellinfo. There are more items that can be extracted.\n    \"\"\"\n\n    def extract_epochs(data):\n        startTime = [\n            ep[\"startTime\"][0][0][0][0]\n            for ep in data[\"cell_metrics\"][\"general\"][0][0][\"epochs\"][0][0][0]\n        ]\n        stopTime = [\n            ep[\"stopTime\"][0][0][0][0]\n            for ep in data[\"cell_metrics\"][\"general\"][0][0][\"epochs\"][0][0][0]\n        ]\n        name = [\n            ep[\"name\"][0][0][0]\n            for ep in data[\"cell_metrics\"][\"general\"][0][0][\"epochs\"][0][0][0]\n        ]\n\n        epochs = pd.DataFrame()\n        epochs[\"name\"] = name\n        epochs[\"startTime\"] = startTime\n        epochs[\"stopTime\"] = stopTime\n        return epochs\n\n    def extract_events(data):\n        psth = {}\n        for dt in data[\"cell_metrics\"][\"events\"][0][0].dtype.names:\n            psth[dt] = pd.DataFrame(\n                index=data[\"cell_metrics\"][\"general\"][0][0][0][\"events\"][0][dt][0][0][\n                    \"x_bins\"\n                ][0][0].T[0]\n                / 1000,\n                data=np.hstack(data[\"cell_metrics\"][\"events\"][0][0][dt][0][0][0]),\n            )\n        return psth\n\n    def extract_general(data):\n        # extract fr per unit with lag zero to ripple\n        try:\n            ripple_fr = [\n                ev.T[0]\n                for ev in data[\"cell_metrics\"][\"events\"][0][0][\"ripples\"][0][0][0]\n            ]\n        except Exception:\n            ripple_fr = []\n        # extract spikes times\n        spikes = [\n            spk.T[0] for spk in data[\"cell_metrics\"][\"spikes\"][0][0][\"times\"][0][0][0]\n        ]\n        # extract epochs\n        try:\n            epochs = extract_epochs(data)\n        except Exception:\n            epochs = []\n\n        # extract events\n        try:\n            events_psth = extract_events(data)\n        except Exception:\n            events_psth = []\n\n        # extract avg waveforms\n        try:\n            waveforms = np.vstack(\n                data[\"cell_metrics\"][\"waveforms\"][0][0][\"filt\"][0][0][0]\n            )\n        except Exception:\n            try:\n                waveforms = [\n                    w.T for w in data[\"cell_metrics\"][\"waveforms\"][0][0][0][0][0][0]\n                ]\n            except Exception:\n                waveforms = [w.T for w in data[\"cell_metrics\"][\"waveforms\"][0][0][0]]\n        # extract chanCoords\n        try:\n            chanCoords_x = data[\"cell_metrics\"][\"general\"][0][0][\"chanCoords\"][0][0][0][\n                0\n            ][\"x\"].T[0]\n            chanCoords_y = data[\"cell_metrics\"][\"general\"][0][0][\"chanCoords\"][0][0][0][\n                0\n            ][\"y\"].T[0]\n        except Exception:\n            chanCoords_x = []\n            chanCoords_y = []\n\n        # add to dictionary\n        data_ = {\n            \"acg_wide\": data[\"cell_metrics\"][\"acg\"][0][0][\"wide\"][0][0],\n            \"acg_narrow\": data[\"cell_metrics\"][\"acg\"][0][0][\"narrow\"][0][0],\n            \"acg_log10\": data[\"cell_metrics\"][\"acg\"][0][0][\"log10\"][0][0],\n            \"ripple_fr\": ripple_fr,\n            \"chanCoords_x\": chanCoords_x,\n            \"chanCoords_y\": chanCoords_y,\n            \"epochs\": epochs,\n            \"spikes\": spikes,\n            \"waveforms\": waveforms,\n            \"events_psth\": events_psth,\n        }\n        return data_\n\n    def un_nest_df(df):\n        # Un-nest some strings are nested within brackets (a better solution exists...)\n        # locate and iterate objects in df\n        for item in df.keys()[df.dtypes == \"object\"]:\n            # if you can get the size of the first item with [0], it is nested\n            # otherwise it fails and is not nested\n            try:\n                df[item][0][0].size\n                # the below line is from: https://www.py4u.net/discuss/140913\n                df[item] = df[item].str.get(0)\n            except Exception:\n                continue\n        return df\n\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".cell_metrics.cellinfo.mat\"\n    )\n    # filename = glob.glob(os.path.join(basepath, \"*.cell_metrics.cellinfo.mat\"))[0]\n\n    # check if saved file exists\n    if not os.path.exists(filename):\n        warnings.warn(\"file does not exist\")\n        if only_metrics:\n            return None\n        return None, None\n\n    # load cell_metrics file\n    data = sio.loadmat(filename)\n\n    # construct data frame with features per neuron\n    df = {}\n    # count units\n    n_cells = data[\"cell_metrics\"][\"UID\"][0][0][0].size\n    dt = data[\"cell_metrics\"].dtype\n    for dn in dt.names:\n        # check if var has the right n of units and is a vector\n        try:\n            if (data[\"cell_metrics\"][dn][0][0][0][0].size == 1) &amp; (\n                data[\"cell_metrics\"][dn][0][0][0].size == n_cells\n            ):\n                # check if nested within brackets\n                try:\n                    df[dn] = [\n                        value[0] if len(value) == 1 else value\n                        for value in data[\"cell_metrics\"][dn][0][0][0]\n                    ]\n                except Exception:\n                    df[dn] = data[\"cell_metrics\"][dn][0][0][0]\n        except Exception:\n            continue\n\n    df = pd.DataFrame(df)\n\n    # load in tag\n    # check if tags exist within cell_metrics\n    if \"tags\" in data.get(\"cell_metrics\").dtype.names:\n        # get names of each tag\n        dt = data[\"cell_metrics\"][\"tags\"][0][0].dtype\n        if len(dt) &gt; 0:\n            # iter through each tag\n            for dn in dt.names:\n                # set up column for tag\n                df[\"tags_\" + dn] = [False] * df.shape[0]\n                # iter through uid\n                for uid in data[\"cell_metrics\"][\"tags\"][0][0][dn][0][0].flatten():\n                    df.loc[df.UID == uid, \"tags_\" + dn] = True\n\n    # add bad unit tag for legacy\n    df[\"bad_unit\"] = [False] * df.shape[0]\n    if \"tags_Bad\" in df.keys():\n        df.bad_unit = df.tags_Bad\n        df.bad_unit = df.bad_unit.replace({np.nan: False})\n\n    # add data from general metrics\n    df[\"basename\"] = data[\"cell_metrics\"][\"general\"][0][0][\"basename\"][0][0][0]\n    df[\"basepath\"] = basepath\n    df[\"sex\"] = data[\"cell_metrics\"][\"general\"][0][0][\"animal\"][0][0][\"sex\"][0][0][0]\n    df[\"species\"] = data[\"cell_metrics\"][\"general\"][0][0][\"animal\"][0][0][\"species\"][0][\n        0\n    ][0]\n    df[\"strain\"] = data[\"cell_metrics\"][\"general\"][0][0][\"animal\"][0][0][\"strain\"][0][\n        0\n    ][0]\n    try:\n        df[\"geneticLine\"] = data[\"cell_metrics\"][\"general\"][0][0][\"animal\"][0][0][\n            \"geneticLine\"\n        ][0][0][0]\n    except Exception:\n        pass\n    df[\"cellCount\"] = data[\"cell_metrics\"][\"general\"][0][0][\"cellCount\"][0][0][0][0]\n\n    # fix nesting issue for strings\n    df = un_nest_df(df)\n\n    # convert nans within tags columns to false\n    cols = df.filter(regex=\"tags_\").columns\n    df[cols] = df[cols].replace({np.nan: False})\n\n    if only_metrics:\n        return df\n\n    # extract other general data and put into dict\n    data_ = extract_general(data)\n\n    return df, data_\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_channel_tags","title":"<code>load_channel_tags(basepath)</code>","text":"<p>Load channel tags from session file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the directory containing the session file.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of channel tags from the session file.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_channel_tags(basepath: str) -&gt; dict:\n    \"\"\"\n    Load channel tags from session file.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the directory containing the session file.\n\n    Returns\n    -------\n    dict\n        A dictionary of channel tags from the session file.\n    \"\"\"\n    filename = glob.glob(os.path.join(basepath, \"*.session.mat\"))[0]\n    data = sio.loadmat(filename, simplify_cells=True)\n    return data[\"session\"][\"channelTags\"]\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_deepSuperficialfromRipple","title":"<code>load_deepSuperficialfromRipple(basepath, bypass_mismatch_exception=False)</code>","text":"<p>Load deepSuperficialfromRipple file created by classification_DeepSuperficial.m.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>bypass_mismatch_exception</code> <code>bool</code> <p>If True, bypass the mismatch exception, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, ndarray, ndarray]</code> <ul> <li>channel_df: DataFrame containing channel information.</li> <li>ripple_average: Array containing average ripple traces.</li> <li>ripple_time_axis: Array containing ripple time axis.</li> </ul> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_deepSuperficialfromRipple(\n    basepath: str, bypass_mismatch_exception: bool = False\n) -&gt; Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n    \"\"\"\n    Load deepSuperficialfromRipple file created by classification_DeepSuperficial.m.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    bypass_mismatch_exception : bool, optional\n        If True, bypass the mismatch exception, by default False.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, np.ndarray, np.ndarray]\n        - channel_df: DataFrame containing channel information.\n        - ripple_average: Array containing average ripple traces.\n        - ripple_time_axis: Array containing ripple time axis.\n    \"\"\"\n    # locate .mat file\n    file_type = \"*.deepSuperficialfromRipple.channelinfo.mat\"\n    filename = glob.glob(basepath + os.sep + file_type)[0]\n\n    # load matfile\n    data = sio.loadmat(filename)\n\n    channel_df = pd.DataFrame()\n    name = \"deepSuperficialfromRipple\"\n\n    # sometimes more channels positons will be in deepSuperficialfromRipple than in xml\n    #   this is because they used channel id as an index.\n    channel_df = pd.DataFrame()\n    channels = np.hstack(data[name][\"channel\"][0][0]) * np.nan\n    shanks = np.hstack(data[name][\"channel\"][0][0]) * np.nan\n\n    channels_, shanks_ = zip(\n        *[\n            (values[0], np.tile(shank, len(values[0])))\n            for shank, values in enumerate(data[name][\"ripple_channels\"][0][0][0])\n        ]\n    )\n    channel_sort_idx = np.hstack(channels_) - 1\n    channels[channel_sort_idx] = np.hstack(channels_)\n    shanks[channel_sort_idx] = np.hstack(shanks_) + 1\n\n    channel_df[\"channel\"] = channels\n    channel_df.loc[np.arange(len(channel_sort_idx)), \"channel_sort_idx\"] = (\n        channel_sort_idx\n    )\n    channel_df[\"shank\"] = shanks\n\n    # add distance from pyr layer (will only be accurate if polarity rev)\n    channel_df[\"channelDistance\"] = data[name][\"channelDistance\"][0][0].T[0]\n\n    # add channel class (deep or superficial)\n    channelClass = []\n    for item in data[name][\"channelClass\"][0][0]:\n        try:\n            channelClass.append(item[0][0])\n        except Exception:\n            channelClass.append(\"unknown\")\n    channel_df[\"channelClass\"] = channelClass\n\n    # add if shank has polarity reversal\n    for shank in channel_df.shank.unique():\n        if channel_df[channel_df.shank == shank].channelClass.unique().shape[0] == 2:\n            channel_df.loc[channel_df.shank == shank, \"polarity_reversal\"] = True\n        else:\n            channel_df.loc[channel_df.shank == shank, \"polarity_reversal\"] = False\n\n    # add ripple and sharp wave features\n    labels = [\"ripple_power\", \"ripple_amplitude\", \"SWR_diff\", \"SWR_amplitude\"]\n    for label in labels:\n        try:\n            channel_df.loc[channel_sort_idx, label] = np.hstack(\n                data[name][label][0][0][0]\n            )[0]\n        except Exception:\n            x = np.arange(len(channel_sort_idx)) * np.nan\n            x[0 : len(np.hstack(data[name][label][0][0][0])[0])] = np.hstack(\n                data[name][label][0][0][0]\n            )[0]\n            channel_df.loc[channel_sort_idx, label] = x\n\n    # pull put avg ripple traces and ts\n    ripple_time_axis = data[name][\"ripple_time_axis\"][0][0][0]\n    ripple_average = np.ones([channel_df.shape[0], len(ripple_time_axis)]) * np.nan\n\n    rip_map = []\n    for ch, values in zip(channels_, data[name][\"ripple_average\"][0][0][0]):\n        if values.shape[1] &gt; 0:\n            rip_map.append(values)\n        else:\n            rip_map.append(np.zeros([len(ripple_time_axis), len(ch)]) * np.nan)\n\n    ripple_average[channel_sort_idx] = np.hstack(rip_map).T\n\n    brainRegions = load_brain_regions(basepath)\n    for key, value in brainRegions.items():\n        if (\"ca1\" in key.lower()) | (\"ca2\" in key.lower()):\n            for shank in value[\"electrodeGroups\"]:\n                channel_df.loc[channel_df.shank == shank, \"ca1_shank\"] = True\n\n    if (ripple_average.shape[0] != channel_df.shape[0]) &amp; (~bypass_mismatch_exception):\n        raise Exception(\n            \"size mismatch \"\n            + str(np.hstack(ripple_average).shape[1])\n            + \" and \"\n            + str(channel_df.shape[0])\n        )\n\n    channel_df[\"basepath\"] = basepath\n\n    return channel_df, ripple_average, ripple_time_axis\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_dentate_spikes","title":"<code>load_dentate_spikes(basepath, dentate_spike_type=['DS1', 'DS2'], manual_events=True, return_epoch_array=False)</code>","text":"<p>Load info from DS*.events.mat and store within a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to your session where DS*.events.mat is located.</p> required <code>dentate_spike_type</code> <code>List[str]</code> <p>List of DS types to load, by default [\"DS1\", \"DS2\"].</p> <code>['DS1', 'DS2']</code> <code>manual_events</code> <code>bool</code> <p>If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.</p> <code>True</code> <code>return_epoch_array</code> <code>bool</code> <p>If True, the output will be an EpochArray, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame with the following fields: - start: start time of DS - stop: end time of DS - peaks: peak time of DS - amplitude: envelope value at peak time - duration: DS duration - detectorName: the name of DS detector used - basepath: path name - basename: session id - animal: animal id</p> Notes <ul> <li>Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.</li> </ul> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_dentate_spikes(\n    basepath: str,\n    dentate_spike_type: List[str] = [\"DS1\", \"DS2\"],\n    manual_events: bool = True,\n    return_epoch_array: bool = False,\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Load info from DS*.events.mat and store within a DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to your session where DS*.events.mat is located.\n    dentate_spike_type : List[str], optional\n        List of DS types to load, by default [\"DS1\", \"DS2\"].\n    manual_events : bool, optional\n        If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.\n    return_epoch_array : bool, optional\n        If True, the output will be an EpochArray, by default False.\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame with the following fields:\n        - start: start time of DS\n        - stop: end time of DS\n        - peaks: peak time of DS\n        - amplitude: envelope value at peak time\n        - duration: DS duration\n        - detectorName: the name of DS detector used\n        - basepath: path name\n        - basename: session id\n        - animal: animal id\n\n    Notes\n    -----\n    * Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.\n    \"\"\"\n\n    def extract_data(s_type, data, manual_events):\n        # make data frame of known fields\n        df = pd.DataFrame()\n        df[\"start\"] = data[s_type][\"timestamps\"][:, 0]\n        df[\"stop\"] = data[s_type][\"timestamps\"][:, 1]\n        df[\"peaks\"] = data[s_type][\"peaks\"]\n        df[\"event_label\"] = s_type\n        df[\"amplitude\"] = data[s_type][\"amplitudes\"]\n        df[\"duration\"] = data[s_type][\"duration\"]\n        df[\"amplitudeUnits\"] = data[s_type][\"amplitudeUnits\"]\n        df[\"detectorName\"] = data[s_type][\"detectorinfo\"][\"detectorname\"]\n        df[\"ml_channel\"] = data[s_type][\"detectorinfo\"][\"ml_channel\"]\n        df[\"h_channel\"] = data[s_type][\"detectorinfo\"][\"h_channel\"]\n\n        # remove flagged ripples, if exist\n        try:\n            df.drop(\n                labels=np.array(data[s_type][\"flagged\"]).T - 1,\n                axis=0,\n                inplace=True,\n            )\n            df.reset_index(inplace=True)\n        except Exception:\n            pass\n\n        # adding manual events\n        if manual_events:\n            try:\n                df = _add_manual_events(df, data[s_type][\"added\"])\n            except Exception:\n                pass\n        return df\n\n    # locate .mat file\n    df = pd.DataFrame()\n    for s_type in dentate_spike_type:\n        filename = glob.glob(basepath + os.sep + \"*\" + s_type + \".events.mat\")\n        if len(filename) == 0:\n            continue\n        # load matfile\n        filename = filename[0]\n        data = sio.loadmat(filename, simplify_cells=True)\n        # pull out data\n        df = pd.concat(\n            [df, extract_data(s_type, data, manual_events)], ignore_index=True\n        )\n\n    if df.shape[0] == 0:\n        return df\n\n    if return_epoch_array:\n        return nel.EpochArray([np.array([df.start, df.stop]).T], label=\"dentate_spike\")\n\n    # get basename and animal\n    normalized_path = os.path.normpath(filename)\n    path_components = normalized_path.split(os.sep)\n    df[\"basepath\"] = basepath\n    df[\"basename\"] = path_components[-2]\n    df[\"animal\"] = path_components[-3]\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_emg","title":"<code>load_emg(basepath, threshold=0.9)</code>","text":"<p>Load EMG data from basename.EMGFromLFP.LFP.mat.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>threshold</code> <code>float</code> <p>Threshold for high epochs (low will be &lt; threshold). Default is 0.9.</p> <code>0.9</code> <p>Returns:</p> Name Type Description <code>emg</code> <code>AnalogSignalArray</code> <p>EMG data.</p> <code>high_emg_epoch</code> <code>EpochArray</code> <p>High EMG epochs.</p> <code>low_emg_epoch</code> <code>EpochArray</code> <p>Low EMG epochs.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_emg(\n    basepath: str, threshold: float = 0.9\n) -&gt; Tuple[nel.AnalogSignalArray, nel.EpochArray, nel.EpochArray]:\n    \"\"\"\n    Load EMG data from basename.EMGFromLFP.LFP.mat.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    threshold : float, optional\n        Threshold for high epochs (low will be &lt; threshold). Default is 0.9.\n\n    Returns\n    -------\n    emg : nel.AnalogSignalArray\n        EMG data.\n    high_emg_epoch : nel.EpochArray\n        High EMG epochs.\n    low_emg_epoch : nel.EpochArray\n        Low EMG epochs.\n    \"\"\"\n    # locate .mat file\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".EMGFromLFP.LFP.mat\"\n    )\n\n    # load matfile\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    # put emg data into AnalogSignalArray\n    emg = nel.AnalogSignalArray(\n        data=data[\"EMGFromLFP\"][\"data\"], timestamps=data[\"EMGFromLFP\"][\"timestamps\"]\n    )\n\n    # get high and low emg epochs\n    high_emg_epoch = find_interval(emg.data.flatten() &gt; threshold)\n    high_emg_epoch = nel.EpochArray(emg.abscissa_vals[high_emg_epoch])\n\n    low_emg_epoch = find_interval(emg.data.flatten() &lt; threshold)\n    low_emg_epoch = nel.EpochArray(emg.abscissa_vals[low_emg_epoch])\n\n    return emg, high_emg_epoch, low_emg_epoch\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_epoch","title":"<code>load_epoch(basepath)</code>","text":"<p>Loads epoch info from cell explorer basename.session and stores in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the following fields: - name: name of the epoch - startTime: start time of the epoch - stopTime: stop time of the epoch - environment: environment during the epoch - manipulation: manipulation during the epoch - behavioralParadigm: behavioral paradigm during the epoch - stimuli: stimuli during the epoch - notes: notes about the epoch - basepath: path to the session folder</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_epoch(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads epoch info from cell explorer basename.session and stores in a DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with the following fields:\n        - name: name of the epoch\n        - startTime: start time of the epoch\n        - stopTime: stop time of the epoch\n        - environment: environment during the epoch\n        - manipulation: manipulation during the epoch\n        - behavioralParadigm: behavioral paradigm during the epoch\n        - stimuli: stimuli during the epoch\n        - notes: notes about the epoch\n        - basepath: path to the session folder\n    \"\"\"\n\n    filename = os.path.join(basepath, os.path.basename(basepath) + \".session.mat\")\n\n    if not os.path.exists(filename):\n        warnings.warn(f\"file {filename} does not exist\")\n        return pd.DataFrame()\n\n    # load file\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    def add_columns(df):\n        \"\"\"add columns to df if they don't exist\"\"\"\n        needed_columns = [\n            \"name\",\n            \"startTime\",\n            \"stopTime\",\n            \"environment\",\n            \"manipulation\",\n            \"behavioralParadigm\",\n            \"stimuli\",\n            \"notes\",\n        ]\n        for col in needed_columns:\n            if col not in df.columns:\n                df[col] = np.nan\n        return df\n\n    try:\n        epoch_df = pd.DataFrame(data[\"session\"][\"epochs\"])\n        epoch_df = add_columns(epoch_df)\n        epoch_df[\"basepath\"] = basepath\n        return epoch_df\n    except Exception:\n        epoch_df = pd.DataFrame([data[\"session\"][\"epochs\"]])\n        epoch_df = add_columns(epoch_df)\n        epoch_df[\"basepath\"] = basepath\n        return epoch_df\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_events","title":"<code>load_events(basepath, epoch_name, load_pandas=False)</code>","text":"<p>Load events from basename.epoch_name.events.mat.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>epoch_name</code> <code>str</code> <p>Name of epoch to load.</p> required <p>Returns:</p> Name Type Description <code>events</code> <code>EpochArray or None or DataFrame</code> <p>Events, or None if the file does not exist.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_events(\n    basepath: str, epoch_name: str, load_pandas=False\n) -&gt; Union[nel.EpochArray, None, pd.DataFrame]:\n    \"\"\"\n    Load events from basename.epoch_name.events.mat.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    epoch_name : str\n        Name of epoch to load.\n\n    Returns\n    -------\n    events : nel.EpochArray or None or pd.DataFrame\n        Events, or None if the file does not exist.\n    \"\"\"\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".\" + epoch_name + \".events.mat\"\n    )\n    # check if filename exist\n    if not os.path.exists(filename):\n        return None\n\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    if load_pandas:\n        n_events = data[epoch_name][\"timestamps\"].shape[0]\n\n        event_df = pd.DataFrame(\n            data[epoch_name][\"timestamps\"], columns=[\"starts\", \"stops\"]\n        )\n        for key in data[epoch_name].keys():\n            if (\n                isinstance(data[epoch_name][key], np.ndarray)\n                and data[epoch_name][key].shape[0] == n_events\n                and data[epoch_name][key].ndim == 1\n            ):\n                event_df[key] = data[epoch_name][key]\n        return event_df\n\n    return nel.EpochArray(data[epoch_name][\"timestamps\"])\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_extracellular_metadata","title":"<code>load_extracellular_metadata(basepath)</code>","text":"<p>Load extracellular metadata from session file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the directory containing the session file.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of extracellular metadata from the session file.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_extracellular_metadata(basepath: str) -&gt; dict:\n    \"\"\"\n    Load extracellular metadata from session file.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the directory containing the session file.\n\n    Returns\n    -------\n    dict\n        A dictionary of extracellular metadata from the session file.\n    \"\"\"\n    filename = os.path.join(basepath, os.path.basename(basepath) + \".session.mat\")\n    # check if filename exist\n    if not os.path.exists(filename):\n        return {}\n    data = sio.loadmat(filename, simplify_cells=True)\n    return data[\"session\"][\"extracellular\"]\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_ied_events","title":"<code>load_ied_events(basepath, manual_events=True, return_epoch_array=False)</code>","text":"<p>Load info from ripples.events.mat and store within a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to your session where ripples.events.mat is located.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, the output will be an EpochArray, by default False.</p> <code>False</code> <code>manual_events</code> <code>bool</code> <p>If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame with the following fields: - start: start time of ripple - stop: end time of ripple - center: center time of ripple - peaks: peak time of ripple</p> Notes <ul> <li>Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.</li> </ul> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_ied_events(\n    basepath: str, manual_events: bool = True, return_epoch_array: bool = False\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Load info from ripples.events.mat and store within a DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to your session where ripples.events.mat is located.\n    return_epoch_array : bool, optional\n        If True, the output will be an EpochArray, by default False.\n    manual_events : bool, optional\n        If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame with the following fields:\n        - start: start time of ripple\n        - stop: end time of ripple\n        - center: center time of ripple\n        - peaks: peak time of ripple\n\n    Notes\n    -----\n    * Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.\n    \"\"\"\n\n    # locate .mat file\n    try:\n        filename = glob.glob(basepath + os.sep + \"*IED.events.mat\")[0]\n    except Exception:\n        try:\n            filename = glob.glob(basepath + os.sep + \"*interictal_spikes.events.mat\")[0]\n        except Exception:\n            # warnings.warn(\"file does not exist\")\n            return pd.DataFrame()\n\n    df = pd.DataFrame()\n\n    data = sio.loadmat(filename, simplify_cells=True)\n    struct_name = list(data.keys())[-1]\n    df[\"start\"] = data[struct_name][\"timestamps\"][:, 0]\n    df[\"stop\"] = data[struct_name][\"timestamps\"][:, 1]\n    df[\"center\"] = data[struct_name][\"peaks\"]\n    df[\"peaks\"] = data[struct_name][\"peaks\"]\n\n    # remove flagged ripples, if exist\n    try:\n        df.drop(\n            labels=np.array(data[struct_name][\"flagged\"]).T - 1,\n            axis=0,\n            inplace=True,\n        )\n        df.reset_index(inplace=True)\n    except Exception:\n        pass\n\n    # adding manual events\n    if manual_events:\n        try:\n            df = _add_manual_events(df, data[struct_name][\"added\"])\n        except Exception:\n            pass\n\n    if return_epoch_array:\n        return nel.EpochArray([np.array([df.start, df.stop]).T], label=\"ied\")\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_manipulation","title":"<code>load_manipulation(basepath, struct_name=None, return_epoch_array=True, merge_gap=None)</code>","text":"<p>Loads the data from the basename.eventName.manipulations.mat file and returns a pandas dataframe.</p> <p>file structure defined here:     https://cellexplorer.org/datastructure/data-structure-and-format/#manipulations</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the basename.eventName.manipulations.mat file.</p> required <code>struct_name</code> <code>Union[str, None]</code> <p>Name of the structure in the mat file to load. If None, loads all the manipulation files, by default None.</p> <code>None</code> <code>return_epoch_array</code> <code>bool</code> <p>If True, returns only the epoch array, by default True.</p> <code>True</code> <code>merge_gap</code> <code>Union[int, None]</code> <p>If not None, merges the epochs that are separated by less than merge_gap (sec). return_epoch_array must be True, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame or EpochArray with the manipulation data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; basepath = r\"Z:\\Data\\Can\\OML22\\day8\"\n&gt;&gt;&gt; df_manipulation = load_manipulation(basepath, struct_name=\"optoStim\", return_epoch_array=False)\n&gt;&gt;&gt; df_manipulation.head(2)\n</code></pre> <p>.. table:: Manipulation Data     :widths: auto</p> <pre><code>====== ========== ========== ========== ========== ========== ========================\n    start      stop       peaks      center     duration    amplitude     amplitudeUnits\n====== ========== ========== ========== ========== ========== ========================\n8426.83650  8426.84845  8426.842475  8426.842475  0.01195   19651       pulse_respect_baseline\n8426.85245  8426.86745  8426.859950  8426.859950  0.01500   17516       pulse_respect_baseline\n====== ========== ========== ========== ========== ========== ========================\n</code></pre> <pre><code>&gt;&gt;&gt; basepath = r\"Z:\\Data\\Can\\OML22\\day8\"\n&gt;&gt;&gt; df_manipulation = load_manipulation(basepath, struct_name=\"optoStim\", return_epoch_array=True)\n&gt;&gt;&gt; df_manipulation\n</code></pre> <p> of length 1:25:656 minutes Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_manipulation(\n    basepath: str,\n    struct_name: Union[str, None] = None,\n    return_epoch_array: bool = True,\n    merge_gap: Union[int, None] = None,\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Loads the data from the basename.eventName.manipulations.mat file and returns a pandas dataframe.\n\n    file structure defined here:\n        https://cellexplorer.org/datastructure/data-structure-and-format/#manipulations\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the basename.eventName.manipulations.mat file.\n    struct_name : Union[str, None], optional\n        Name of the structure in the mat file to load. If None, loads all the manipulation files, by default None.\n    return_epoch_array : bool, optional\n        If True, returns only the epoch array, by default True.\n    merge_gap : Union[int, None], optional\n        If not None, merges the epochs that are separated by less than merge_gap (sec). return_epoch_array must be True, by default None.\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame or EpochArray with the manipulation data.\n\n    Examples\n    -------\n    &gt;&gt;&gt; basepath = r\"Z:\\Data\\Can\\OML22\\day8\"\n    &gt;&gt;&gt; df_manipulation = load_manipulation(basepath, struct_name=\"optoStim\", return_epoch_array=False)\n    &gt;&gt;&gt; df_manipulation.head(2)\n\n    .. table:: Manipulation Data\n        :widths: auto\n\n        ====== ========== ========== ========== ========== ========== ========================\n            start      stop       peaks      center     duration    amplitude     amplitudeUnits\n        ====== ========== ========== ========== ========== ========== ========================\n        8426.83650  8426.84845  8426.842475  8426.842475  0.01195   19651       pulse_respect_baseline\n        8426.85245  8426.86745  8426.859950  8426.859950  0.01500   17516       pulse_respect_baseline\n        ====== ========== ========== ========== ========== ========== ========================\n\n    &gt;&gt;&gt; basepath = r\"Z:\\Data\\Can\\OML22\\day8\"\n    &gt;&gt;&gt; df_manipulation = load_manipulation(basepath, struct_name=\"optoStim\", return_epoch_array=True)\n    &gt;&gt;&gt; df_manipulation\n\n    &lt;EpochArray at 0x1faba577520: 5,774 epochs&gt; of length 1:25:656 minutes\n    \"\"\"\n    try:\n        if struct_name is None:\n            filename = glob.glob(basepath + os.sep + \"*manipulation.mat\")\n            print(filename)\n            if len(filename) &gt; 1:\n                raise ValueError(\n                    \"multi-file not implemented yet...than one manipulation file found\"\n                )\n            filename = filename[0]\n        else:\n            filename = glob.glob(\n                basepath + os.sep + \"*\" + struct_name + \".manipulation.mat\"\n            )[0]\n    except Exception:\n        return None\n    # load matfile\n    data = sio.loadmat(filename)\n\n    if struct_name is None:\n        struct_name = list(data.keys())[-1]\n\n    df = pd.DataFrame()\n    df[\"start\"] = data[struct_name][\"timestamps\"][0][0][:, 0]\n    df[\"stop\"] = data[struct_name][\"timestamps\"][0][0][:, 1]\n    df[\"peaks\"] = data[struct_name][\"peaks\"][0][0]\n    df[\"center\"] = data[struct_name][\"center\"][0][0]\n    df[\"duration\"] = data[struct_name][\"duration\"][0][0]\n    df[\"amplitude\"] = data[struct_name][\"amplitude\"][0][0]\n    df[\"amplitudeUnits\"] = data[struct_name][\"amplitudeUnits\"][0][0][0]\n\n    # extract event label names\n    eventIDlabels = []\n    for name in data[struct_name][\"eventIDlabels\"][0][0][0]:\n        eventIDlabels.append(name[0])\n\n    # extract numeric category labels associated with label names\n    eventID = np.array(data[struct_name][\"eventID\"][0][0]).ravel()\n\n    # add eventIDlabels and eventID to df\n    for ev_label, ev_num in zip(eventIDlabels, np.unique(eventID)):\n        df.loc[eventID == ev_num, \"ev_label\"] = ev_label\n\n    if return_epoch_array:\n        # get session epochs to add support for epochs\n        epoch_df = load_epoch(basepath)\n        # get session bounds to provide support\n        session_bounds = nel.EpochArray(\n            [epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]]\n        )\n        # if many types of manipulations, add them to dictinary\n        if df.ev_label.unique().size &gt; 1:\n            manipulation_epoch = {}\n            for label in df.ev_label.unique():\n                manipulation_epoch_ = nel.EpochArray(\n                    np.array(\n                        [\n                            df[df.ev_label == label][\"start\"],\n                            df[df.ev_label == label][\"stop\"],\n                        ]\n                    ).T,\n                    domain=session_bounds,\n                )\n                if merge_gap is not None:\n                    manipulation_epoch_ = manipulation_epoch_.merge(gap=merge_gap)\n\n                manipulation_epoch[label] = manipulation_epoch_\n        else:\n            manipulation_epoch = nel.EpochArray(\n                np.array([df[\"start\"], df[\"stop\"]]).T, domain=session_bounds\n            )\n            if merge_gap is not None:\n                manipulation_epoch = manipulation_epoch.merge(gap=merge_gap)\n\n        return manipulation_epoch\n    else:\n        return df\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_mua_events","title":"<code>load_mua_events(basepath)</code>","text":"<p>Loads the MUA data from the basepath. Meant to load .mat file created by find_HSE.m.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The path to the folder containing the MUA data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The pandas DataFrame containing the MUA data.</p> TODO <p>If none exist in basepath, create one.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_mua_events(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads the MUA data from the basepath.\n    Meant to load .mat file created by find_HSE.m.\n\n    Parameters\n    ----------\n    basepath : str\n        The path to the folder containing the MUA data.\n\n    Returns\n    -------\n    pd.DataFrame\n        The pandas DataFrame containing the MUA data.\n\n    TODO\n    ----\n    If none exist in basepath, create one.\n    \"\"\"\n\n    # locate .mat file\n    try:\n        filename = glob.glob(basepath + os.sep + \"*mua_ca1_pyr.events.mat\")[0]\n    except Exception:\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame()\n\n    # load matfile\n    data = sio.loadmat(filename)\n\n    # pull out and package data\n    df = pd.DataFrame()\n    df[\"start\"] = data[\"HSE\"][\"timestamps\"][0][0][:, 0]\n    df[\"stop\"] = data[\"HSE\"][\"timestamps\"][0][0][:, 1]\n    df[\"peaks\"] = data[\"HSE\"][\"peaks\"][0][0]\n    df[\"center\"] = data[\"HSE\"][\"center\"][0][0]\n    df[\"duration\"] = data[\"HSE\"][\"duration\"][0][0]\n    df[\"amplitude\"] = data[\"HSE\"][\"amplitudes\"][0][0]\n    df[\"amplitudeUnits\"] = data[\"HSE\"][\"amplitudeUnits\"][0][0][0]\n    df[\"detectorName\"] = data[\"HSE\"][\"detectorinfo\"][0][0][\"detectorname\"][0][0][0]\n\n    # get basename and animal\n    normalized_path = os.path.normpath(filename)\n    path_components = normalized_path.split(os.sep)\n    df[\"basepath\"] = basepath\n    df[\"basename\"] = path_components[-2]\n    df[\"animal\"] = path_components[-3]\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_position","title":"<code>load_position(basepath, fs=39.0625)</code>","text":"<p>Load position data from a .whl file in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the directory containing the .whl file.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency, by default 39.0625.</p> <code>39.0625</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, float]</code> <p>DataFrame containing position data and the sampling frequency.</p> Notes <p>If the directory does not exist or contains no .whl files, the function will exit.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_position(basepath: str, fs: float = 39.0625) -&gt; Tuple[pd.DataFrame, float]:\n    \"\"\"\n    Load position data from a .whl file in the specified directory.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the directory containing the .whl file.\n    fs : float, optional\n        Sampling frequency, by default 39.0625.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, float]\n        DataFrame containing position data and the sampling frequency.\n\n    Notes\n    -----\n    If the directory does not exist or contains no .whl files, the function will exit.\n    \"\"\"\n    if not os.path.exists(basepath):\n        print(\"The path \" + basepath + \" doesn't exist; Exiting ...\")\n        sys.exit()\n    listdir = os.listdir(basepath)\n    whlfiles = [f for f in listdir if f.endswith(\".whl\")]\n    if not len(whlfiles):\n        print(\"Folder contains no whl files; Exiting ...\")\n        sys.exit()\n    new_path = os.path.join(basepath, whlfiles[0])\n    df = pd.read_csv(new_path, delimiter=\"\\t\", header=0, names=[\"x1\", \"y1\", \"x2\", \"y2\"])\n    df[df == -1] = np.nan\n    return df, fs\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_probe_layout","title":"<code>load_probe_layout(basepath)</code>","text":"<p>Load electrode coordinates and grouping from the session.extracellular.mat file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <p>Returns:</p> Name Type Description <code>probe_layout</code> <code>DataFrame</code> <p>DataFrame with x, y coordinates and shank number.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_probe_layout(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Load electrode coordinates and grouping from the session.extracellular.mat file.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n\n    Returns\n    -------\n    probe_layout : pd.DataFrame\n        DataFrame with x, y coordinates and shank number.\n    \"\"\"\n\n    # load session file\n    filename = glob.glob(os.path.join(basepath, \"*.session.mat\"))[0]\n\n    # load file\n    data = sio.loadmat(filename, simplify_cells=True)\n    x = data[\"session\"][\"extracellular\"][\"chanCoords\"][\"x\"]\n    y = data[\"session\"][\"extracellular\"][\"chanCoords\"][\"y\"]\n\n    if (len(x) == 0) &amp; (len(y) == 0):\n        warnings.warn(\n            \"The coordinates are empty in session.extracellular.chanCoords. Returning None - check session file\"\n        )\n        return None\n\n    electrode_groups = data[\"session\"][\"extracellular\"][\"electrodeGroups\"][\"channels\"]\n\n    # for each group in electrodeGroups\n    mapped_shanks = []\n    mapped_channels = []\n\n    n_groups = data[\"session\"][\"extracellular\"][\"nElectrodeGroups\"]\n\n    if n_groups &gt; 1:\n        # loop through electrode groups\n        for group_i in np.arange(n_groups):\n            mapped_channels.append(\n                electrode_groups[group_i] - 1\n            )  # -1 to make 0 indexed\n            mapped_shanks.append(np.repeat(group_i, len(electrode_groups[group_i])))\n\n    elif n_groups == 1:\n        mapped_channels.append(electrode_groups - 1)  # -1 to make 0 indexed\n        mapped_shanks.append(\n            np.repeat(0, len(electrode_groups))\n        )  # electrode group for single shank always 0\n\n    #  unpack to lists\n    mapped_channels = list(chain(*mapped_channels))\n    shanks = list(chain(*mapped_shanks))\n\n    # get shank in same dimension as channels\n    shanks = np.expand_dims(shanks, axis=1)\n\n    probe_layout = (\n        pd.DataFrame({\"x\": x.flatten(), \"y\": y.flatten()})\n        .iloc[mapped_channels]\n        .reset_index(drop=True)\n    )\n    probe_layout[\"shank\"] = shanks\n    probe_layout[\"channels\"] = mapped_channels\n\n    return probe_layout\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_ripples_events","title":"<code>load_ripples_events(basepath, return_epoch_array=False, manual_events=True)</code>","text":"<p>Load info from ripples.events.mat and store within a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to your session where ripples.events.mat is located.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, the output will be an EpochArray, by default False.</p> <code>False</code> <code>manual_events</code> <code>bool</code> <p>If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame with the following fields: - start: start time of ripple - stop: end time of ripple - peaks: peak time of ripple - amplitude: envelope value at peak time - duration: ripple duration - frequency: instant frequency at peak - detectorName: the name of ripple detector used - event_spk_thres: 1 or 0 for if a mua threshold was used - basepath: path name - basename: session id - animal: animal id</p> Notes <ul> <li>Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.</li> </ul> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_ripples_events(\n    basepath: str, return_epoch_array: bool = False, manual_events: bool = True\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Load info from ripples.events.mat and store within a DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to your session where ripples.events.mat is located.\n    return_epoch_array : bool, optional\n        If True, the output will be an EpochArray, by default False.\n    manual_events : bool, optional\n        If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame with the following fields:\n        - start: start time of ripple\n        - stop: end time of ripple\n        - peaks: peak time of ripple\n        - amplitude: envelope value at peak time\n        - duration: ripple duration\n        - frequency: instant frequency at peak\n        - detectorName: the name of ripple detector used\n        - event_spk_thres: 1 or 0 for if a mua threshold was used\n        - basepath: path name\n        - basename: session id\n        - animal: animal id\n\n    Notes\n    -----\n    * Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.\n    \"\"\"\n\n    # locate .mat file\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".ripples.events.mat\"\n    )\n    if not os.path.exists(filename):\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame()\n\n    # load matfile\n    data = sio.loadmat(filename)\n\n    # make data frame of known fields\n    df = pd.DataFrame()\n    try:\n        df[\"start\"] = data[\"ripples\"][\"timestamps\"][0][0][:, 0]\n        df[\"stop\"] = data[\"ripples\"][\"timestamps\"][0][0][:, 1]\n    except Exception:\n        df[\"start\"] = data[\"ripples\"][\"times\"][0][0][:, 0]\n        df[\"stop\"] = data[\"ripples\"][\"times\"][0][0][:, 1]\n\n    for name in [\"peaks\", \"amplitude\", \"duration\", \"frequency\", \"peakNormedPower\"]:\n        try:\n            df[name] = data[\"ripples\"][name][0][0]\n        except Exception:\n            df[name] = np.nan\n\n    if df.duration.isna().all():\n        df[\"duration\"] = df.stop - df.start\n\n    try:\n        df[\"detectorName\"] = data[\"ripples\"][\"detectorinfo\"][0][0][\"detectorname\"][0][\n            0\n        ][0]\n    except Exception:\n        try:\n            df[\"detectorName\"] = data[\"ripples\"][\"detectorName\"][0][0][0]\n        except Exception:\n            df[\"detectorName\"] = \"unknown\"\n\n    # find ripple channel (this can be in several places depending on the file)\n    try:\n        df[\"ripple_channel\"] = data[\"ripples\"][\"detectorinfo\"][0][0][\"detectionparms\"][\n            0\n        ][0][\"Channels\"][0][0][0][0]\n    except Exception:\n        try:\n            df[\"ripple_channel\"] = data[\"ripples\"][\"detectorParams\"][0][0][\"channel\"][\n                0\n            ][0][0][0]\n        except Exception:\n            try:\n                df[\"ripple_channel\"] = data[\"ripples\"][\"detectorinfo\"][0][0][\n                    \"detectionparms\"\n                ][0][0][\"channel\"][0][0][0][0]\n            except Exception:\n                try:\n                    df[\"ripple_channel\"] = data[\"ripples\"][\"detectorinfo\"][0][0][\n                        \"detectionparms\"\n                    ][0][0][\"ripple_channel\"][0][0][0][0]\n                except Exception:\n                    try:\n                        df[\"ripple_channel\"] = data[\"ripples\"][\"detectorinfo\"][0][0][\n                            \"detectionchannel1\"\n                        ][0][0][0][0]\n                    except Exception:\n                        df[\"ripple_channel\"] = np.nan\n\n    # remove flagged ripples, if exist\n    try:\n        df.drop(\n            labels=np.array(data[\"ripples\"][\"flagged\"][0][0]).T[0] - 1,\n            axis=0,\n            inplace=True,\n        )\n        df.reset_index(inplace=True)\n    except Exception:\n        pass\n\n    # adding manual events\n    if manual_events:\n        try:\n            df = _add_manual_events(df, data[\"ripples\"][\"added\"][0][0].T[0])\n        except Exception:\n            pass\n\n    # adding if ripples were restricted by spikes\n    dt = data[\"ripples\"].dtype\n    if \"eventSpikingParameters\" in dt.names:\n        df[\"event_spk_thres\"] = 1\n    else:\n        df[\"event_spk_thres\"] = 0\n\n    # get basename and animal\n    normalized_path = os.path.normpath(filename)\n    path_components = normalized_path.split(os.sep)\n    df[\"basepath\"] = basepath\n    df[\"basename\"] = path_components[-2]\n    df[\"animal\"] = path_components[-3]\n\n    if return_epoch_array:\n        return nel.EpochArray([np.array([df.start, df.stop]).T], label=\"ripples\")\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_spikes","title":"<code>load_spikes(basepath, putativeCellType=[], brainRegion=[], remove_bad_unit=True, brain_state=[], other_metric=None, other_metric_value=None, support=None, remove_unstable=False, stable_interval_width=600)</code>","text":"<p>Load specific cells' spike times.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>putativeCellType</code> <code>List[str]</code> <p>List of putative cell types to restrict spikes to, by default [].</p> <code>[]</code> <code>brainRegion</code> <code>List[str]</code> <p>List of brain regions to restrict spikes to, by default [].</p> <code>[]</code> <code>remove_bad_unit</code> <code>bool</code> <p>If True, do not load bad cells (tagged in CE), by default True.</p> <code>True</code> <code>brain_state</code> <code>List[str]</code> <p>List of brain states to restrict spikes to, by default [].</p> <code>[]</code> <code>other_metric</code> <code>Union[str, None]</code> <p>Metric to restrict spikes to, by default None.</p> <code>None</code> <code>other_metric_value</code> <code>Union[str, None]</code> <p>Value of the metric to restrict spikes to, by default None.</p> <code>None</code> <code>support</code> <code>Union[EpochArray, None]</code> <p>Time support to provide, by default None.</p> <code>None</code> <code>remove_unstable</code> <code>bool</code> <p>If True, remove unstable cells, by default False.</p> <code>False</code> <code>stable_interval_width</code> <code>int</code> <p>Width of the stable interval in seconds, by default 600.</p> <code>600</code> <p>Returns:</p> Type Description <code>Tuple[Union[SpikeTrainArray, None], Union[DataFrame, None]]</code> <p>Spike train array and cell metrics DataFrame.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_spikes(\n    basepath: str,\n    putativeCellType: List[str] = [],\n    brainRegion: List[str] = [],\n    remove_bad_unit: bool = True,\n    brain_state: List[str] = [],\n    other_metric: Union[str, None] = None,\n    other_metric_value: Union[str, None] = None,\n    support: Union[nel.EpochArray, None] = None,\n    remove_unstable: bool = False,\n    stable_interval_width: int = 600,\n) -&gt; Tuple[Union[nel.SpikeTrainArray, None], Union[pd.DataFrame, None]]:\n    \"\"\"\n    Load specific cells' spike times.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    putativeCellType : List[str], optional\n        List of putative cell types to restrict spikes to, by default [].\n    brainRegion : List[str], optional\n        List of brain regions to restrict spikes to, by default [].\n    remove_bad_unit : bool, optional\n        If True, do not load bad cells (tagged in CE), by default True.\n    brain_state : List[str], optional\n        List of brain states to restrict spikes to, by default [].\n    other_metric : Union[str, None], optional\n        Metric to restrict spikes to, by default None.\n    other_metric_value : Union[str, None], optional\n        Value of the metric to restrict spikes to, by default None.\n    support : Union[nel.EpochArray, None], optional\n        Time support to provide, by default None.\n    remove_unstable : bool, optional\n        If True, remove unstable cells, by default False.\n    stable_interval_width : int, optional\n        Width of the stable interval in seconds, by default 600.\n\n    Returns\n    -------\n    Tuple[Union[nel.SpikeTrainArray, None], Union[pd.DataFrame, None]]\n        Spike train array and cell metrics DataFrame.\n    \"\"\"\n    if not isinstance(putativeCellType, list):\n        putativeCellType = [putativeCellType]\n    if not isinstance(brainRegion, list):\n        brainRegion = [brainRegion]\n\n    # get sample rate from session\n    fs_dat = load_extracellular_metadata(basepath).get(\"sr\", None)\n\n    if fs_dat is None:\n        return None, None\n\n    # load cell metrics and spike data\n    cell_metrics, data = load_cell_metrics(basepath)\n\n    if cell_metrics is None or data is None:\n        return None, None\n\n    # put spike data into array st\n    st = np.array(data[\"spikes\"], dtype=object)\n\n    # restrict cell metrics\n    if len(putativeCellType) &gt; 0:\n        restrict_idx = []\n        for cell_type in putativeCellType:\n            restrict_idx.append(\n                cell_metrics.putativeCellType.str.contains(cell_type).values\n            )\n        restrict_idx = np.any(restrict_idx, axis=0)\n        cell_metrics = cell_metrics[restrict_idx]\n        st = st[restrict_idx]\n\n    if len(brainRegion) &gt; 0:\n        restrict_idx = []\n        for brain_region in brainRegion:\n            restrict_idx.append(\n                cell_metrics.brainRegion.str.contains(brain_region).values\n            )\n        restrict_idx = np.any(restrict_idx, axis=0)\n        cell_metrics = cell_metrics[restrict_idx]\n        st = st[restrict_idx]\n\n    # restrict cell metrics by arbitrary metric\n    if other_metric is not None:\n        # make other_metric_value a list if not already\n        if not isinstance(other_metric, list):\n            other_metric = [other_metric]\n        if not isinstance(other_metric_value, list):\n            other_metric_value = [other_metric_value]\n        # check that other_metric_value is the same length as other_metric\n        if len(other_metric) != len(other_metric_value):\n            raise ValueError(\n                \"other_metric and other_metric_value must be of same length\"\n            )\n\n        restrict_idx = []\n        for metric, value in zip(other_metric, other_metric_value):\n            restrict_idx.append(cell_metrics[metric].str.contains(value).values)\n        restrict_idx = np.any(restrict_idx, axis=0)\n        cell_metrics = cell_metrics[restrict_idx]\n        st = st[restrict_idx]\n\n    if remove_bad_unit:\n        # bad units will be tagged true, so only keep false values\n        restrict_idx = ~cell_metrics.bad_unit.values\n        cell_metrics = cell_metrics[restrict_idx]\n        st = st[restrict_idx]\n\n    if remove_unstable and len(st) &gt; 0:\n        starts = np.arange(\n            np.hstack(st).min(),\n            np.hstack(st).max() - stable_interval_width,\n            stable_interval_width,\n        )\n        stops = starts + stable_interval_width\n\n        bst = npy.process.count_in_interval(st, starts, stops, \"counts\")\n        restrict_idx = np.sum(bst == 0, axis=1) &lt; 2  # allow for 1 unstable interval max\n        cell_metrics = cell_metrics[restrict_idx]\n        st = st[restrict_idx]\n\n    # get spike train array\n    try:\n        if support is not None:\n            st = nel.SpikeTrainArray(timestamps=st, fs=fs_dat, support=support)\n        else:\n            st = nel.SpikeTrainArray(timestamps=st, fs=fs_dat)\n    except Exception:  # if only single cell... should prob just skip session\n        if support is not None:\n            st = nel.SpikeTrainArray(timestamps=st[0], fs=fs_dat, support=support)\n        else:\n            st = nel.SpikeTrainArray(timestamps=st[0], fs=fs_dat)\n\n    if len(brain_state) &gt; 0:\n        # get brain states\n        brain_states = [\"WAKEstate\", \"NREMstate\", \"REMstate\", \"THETA\", \"nonTHETA\"]\n        if brain_state not in brain_states:\n            assert print(\"not correct brain state. Pick one\", brain_states)\n        else:\n            state_dict = load_SleepState_states(basepath)\n            state_epoch = nel.EpochArray(state_dict[brain_state])\n            st = st[state_epoch]\n\n    return st, cell_metrics\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_theta_cycles","title":"<code>load_theta_cycles(basepath, return_epoch_array=False)</code>","text":"<p>Load theta cycles calculated from auto_theta_cycles.m.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to your session where thetacycles.events.mat is located.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, the output will be an EpochArray, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame with the following fields: - start: start time of theta cycle - stop: end time of theta cycle - duration: theta cycle duration - center: center time of theta cycle - trough: trough time of theta cycle - theta_channel: the theta channel used for detection</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_theta_cycles(\n    basepath: str, return_epoch_array: bool = False\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Load theta cycles calculated from auto_theta_cycles.m.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to your session where thetacycles.events.mat is located.\n    return_epoch_array : bool, optional\n        If True, the output will be an EpochArray, by default False.\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame with the following fields:\n        - start: start time of theta cycle\n        - stop: end time of theta cycle\n        - duration: theta cycle duration\n        - center: center time of theta cycle\n        - trough: trough time of theta cycle\n        - theta_channel: the theta channel used for detection\n    \"\"\"\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".thetacycles.events.mat\"\n    )\n    if not os.path.exists(filename):\n        warnings.warn(\"file does not exist\")\n        if return_epoch_array:\n            return nel.EpochArray()\n        return pd.DataFrame()\n\n    data = sio.loadmat(filename, simplify_cells=True)\n    df = pd.DataFrame()\n    df[\"start\"] = data[\"thetacycles\"][\"timestamps\"][:, 0]\n    df[\"stop\"] = data[\"thetacycles\"][\"timestamps\"][:, 1]\n    df[\"duration\"] = data[\"thetacycles\"][\"duration\"]\n    df[\"center\"] = data[\"thetacycles\"][\"center\"]\n    df[\"trough\"] = data[\"thetacycles\"][\"peaks\"]\n    df[\"theta_channel\"] = data[\"thetacycles\"][\"detectorinfo\"][\"theta_channel\"]\n\n    if return_epoch_array:\n        return nel.EpochArray([np.array([df.start, df.stop]).T], label=\"theta_cycles\")\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_theta_rem_shift","title":"<code>load_theta_rem_shift(basepath)</code>","text":"<p>Load theta REM shift data from get_rem_shift.m.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to your session where theta_rem_shift.mat is located.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, dict]</code> <p>DataFrame with the following fields: - UID: unique identifier for each unit - circ_dist: circular distance - rem_shift: REM shift - non_rem_shift: non-REM shift - m_rem: mean phase locking value during REM - r_rem: resultant vector length during REM - k_rem: concentration parameter during REM - p_rem: p-value of phase locking during REM - mode_rem: mode of phase locking during REM - m_wake: mean phase locking value during wake - r_wake: resultant vector length during wake - k_wake: concentration parameter during wake - p_wake: p-value of phase locking during wake - mode_wake: mode of phase locking during wake</p> <code>dict</code> <p>Dictionary with phase distributions and spike phases for REM and wake states.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_theta_rem_shift(basepath: str) -&gt; Tuple[pd.DataFrame, dict]:\n    \"\"\"\n    Load theta REM shift data from get_rem_shift.m.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to your session where theta_rem_shift.mat is located.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, dict]\n        DataFrame with the following fields:\n        - UID: unique identifier for each unit\n        - circ_dist: circular distance\n        - rem_shift: REM shift\n        - non_rem_shift: non-REM shift\n        - m_rem: mean phase locking value during REM\n        - r_rem: resultant vector length during REM\n        - k_rem: concentration parameter during REM\n        - p_rem: p-value of phase locking during REM\n        - mode_rem: mode of phase locking during REM\n        - m_wake: mean phase locking value during wake\n        - r_wake: resultant vector length during wake\n        - k_wake: concentration parameter during wake\n        - p_wake: p-value of phase locking during wake\n        - mode_wake: mode of phase locking during wake\n\n    dict\n        Dictionary with phase distributions and spike phases for REM and wake states.\n    \"\"\"\n    try:\n        filename = glob.glob(basepath + os.sep + \"*theta_rem_shift.mat\")[0]\n    except Exception:\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame(), np.nan\n\n    data = sio.loadmat(filename)\n\n    df = pd.DataFrame()\n\n    df[\"UID\"] = data[\"rem_shift_data\"][\"UID\"][0][0][0]\n    df[\"circ_dist\"] = data[\"rem_shift_data\"][\"circ_dist\"][0][0][0]\n    df[\"rem_shift\"] = data[\"rem_shift_data\"][\"rem_shift\"][0][0][0]\n    df[\"non_rem_shift\"] = data[\"rem_shift_data\"][\"non_rem_shift\"][0][0][0]\n\n    # rem metrics\n    df[\"m_rem\"] = data[\"rem_shift_data\"][\"PhaseLockingData_rem\"][0][0][\"phasestats\"][0][\n        0\n    ][\"m\"][0][0][0]\n    df[\"r_rem\"] = data[\"rem_shift_data\"][\"PhaseLockingData_rem\"][0][0][\"phasestats\"][0][\n        0\n    ][\"r\"][0][0][0]\n    df[\"k_rem\"] = data[\"rem_shift_data\"][\"PhaseLockingData_rem\"][0][0][\"phasestats\"][0][\n        0\n    ][\"k\"][0][0][0]\n    df[\"p_rem\"] = data[\"rem_shift_data\"][\"PhaseLockingData_rem\"][0][0][\"phasestats\"][0][\n        0\n    ][\"p\"][0][0][0]\n    df[\"mode_rem\"] = data[\"rem_shift_data\"][\"PhaseLockingData_rem\"][0][0][\"phasestats\"][\n        0\n    ][0][\"mode\"][0][0][0]\n\n    # wake metrics\n    df[\"m_wake\"] = data[\"rem_shift_data\"][\"PhaseLockingData_wake\"][0][0][\"phasestats\"][\n        0\n    ][0][\"m\"][0][0][0]\n    df[\"r_wake\"] = data[\"rem_shift_data\"][\"PhaseLockingData_wake\"][0][0][\"phasestats\"][\n        0\n    ][0][\"r\"][0][0][0]\n    df[\"k_wake\"] = data[\"rem_shift_data\"][\"PhaseLockingData_wake\"][0][0][\"phasestats\"][\n        0\n    ][0][\"k\"][0][0][0]\n    df[\"p_wake\"] = data[\"rem_shift_data\"][\"PhaseLockingData_wake\"][0][0][\"phasestats\"][\n        0\n    ][0][\"p\"][0][0][0]\n    df[\"mode_wake\"] = data[\"rem_shift_data\"][\"PhaseLockingData_wake\"][0][0][\n        \"phasestats\"\n    ][0][0][\"mode\"][0][0][0]\n\n    def get_distros(data, state):\n        return np.vstack(data[\"rem_shift_data\"][state][0][0][\"phasedistros\"][0][0].T)\n\n    def get_spikephases(data, state):\n        return data[\"rem_shift_data\"][state][0][0][\"spkphases\"][0][0][0]\n\n    # add to dictionary\n    data_dict = {\n        \"rem\": {\n            \"phasedistros\": get_distros(data, \"PhaseLockingData_rem\"),\n            \"spkphases\": get_spikephases(data, \"PhaseLockingData_rem\"),\n        },\n        \"wake\": {\n            \"phasedistros\": get_distros(data, \"PhaseLockingData_wake\"),\n            \"spkphases\": get_spikephases(data, \"PhaseLockingData_wake\"),\n        },\n    }\n\n    return df, data_dict\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.load_trials","title":"<code>load_trials(basepath)</code>","text":"<p>Loads trials from cell explorer basename.animal.behavior and stores in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the following fields: - startTime: start time of the trial, in seconds - stopTime: stop time of the trial, in seconds - trialsID: ID of the trial</p> References <p>https://cellexplorer.org/datastructure/data-structure-and-format/#behavior</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_trials(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads trials from cell explorer basename.animal.behavior and stores in a DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with the following fields:\n        - startTime: start time of the trial, in seconds\n        - stopTime: stop time of the trial, in seconds\n        - trialsID: ID of the trial\n\n    References\n    ----------\n    https://cellexplorer.org/datastructure/data-structure-and-format/#behavior\n    \"\"\"\n\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".animal.behavior.mat\"\n    )\n\n    if not os.path.exists(filename):\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame()\n\n    # load file\n    data = sio.loadmat(filename, simplify_cells=True)\n    if \"trials\" not in data[\"behavior\"].keys():\n        warnings.warn(\"trials not found in file\")\n        return pd.DataFrame()\n\n    # current standard is\n    #   behavior.trials.*name of trial*.start\n    #   behavior.trials.*name of trial*.stop\n    if (\n        isinstance(data[\"behavior\"][\"trials\"], dict)\n        and \"starts\" in data[\"behavior\"][\"trials\"].keys()\n        and \"stops\" in data[\"behavior\"][\"trials\"].keys()\n    ):\n        df = pd.DataFrame(\n            data=np.array(\n                [\n                    data[\"behavior\"][\"trials\"][\"starts\"],\n                    data[\"behavior\"][\"trials\"][\"stops\"],\n                ]\n            ).T\n        )\n        df.columns = [\"startTime\", \"stopTime\"]\n        df[\"trialsID\"] = data[\"behavior\"][\"trials\"][\"stateName\"]\n\n    # old standard\n    #   behavior.trials.*[starts,stops]*\n    else:\n        # check if trials is empty\n        if len(data[\"behavior\"][\"trials\"]) == 0:\n            warnings.warn(\"trials is empty\")\n            return pd.DataFrame()\n        try:\n            df = pd.DataFrame(data=data[\"behavior\"][\"trials\"])\n            df.columns = [\"startTime\", \"stopTime\"]\n            # check if trialsID exists\n            if \"trialsID\" in data[\"behavior\"].keys():\n                df[\"trialsID\"] = data[\"behavior\"][\"trialsID\"]\n        except Exception:\n            df = pd.DataFrame(data=[data[\"behavior\"][\"trials\"]])\n            df.columns = [\"startTime\", \"stopTime\"]\n            # check if trialsID exists\n            if \"trialsID\" in data[\"behavior\"].keys():\n                if type(data[\"behavior\"][\"trialsID\"]) is str:\n                    df[\"trialsID\"] = data[\"behavior\"][\"trialsID\"]\n\n                elif len(data[\"behavior\"][\"trialsID\"]) == df.shape[0]:\n                    df[\"trialsID\"] = data[\"behavior\"][\"trialsID\"]\n                else:\n                    warnings.warn(\"trials or trialsID not correct shape\")\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/#neuro_py.io.writeNeuroscopeEvents","title":"<code>writeNeuroscopeEvents(path, ep, name)</code>","text":"<p>Write events to a Neuroscope-compatible file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the output file.</p> required <code>ep</code> <code>Any</code> <p>Epoch data containing start and end times.</p> required <code>name</code> <code>str</code> <p>Name of the event.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def writeNeuroscopeEvents(path: str, ep: Any, name: str) -&gt; None:\n    \"\"\"\n    Write events to a Neuroscope-compatible file.\n\n    Parameters\n    ----------\n    path : str\n        Path to the output file.\n    ep : Any\n        Epoch data containing start and end times.\n    name : str\n        Name of the event.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    f = open(path, \"w\")\n    for i in range(len(ep)):\n        f.writelines(\n            str(ep.as_units(\"ms\").iloc[i][\"start\"])\n            + \" \"\n            + name\n            + \" start \"\n            + str(1)\n            + \"\\n\"\n        )\n        # f.writelines(str(ep.as_units('ms').iloc[i]['peak']) + \" \"+name+\" start \"+ str(1)+\"\\n\")\n        f.writelines(\n            str(ep.as_units(\"ms\").iloc[i][\"end\"]) + \" \" + name + \" end \" + str(1) + \"\\n\"\n        )\n    f.close()\n</code></pre>"},{"location":"reference/neuro_py/io/loading/","title":"neuro_py.io.loading","text":"<p>Loading functions for cell explorer format</p>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.LFPLoader","title":"<code>LFPLoader</code>","text":"<p>               Bases: <code>object</code></p> <p>Simple class to load LFP or wideband data from a recording folder.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the recording folder.</p> required <code>channels</code> <code>Union[int, list, None]</code> <p>Channel number or list of channel numbers, by default None (load all channels memmap).</p> <code>None</code> <code>ext</code> <code>str</code> <p>File extension, by default \"lfp\".</p> <code>'lfp'</code> <code>epoch</code> <code>Union[ndarray, EpochArray, None]</code> <p>Epoch array or ndarray, by default None (load all data).</p> <code>None</code> <p>Returns:</p> Type Description <code>AnalogSignalArray</code> <p>Analog signal array of shape (n_channels, n_samples).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # load lfp file\n&gt;&gt;&gt; basepath = r\"X:/data/Barrage/NN10/day10\"\n&gt;&gt;&gt; lfp = loading.LFPLoader(basepath,ext=\"lfp\")\n&gt;&gt;&gt; lfp\n    &lt;AnalogSignalArray at 0x25ba1576640: 128 signals&gt; for a total of 5:33:58:789 hours\n</code></pre> <pre><code>&gt;&gt;&gt; # Loading dat file\n&gt;&gt;&gt; dat = loading.LFPLoader(basepath,ext=\"dat\")\n&gt;&gt;&gt; dat\n    &lt;AnalogSignalArray at 0x25ba4fedc40: 128 signals&gt; for a total of 5:33:58:790 hours\n&gt;&gt;&gt; dat.lfp.data.shape\n    (128, 400775808)\n&gt;&gt;&gt; type(dat.lfp.data)\n    numpy.memmap\n</code></pre> Source code in <code>neuro_py/io/loading.py</code> <pre><code>class LFPLoader(object):\n    \"\"\"\n    Simple class to load LFP or wideband data from a recording folder.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the recording folder.\n    channels : Union[int, list, None], optional\n        Channel number or list of channel numbers, by default None (load all channels memmap).\n    ext : str, optional\n        File extension, by default \"lfp\".\n    epoch : Union[np.ndarray, nel.EpochArray, None], optional\n        Epoch array or ndarray, by default None (load all data).\n\n    Returns\n    -------\n    nelpy.AnalogSignalArray\n        Analog signal array of shape (n_channels, n_samples).\n\n    Examples\n    --------\n    &gt;&gt;&gt; # load lfp file\n    &gt;&gt;&gt; basepath = r\"X:/data/Barrage/NN10/day10\"\n    &gt;&gt;&gt; lfp = loading.LFPLoader(basepath,ext=\"lfp\")\n    &gt;&gt;&gt; lfp\n        &lt;AnalogSignalArray at 0x25ba1576640: 128 signals&gt; for a total of 5:33:58:789 hours\n\n    &gt;&gt;&gt; # Loading dat file\n    &gt;&gt;&gt; dat = loading.LFPLoader(basepath,ext=\"dat\")\n    &gt;&gt;&gt; dat\n        &lt;AnalogSignalArray at 0x25ba4fedc40: 128 signals&gt; for a total of 5:33:58:790 hours\n    &gt;&gt;&gt; dat.lfp.data.shape\n        (128, 400775808)\n    &gt;&gt;&gt; type(dat.lfp.data)\n        numpy.memmap\n    \"\"\"\n\n    def __init__(\n        self,\n        basepath: str,\n        channels: Union[int, list, None] = None,\n        ext: str = \"lfp\",\n        epoch: Union[np.ndarray, nel.EpochArray, None] = None,\n    ) -&gt; None:\n        self.basepath = basepath  # path to the recording folder\n        self.channels = channels  # channel number or list of channel numbers\n        self.ext = ext  # lfp or dat\n        self.epoch = epoch\n\n        # get xml data\n        self.get_xml_data()\n\n        # set sampling rate based on the extension of the file (lfp or dat)\n        if self.ext == \"dat\":\n            self.fs = self.fs_dat\n\n        # load lfp\n        self.load_lfp()\n\n    def get_xml_data(self) -&gt; None:\n        nChannels, fs, fs_dat, shank_to_channel = loadXML(self.basepath)\n        self.nChannels = nChannels\n        self.fs = fs\n        self.fs_dat = fs_dat\n        self.shank_to_channel = shank_to_channel\n\n    def load_lfp(self) -&gt; None:\n        lfp, timestep = loadLFP(\n            self.basepath,\n            n_channels=self.nChannels,\n            channel=self.channels,\n            frequency=self.fs,\n            ext=self.ext,\n        )\n\n        if isinstance(self.epoch, nel.EpochArray):\n            intervals = self.epoch.data\n        elif isinstance(self.epoch, np.ndarray):\n            intervals = self.epoch\n            if intervals.ndim == 1:\n                intervals = intervals[np.newaxis, :]\n        else:\n            intervals = np.array([0, timestep.shape[0] / self.fs])[np.newaxis, :]\n\n        idx = in_intervals(timestep, intervals)\n\n        # if loading all, don't index as to preserve memmap\n        if idx.all():\n            self.lfp = nel.AnalogSignalArray(\n                data=lfp.T,\n                timestamps=timestep,\n                fs=self.fs,\n                support=nel.EpochArray(intervals),\n            )\n        else:\n            self.lfp = nel.AnalogSignalArray(\n                data=lfp[idx, None].T,\n                timestamps=timestep[idx],\n                fs=self.fs,\n                support=nel.EpochArray(\n                    np.array([min(timestep[idx]), max(timestep[idx])])\n                ),\n            )\n\n    def __repr__(self) -&gt; None:\n        return self.lfp.__repr__()\n\n    def get_phase(self, band2filter: list = [6, 12], ford: int = 3) -&gt; np.ndarray:\n        \"\"\"\n        Get the phase of the LFP signal using a bandpass filter and Hilbert transform.\n\n        Parameters\n        ----------\n        band2filter : list, optional\n            The frequency band to filter, by default [6, 12].\n        ford : int, optional\n            The order of the Butterworth filter, by default 3.\n\n        Returns\n        -------\n        np.ndarray\n            The phase of the LFP signal.\n        \"\"\"\n        band2filter = np.array(band2filter, dtype=float)\n        b, a = signal.butter(ford, band2filter / (self.fs / 2), btype=\"bandpass\")\n        filt_sig = signal.filtfilt(b, a, self.lfp.data, padtype=\"odd\")\n        return np.angle(signal.hilbert(filt_sig))\n\n    def get_freq_phase_amp(\n        self, band2filter: list = [6, 12], ford: int = 3, kernel_size: int = 13\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Get the filtered signal, phase, amplitude, and filtered amplitude of the LFP signal.\n\n        Parameters\n        ----------\n        band2filter : list, optional\n            The frequency band to filter, by default [6, 12].\n        ford : int, optional\n            The order of the Butterworth filter, by default 3.\n        kernel_size : int, optional\n            The kernel size for the median filter, by default 13.\n\n        Returns\n        -------\n        filt_sig : np.ndarray\n            The filtered signal.\n        phase : np.ndarray\n            The phase of the LFP signal.\n        amplitude : np.ndarray\n            The amplitude of the LFP signal.\n        amplitude_filtered : np.ndarray\n            The filtered amplitude of the LFP signal.\n        frequency : np.ndarray\n            The instantaneous frequency of the LFP signal.\n        \"\"\"\n\n        band2filter = np.array(band2filter, dtype=float)\n\n        b, a = signal.butter(ford, band2filter / (self.fs / 2), btype=\"bandpass\")\n\n        filt_sig = signal.filtfilt(b, a, self.lfp.data, padtype=\"odd\")\n        phase = np.angle(signal.hilbert(filt_sig))\n        amplitude = np.abs(signal.hilbert(filt_sig))\n        amplitude_filtered = signal.filtfilt(b, a, amplitude, padtype=\"odd\")\n\n        # calculate the frequency\n        # median filter to smooth the unwrapped phase (this is to avoid jumps in the frequency)\n        filtered_signal = signal.medfilt2d(\n            np.unwrap(phase), kernel_size=[1, kernel_size]\n        )\n\n        # Calculate the derivative of the unwrapped phase to get frequency\n        dt = np.diff(self.lfp.abscissa_vals)\n        if np.allclose(dt, dt[0]):  # Check if sampling is uniform\n            dt = dt[0]  # Use a single scalar for uniform sampling\n        else:\n            dt = np.hstack((dt[0], dt))  # Use an array for non-uniform sampling\n        derivative = np.gradient(filtered_signal, dt, axis=-1)\n        frequency = derivative / (2 * np.pi)\n\n        return filt_sig, phase, amplitude, amplitude_filtered, frequency\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.LFPLoader.get_freq_phase_amp","title":"<code>get_freq_phase_amp(band2filter=[6, 12], ford=3, kernel_size=13)</code>","text":"<p>Get the filtered signal, phase, amplitude, and filtered amplitude of the LFP signal.</p> <p>Parameters:</p> Name Type Description Default <code>band2filter</code> <code>list</code> <p>The frequency band to filter, by default [6, 12].</p> <code>[6, 12]</code> <code>ford</code> <code>int</code> <p>The order of the Butterworth filter, by default 3.</p> <code>3</code> <code>kernel_size</code> <code>int</code> <p>The kernel size for the median filter, by default 13.</p> <code>13</code> <p>Returns:</p> Name Type Description <code>filt_sig</code> <code>ndarray</code> <p>The filtered signal.</p> <code>phase</code> <code>ndarray</code> <p>The phase of the LFP signal.</p> <code>amplitude</code> <code>ndarray</code> <p>The amplitude of the LFP signal.</p> <code>amplitude_filtered</code> <code>ndarray</code> <p>The filtered amplitude of the LFP signal.</p> <code>frequency</code> <code>ndarray</code> <p>The instantaneous frequency of the LFP signal.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def get_freq_phase_amp(\n    self, band2filter: list = [6, 12], ford: int = 3, kernel_size: int = 13\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Get the filtered signal, phase, amplitude, and filtered amplitude of the LFP signal.\n\n    Parameters\n    ----------\n    band2filter : list, optional\n        The frequency band to filter, by default [6, 12].\n    ford : int, optional\n        The order of the Butterworth filter, by default 3.\n    kernel_size : int, optional\n        The kernel size for the median filter, by default 13.\n\n    Returns\n    -------\n    filt_sig : np.ndarray\n        The filtered signal.\n    phase : np.ndarray\n        The phase of the LFP signal.\n    amplitude : np.ndarray\n        The amplitude of the LFP signal.\n    amplitude_filtered : np.ndarray\n        The filtered amplitude of the LFP signal.\n    frequency : np.ndarray\n        The instantaneous frequency of the LFP signal.\n    \"\"\"\n\n    band2filter = np.array(band2filter, dtype=float)\n\n    b, a = signal.butter(ford, band2filter / (self.fs / 2), btype=\"bandpass\")\n\n    filt_sig = signal.filtfilt(b, a, self.lfp.data, padtype=\"odd\")\n    phase = np.angle(signal.hilbert(filt_sig))\n    amplitude = np.abs(signal.hilbert(filt_sig))\n    amplitude_filtered = signal.filtfilt(b, a, amplitude, padtype=\"odd\")\n\n    # calculate the frequency\n    # median filter to smooth the unwrapped phase (this is to avoid jumps in the frequency)\n    filtered_signal = signal.medfilt2d(\n        np.unwrap(phase), kernel_size=[1, kernel_size]\n    )\n\n    # Calculate the derivative of the unwrapped phase to get frequency\n    dt = np.diff(self.lfp.abscissa_vals)\n    if np.allclose(dt, dt[0]):  # Check if sampling is uniform\n        dt = dt[0]  # Use a single scalar for uniform sampling\n    else:\n        dt = np.hstack((dt[0], dt))  # Use an array for non-uniform sampling\n    derivative = np.gradient(filtered_signal, dt, axis=-1)\n    frequency = derivative / (2 * np.pi)\n\n    return filt_sig, phase, amplitude, amplitude_filtered, frequency\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.LFPLoader.get_phase","title":"<code>get_phase(band2filter=[6, 12], ford=3)</code>","text":"<p>Get the phase of the LFP signal using a bandpass filter and Hilbert transform.</p> <p>Parameters:</p> Name Type Description Default <code>band2filter</code> <code>list</code> <p>The frequency band to filter, by default [6, 12].</p> <code>[6, 12]</code> <code>ford</code> <code>int</code> <p>The order of the Butterworth filter, by default 3.</p> <code>3</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The phase of the LFP signal.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def get_phase(self, band2filter: list = [6, 12], ford: int = 3) -&gt; np.ndarray:\n    \"\"\"\n    Get the phase of the LFP signal using a bandpass filter and Hilbert transform.\n\n    Parameters\n    ----------\n    band2filter : list, optional\n        The frequency band to filter, by default [6, 12].\n    ford : int, optional\n        The order of the Butterworth filter, by default 3.\n\n    Returns\n    -------\n    np.ndarray\n        The phase of the LFP signal.\n    \"\"\"\n    band2filter = np.array(band2filter, dtype=float)\n    b, a = signal.butter(ford, band2filter / (self.fs / 2), btype=\"bandpass\")\n    filt_sig = signal.filtfilt(b, a, self.lfp.data, padtype=\"odd\")\n    return np.angle(signal.hilbert(filt_sig))\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading._add_manual_events","title":"<code>_add_manual_events(df, added_ts)</code>","text":"<p>Add new rows to a dataframe representing manual events (from Neuroscope2) with durations equal to the mean duration of the existing events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataframe, with at least two columns called 'start' and 'stop', representing the start and stop times of the events.</p> required <code>added_ts</code> <code>list</code> <p>A list of timestamps representing the peaks of the new events to be added to the dataframe.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The modified dataframe with the new rows added and sorted by the 'peaks' column.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def _add_manual_events(df: pd.DataFrame, added_ts: list) -&gt; pd.DataFrame:\n    \"\"\"\n    Add new rows to a dataframe representing manual events (from Neuroscope2)\n    with durations equal to the mean duration of the existing events.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input dataframe, with at least two columns called 'start' and 'stop',\n        representing the start and stop times of the events.\n    added_ts : list\n        A list of timestamps representing the peaks of the new events to be added\n        to the dataframe.\n\n    Returns\n    -------\n    pd.DataFrame\n        The modified dataframe with the new rows added and sorted by the 'peaks' column.\n    \"\"\"\n    # Calculate the mean duration of the existing events\n    mean_duration = (df[\"stop\"] - df[\"start\"]).mean()\n\n    # Create a new dataframe with a 'peaks' column equal to the added_ts values\n    df_added = pd.DataFrame()\n    df_added[\"peaks\"] = added_ts\n\n    # Calculate the start and stop times of the new events based on the mean duration\n    df_added[\"start\"] = added_ts - mean_duration / 2\n    df_added[\"stop\"] = added_ts + mean_duration / 2\n\n    # Calculate the duration of the new events as the mean duration\n    df_added[\"duration\"] = df_added.stop.values - df_added.start.values\n\n    # Append the new events to the original dataframe\n    df = pd.concat([df, df_added], ignore_index=True)\n\n    # Sort the dataframe by the 'peaks' column\n    df.sort_values(by=[\"peaks\"], ignore_index=True, inplace=True)\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.add_animal_id","title":"<code>add_animal_id(df)</code>","text":"<p>Add animal_id column to a dataframe based on the basepath column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe with a basepath column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with an additional animal_id column.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def add_animal_id(df: pd.core.frame.DataFrame) -&gt; pd.core.frame.DataFrame:\n    \"\"\"\n    Add animal_id column to a dataframe based on the basepath column.\n\n    Parameters\n    ----------\n    df : pd.core.frame.DataFrame\n        Dataframe with a basepath column.\n\n    Returns\n    -------\n    pd.core.frame.DataFrame\n        Dataframe with an additional animal_id column.\n    \"\"\"\n    df[\"animal_id\"] = df.basepath.map(\n        dict([(basepath, get_animal_id(basepath)) for basepath in df.basepath.unique()])\n    )\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.get_animal_id","title":"<code>get_animal_id(basepath)</code>","text":"<p>Return animal ID from basepath using basename.session.mat.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to session folder.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Animal ID.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def get_animal_id(basepath: str) -&gt; str:\n    \"\"\"\n    Return animal ID from basepath using basename.session.mat.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to session folder.\n\n    Returns\n    -------\n    str\n        Animal ID.\n    \"\"\"\n    try:\n        filename = glob.glob(os.path.join(basepath, \"*.session.mat\"))[0]\n    except Exception:\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame()\n\n    # load file\n    data = sio.loadmat(filename)\n    return data[\"session\"][0][0][\"animal\"][0][0][\"name\"][0]\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.loadLFP","title":"<code>loadLFP(basepath, n_channels=90, channel=None, frequency=1250.0, precision='int16', ext='lfp', filename=None)</code>","text":"<p>Load LFP data from a specified file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder containing the LFP file.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels, by default 90.</p> <code>90</code> <code>channel</code> <code>Optional[Union[int, list]]</code> <p>Specific channel(s) to load, by default None.</p> <code>None</code> <code>frequency</code> <code>float</code> <p>Sampling frequency, by default 1250.0.</p> <code>1250.0</code> <code>precision</code> <code>str</code> <p>Data precision, by default \"int16\".</p> <code>'int16'</code> <code>ext</code> <code>str</code> <p>File extension, by default \"lfp\".</p> <code>'lfp'</code> <code>filename</code> <code>Optional[str]</code> <p>Name of the file to load, located in basepath, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Tuple[ndarray, ndarray]]</code> <p>Data and corresponding timestamps.</p> Notes <p>If both .lfp and .eeg files are present, .lfp file is prioritized. If neither are present, returns None.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def loadLFP(\n    basepath: str,\n    n_channels: int = 90,\n    channel: Union[int, None] = None,\n    frequency: float = 1250.0,\n    precision: str = \"int16\",\n    ext: str = \"lfp\",\n    filename: str = None,  # name of file to load, located in basepath\n):\n    \"\"\"\n    Load LFP data from a specified file.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder containing the LFP file.\n    n_channels : int, optional\n        Number of channels, by default 90.\n    channel : Optional[Union[int, list]], optional\n        Specific channel(s) to load, by default None.\n    frequency : float, optional\n        Sampling frequency, by default 1250.0.\n    precision : str, optional\n        Data precision, by default \"int16\".\n    ext : str, optional\n        File extension, by default \"lfp\".\n    filename : Optional[str], optional\n        Name of the file to load, located in basepath, by default None.\n\n    Returns\n    -------\n    Optional[Tuple[np.ndarray, np.ndarray]]\n        Data and corresponding timestamps.\n\n    Notes\n    -----\n    If both .lfp and .eeg files are present, .lfp file is prioritized.\n    If neither are present, returns None.\n    \"\"\"\n    if filename is not None:\n        path = os.path.join(basepath, filename)\n    else:\n        path = \"\"\n        if ext == \"lfp\":\n            path = os.path.join(basepath, os.path.basename(basepath) + \".lfp\")\n            if not os.path.exists(path):\n                path = os.path.join(basepath, os.path.basename(basepath) + \".eeg\")\n        if ext == \"dat\":\n            path = os.path.join(basepath, os.path.basename(basepath) + \".dat\")\n\n    # check if saved file exists\n    if not os.path.exists(path):\n        warnings.warn(\"file does not exist\")\n        return\n    if channel is None:\n        n_channels = int(n_channels)\n\n        f = open(path, \"rb\")\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        bytes_size = 2\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n        f.close()\n        data = np.memmap(path, np.int16, \"r\", shape=(n_samples, n_channels))\n        timestep = np.arange(0, n_samples) / frequency\n        return data, timestep\n\n    if type(channel) is not list:\n        f = open(path, \"rb\")\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        bytes_size = 2\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n        f.close()\n        with open(path, \"rb\") as f:\n            data = np.fromfile(f, np.int16).reshape((n_samples, n_channels))[:, channel]\n            timestep = np.arange(0, len(data)) / frequency\n            # check if lfp time stamps exist\n            lfp_ts_path = os.path.join(\n                os.path.dirname(os.path.abspath(path)), \"lfp_ts.npy\"\n            )\n            if os.path.exists(lfp_ts_path):\n                timestep = np.load(lfp_ts_path).reshape(-1)\n\n            return data, timestep\n\n    elif type(channel) is list:\n        f = open(path, \"rb\")\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        bytes_size = 2\n\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n        f.close()\n        with open(path, \"rb\") as f:\n            data = np.fromfile(f, np.int16).reshape((n_samples, n_channels))[:, channel]\n            timestep = np.arange(0, len(data)) / frequency\n            # check if lfp time stamps exist\n            lfp_ts_path = os.path.join(\n                os.path.dirname(os.path.abspath(path)), \"lfp_ts.npy\"\n            )\n            if os.path.exists(lfp_ts_path):\n                timestep = np.load(lfp_ts_path).reshape(-1)\n            return data, timestep\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.loadXML","title":"<code>loadXML(basepath)</code>","text":"<p>Load XML file and extract relevant information.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder session containing the XML file.</p> required <p>Returns:</p> Type Description <code>Union[Tuple[int, int, int, Dict[int, list]], None]</code> <p>A tuple containing: - The number of channels (int) - The sampling frequency of the dat file (int) - The sampling frequency of the eeg file (int) - The mappings shanks to channels as a dict (Dict[int, list])</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def loadXML(basepath: str) -&gt; Union[Tuple[int, int, int, Dict[int, list]], None]:\n    \"\"\"\n    Load XML file and extract relevant information.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder session containing the XML file.\n\n    Returns\n    -------\n    Union[Tuple[int, int, int, Dict[int, list]], None]\n        A tuple containing:\n        - The number of channels (int)\n        - The sampling frequency of the dat file (int)\n        - The sampling frequency of the eeg file (int)\n        - The mappings shanks to channels as a dict (Dict[int, list])\n    \"\"\"\n    # check if saved file exists\n    try:\n        basename = os.path.basename(basepath)\n        filename = glob.glob(os.path.join(basepath, basename + \".xml\"))[0]\n    except Exception:\n        warnings.warn(\"xml file does not exist\")\n        return\n\n    xmldoc = minidom.parse(filename)\n    nChannels = (\n        xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n        .getElementsByTagName(\"nChannels\")[0]\n        .firstChild.data\n    )\n    fs_dat = (\n        xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n        .getElementsByTagName(\"samplingRate\")[0]\n        .firstChild.data\n    )\n    fs = (\n        xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n        .getElementsByTagName(\"lfpSamplingRate\")[0]\n        .firstChild.data\n    )\n\n    shank_to_channel = {}\n    groups = (\n        xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n        .getElementsByTagName(\"channelGroups\")[0]\n        .getElementsByTagName(\"group\")\n    )\n    for i in range(len(groups)):\n        shank_to_channel[i] = [\n            int(child.firstChild.data)\n            for child in groups[i].getElementsByTagName(\"channel\")\n        ]\n    return int(nChannels), int(fs), int(fs_dat), shank_to_channel\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_SWRunitMetrics","title":"<code>load_SWRunitMetrics(basepath)</code>","text":"<p>Load SWRunitMetrics.mat into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder containing the SWRunitMetrics.mat file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the following fields: - particip: the probability of participation into ripples for each unit - FRall: mean firing rate during ripples - FRparticip: mean firing rate for ripples with at least 1 spike - nSpkAll: mean number of spikes in all ripples - nSpkParticip: mean number of spikes in ripples with at least 1 spike - epoch: behavioral epoch label</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_SWRunitMetrics(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Load SWRunitMetrics.mat into a pandas DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder containing the SWRunitMetrics.mat file.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with the following fields:\n        - particip: the probability of participation into ripples for each unit\n        - FRall: mean firing rate during ripples\n        - FRparticip: mean firing rate for ripples with at least 1 spike\n        - nSpkAll: mean number of spikes in all ripples\n        - nSpkParticip: mean number of spikes in ripples with at least 1 spike\n        - epoch: behavioral epoch label\n    \"\"\"\n\n    def extract_swr_epoch_data(data, epoch):\n        # get var names\n        dt = data[\"SWRunitMetrics\"][epoch][0][0].dtype\n\n        df2 = pd.DataFrame()\n\n        # get n units\n        # there might be other fields within here like the epoch timestamps\n        # skip those by returning empty df\n        try:\n            n_cells = data[\"SWRunitMetrics\"][epoch][0][0][0][\"particip\"][0].shape[0]\n        except Exception:\n            return df2\n\n        for dn in dt.names:\n            if (data[\"SWRunitMetrics\"][epoch][0][0][0][dn][0].shape[1] == 1) &amp; (\n                data[\"SWRunitMetrics\"][epoch][0][0][0][dn][0].shape[0] == n_cells\n            ):\n                df2[dn] = data[\"SWRunitMetrics\"][epoch][0][0][0][dn][0].T[0]\n        df2[\"epoch\"] = epoch\n        return df2\n\n    try:\n        filename = glob.glob(os.path.join(basepath, \"*.SWRunitMetrics.mat\"))[0]\n    except Exception:\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame()\n\n    # load file\n    data = sio.loadmat(filename)\n\n    df2 = pd.DataFrame()\n    # loop through each available epoch and pull out contents\n    for epoch in data[\"SWRunitMetrics\"].dtype.names:\n        if data[\"SWRunitMetrics\"][epoch][0][0].size &gt; 0:  # not empty\n            # call content extractor\n            df_ = extract_swr_epoch_data(data, epoch)\n\n            # append conents to overall data frame\n            if df_.size &gt; 0:\n                df2 = pd.concat([df2, df_], ignore_index=True)\n\n    return df2\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_SleepState_states","title":"<code>load_SleepState_states(basepath, return_epoch_array=False, states_list=['WAKEstate', 'NREMstate', 'REMstate', 'THETA', 'nonTHETA'])</code>","text":"<p>Loader of SleepState.states.mat.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder containing the SleepState.states.mat file.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, return an dict of EpochArrays, by default False.</p> <code>False</code> <code>states_list</code> <code>list</code> <p>List of states to load, by default [\"WAKEstate\", \"NREMstate\", \"REMstate\", \"THETA\", \"nonTHETA\"].</p> <code>['WAKEstate', 'NREMstate', 'REMstate', 'THETA', 'nonTHETA']</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the contents of the SleepState.states.mat file.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_SleepState_states(\n    basepath: str,\n    return_epoch_array: bool = False,\n    states_list: list = [\"WAKEstate\", \"NREMstate\", \"REMstate\", \"THETA\", \"nonTHETA\"],\n) -&gt; dict:\n    \"\"\"\n    Loader of SleepState.states.mat.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder containing the SleepState.states.mat file.\n    return_epoch_array : bool, optional\n        If True, return an dict of EpochArrays, by default False.\n    states_list : list, optional\n        List of states to load, by default [\"WAKEstate\", \"NREMstate\", \"REMstate\", \"THETA\", \"nonTHETA\"].\n\n    Returns\n    -------\n    dict\n        Dictionary containing the contents of the SleepState.states.mat file.\n    \"\"\"\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".SleepState.states.mat\"\n    )\n    if not os.path.exists(filename):\n        warnings.warn(\"file does not exist\")\n        return None\n\n    # load cell_metrics file\n    data = sio.loadmat(filename)\n\n    # get epoch id\n    wake_id = (\n        np.where(data[\"SleepState\"][\"idx\"][0][0][\"statenames\"][0][0][0] == \"WAKE\")[0][0]\n        + 1\n    )\n    rem_id = (\n        np.where(data[\"SleepState\"][\"idx\"][0][0][\"statenames\"][0][0][0] == \"REM\")[0][0]\n        + 1\n    )\n    nrem_id = (\n        np.where(data[\"SleepState\"][\"idx\"][0][0][\"statenames\"][0][0][0] == \"NREM\")[0][0]\n        + 1\n    )\n\n    # get states and timestamps vectors\n    states = data[\"SleepState\"][\"idx\"][0][0][\"states\"][0][0]\n    timestamps = data[\"SleepState\"][\"idx\"][0][0][\"timestamps\"][0][0]\n\n    # set up dict\n    dict_ = {\n        \"wake_id\": wake_id,\n        \"rem_id\": rem_id,\n        \"nrem_id\": nrem_id,\n        \"states\": states,\n        \"timestamps\": timestamps,\n    }\n\n    # iter through states and add to dict\n    dt = data[\"SleepState\"][\"ints\"][0][0].dtype\n    for dn in dt.names:\n        dict_[dn] = data[\"SleepState\"][\"ints\"][0][0][dn][0][0]\n\n    if not return_epoch_array:\n        return dict_\n    else:\n        epoch_df = load_epoch(basepath)\n        # get session bounds to provide support\n        session_domain = nel.EpochArray(\n            [epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]]\n        )\n        states_dict = {}\n        for state in states_list:\n            states_dict[state] = nel.EpochArray(\n                dict_.get(state, []), domain=session_domain\n            )\n        return states_dict\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_all_cell_metrics","title":"<code>load_all_cell_metrics(basepaths)</code>","text":"<p>Load cell metrics from multiple sessions.</p> <p>Parameters:</p> Name Type Description Default <code>basepaths</code> <code>List[str]</code> <p>List of basepaths, can be a pandas column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Concatenated pandas DataFrame with metrics.</p> Notes <p>To get waveforms, spike times, etc., use load_cell_metrics.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_all_cell_metrics(basepaths: List[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    Load cell metrics from multiple sessions.\n\n    Parameters\n    ----------\n    basepaths : List[str]\n        List of basepaths, can be a pandas column.\n\n    Returns\n    -------\n    pd.DataFrame\n        Concatenated pandas DataFrame with metrics.\n\n    Notes\n    -----\n    To get waveforms, spike times, etc., use load_cell_metrics.\n    \"\"\"\n\n    # to speed up, use parallel\n    num_cores = multiprocessing.cpu_count()\n    cell_metrics = Parallel(n_jobs=num_cores)(\n        delayed(load_cell_metrics)(basepath, True) for basepath in basepaths\n    )\n\n    return pd.concat(cell_metrics, ignore_index=True)\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_animal_behavior","title":"<code>load_animal_behavior(basepath, alternative_file=None)</code>","text":"<p>load_animal_behavior loads basename.animal.behavior.mat files created by general_behavior_file.m The output is a pandas data frame with [time,x,y,z,linearized,speed,acceleration,trials,epochs]</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>alternative_file</code> <code>Union[str, None]</code> <p>Alternative file name to load, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the following fields: - time: timestamps - x: x-coordinate - y: y-coordinate - z: z-coordinate - linearized: linearized position - speed: speed of the animal - acceleration: acceleration of the animal - trials: trial numbers - epochs: epoch names - environment: environment names</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_animal_behavior(\n    basepath: str, alternative_file: Union[str, None] = None\n) -&gt; pd.DataFrame:\n    \"\"\"\n    load_animal_behavior loads basename.animal.behavior.mat files created by general_behavior_file.m\n    The output is a pandas data frame with [time,x,y,z,linearized,speed,acceleration,trials,epochs]\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    alternative_file : Union[str, None], optional\n        Alternative file name to load, by default None.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with the following fields:\n        - time: timestamps\n        - x: x-coordinate\n        - y: y-coordinate\n        - z: z-coordinate\n        - linearized: linearized position\n        - speed: speed of the animal\n        - acceleration: acceleration of the animal\n        - trials: trial numbers\n        - epochs: epoch names\n        - environment: environment names\n    \"\"\"\n    df = pd.DataFrame()\n\n    if alternative_file is None:\n        try:\n            filename = glob.glob(os.path.join(basepath, \"*.animal.behavior.mat\"))[0]\n        except Exception:\n            warnings.warn(\"file does not exist\")\n            return df\n    else:\n        try:\n            filename = glob.glob(\n                os.path.join(basepath, \"*\" + alternative_file + \".mat\")\n            )[0]\n        except Exception:\n            warnings.warn(\"file does not exist\")\n            return df\n\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    # add timestamps first which provide the correct shape of df\n    # here, I'm naming them time, but this should be deprecated\n    df[\"time\"] = data[\"behavior\"][\"timestamps\"]\n\n    # add all other position coordinates to df (will add everything it can within position)\n    for key in data[\"behavior\"][\"position\"].keys():\n        values = data[\"behavior\"][\"position\"][key]\n        if isinstance(values, (list, np.ndarray)) and len(values) == 0:\n            continue\n        df[key] = values\n\n    # add other fields from behavior to df (acceleration,speed,states)\n    for key in data[\"behavior\"].keys():\n        values = data[\"behavior\"][key]\n        if isinstance(values, (list, np.ndarray)) and len(values) != len(df):\n            continue\n        df[key] = values\n\n    # add speed and acceleration\n    if \"speed\" not in df.columns:\n        df[\"speed\"] = get_speed(df[[\"x\", \"y\"]].values, df.time.values)\n    if \"acceleration\" not in df.columns:  # using backward difference\n        df.loc[1:, \"acceleration\"] = np.diff(df[\"speed\"]) / np.diff(df[\"time\"])\n        df.loc[0, \"acceleration\"] = 0  # assuming no acceleration at start\n\n    trials = data[\"behavior\"][\"trials\"]\n    try:\n        for t in range(trials.shape[0]):\n            idx = (df.time &gt;= trials[t, 0]) &amp; (df.time &lt;= trials[t, 1])\n            df.loc[idx, \"trials\"] = t\n    except Exception:\n        pass\n\n    epochs = load_epoch(basepath)\n    for t in range(epochs.shape[0]):\n        idx = (df.time &gt;= epochs.startTime.iloc[t]) &amp; (\n            df.time &lt;= epochs.stopTime.iloc[t]\n        )\n        df.loc[idx, \"epochs\"] = epochs.name.iloc[t]\n        df.loc[idx, \"environment\"] = epochs.environment.iloc[t]\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_barrage_events","title":"<code>load_barrage_events(basepath, return_epoch_array=False, restrict_to_nrem=True, min_duration=0.0)</code>","text":"<p>Load barrage events from the .HSEn2.events.mat file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Basepath to the session folder.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, return an EpochArray instead of a DataFrame, by default False</p> <code>False</code> <code>restrict_to_nrem</code> <code>bool</code> <p>If True, restrict to NREM sleep, by default True</p> <code>True</code> <code>min_duration</code> <code>float</code> <p>Minimum duration of a barrage, by default 0.0</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame with barrage events.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_barrage_events(\n    basepath: str,\n    return_epoch_array: bool = False,\n    restrict_to_nrem: bool = True,\n    min_duration: float = 0.0,\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Load barrage events from the .HSEn2.events.mat file.\n\n    Parameters\n    ----------\n    basepath : str\n        Basepath to the session folder.\n    return_epoch_array : bool, optional\n        If True, return an EpochArray instead of a DataFrame, by default False\n    restrict_to_nrem : bool, optional\n        If True, restrict to NREM sleep, by default True\n    min_duration : float, optional\n        Minimum duration of a barrage, by default 0.0\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame with barrage events.\n    \"\"\"\n\n    # locate barrage file\n    filename = os.path.join(basepath, os.path.basename(basepath) + \".HSEn2.events.mat\")\n\n    # check if file exists\n    if os.path.exists(filename) is False:\n        warnings.warn(\"No barrage file found for {}\".format(basepath))\n        if return_epoch_array:\n            return nel.EpochArray()\n        return pd.DataFrame()\n\n    # load data from file and extract relevant data\n    data = sio.loadmat(filename, simplify_cells=True)\n    data = data[\"HSEn2\"]\n\n    # convert to DataFrame\n    df = pd.DataFrame()\n    df[\"start\"] = data[\"timestamps\"][:, 0]\n    df[\"stop\"] = data[\"timestamps\"][:, 1]\n    df[\"peaks\"] = data[\"peaks\"]\n    df[\"duration\"] = data[\"timestamps\"][:, 1] - data[\"timestamps\"][:, 0]\n\n    # restrict to NREM sleep\n    if restrict_to_nrem:\n        state_dict = load_SleepState_states(basepath)\n        nrem_epochs = nel.EpochArray(state_dict[\"NREMstate\"]).expand(2)\n        idx = in_intervals(df[\"start\"].values, nrem_epochs.data)\n        df = df[idx].reset_index(drop=True)\n\n    # restrict to barrages with a minimum duration\n    df = df[df.duration &gt; min_duration].reset_index(drop=True)\n\n    # make sure each barrage has some ca2 activity\n    # load ca2 pyr cells\n    st, _ = load_spikes(basepath, putativeCellType=\"Pyr\", brainRegion=\"CA2\")\n    # bin spikes into barrages\n    bst = get_participation(st.data, df[\"start\"].values, df[\"stop\"].values)\n    # keep only barrages with some activity\n    df = df[np.sum(bst &gt; 0, axis=0) &gt; 0].reset_index(drop=True)\n\n    if return_epoch_array:\n        return nel.EpochArray([np.array([df.start, df.stop]).T], label=\"barrage\")\n\n    # get basename and animal\n    normalized_path = os.path.normpath(filename)\n    path_components = normalized_path.split(os.sep)\n    df[\"basepath\"] = basepath\n    df[\"basename\"] = path_components[-2]\n    df[\"animal\"] = path_components[-3]\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_basic_data","title":"<code>load_basic_data(basepath)</code>","text":"<p>Load basic data from the specified basepath.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, dict, DataFrame, float]</code> <ul> <li>cell_metrics: DataFrame containing cell metrics.</li> <li>data: Dictionary containing additional data.</li> <li>ripples: DataFrame containing ripple events.</li> <li>fs_dat: Sampling rate of the data.</li> </ul> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_basic_data(basepath: str) -&gt; Tuple[pd.DataFrame, dict, pd.DataFrame, float]:\n    \"\"\"\n    Load basic data from the specified basepath.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, dict, pd.DataFrame, float]\n        - cell_metrics: DataFrame containing cell metrics.\n        - data: Dictionary containing additional data.\n        - ripples: DataFrame containing ripple events.\n        - fs_dat: Sampling rate of the data.\n    \"\"\"\n    try:\n        nChannels, fs, fs_dat, shank_to_channel = loadXML(basepath)\n    except Exception:\n        fs_dat = load_extracellular_metadata(basepath).get(\"sr\")\n\n    ripples = load_ripples_events(basepath)\n    cell_metrics, data = load_cell_metrics(basepath)\n\n    return cell_metrics, data, ripples, fs_dat\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_brain_regions","title":"<code>load_brain_regions(basepath, out_format='dict')</code>","text":"<p>Loads brain region info from cell explorer basename.session and stores in dict (default) or DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>out_format</code> <code>str</code> <p>Output format, either 'dict' or 'DataFrame', by default 'dict'.</p> <code>'dict'</code> <p>Returns:</p> Type Description <code>Union[dict, DataFrame]</code> <p>Dictionary or DataFrame with brain region information.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; brainRegions = load_brain_regions(\"Z:\\Data\\GirardeauG\\Rat09\\Rat09-20140327\")\n&gt;&gt;&gt; print(brainRegions.keys())\ndict_keys(['CA1', 'Unknown', 'blv', 'bmp', 'ven'])\n&gt;&gt;&gt; print(brainRegions['CA1'].keys())\ndict_keys(['channels', 'electrodeGroups'])\n&gt;&gt;&gt; print(brainRegions['CA1']['channels'])\n[145 146 147 148 149 153 155 157 150 151 154 159 156 152 158 160 137 140\n129 136 138 134 130 132 142 143 144 141 131 139 133 135]\n&gt;&gt;&gt; print(brainRegions['CA1']['electrodeGroups'])\n    [17 18 19 20]\n</code></pre> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_brain_regions(\n    basepath: str, out_format: str = \"dict\"\n) -&gt; Union[dict, pd.DataFrame]:\n    \"\"\"\n    Loads brain region info from cell explorer basename.session and stores in dict (default) or DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    out_format : str, optional\n        Output format, either 'dict' or 'DataFrame', by default 'dict'.\n\n    Returns\n    -------\n    Union[dict, pd.DataFrame]\n        Dictionary or DataFrame with brain region information.\n\n    Examples\n    -------\n    &gt;&gt;&gt; brainRegions = load_brain_regions(\"Z:\\\\Data\\\\GirardeauG\\\\Rat09\\\\Rat09-20140327\")\n    &gt;&gt;&gt; print(brainRegions.keys())\n    dict_keys(['CA1', 'Unknown', 'blv', 'bmp', 'ven'])\n    &gt;&gt;&gt; print(brainRegions['CA1'].keys())\n    dict_keys(['channels', 'electrodeGroups'])\n    &gt;&gt;&gt; print(brainRegions['CA1']['channels'])\n    [145 146 147 148 149 153 155 157 150 151 154 159 156 152 158 160 137 140\n    129 136 138 134 130 132 142 143 144 141 131 139 133 135]\n    &gt;&gt;&gt; print(brainRegions['CA1']['electrodeGroups'])\n        [17 18 19 20]\n    \"\"\"\n    filename = os.path.join(basepath, os.path.basename(basepath) + \".session.mat\")\n\n    if not os.path.exists(filename):\n        warnings.warn(f\"file {filename} does not exist\")\n        if out_format == \"DataFrame\":\n            return pd.DataFrame()\n        else:\n            return {}\n\n    # load file\n    data = sio.loadmat(filename, simplify_cells=True)\n    data = data[\"session\"]\n\n    if \"brainRegions\" not in data.keys():\n        warnings.warn(\"brainRegions not found in file\")\n        if out_format == \"DataFrame\":\n            return pd.DataFrame()\n        else:\n            return {}\n\n    brainRegions = {}\n    for region in data[\"brainRegions\"].keys():\n        if len(data[\"brainRegions\"][region]) == 0:\n            continue\n        channels = data[\"brainRegions\"][region][\"channels\"] - 1\n        try:\n            electrodeGroups = data[\"brainRegions\"][region][\"electrodeGroups\"]\n        except Exception:\n            electrodeGroups = np.nan\n\n        brainRegions[region] = {\n            \"channels\": channels,\n            \"electrodeGroups\": electrodeGroups,\n        }\n\n    if out_format == \"DataFrame\":  # return as DataFrame\n        # get channel order from electrodeGroups in session file\n        shank_to_channel = data[\"extracellular\"][\"electrodeGroups\"][\"channels\"] - 1\n\n        # check if nested array for multi shank\n        if is_nested(shank_to_channel) or shank_to_channel.ndim &gt; 1:\n            channels = np.hstack(shank_to_channel)\n            shanks = np.hstack(\n                [\n                    np.repeat(i, len(shank_to_channel[i]))\n                    for i in range(len(shank_to_channel))\n                ]\n            )\n        else:\n            channels = shank_to_channel\n            shanks = np.zeros(len(channels))\n\n        mapped_df = pd.DataFrame(columns=[\"channels\", \"region\"])\n        mapped_df[\"channels\"] = channels\n        mapped_df[\"region\"] = \"Unknown\"\n        mapped_df[\"shank\"] = shanks\n\n        for key in brainRegions.keys():\n            idx = np.in1d(channels, brainRegions[key][\"channels\"])\n            mapped_df.loc[idx, \"region\"] = key\n\n        # save channel as zero-indexed\n        mapped_df[\"channels\"] = mapped_df[\"channels\"]\n\n        return mapped_df.reset_index(drop=True)\n\n    elif out_format == \"dict\":\n        return brainRegions\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_cell_metrics","title":"<code>load_cell_metrics(basepath, only_metrics=False)</code>","text":"<p>Loader of cell-explorer cell_metrics.cellinfo.mat.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to folder with cell_metrics.cellinfo.mat.</p> required <code>only_metrics</code> <code>bool</code> <p>If True, only metrics are loaded, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[DataFrame, Tuple[DataFrame, dict]]</code> <p>DataFrame of single unit features and a dictionary with data that does not fit nicely into a DataFrame (waveforms, acgs, epochs, etc.).</p> Notes <p>See https://cellexplorer.org/datastructure/standard-cell-metrics/ for details.</p> <p>TODO: Extract all fields from cell_metrics.cellinfo. There are more items that can be extracted.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_cell_metrics(\n    basepath: str, only_metrics: bool = False\n) -&gt; Union[pd.DataFrame, Tuple[pd.DataFrame, dict]]:\n    \"\"\"\n    Loader of cell-explorer cell_metrics.cellinfo.mat.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to folder with cell_metrics.cellinfo.mat.\n    only_metrics : bool, optional\n        If True, only metrics are loaded, by default False.\n\n    Returns\n    -------\n    Union[pd.DataFrame, Tuple[pd.DataFrame, dict]]\n        DataFrame of single unit features and a dictionary with data that does not fit nicely into a DataFrame (waveforms, acgs, epochs, etc.).\n\n    Notes\n    -----\n    See https://cellexplorer.org/datastructure/standard-cell-metrics/ for details.\n\n    TODO: Extract all fields from cell_metrics.cellinfo. There are more items that can be extracted.\n    \"\"\"\n\n    def extract_epochs(data):\n        startTime = [\n            ep[\"startTime\"][0][0][0][0]\n            for ep in data[\"cell_metrics\"][\"general\"][0][0][\"epochs\"][0][0][0]\n        ]\n        stopTime = [\n            ep[\"stopTime\"][0][0][0][0]\n            for ep in data[\"cell_metrics\"][\"general\"][0][0][\"epochs\"][0][0][0]\n        ]\n        name = [\n            ep[\"name\"][0][0][0]\n            for ep in data[\"cell_metrics\"][\"general\"][0][0][\"epochs\"][0][0][0]\n        ]\n\n        epochs = pd.DataFrame()\n        epochs[\"name\"] = name\n        epochs[\"startTime\"] = startTime\n        epochs[\"stopTime\"] = stopTime\n        return epochs\n\n    def extract_events(data):\n        psth = {}\n        for dt in data[\"cell_metrics\"][\"events\"][0][0].dtype.names:\n            psth[dt] = pd.DataFrame(\n                index=data[\"cell_metrics\"][\"general\"][0][0][0][\"events\"][0][dt][0][0][\n                    \"x_bins\"\n                ][0][0].T[0]\n                / 1000,\n                data=np.hstack(data[\"cell_metrics\"][\"events\"][0][0][dt][0][0][0]),\n            )\n        return psth\n\n    def extract_general(data):\n        # extract fr per unit with lag zero to ripple\n        try:\n            ripple_fr = [\n                ev.T[0]\n                for ev in data[\"cell_metrics\"][\"events\"][0][0][\"ripples\"][0][0][0]\n            ]\n        except Exception:\n            ripple_fr = []\n        # extract spikes times\n        spikes = [\n            spk.T[0] for spk in data[\"cell_metrics\"][\"spikes\"][0][0][\"times\"][0][0][0]\n        ]\n        # extract epochs\n        try:\n            epochs = extract_epochs(data)\n        except Exception:\n            epochs = []\n\n        # extract events\n        try:\n            events_psth = extract_events(data)\n        except Exception:\n            events_psth = []\n\n        # extract avg waveforms\n        try:\n            waveforms = np.vstack(\n                data[\"cell_metrics\"][\"waveforms\"][0][0][\"filt\"][0][0][0]\n            )\n        except Exception:\n            try:\n                waveforms = [\n                    w.T for w in data[\"cell_metrics\"][\"waveforms\"][0][0][0][0][0][0]\n                ]\n            except Exception:\n                waveforms = [w.T for w in data[\"cell_metrics\"][\"waveforms\"][0][0][0]]\n        # extract chanCoords\n        try:\n            chanCoords_x = data[\"cell_metrics\"][\"general\"][0][0][\"chanCoords\"][0][0][0][\n                0\n            ][\"x\"].T[0]\n            chanCoords_y = data[\"cell_metrics\"][\"general\"][0][0][\"chanCoords\"][0][0][0][\n                0\n            ][\"y\"].T[0]\n        except Exception:\n            chanCoords_x = []\n            chanCoords_y = []\n\n        # add to dictionary\n        data_ = {\n            \"acg_wide\": data[\"cell_metrics\"][\"acg\"][0][0][\"wide\"][0][0],\n            \"acg_narrow\": data[\"cell_metrics\"][\"acg\"][0][0][\"narrow\"][0][0],\n            \"acg_log10\": data[\"cell_metrics\"][\"acg\"][0][0][\"log10\"][0][0],\n            \"ripple_fr\": ripple_fr,\n            \"chanCoords_x\": chanCoords_x,\n            \"chanCoords_y\": chanCoords_y,\n            \"epochs\": epochs,\n            \"spikes\": spikes,\n            \"waveforms\": waveforms,\n            \"events_psth\": events_psth,\n        }\n        return data_\n\n    def un_nest_df(df):\n        # Un-nest some strings are nested within brackets (a better solution exists...)\n        # locate and iterate objects in df\n        for item in df.keys()[df.dtypes == \"object\"]:\n            # if you can get the size of the first item with [0], it is nested\n            # otherwise it fails and is not nested\n            try:\n                df[item][0][0].size\n                # the below line is from: https://www.py4u.net/discuss/140913\n                df[item] = df[item].str.get(0)\n            except Exception:\n                continue\n        return df\n\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".cell_metrics.cellinfo.mat\"\n    )\n    # filename = glob.glob(os.path.join(basepath, \"*.cell_metrics.cellinfo.mat\"))[0]\n\n    # check if saved file exists\n    if not os.path.exists(filename):\n        warnings.warn(\"file does not exist\")\n        if only_metrics:\n            return None\n        return None, None\n\n    # load cell_metrics file\n    data = sio.loadmat(filename)\n\n    # construct data frame with features per neuron\n    df = {}\n    # count units\n    n_cells = data[\"cell_metrics\"][\"UID\"][0][0][0].size\n    dt = data[\"cell_metrics\"].dtype\n    for dn in dt.names:\n        # check if var has the right n of units and is a vector\n        try:\n            if (data[\"cell_metrics\"][dn][0][0][0][0].size == 1) &amp; (\n                data[\"cell_metrics\"][dn][0][0][0].size == n_cells\n            ):\n                # check if nested within brackets\n                try:\n                    df[dn] = [\n                        value[0] if len(value) == 1 else value\n                        for value in data[\"cell_metrics\"][dn][0][0][0]\n                    ]\n                except Exception:\n                    df[dn] = data[\"cell_metrics\"][dn][0][0][0]\n        except Exception:\n            continue\n\n    df = pd.DataFrame(df)\n\n    # load in tag\n    # check if tags exist within cell_metrics\n    if \"tags\" in data.get(\"cell_metrics\").dtype.names:\n        # get names of each tag\n        dt = data[\"cell_metrics\"][\"tags\"][0][0].dtype\n        if len(dt) &gt; 0:\n            # iter through each tag\n            for dn in dt.names:\n                # set up column for tag\n                df[\"tags_\" + dn] = [False] * df.shape[0]\n                # iter through uid\n                for uid in data[\"cell_metrics\"][\"tags\"][0][0][dn][0][0].flatten():\n                    df.loc[df.UID == uid, \"tags_\" + dn] = True\n\n    # add bad unit tag for legacy\n    df[\"bad_unit\"] = [False] * df.shape[0]\n    if \"tags_Bad\" in df.keys():\n        df.bad_unit = df.tags_Bad\n        df.bad_unit = df.bad_unit.replace({np.nan: False})\n\n    # add data from general metrics\n    df[\"basename\"] = data[\"cell_metrics\"][\"general\"][0][0][\"basename\"][0][0][0]\n    df[\"basepath\"] = basepath\n    df[\"sex\"] = data[\"cell_metrics\"][\"general\"][0][0][\"animal\"][0][0][\"sex\"][0][0][0]\n    df[\"species\"] = data[\"cell_metrics\"][\"general\"][0][0][\"animal\"][0][0][\"species\"][0][\n        0\n    ][0]\n    df[\"strain\"] = data[\"cell_metrics\"][\"general\"][0][0][\"animal\"][0][0][\"strain\"][0][\n        0\n    ][0]\n    try:\n        df[\"geneticLine\"] = data[\"cell_metrics\"][\"general\"][0][0][\"animal\"][0][0][\n            \"geneticLine\"\n        ][0][0][0]\n    except Exception:\n        pass\n    df[\"cellCount\"] = data[\"cell_metrics\"][\"general\"][0][0][\"cellCount\"][0][0][0][0]\n\n    # fix nesting issue for strings\n    df = un_nest_df(df)\n\n    # convert nans within tags columns to false\n    cols = df.filter(regex=\"tags_\").columns\n    df[cols] = df[cols].replace({np.nan: False})\n\n    if only_metrics:\n        return df\n\n    # extract other general data and put into dict\n    data_ = extract_general(data)\n\n    return df, data_\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_channel_tags","title":"<code>load_channel_tags(basepath)</code>","text":"<p>Load channel tags from session file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the directory containing the session file.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of channel tags from the session file.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_channel_tags(basepath: str) -&gt; dict:\n    \"\"\"\n    Load channel tags from session file.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the directory containing the session file.\n\n    Returns\n    -------\n    dict\n        A dictionary of channel tags from the session file.\n    \"\"\"\n    filename = glob.glob(os.path.join(basepath, \"*.session.mat\"))[0]\n    data = sio.loadmat(filename, simplify_cells=True)\n    return data[\"session\"][\"channelTags\"]\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_deepSuperficialfromRipple","title":"<code>load_deepSuperficialfromRipple(basepath, bypass_mismatch_exception=False)</code>","text":"<p>Load deepSuperficialfromRipple file created by classification_DeepSuperficial.m.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>bypass_mismatch_exception</code> <code>bool</code> <p>If True, bypass the mismatch exception, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, ndarray, ndarray]</code> <ul> <li>channel_df: DataFrame containing channel information.</li> <li>ripple_average: Array containing average ripple traces.</li> <li>ripple_time_axis: Array containing ripple time axis.</li> </ul> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_deepSuperficialfromRipple(\n    basepath: str, bypass_mismatch_exception: bool = False\n) -&gt; Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n    \"\"\"\n    Load deepSuperficialfromRipple file created by classification_DeepSuperficial.m.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    bypass_mismatch_exception : bool, optional\n        If True, bypass the mismatch exception, by default False.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, np.ndarray, np.ndarray]\n        - channel_df: DataFrame containing channel information.\n        - ripple_average: Array containing average ripple traces.\n        - ripple_time_axis: Array containing ripple time axis.\n    \"\"\"\n    # locate .mat file\n    file_type = \"*.deepSuperficialfromRipple.channelinfo.mat\"\n    filename = glob.glob(basepath + os.sep + file_type)[0]\n\n    # load matfile\n    data = sio.loadmat(filename)\n\n    channel_df = pd.DataFrame()\n    name = \"deepSuperficialfromRipple\"\n\n    # sometimes more channels positons will be in deepSuperficialfromRipple than in xml\n    #   this is because they used channel id as an index.\n    channel_df = pd.DataFrame()\n    channels = np.hstack(data[name][\"channel\"][0][0]) * np.nan\n    shanks = np.hstack(data[name][\"channel\"][0][0]) * np.nan\n\n    channels_, shanks_ = zip(\n        *[\n            (values[0], np.tile(shank, len(values[0])))\n            for shank, values in enumerate(data[name][\"ripple_channels\"][0][0][0])\n        ]\n    )\n    channel_sort_idx = np.hstack(channels_) - 1\n    channels[channel_sort_idx] = np.hstack(channels_)\n    shanks[channel_sort_idx] = np.hstack(shanks_) + 1\n\n    channel_df[\"channel\"] = channels\n    channel_df.loc[np.arange(len(channel_sort_idx)), \"channel_sort_idx\"] = (\n        channel_sort_idx\n    )\n    channel_df[\"shank\"] = shanks\n\n    # add distance from pyr layer (will only be accurate if polarity rev)\n    channel_df[\"channelDistance\"] = data[name][\"channelDistance\"][0][0].T[0]\n\n    # add channel class (deep or superficial)\n    channelClass = []\n    for item in data[name][\"channelClass\"][0][0]:\n        try:\n            channelClass.append(item[0][0])\n        except Exception:\n            channelClass.append(\"unknown\")\n    channel_df[\"channelClass\"] = channelClass\n\n    # add if shank has polarity reversal\n    for shank in channel_df.shank.unique():\n        if channel_df[channel_df.shank == shank].channelClass.unique().shape[0] == 2:\n            channel_df.loc[channel_df.shank == shank, \"polarity_reversal\"] = True\n        else:\n            channel_df.loc[channel_df.shank == shank, \"polarity_reversal\"] = False\n\n    # add ripple and sharp wave features\n    labels = [\"ripple_power\", \"ripple_amplitude\", \"SWR_diff\", \"SWR_amplitude\"]\n    for label in labels:\n        try:\n            channel_df.loc[channel_sort_idx, label] = np.hstack(\n                data[name][label][0][0][0]\n            )[0]\n        except Exception:\n            x = np.arange(len(channel_sort_idx)) * np.nan\n            x[0 : len(np.hstack(data[name][label][0][0][0])[0])] = np.hstack(\n                data[name][label][0][0][0]\n            )[0]\n            channel_df.loc[channel_sort_idx, label] = x\n\n    # pull put avg ripple traces and ts\n    ripple_time_axis = data[name][\"ripple_time_axis\"][0][0][0]\n    ripple_average = np.ones([channel_df.shape[0], len(ripple_time_axis)]) * np.nan\n\n    rip_map = []\n    for ch, values in zip(channels_, data[name][\"ripple_average\"][0][0][0]):\n        if values.shape[1] &gt; 0:\n            rip_map.append(values)\n        else:\n            rip_map.append(np.zeros([len(ripple_time_axis), len(ch)]) * np.nan)\n\n    ripple_average[channel_sort_idx] = np.hstack(rip_map).T\n\n    brainRegions = load_brain_regions(basepath)\n    for key, value in brainRegions.items():\n        if (\"ca1\" in key.lower()) | (\"ca2\" in key.lower()):\n            for shank in value[\"electrodeGroups\"]:\n                channel_df.loc[channel_df.shank == shank, \"ca1_shank\"] = True\n\n    if (ripple_average.shape[0] != channel_df.shape[0]) &amp; (~bypass_mismatch_exception):\n        raise Exception(\n            \"size mismatch \"\n            + str(np.hstack(ripple_average).shape[1])\n            + \" and \"\n            + str(channel_df.shape[0])\n        )\n\n    channel_df[\"basepath\"] = basepath\n\n    return channel_df, ripple_average, ripple_time_axis\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_dentate_spikes","title":"<code>load_dentate_spikes(basepath, dentate_spike_type=['DS1', 'DS2'], manual_events=True, return_epoch_array=False)</code>","text":"<p>Load info from DS*.events.mat and store within a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to your session where DS*.events.mat is located.</p> required <code>dentate_spike_type</code> <code>List[str]</code> <p>List of DS types to load, by default [\"DS1\", \"DS2\"].</p> <code>['DS1', 'DS2']</code> <code>manual_events</code> <code>bool</code> <p>If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.</p> <code>True</code> <code>return_epoch_array</code> <code>bool</code> <p>If True, the output will be an EpochArray, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame with the following fields: - start: start time of DS - stop: end time of DS - peaks: peak time of DS - amplitude: envelope value at peak time - duration: DS duration - detectorName: the name of DS detector used - basepath: path name - basename: session id - animal: animal id</p> Notes <ul> <li>Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.</li> </ul> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_dentate_spikes(\n    basepath: str,\n    dentate_spike_type: List[str] = [\"DS1\", \"DS2\"],\n    manual_events: bool = True,\n    return_epoch_array: bool = False,\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Load info from DS*.events.mat and store within a DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to your session where DS*.events.mat is located.\n    dentate_spike_type : List[str], optional\n        List of DS types to load, by default [\"DS1\", \"DS2\"].\n    manual_events : bool, optional\n        If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.\n    return_epoch_array : bool, optional\n        If True, the output will be an EpochArray, by default False.\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame with the following fields:\n        - start: start time of DS\n        - stop: end time of DS\n        - peaks: peak time of DS\n        - amplitude: envelope value at peak time\n        - duration: DS duration\n        - detectorName: the name of DS detector used\n        - basepath: path name\n        - basename: session id\n        - animal: animal id\n\n    Notes\n    -----\n    * Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.\n    \"\"\"\n\n    def extract_data(s_type, data, manual_events):\n        # make data frame of known fields\n        df = pd.DataFrame()\n        df[\"start\"] = data[s_type][\"timestamps\"][:, 0]\n        df[\"stop\"] = data[s_type][\"timestamps\"][:, 1]\n        df[\"peaks\"] = data[s_type][\"peaks\"]\n        df[\"event_label\"] = s_type\n        df[\"amplitude\"] = data[s_type][\"amplitudes\"]\n        df[\"duration\"] = data[s_type][\"duration\"]\n        df[\"amplitudeUnits\"] = data[s_type][\"amplitudeUnits\"]\n        df[\"detectorName\"] = data[s_type][\"detectorinfo\"][\"detectorname\"]\n        df[\"ml_channel\"] = data[s_type][\"detectorinfo\"][\"ml_channel\"]\n        df[\"h_channel\"] = data[s_type][\"detectorinfo\"][\"h_channel\"]\n\n        # remove flagged ripples, if exist\n        try:\n            df.drop(\n                labels=np.array(data[s_type][\"flagged\"]).T - 1,\n                axis=0,\n                inplace=True,\n            )\n            df.reset_index(inplace=True)\n        except Exception:\n            pass\n\n        # adding manual events\n        if manual_events:\n            try:\n                df = _add_manual_events(df, data[s_type][\"added\"])\n            except Exception:\n                pass\n        return df\n\n    # locate .mat file\n    df = pd.DataFrame()\n    for s_type in dentate_spike_type:\n        filename = glob.glob(basepath + os.sep + \"*\" + s_type + \".events.mat\")\n        if len(filename) == 0:\n            continue\n        # load matfile\n        filename = filename[0]\n        data = sio.loadmat(filename, simplify_cells=True)\n        # pull out data\n        df = pd.concat(\n            [df, extract_data(s_type, data, manual_events)], ignore_index=True\n        )\n\n    if df.shape[0] == 0:\n        return df\n\n    if return_epoch_array:\n        return nel.EpochArray([np.array([df.start, df.stop]).T], label=\"dentate_spike\")\n\n    # get basename and animal\n    normalized_path = os.path.normpath(filename)\n    path_components = normalized_path.split(os.sep)\n    df[\"basepath\"] = basepath\n    df[\"basename\"] = path_components[-2]\n    df[\"animal\"] = path_components[-3]\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_emg","title":"<code>load_emg(basepath, threshold=0.9)</code>","text":"<p>Load EMG data from basename.EMGFromLFP.LFP.mat.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>threshold</code> <code>float</code> <p>Threshold for high epochs (low will be &lt; threshold). Default is 0.9.</p> <code>0.9</code> <p>Returns:</p> Name Type Description <code>emg</code> <code>AnalogSignalArray</code> <p>EMG data.</p> <code>high_emg_epoch</code> <code>EpochArray</code> <p>High EMG epochs.</p> <code>low_emg_epoch</code> <code>EpochArray</code> <p>Low EMG epochs.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_emg(\n    basepath: str, threshold: float = 0.9\n) -&gt; Tuple[nel.AnalogSignalArray, nel.EpochArray, nel.EpochArray]:\n    \"\"\"\n    Load EMG data from basename.EMGFromLFP.LFP.mat.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    threshold : float, optional\n        Threshold for high epochs (low will be &lt; threshold). Default is 0.9.\n\n    Returns\n    -------\n    emg : nel.AnalogSignalArray\n        EMG data.\n    high_emg_epoch : nel.EpochArray\n        High EMG epochs.\n    low_emg_epoch : nel.EpochArray\n        Low EMG epochs.\n    \"\"\"\n    # locate .mat file\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".EMGFromLFP.LFP.mat\"\n    )\n\n    # load matfile\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    # put emg data into AnalogSignalArray\n    emg = nel.AnalogSignalArray(\n        data=data[\"EMGFromLFP\"][\"data\"], timestamps=data[\"EMGFromLFP\"][\"timestamps\"]\n    )\n\n    # get high and low emg epochs\n    high_emg_epoch = find_interval(emg.data.flatten() &gt; threshold)\n    high_emg_epoch = nel.EpochArray(emg.abscissa_vals[high_emg_epoch])\n\n    low_emg_epoch = find_interval(emg.data.flatten() &lt; threshold)\n    low_emg_epoch = nel.EpochArray(emg.abscissa_vals[low_emg_epoch])\n\n    return emg, high_emg_epoch, low_emg_epoch\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_epoch","title":"<code>load_epoch(basepath)</code>","text":"<p>Loads epoch info from cell explorer basename.session and stores in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the following fields: - name: name of the epoch - startTime: start time of the epoch - stopTime: stop time of the epoch - environment: environment during the epoch - manipulation: manipulation during the epoch - behavioralParadigm: behavioral paradigm during the epoch - stimuli: stimuli during the epoch - notes: notes about the epoch - basepath: path to the session folder</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_epoch(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads epoch info from cell explorer basename.session and stores in a DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with the following fields:\n        - name: name of the epoch\n        - startTime: start time of the epoch\n        - stopTime: stop time of the epoch\n        - environment: environment during the epoch\n        - manipulation: manipulation during the epoch\n        - behavioralParadigm: behavioral paradigm during the epoch\n        - stimuli: stimuli during the epoch\n        - notes: notes about the epoch\n        - basepath: path to the session folder\n    \"\"\"\n\n    filename = os.path.join(basepath, os.path.basename(basepath) + \".session.mat\")\n\n    if not os.path.exists(filename):\n        warnings.warn(f\"file {filename} does not exist\")\n        return pd.DataFrame()\n\n    # load file\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    def add_columns(df):\n        \"\"\"add columns to df if they don't exist\"\"\"\n        needed_columns = [\n            \"name\",\n            \"startTime\",\n            \"stopTime\",\n            \"environment\",\n            \"manipulation\",\n            \"behavioralParadigm\",\n            \"stimuli\",\n            \"notes\",\n        ]\n        for col in needed_columns:\n            if col not in df.columns:\n                df[col] = np.nan\n        return df\n\n    try:\n        epoch_df = pd.DataFrame(data[\"session\"][\"epochs\"])\n        epoch_df = add_columns(epoch_df)\n        epoch_df[\"basepath\"] = basepath\n        return epoch_df\n    except Exception:\n        epoch_df = pd.DataFrame([data[\"session\"][\"epochs\"]])\n        epoch_df = add_columns(epoch_df)\n        epoch_df[\"basepath\"] = basepath\n        return epoch_df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_events","title":"<code>load_events(basepath, epoch_name, load_pandas=False)</code>","text":"<p>Load events from basename.epoch_name.events.mat.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>epoch_name</code> <code>str</code> <p>Name of epoch to load.</p> required <p>Returns:</p> Name Type Description <code>events</code> <code>EpochArray or None or DataFrame</code> <p>Events, or None if the file does not exist.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_events(\n    basepath: str, epoch_name: str, load_pandas=False\n) -&gt; Union[nel.EpochArray, None, pd.DataFrame]:\n    \"\"\"\n    Load events from basename.epoch_name.events.mat.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    epoch_name : str\n        Name of epoch to load.\n\n    Returns\n    -------\n    events : nel.EpochArray or None or pd.DataFrame\n        Events, or None if the file does not exist.\n    \"\"\"\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".\" + epoch_name + \".events.mat\"\n    )\n    # check if filename exist\n    if not os.path.exists(filename):\n        return None\n\n    data = sio.loadmat(filename, simplify_cells=True)\n\n    if load_pandas:\n        n_events = data[epoch_name][\"timestamps\"].shape[0]\n\n        event_df = pd.DataFrame(\n            data[epoch_name][\"timestamps\"], columns=[\"starts\", \"stops\"]\n        )\n        for key in data[epoch_name].keys():\n            if (\n                isinstance(data[epoch_name][key], np.ndarray)\n                and data[epoch_name][key].shape[0] == n_events\n                and data[epoch_name][key].ndim == 1\n            ):\n                event_df[key] = data[epoch_name][key]\n        return event_df\n\n    return nel.EpochArray(data[epoch_name][\"timestamps\"])\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_extracellular_metadata","title":"<code>load_extracellular_metadata(basepath)</code>","text":"<p>Load extracellular metadata from session file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path to the directory containing the session file.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of extracellular metadata from the session file.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_extracellular_metadata(basepath: str) -&gt; dict:\n    \"\"\"\n    Load extracellular metadata from session file.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path to the directory containing the session file.\n\n    Returns\n    -------\n    dict\n        A dictionary of extracellular metadata from the session file.\n    \"\"\"\n    filename = os.path.join(basepath, os.path.basename(basepath) + \".session.mat\")\n    # check if filename exist\n    if not os.path.exists(filename):\n        return {}\n    data = sio.loadmat(filename, simplify_cells=True)\n    return data[\"session\"][\"extracellular\"]\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_ied_events","title":"<code>load_ied_events(basepath, manual_events=True, return_epoch_array=False)</code>","text":"<p>Load info from ripples.events.mat and store within a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to your session where ripples.events.mat is located.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, the output will be an EpochArray, by default False.</p> <code>False</code> <code>manual_events</code> <code>bool</code> <p>If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame with the following fields: - start: start time of ripple - stop: end time of ripple - center: center time of ripple - peaks: peak time of ripple</p> Notes <ul> <li>Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.</li> </ul> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_ied_events(\n    basepath: str, manual_events: bool = True, return_epoch_array: bool = False\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Load info from ripples.events.mat and store within a DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to your session where ripples.events.mat is located.\n    return_epoch_array : bool, optional\n        If True, the output will be an EpochArray, by default False.\n    manual_events : bool, optional\n        If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame with the following fields:\n        - start: start time of ripple\n        - stop: end time of ripple\n        - center: center time of ripple\n        - peaks: peak time of ripple\n\n    Notes\n    -----\n    * Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.\n    \"\"\"\n\n    # locate .mat file\n    try:\n        filename = glob.glob(basepath + os.sep + \"*IED.events.mat\")[0]\n    except Exception:\n        try:\n            filename = glob.glob(basepath + os.sep + \"*interictal_spikes.events.mat\")[0]\n        except Exception:\n            # warnings.warn(\"file does not exist\")\n            return pd.DataFrame()\n\n    df = pd.DataFrame()\n\n    data = sio.loadmat(filename, simplify_cells=True)\n    struct_name = list(data.keys())[-1]\n    df[\"start\"] = data[struct_name][\"timestamps\"][:, 0]\n    df[\"stop\"] = data[struct_name][\"timestamps\"][:, 1]\n    df[\"center\"] = data[struct_name][\"peaks\"]\n    df[\"peaks\"] = data[struct_name][\"peaks\"]\n\n    # remove flagged ripples, if exist\n    try:\n        df.drop(\n            labels=np.array(data[struct_name][\"flagged\"]).T - 1,\n            axis=0,\n            inplace=True,\n        )\n        df.reset_index(inplace=True)\n    except Exception:\n        pass\n\n    # adding manual events\n    if manual_events:\n        try:\n            df = _add_manual_events(df, data[struct_name][\"added\"])\n        except Exception:\n            pass\n\n    if return_epoch_array:\n        return nel.EpochArray([np.array([df.start, df.stop]).T], label=\"ied\")\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_manipulation","title":"<code>load_manipulation(basepath, struct_name=None, return_epoch_array=True, merge_gap=None)</code>","text":"<p>Loads the data from the basename.eventName.manipulations.mat file and returns a pandas dataframe.</p> <p>file structure defined here:     https://cellexplorer.org/datastructure/data-structure-and-format/#manipulations</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the basename.eventName.manipulations.mat file.</p> required <code>struct_name</code> <code>Union[str, None]</code> <p>Name of the structure in the mat file to load. If None, loads all the manipulation files, by default None.</p> <code>None</code> <code>return_epoch_array</code> <code>bool</code> <p>If True, returns only the epoch array, by default True.</p> <code>True</code> <code>merge_gap</code> <code>Union[int, None]</code> <p>If not None, merges the epochs that are separated by less than merge_gap (sec). return_epoch_array must be True, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame or EpochArray with the manipulation data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; basepath = r\"Z:\\Data\\Can\\OML22\\day8\"\n&gt;&gt;&gt; df_manipulation = load_manipulation(basepath, struct_name=\"optoStim\", return_epoch_array=False)\n&gt;&gt;&gt; df_manipulation.head(2)\n</code></pre> <p>.. table:: Manipulation Data     :widths: auto</p> <pre><code>====== ========== ========== ========== ========== ========== ========================\n    start      stop       peaks      center     duration    amplitude     amplitudeUnits\n====== ========== ========== ========== ========== ========== ========================\n8426.83650  8426.84845  8426.842475  8426.842475  0.01195   19651       pulse_respect_baseline\n8426.85245  8426.86745  8426.859950  8426.859950  0.01500   17516       pulse_respect_baseline\n====== ========== ========== ========== ========== ========== ========================\n</code></pre> <pre><code>&gt;&gt;&gt; basepath = r\"Z:\\Data\\Can\\OML22\\day8\"\n&gt;&gt;&gt; df_manipulation = load_manipulation(basepath, struct_name=\"optoStim\", return_epoch_array=True)\n&gt;&gt;&gt; df_manipulation\n</code></pre> <p> of length 1:25:656 minutes Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_manipulation(\n    basepath: str,\n    struct_name: Union[str, None] = None,\n    return_epoch_array: bool = True,\n    merge_gap: Union[int, None] = None,\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Loads the data from the basename.eventName.manipulations.mat file and returns a pandas dataframe.\n\n    file structure defined here:\n        https://cellexplorer.org/datastructure/data-structure-and-format/#manipulations\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the basename.eventName.manipulations.mat file.\n    struct_name : Union[str, None], optional\n        Name of the structure in the mat file to load. If None, loads all the manipulation files, by default None.\n    return_epoch_array : bool, optional\n        If True, returns only the epoch array, by default True.\n    merge_gap : Union[int, None], optional\n        If not None, merges the epochs that are separated by less than merge_gap (sec). return_epoch_array must be True, by default None.\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame or EpochArray with the manipulation data.\n\n    Examples\n    -------\n    &gt;&gt;&gt; basepath = r\"Z:\\Data\\Can\\OML22\\day8\"\n    &gt;&gt;&gt; df_manipulation = load_manipulation(basepath, struct_name=\"optoStim\", return_epoch_array=False)\n    &gt;&gt;&gt; df_manipulation.head(2)\n\n    .. table:: Manipulation Data\n        :widths: auto\n\n        ====== ========== ========== ========== ========== ========== ========================\n            start      stop       peaks      center     duration    amplitude     amplitudeUnits\n        ====== ========== ========== ========== ========== ========== ========================\n        8426.83650  8426.84845  8426.842475  8426.842475  0.01195   19651       pulse_respect_baseline\n        8426.85245  8426.86745  8426.859950  8426.859950  0.01500   17516       pulse_respect_baseline\n        ====== ========== ========== ========== ========== ========== ========================\n\n    &gt;&gt;&gt; basepath = r\"Z:\\Data\\Can\\OML22\\day8\"\n    &gt;&gt;&gt; df_manipulation = load_manipulation(basepath, struct_name=\"optoStim\", return_epoch_array=True)\n    &gt;&gt;&gt; df_manipulation\n\n    &lt;EpochArray at 0x1faba577520: 5,774 epochs&gt; of length 1:25:656 minutes\n    \"\"\"\n    try:\n        if struct_name is None:\n            filename = glob.glob(basepath + os.sep + \"*manipulation.mat\")\n            print(filename)\n            if len(filename) &gt; 1:\n                raise ValueError(\n                    \"multi-file not implemented yet...than one manipulation file found\"\n                )\n            filename = filename[0]\n        else:\n            filename = glob.glob(\n                basepath + os.sep + \"*\" + struct_name + \".manipulation.mat\"\n            )[0]\n    except Exception:\n        return None\n    # load matfile\n    data = sio.loadmat(filename)\n\n    if struct_name is None:\n        struct_name = list(data.keys())[-1]\n\n    df = pd.DataFrame()\n    df[\"start\"] = data[struct_name][\"timestamps\"][0][0][:, 0]\n    df[\"stop\"] = data[struct_name][\"timestamps\"][0][0][:, 1]\n    df[\"peaks\"] = data[struct_name][\"peaks\"][0][0]\n    df[\"center\"] = data[struct_name][\"center\"][0][0]\n    df[\"duration\"] = data[struct_name][\"duration\"][0][0]\n    df[\"amplitude\"] = data[struct_name][\"amplitude\"][0][0]\n    df[\"amplitudeUnits\"] = data[struct_name][\"amplitudeUnits\"][0][0][0]\n\n    # extract event label names\n    eventIDlabels = []\n    for name in data[struct_name][\"eventIDlabels\"][0][0][0]:\n        eventIDlabels.append(name[0])\n\n    # extract numeric category labels associated with label names\n    eventID = np.array(data[struct_name][\"eventID\"][0][0]).ravel()\n\n    # add eventIDlabels and eventID to df\n    for ev_label, ev_num in zip(eventIDlabels, np.unique(eventID)):\n        df.loc[eventID == ev_num, \"ev_label\"] = ev_label\n\n    if return_epoch_array:\n        # get session epochs to add support for epochs\n        epoch_df = load_epoch(basepath)\n        # get session bounds to provide support\n        session_bounds = nel.EpochArray(\n            [epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]]\n        )\n        # if many types of manipulations, add them to dictinary\n        if df.ev_label.unique().size &gt; 1:\n            manipulation_epoch = {}\n            for label in df.ev_label.unique():\n                manipulation_epoch_ = nel.EpochArray(\n                    np.array(\n                        [\n                            df[df.ev_label == label][\"start\"],\n                            df[df.ev_label == label][\"stop\"],\n                        ]\n                    ).T,\n                    domain=session_bounds,\n                )\n                if merge_gap is not None:\n                    manipulation_epoch_ = manipulation_epoch_.merge(gap=merge_gap)\n\n                manipulation_epoch[label] = manipulation_epoch_\n        else:\n            manipulation_epoch = nel.EpochArray(\n                np.array([df[\"start\"], df[\"stop\"]]).T, domain=session_bounds\n            )\n            if merge_gap is not None:\n                manipulation_epoch = manipulation_epoch.merge(gap=merge_gap)\n\n        return manipulation_epoch\n    else:\n        return df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_mua_events","title":"<code>load_mua_events(basepath)</code>","text":"<p>Loads the MUA data from the basepath. Meant to load .mat file created by find_HSE.m.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The path to the folder containing the MUA data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The pandas DataFrame containing the MUA data.</p> TODO <p>If none exist in basepath, create one.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_mua_events(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads the MUA data from the basepath.\n    Meant to load .mat file created by find_HSE.m.\n\n    Parameters\n    ----------\n    basepath : str\n        The path to the folder containing the MUA data.\n\n    Returns\n    -------\n    pd.DataFrame\n        The pandas DataFrame containing the MUA data.\n\n    TODO\n    ----\n    If none exist in basepath, create one.\n    \"\"\"\n\n    # locate .mat file\n    try:\n        filename = glob.glob(basepath + os.sep + \"*mua_ca1_pyr.events.mat\")[0]\n    except Exception:\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame()\n\n    # load matfile\n    data = sio.loadmat(filename)\n\n    # pull out and package data\n    df = pd.DataFrame()\n    df[\"start\"] = data[\"HSE\"][\"timestamps\"][0][0][:, 0]\n    df[\"stop\"] = data[\"HSE\"][\"timestamps\"][0][0][:, 1]\n    df[\"peaks\"] = data[\"HSE\"][\"peaks\"][0][0]\n    df[\"center\"] = data[\"HSE\"][\"center\"][0][0]\n    df[\"duration\"] = data[\"HSE\"][\"duration\"][0][0]\n    df[\"amplitude\"] = data[\"HSE\"][\"amplitudes\"][0][0]\n    df[\"amplitudeUnits\"] = data[\"HSE\"][\"amplitudeUnits\"][0][0][0]\n    df[\"detectorName\"] = data[\"HSE\"][\"detectorinfo\"][0][0][\"detectorname\"][0][0][0]\n\n    # get basename and animal\n    normalized_path = os.path.normpath(filename)\n    path_components = normalized_path.split(os.sep)\n    df[\"basepath\"] = basepath\n    df[\"basename\"] = path_components[-2]\n    df[\"animal\"] = path_components[-3]\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_position","title":"<code>load_position(basepath, fs=39.0625)</code>","text":"<p>Load position data from a .whl file in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the directory containing the .whl file.</p> required <code>fs</code> <code>float</code> <p>Sampling frequency, by default 39.0625.</p> <code>39.0625</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, float]</code> <p>DataFrame containing position data and the sampling frequency.</p> Notes <p>If the directory does not exist or contains no .whl files, the function will exit.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_position(basepath: str, fs: float = 39.0625) -&gt; Tuple[pd.DataFrame, float]:\n    \"\"\"\n    Load position data from a .whl file in the specified directory.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the directory containing the .whl file.\n    fs : float, optional\n        Sampling frequency, by default 39.0625.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, float]\n        DataFrame containing position data and the sampling frequency.\n\n    Notes\n    -----\n    If the directory does not exist or contains no .whl files, the function will exit.\n    \"\"\"\n    if not os.path.exists(basepath):\n        print(\"The path \" + basepath + \" doesn't exist; Exiting ...\")\n        sys.exit()\n    listdir = os.listdir(basepath)\n    whlfiles = [f for f in listdir if f.endswith(\".whl\")]\n    if not len(whlfiles):\n        print(\"Folder contains no whl files; Exiting ...\")\n        sys.exit()\n    new_path = os.path.join(basepath, whlfiles[0])\n    df = pd.read_csv(new_path, delimiter=\"\\t\", header=0, names=[\"x1\", \"y1\", \"x2\", \"y2\"])\n    df[df == -1] = np.nan\n    return df, fs\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_probe_layout","title":"<code>load_probe_layout(basepath)</code>","text":"<p>Load electrode coordinates and grouping from the session.extracellular.mat file.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <p>Returns:</p> Name Type Description <code>probe_layout</code> <code>DataFrame</code> <p>DataFrame with x, y coordinates and shank number.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_probe_layout(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Load electrode coordinates and grouping from the session.extracellular.mat file.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n\n    Returns\n    -------\n    probe_layout : pd.DataFrame\n        DataFrame with x, y coordinates and shank number.\n    \"\"\"\n\n    # load session file\n    filename = glob.glob(os.path.join(basepath, \"*.session.mat\"))[0]\n\n    # load file\n    data = sio.loadmat(filename, simplify_cells=True)\n    x = data[\"session\"][\"extracellular\"][\"chanCoords\"][\"x\"]\n    y = data[\"session\"][\"extracellular\"][\"chanCoords\"][\"y\"]\n\n    if (len(x) == 0) &amp; (len(y) == 0):\n        warnings.warn(\n            \"The coordinates are empty in session.extracellular.chanCoords. Returning None - check session file\"\n        )\n        return None\n\n    electrode_groups = data[\"session\"][\"extracellular\"][\"electrodeGroups\"][\"channels\"]\n\n    # for each group in electrodeGroups\n    mapped_shanks = []\n    mapped_channels = []\n\n    n_groups = data[\"session\"][\"extracellular\"][\"nElectrodeGroups\"]\n\n    if n_groups &gt; 1:\n        # loop through electrode groups\n        for group_i in np.arange(n_groups):\n            mapped_channels.append(\n                electrode_groups[group_i] - 1\n            )  # -1 to make 0 indexed\n            mapped_shanks.append(np.repeat(group_i, len(electrode_groups[group_i])))\n\n    elif n_groups == 1:\n        mapped_channels.append(electrode_groups - 1)  # -1 to make 0 indexed\n        mapped_shanks.append(\n            np.repeat(0, len(electrode_groups))\n        )  # electrode group for single shank always 0\n\n    #  unpack to lists\n    mapped_channels = list(chain(*mapped_channels))\n    shanks = list(chain(*mapped_shanks))\n\n    # get shank in same dimension as channels\n    shanks = np.expand_dims(shanks, axis=1)\n\n    probe_layout = (\n        pd.DataFrame({\"x\": x.flatten(), \"y\": y.flatten()})\n        .iloc[mapped_channels]\n        .reset_index(drop=True)\n    )\n    probe_layout[\"shank\"] = shanks\n    probe_layout[\"channels\"] = mapped_channels\n\n    return probe_layout\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_ripples_events","title":"<code>load_ripples_events(basepath, return_epoch_array=False, manual_events=True)</code>","text":"<p>Load info from ripples.events.mat and store within a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to your session where ripples.events.mat is located.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, the output will be an EpochArray, by default False.</p> <code>False</code> <code>manual_events</code> <code>bool</code> <p>If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame with the following fields: - start: start time of ripple - stop: end time of ripple - peaks: peak time of ripple - amplitude: envelope value at peak time - duration: ripple duration - frequency: instant frequency at peak - detectorName: the name of ripple detector used - event_spk_thres: 1 or 0 for if a mua threshold was used - basepath: path name - basename: session id - animal: animal id</p> Notes <ul> <li>Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.</li> </ul> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_ripples_events(\n    basepath: str, return_epoch_array: bool = False, manual_events: bool = True\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Load info from ripples.events.mat and store within a DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to your session where ripples.events.mat is located.\n    return_epoch_array : bool, optional\n        If True, the output will be an EpochArray, by default False.\n    manual_events : bool, optional\n        If True, add manually added events from Neuroscope2 (interval will be calculated from mean event duration), by default True.\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame with the following fields:\n        - start: start time of ripple\n        - stop: end time of ripple\n        - peaks: peak time of ripple\n        - amplitude: envelope value at peak time\n        - duration: ripple duration\n        - frequency: instant frequency at peak\n        - detectorName: the name of ripple detector used\n        - event_spk_thres: 1 or 0 for if a mua threshold was used\n        - basepath: path name\n        - basename: session id\n        - animal: animal id\n\n    Notes\n    -----\n    * Note that basepath/basename/animal relies on specific folder structure and may be incorrect for some data structures.\n    \"\"\"\n\n    # locate .mat file\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".ripples.events.mat\"\n    )\n    if not os.path.exists(filename):\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame()\n\n    # load matfile\n    data = sio.loadmat(filename)\n\n    # make data frame of known fields\n    df = pd.DataFrame()\n    try:\n        df[\"start\"] = data[\"ripples\"][\"timestamps\"][0][0][:, 0]\n        df[\"stop\"] = data[\"ripples\"][\"timestamps\"][0][0][:, 1]\n    except Exception:\n        df[\"start\"] = data[\"ripples\"][\"times\"][0][0][:, 0]\n        df[\"stop\"] = data[\"ripples\"][\"times\"][0][0][:, 1]\n\n    for name in [\"peaks\", \"amplitude\", \"duration\", \"frequency\", \"peakNormedPower\"]:\n        try:\n            df[name] = data[\"ripples\"][name][0][0]\n        except Exception:\n            df[name] = np.nan\n\n    if df.duration.isna().all():\n        df[\"duration\"] = df.stop - df.start\n\n    try:\n        df[\"detectorName\"] = data[\"ripples\"][\"detectorinfo\"][0][0][\"detectorname\"][0][\n            0\n        ][0]\n    except Exception:\n        try:\n            df[\"detectorName\"] = data[\"ripples\"][\"detectorName\"][0][0][0]\n        except Exception:\n            df[\"detectorName\"] = \"unknown\"\n\n    # find ripple channel (this can be in several places depending on the file)\n    try:\n        df[\"ripple_channel\"] = data[\"ripples\"][\"detectorinfo\"][0][0][\"detectionparms\"][\n            0\n        ][0][\"Channels\"][0][0][0][0]\n    except Exception:\n        try:\n            df[\"ripple_channel\"] = data[\"ripples\"][\"detectorParams\"][0][0][\"channel\"][\n                0\n            ][0][0][0]\n        except Exception:\n            try:\n                df[\"ripple_channel\"] = data[\"ripples\"][\"detectorinfo\"][0][0][\n                    \"detectionparms\"\n                ][0][0][\"channel\"][0][0][0][0]\n            except Exception:\n                try:\n                    df[\"ripple_channel\"] = data[\"ripples\"][\"detectorinfo\"][0][0][\n                        \"detectionparms\"\n                    ][0][0][\"ripple_channel\"][0][0][0][0]\n                except Exception:\n                    try:\n                        df[\"ripple_channel\"] = data[\"ripples\"][\"detectorinfo\"][0][0][\n                            \"detectionchannel1\"\n                        ][0][0][0][0]\n                    except Exception:\n                        df[\"ripple_channel\"] = np.nan\n\n    # remove flagged ripples, if exist\n    try:\n        df.drop(\n            labels=np.array(data[\"ripples\"][\"flagged\"][0][0]).T[0] - 1,\n            axis=0,\n            inplace=True,\n        )\n        df.reset_index(inplace=True)\n    except Exception:\n        pass\n\n    # adding manual events\n    if manual_events:\n        try:\n            df = _add_manual_events(df, data[\"ripples\"][\"added\"][0][0].T[0])\n        except Exception:\n            pass\n\n    # adding if ripples were restricted by spikes\n    dt = data[\"ripples\"].dtype\n    if \"eventSpikingParameters\" in dt.names:\n        df[\"event_spk_thres\"] = 1\n    else:\n        df[\"event_spk_thres\"] = 0\n\n    # get basename and animal\n    normalized_path = os.path.normpath(filename)\n    path_components = normalized_path.split(os.sep)\n    df[\"basepath\"] = basepath\n    df[\"basename\"] = path_components[-2]\n    df[\"animal\"] = path_components[-3]\n\n    if return_epoch_array:\n        return nel.EpochArray([np.array([df.start, df.stop]).T], label=\"ripples\")\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_spikes","title":"<code>load_spikes(basepath, putativeCellType=[], brainRegion=[], remove_bad_unit=True, brain_state=[], other_metric=None, other_metric_value=None, support=None, remove_unstable=False, stable_interval_width=600)</code>","text":"<p>Load specific cells' spike times.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <code>putativeCellType</code> <code>List[str]</code> <p>List of putative cell types to restrict spikes to, by default [].</p> <code>[]</code> <code>brainRegion</code> <code>List[str]</code> <p>List of brain regions to restrict spikes to, by default [].</p> <code>[]</code> <code>remove_bad_unit</code> <code>bool</code> <p>If True, do not load bad cells (tagged in CE), by default True.</p> <code>True</code> <code>brain_state</code> <code>List[str]</code> <p>List of brain states to restrict spikes to, by default [].</p> <code>[]</code> <code>other_metric</code> <code>Union[str, None]</code> <p>Metric to restrict spikes to, by default None.</p> <code>None</code> <code>other_metric_value</code> <code>Union[str, None]</code> <p>Value of the metric to restrict spikes to, by default None.</p> <code>None</code> <code>support</code> <code>Union[EpochArray, None]</code> <p>Time support to provide, by default None.</p> <code>None</code> <code>remove_unstable</code> <code>bool</code> <p>If True, remove unstable cells, by default False.</p> <code>False</code> <code>stable_interval_width</code> <code>int</code> <p>Width of the stable interval in seconds, by default 600.</p> <code>600</code> <p>Returns:</p> Type Description <code>Tuple[Union[SpikeTrainArray, None], Union[DataFrame, None]]</code> <p>Spike train array and cell metrics DataFrame.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_spikes(\n    basepath: str,\n    putativeCellType: List[str] = [],\n    brainRegion: List[str] = [],\n    remove_bad_unit: bool = True,\n    brain_state: List[str] = [],\n    other_metric: Union[str, None] = None,\n    other_metric_value: Union[str, None] = None,\n    support: Union[nel.EpochArray, None] = None,\n    remove_unstable: bool = False,\n    stable_interval_width: int = 600,\n) -&gt; Tuple[Union[nel.SpikeTrainArray, None], Union[pd.DataFrame, None]]:\n    \"\"\"\n    Load specific cells' spike times.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n    putativeCellType : List[str], optional\n        List of putative cell types to restrict spikes to, by default [].\n    brainRegion : List[str], optional\n        List of brain regions to restrict spikes to, by default [].\n    remove_bad_unit : bool, optional\n        If True, do not load bad cells (tagged in CE), by default True.\n    brain_state : List[str], optional\n        List of brain states to restrict spikes to, by default [].\n    other_metric : Union[str, None], optional\n        Metric to restrict spikes to, by default None.\n    other_metric_value : Union[str, None], optional\n        Value of the metric to restrict spikes to, by default None.\n    support : Union[nel.EpochArray, None], optional\n        Time support to provide, by default None.\n    remove_unstable : bool, optional\n        If True, remove unstable cells, by default False.\n    stable_interval_width : int, optional\n        Width of the stable interval in seconds, by default 600.\n\n    Returns\n    -------\n    Tuple[Union[nel.SpikeTrainArray, None], Union[pd.DataFrame, None]]\n        Spike train array and cell metrics DataFrame.\n    \"\"\"\n    if not isinstance(putativeCellType, list):\n        putativeCellType = [putativeCellType]\n    if not isinstance(brainRegion, list):\n        brainRegion = [brainRegion]\n\n    # get sample rate from session\n    fs_dat = load_extracellular_metadata(basepath).get(\"sr\", None)\n\n    if fs_dat is None:\n        return None, None\n\n    # load cell metrics and spike data\n    cell_metrics, data = load_cell_metrics(basepath)\n\n    if cell_metrics is None or data is None:\n        return None, None\n\n    # put spike data into array st\n    st = np.array(data[\"spikes\"], dtype=object)\n\n    # restrict cell metrics\n    if len(putativeCellType) &gt; 0:\n        restrict_idx = []\n        for cell_type in putativeCellType:\n            restrict_idx.append(\n                cell_metrics.putativeCellType.str.contains(cell_type).values\n            )\n        restrict_idx = np.any(restrict_idx, axis=0)\n        cell_metrics = cell_metrics[restrict_idx]\n        st = st[restrict_idx]\n\n    if len(brainRegion) &gt; 0:\n        restrict_idx = []\n        for brain_region in brainRegion:\n            restrict_idx.append(\n                cell_metrics.brainRegion.str.contains(brain_region).values\n            )\n        restrict_idx = np.any(restrict_idx, axis=0)\n        cell_metrics = cell_metrics[restrict_idx]\n        st = st[restrict_idx]\n\n    # restrict cell metrics by arbitrary metric\n    if other_metric is not None:\n        # make other_metric_value a list if not already\n        if not isinstance(other_metric, list):\n            other_metric = [other_metric]\n        if not isinstance(other_metric_value, list):\n            other_metric_value = [other_metric_value]\n        # check that other_metric_value is the same length as other_metric\n        if len(other_metric) != len(other_metric_value):\n            raise ValueError(\n                \"other_metric and other_metric_value must be of same length\"\n            )\n\n        restrict_idx = []\n        for metric, value in zip(other_metric, other_metric_value):\n            restrict_idx.append(cell_metrics[metric].str.contains(value).values)\n        restrict_idx = np.any(restrict_idx, axis=0)\n        cell_metrics = cell_metrics[restrict_idx]\n        st = st[restrict_idx]\n\n    if remove_bad_unit:\n        # bad units will be tagged true, so only keep false values\n        restrict_idx = ~cell_metrics.bad_unit.values\n        cell_metrics = cell_metrics[restrict_idx]\n        st = st[restrict_idx]\n\n    if remove_unstable and len(st) &gt; 0:\n        starts = np.arange(\n            np.hstack(st).min(),\n            np.hstack(st).max() - stable_interval_width,\n            stable_interval_width,\n        )\n        stops = starts + stable_interval_width\n\n        bst = npy.process.count_in_interval(st, starts, stops, \"counts\")\n        restrict_idx = np.sum(bst == 0, axis=1) &lt; 2  # allow for 1 unstable interval max\n        cell_metrics = cell_metrics[restrict_idx]\n        st = st[restrict_idx]\n\n    # get spike train array\n    try:\n        if support is not None:\n            st = nel.SpikeTrainArray(timestamps=st, fs=fs_dat, support=support)\n        else:\n            st = nel.SpikeTrainArray(timestamps=st, fs=fs_dat)\n    except Exception:  # if only single cell... should prob just skip session\n        if support is not None:\n            st = nel.SpikeTrainArray(timestamps=st[0], fs=fs_dat, support=support)\n        else:\n            st = nel.SpikeTrainArray(timestamps=st[0], fs=fs_dat)\n\n    if len(brain_state) &gt; 0:\n        # get brain states\n        brain_states = [\"WAKEstate\", \"NREMstate\", \"REMstate\", \"THETA\", \"nonTHETA\"]\n        if brain_state not in brain_states:\n            assert print(\"not correct brain state. Pick one\", brain_states)\n        else:\n            state_dict = load_SleepState_states(basepath)\n            state_epoch = nel.EpochArray(state_dict[brain_state])\n            st = st[state_epoch]\n\n    return st, cell_metrics\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_theta_cycles","title":"<code>load_theta_cycles(basepath, return_epoch_array=False)</code>","text":"<p>Load theta cycles calculated from auto_theta_cycles.m.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to your session where thetacycles.events.mat is located.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, the output will be an EpochArray, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[DataFrame, EpochArray]</code> <p>DataFrame with the following fields: - start: start time of theta cycle - stop: end time of theta cycle - duration: theta cycle duration - center: center time of theta cycle - trough: trough time of theta cycle - theta_channel: the theta channel used for detection</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_theta_cycles(\n    basepath: str, return_epoch_array: bool = False\n) -&gt; Union[pd.DataFrame, nel.EpochArray]:\n    \"\"\"\n    Load theta cycles calculated from auto_theta_cycles.m.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to your session where thetacycles.events.mat is located.\n    return_epoch_array : bool, optional\n        If True, the output will be an EpochArray, by default False.\n\n    Returns\n    -------\n    Union[pd.DataFrame, nel.EpochArray]\n        DataFrame with the following fields:\n        - start: start time of theta cycle\n        - stop: end time of theta cycle\n        - duration: theta cycle duration\n        - center: center time of theta cycle\n        - trough: trough time of theta cycle\n        - theta_channel: the theta channel used for detection\n    \"\"\"\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".thetacycles.events.mat\"\n    )\n    if not os.path.exists(filename):\n        warnings.warn(\"file does not exist\")\n        if return_epoch_array:\n            return nel.EpochArray()\n        return pd.DataFrame()\n\n    data = sio.loadmat(filename, simplify_cells=True)\n    df = pd.DataFrame()\n    df[\"start\"] = data[\"thetacycles\"][\"timestamps\"][:, 0]\n    df[\"stop\"] = data[\"thetacycles\"][\"timestamps\"][:, 1]\n    df[\"duration\"] = data[\"thetacycles\"][\"duration\"]\n    df[\"center\"] = data[\"thetacycles\"][\"center\"]\n    df[\"trough\"] = data[\"thetacycles\"][\"peaks\"]\n    df[\"theta_channel\"] = data[\"thetacycles\"][\"detectorinfo\"][\"theta_channel\"]\n\n    if return_epoch_array:\n        return nel.EpochArray([np.array([df.start, df.stop]).T], label=\"theta_cycles\")\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_theta_rem_shift","title":"<code>load_theta_rem_shift(basepath)</code>","text":"<p>Load theta REM shift data from get_rem_shift.m.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to your session where theta_rem_shift.mat is located.</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, dict]</code> <p>DataFrame with the following fields: - UID: unique identifier for each unit - circ_dist: circular distance - rem_shift: REM shift - non_rem_shift: non-REM shift - m_rem: mean phase locking value during REM - r_rem: resultant vector length during REM - k_rem: concentration parameter during REM - p_rem: p-value of phase locking during REM - mode_rem: mode of phase locking during REM - m_wake: mean phase locking value during wake - r_wake: resultant vector length during wake - k_wake: concentration parameter during wake - p_wake: p-value of phase locking during wake - mode_wake: mode of phase locking during wake</p> <code>dict</code> <p>Dictionary with phase distributions and spike phases for REM and wake states.</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_theta_rem_shift(basepath: str) -&gt; Tuple[pd.DataFrame, dict]:\n    \"\"\"\n    Load theta REM shift data from get_rem_shift.m.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to your session where theta_rem_shift.mat is located.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, dict]\n        DataFrame with the following fields:\n        - UID: unique identifier for each unit\n        - circ_dist: circular distance\n        - rem_shift: REM shift\n        - non_rem_shift: non-REM shift\n        - m_rem: mean phase locking value during REM\n        - r_rem: resultant vector length during REM\n        - k_rem: concentration parameter during REM\n        - p_rem: p-value of phase locking during REM\n        - mode_rem: mode of phase locking during REM\n        - m_wake: mean phase locking value during wake\n        - r_wake: resultant vector length during wake\n        - k_wake: concentration parameter during wake\n        - p_wake: p-value of phase locking during wake\n        - mode_wake: mode of phase locking during wake\n\n    dict\n        Dictionary with phase distributions and spike phases for REM and wake states.\n    \"\"\"\n    try:\n        filename = glob.glob(basepath + os.sep + \"*theta_rem_shift.mat\")[0]\n    except Exception:\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame(), np.nan\n\n    data = sio.loadmat(filename)\n\n    df = pd.DataFrame()\n\n    df[\"UID\"] = data[\"rem_shift_data\"][\"UID\"][0][0][0]\n    df[\"circ_dist\"] = data[\"rem_shift_data\"][\"circ_dist\"][0][0][0]\n    df[\"rem_shift\"] = data[\"rem_shift_data\"][\"rem_shift\"][0][0][0]\n    df[\"non_rem_shift\"] = data[\"rem_shift_data\"][\"non_rem_shift\"][0][0][0]\n\n    # rem metrics\n    df[\"m_rem\"] = data[\"rem_shift_data\"][\"PhaseLockingData_rem\"][0][0][\"phasestats\"][0][\n        0\n    ][\"m\"][0][0][0]\n    df[\"r_rem\"] = data[\"rem_shift_data\"][\"PhaseLockingData_rem\"][0][0][\"phasestats\"][0][\n        0\n    ][\"r\"][0][0][0]\n    df[\"k_rem\"] = data[\"rem_shift_data\"][\"PhaseLockingData_rem\"][0][0][\"phasestats\"][0][\n        0\n    ][\"k\"][0][0][0]\n    df[\"p_rem\"] = data[\"rem_shift_data\"][\"PhaseLockingData_rem\"][0][0][\"phasestats\"][0][\n        0\n    ][\"p\"][0][0][0]\n    df[\"mode_rem\"] = data[\"rem_shift_data\"][\"PhaseLockingData_rem\"][0][0][\"phasestats\"][\n        0\n    ][0][\"mode\"][0][0][0]\n\n    # wake metrics\n    df[\"m_wake\"] = data[\"rem_shift_data\"][\"PhaseLockingData_wake\"][0][0][\"phasestats\"][\n        0\n    ][0][\"m\"][0][0][0]\n    df[\"r_wake\"] = data[\"rem_shift_data\"][\"PhaseLockingData_wake\"][0][0][\"phasestats\"][\n        0\n    ][0][\"r\"][0][0][0]\n    df[\"k_wake\"] = data[\"rem_shift_data\"][\"PhaseLockingData_wake\"][0][0][\"phasestats\"][\n        0\n    ][0][\"k\"][0][0][0]\n    df[\"p_wake\"] = data[\"rem_shift_data\"][\"PhaseLockingData_wake\"][0][0][\"phasestats\"][\n        0\n    ][0][\"p\"][0][0][0]\n    df[\"mode_wake\"] = data[\"rem_shift_data\"][\"PhaseLockingData_wake\"][0][0][\n        \"phasestats\"\n    ][0][0][\"mode\"][0][0][0]\n\n    def get_distros(data, state):\n        return np.vstack(data[\"rem_shift_data\"][state][0][0][\"phasedistros\"][0][0].T)\n\n    def get_spikephases(data, state):\n        return data[\"rem_shift_data\"][state][0][0][\"spkphases\"][0][0][0]\n\n    # add to dictionary\n    data_dict = {\n        \"rem\": {\n            \"phasedistros\": get_distros(data, \"PhaseLockingData_rem\"),\n            \"spkphases\": get_spikephases(data, \"PhaseLockingData_rem\"),\n        },\n        \"wake\": {\n            \"phasedistros\": get_distros(data, \"PhaseLockingData_wake\"),\n            \"spkphases\": get_spikephases(data, \"PhaseLockingData_wake\"),\n        },\n    }\n\n    return df, data_dict\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.load_trials","title":"<code>load_trials(basepath)</code>","text":"<p>Loads trials from cell explorer basename.animal.behavior and stores in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session folder.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the following fields: - startTime: start time of the trial, in seconds - stopTime: stop time of the trial, in seconds - trialsID: ID of the trial</p> References <p>https://cellexplorer.org/datastructure/data-structure-and-format/#behavior</p> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def load_trials(basepath: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Loads trials from cell explorer basename.animal.behavior and stores in a DataFrame.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session folder.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with the following fields:\n        - startTime: start time of the trial, in seconds\n        - stopTime: stop time of the trial, in seconds\n        - trialsID: ID of the trial\n\n    References\n    ----------\n    https://cellexplorer.org/datastructure/data-structure-and-format/#behavior\n    \"\"\"\n\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".animal.behavior.mat\"\n    )\n\n    if not os.path.exists(filename):\n        warnings.warn(\"file does not exist\")\n        return pd.DataFrame()\n\n    # load file\n    data = sio.loadmat(filename, simplify_cells=True)\n    if \"trials\" not in data[\"behavior\"].keys():\n        warnings.warn(\"trials not found in file\")\n        return pd.DataFrame()\n\n    # current standard is\n    #   behavior.trials.*name of trial*.start\n    #   behavior.trials.*name of trial*.stop\n    if (\n        isinstance(data[\"behavior\"][\"trials\"], dict)\n        and \"starts\" in data[\"behavior\"][\"trials\"].keys()\n        and \"stops\" in data[\"behavior\"][\"trials\"].keys()\n    ):\n        df = pd.DataFrame(\n            data=np.array(\n                [\n                    data[\"behavior\"][\"trials\"][\"starts\"],\n                    data[\"behavior\"][\"trials\"][\"stops\"],\n                ]\n            ).T\n        )\n        df.columns = [\"startTime\", \"stopTime\"]\n        df[\"trialsID\"] = data[\"behavior\"][\"trials\"][\"stateName\"]\n\n    # old standard\n    #   behavior.trials.*[starts,stops]*\n    else:\n        # check if trials is empty\n        if len(data[\"behavior\"][\"trials\"]) == 0:\n            warnings.warn(\"trials is empty\")\n            return pd.DataFrame()\n        try:\n            df = pd.DataFrame(data=data[\"behavior\"][\"trials\"])\n            df.columns = [\"startTime\", \"stopTime\"]\n            # check if trialsID exists\n            if \"trialsID\" in data[\"behavior\"].keys():\n                df[\"trialsID\"] = data[\"behavior\"][\"trialsID\"]\n        except Exception:\n            df = pd.DataFrame(data=[data[\"behavior\"][\"trials\"]])\n            df.columns = [\"startTime\", \"stopTime\"]\n            # check if trialsID exists\n            if \"trialsID\" in data[\"behavior\"].keys():\n                if type(data[\"behavior\"][\"trialsID\"]) is str:\n                    df[\"trialsID\"] = data[\"behavior\"][\"trialsID\"]\n\n                elif len(data[\"behavior\"][\"trialsID\"]) == df.shape[0]:\n                    df[\"trialsID\"] = data[\"behavior\"][\"trialsID\"]\n                else:\n                    warnings.warn(\"trials or trialsID not correct shape\")\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/io/loading/#neuro_py.io.loading.writeNeuroscopeEvents","title":"<code>writeNeuroscopeEvents(path, ep, name)</code>","text":"<p>Write events to a Neuroscope-compatible file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the output file.</p> required <code>ep</code> <code>Any</code> <p>Epoch data containing start and end times.</p> required <code>name</code> <code>str</code> <p>Name of the event.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/io/loading.py</code> <pre><code>def writeNeuroscopeEvents(path: str, ep: Any, name: str) -&gt; None:\n    \"\"\"\n    Write events to a Neuroscope-compatible file.\n\n    Parameters\n    ----------\n    path : str\n        Path to the output file.\n    ep : Any\n        Epoch data containing start and end times.\n    name : str\n        Name of the event.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    f = open(path, \"w\")\n    for i in range(len(ep)):\n        f.writelines(\n            str(ep.as_units(\"ms\").iloc[i][\"start\"])\n            + \" \"\n            + name\n            + \" start \"\n            + str(1)\n            + \"\\n\"\n        )\n        # f.writelines(str(ep.as_units('ms').iloc[i]['peak']) + \" \"+name+\" start \"+ str(1)+\"\\n\")\n        f.writelines(\n            str(ep.as_units(\"ms\").iloc[i][\"end\"]) + \" \" + name + \" end \" + str(1) + \"\\n\"\n        )\n    f.close()\n</code></pre>"},{"location":"reference/neuro_py/io/saving/","title":"neuro_py.io.saving","text":""},{"location":"reference/neuro_py/io/saving/#neuro_py.io.saving.epoch_to_mat","title":"<code>epoch_to_mat(epoch, basepath, epoch_name, detection_name=None)</code>","text":"<p>Save an EpochArray to a .mat file in the Cell Explorer format.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>EpochArray to save.</p> required <code>basepath</code> <code>str</code> <p>Basepath to save the file to.</p> required <code>epoch_name</code> <code>str</code> <p>Name of the epoch.</p> required <code>detection_name</code> <code>Union[None, str]</code> <p>Name of the detection, by default None.</p> <code>None</code> Source code in <code>neuro_py/io/saving.py</code> <pre><code>def epoch_to_mat(\n    epoch: nel.EpochArray,\n    basepath: str,\n    epoch_name: str,\n    detection_name: Union[None, str] = None,\n) -&gt; None:\n    \"\"\"\n    Save an EpochArray to a .mat file in the Cell Explorer format.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray\n        EpochArray to save.\n    basepath : str\n        Basepath to save the file to.\n    epoch_name : str\n        Name of the epoch.\n    detection_name : Union[None, str], optional\n        Name of the detection, by default None.\n    \"\"\"\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".\" + epoch_name + \".events.mat\"\n    )\n    data = {}\n    data[epoch_name] = {}\n\n    data[epoch_name][\"timestamps\"] = epoch.data\n\n    # check if only single epoch\n    if epoch.data.ndim == 1:\n        data[epoch_name][\"peaks\"] = np.median(epoch.data, axis=0)\n    else:\n        data[epoch_name][\"peaks\"] = np.median(epoch.data, axis=1)\n\n    data[epoch_name][\"amplitudes\"] = []\n    data[epoch_name][\"amplitudeUnits\"] = []\n    data[epoch_name][\"eventID\"] = []\n    data[epoch_name][\"eventIDlabels\"] = []\n    data[epoch_name][\"eventIDbinary\"] = []\n\n    # check if only single epoch\n    if epoch.data.ndim == 1:\n        data[epoch_name][\"duration\"] = epoch.data[1] - epoch.data[0]\n    else:\n        data[epoch_name][\"duration\"] = epoch.durations\n\n    data[epoch_name][\"center\"] = data[epoch_name][\"peaks\"]\n    data[epoch_name][\"detectorinfo\"] = {}\n    if detection_name is None:\n        data[epoch_name][\"detectorinfo\"][\"detectorname\"] = []\n    else:\n        data[epoch_name][\"detectorinfo\"][\"detectorname\"] = detection_name\n    data[epoch_name][\"detectorinfo\"][\"detectionparms\"] = []\n    data[epoch_name][\"detectorinfo\"][\"detectionintervals\"] = []\n\n    savemat(filename, data, long_field_names=True)\n</code></pre>"},{"location":"reference/neuro_py/lfp/","title":"neuro_py.lfp","text":""},{"location":"reference/neuro_py/lfp/#neuro_py.lfp.clean_lfp","title":"<code>clean_lfp(lfp, t=None, thresholds=(5, 10), artifact_time_expand=(0.25, 0.1), return_bad_intervals=False)</code>","text":"<p>Remove artefacts and noise from a local field potential (LFP) signal.</p> <p>Parameters:</p> Name Type Description Default <code>lfp</code> <code>AnalogSignalArray</code> <p>The LFP signal to be cleaned. Single signal only.</p> required <code>thresholds</code> <code>tuple of float</code> <p>A tuple of two thresholds for detecting artefacts and noise. The first threshold is used to detect large global artefacts by finding values in the z-scored LFP signal that deviate by more than the threshold number of sigmas from the mean. The second threshold is used to detect noise by finding values in the derivative of the z-scored LFP signal that are greater than the threshold. Default is (5, 10).</p> <code>(5, 10)</code> <code>artifact_time_expand</code> <code>tuple of float</code> <p>A tuple of two time intervals around detected artefacts and noise. The first interval is used to expand the detected large global artefacts. The second interval is used to expand the detected noise. Default is (0.25, 0.1).</p> <code>(0.25, 0.1)</code> <code>return_bad_intervals</code> <code>bool</code> <p>If True, also returns intervals of artefacts and noise as an <code>nel.EpochArray</code>. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, EpochArray]]</code> <p>The cleaned LFP signal. If <code>return_bad_intervals</code> is True, also returns an <code>nel.EpochArray</code> representing the intervals of artefacts and noise.</p> Notes <p>Based on https://github.com/ayalab1/neurocode/blob/master/lfp/CleanLFP.m by Ralitsa Todorova</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lfp = nel.AnalogSignalArray(data=np.random.randn(1250), timestamps=np.arange(1250)/1250)\n&gt;&gt;&gt; clean_lfp(lfp)\narray([-1.73104885,  1.08192036,  1.40332741, ..., -2.78671212,\n    -1.63661574, -1.10868426])\n</code></pre> Source code in <code>neuro_py/lfp/preprocessing.py</code> <pre><code>def clean_lfp(\n    lfp: Union[nel.AnalogSignalArray, np.ndarray],\n    t: np.ndarray = None,\n    thresholds: Tuple[float, float] = (5, 10),\n    artifact_time_expand: Tuple[float, float] = (0.25, 0.1),\n    return_bad_intervals: bool = False,\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, nel.EpochArray]]:\n    \"\"\"\n    Remove artefacts and noise from a local field potential (LFP) signal.\n\n    Parameters\n    ----------\n    lfp : nel.AnalogSignalArray\n        The LFP signal to be cleaned. Single signal only.\n    thresholds : tuple of float, optional\n        A tuple of two thresholds for detecting artefacts and noise. The first threshold is used to detect large global\n        artefacts by finding values in the z-scored LFP signal that deviate by more than the threshold number of sigmas\n        from the mean. The second threshold is used to detect noise by finding values in the derivative of the z-scored\n        LFP signal that are greater than the threshold. Default is (5, 10).\n    artifact_time_expand : tuple of float, optional\n        A tuple of two time intervals around detected artefacts and noise. The first interval is used to expand the detected\n        large global artefacts. The second interval is used to expand the detected noise. Default is (0.25, 0.1).\n    return_bad_intervals : bool, optional\n        If True, also returns intervals of artefacts and noise as an `nel.EpochArray`. Default is False.\n\n    Returns\n    -------\n    Union[np.ndarray, Tuple[np.ndarray, nel.EpochArray]]\n        The cleaned LFP signal. If `return_bad_intervals` is True, also returns an `nel.EpochArray`\n        representing the intervals of artefacts and noise.\n\n    Notes\n    -----\n    Based on https://github.com/ayalab1/neurocode/blob/master/lfp/CleanLFP.m by Ralitsa Todorova\n\n    Examples\n    --------\n    &gt;&gt;&gt; lfp = nel.AnalogSignalArray(data=np.random.randn(1250), timestamps=np.arange(1250)/1250)\n    &gt;&gt;&gt; clean_lfp(lfp)\n    array([-1.73104885,  1.08192036,  1.40332741, ..., -2.78671212,\n        -1.63661574, -1.10868426])\n    \"\"\"\n    threshold1 = thresholds[0]  # in sigmas deviating from the mean\n    aroundArtefact1 = artifact_time_expand[\n        0\n    ]  # interval to expand large global artefacts\n\n    threshold2 = thresholds[1]  # for derivative of z-scored signal\n    aroundArtefact2 = artifact_time_expand[1]  # interval to expand detected noise\n\n    if isinstance(lfp, nel.AnalogSignalArray):\n        t = lfp.time  # time points of LFP signal\n        values = lfp.copy().data.flatten()  # values of LFP signal\n        z = lfp.zscore().data.flatten()  # z-scored values of LFP signal\n    elif isinstance(lfp, np.ndarray):\n        if t is None:\n            raise ValueError(\"t must be provided when lfp is np.ndarray\")\n        values = lfp.flatten()\n        z = (values - np.mean(values)) / np.std(values)\n    else:\n        raise ValueError(\"lfp must be nel.AnalogSignalArray or np.ndarray\")\n\n    d = np.append(np.diff(z), 0)  # derivative of z-scored LFP signal\n\n    # Detect large global artefacts [0]\n    artefactInterval = t[\n        np.array(intervals.find_interval(np.abs(z) &gt; threshold1), dtype=int)\n    ]\n    artefactInterval = nel.EpochArray(artefactInterval)\n    if not artefactInterval.isempty:\n        artefactInterval = artefactInterval.expand(aroundArtefact1)\n\n    # Find noise using the derivative of the z-scored signal [1]\n    noisyInterval = t[\n        np.array(intervals.find_interval(np.abs(d) &gt; threshold2), dtype=int)\n    ]\n    noisyInterval = nel.EpochArray(noisyInterval)\n    if not noisyInterval.isempty:\n        noisyInterval = noisyInterval.expand(aroundArtefact2)\n\n    # Combine intervals for artefacts and noise\n    bad = (artefactInterval | noisyInterval).merge()\n\n    if bad.isempty:\n        return values\n\n    # Find timestamps within intervals for artefacts and noise\n    in_interval = intervals.in_intervals(t, bad.data)\n\n    # Interpolate values for timestamps within intervals for artefacts and noise\n    values[in_interval] = np.interp(\n        t[in_interval], t[~in_interval], values[~in_interval]\n    )\n\n    return (values, bad) if return_bad_intervals else values\n</code></pre>"},{"location":"reference/neuro_py/lfp/#neuro_py.lfp.event_triggered_wavelet","title":"<code>event_triggered_wavelet(signal, timestamps, events, max_lag=1, freq_min=4, freq_max=100, freq_step=4, return_pandas=False, parallel=True, whiten=True, whiten_order=2, fs=None, **kwargs)</code>","text":"<p>Compute the event-triggered wavelet transform of a signal.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>1d array</code> <p>Time series.</p> required <code>timestamps</code> <code>1d array</code> <p>Time points for each sample in the signal.</p> required <code>events</code> <code>1d array</code> <p>Time points of events.</p> required <code>max_lag</code> <code>float</code> <p>Maximum lag to consider, in seconds.</p> <code>1</code> <code>freq_min</code> <code>float</code> <p>Minimum frequency to consider, in Hz.</p> <code>4</code> <code>freq_max</code> <code>float</code> <p>Maximum frequency to consider, in Hz.</p> <code>100</code> <code>freq_step</code> <code>float</code> <p>Step size for frequency range, in Hz.</p> <code>4</code> <code>return_pandas</code> <code>bool</code> <p>If True, return the output as pandas objects.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>If True, use parallel processing to compute the wavelet transform.</p> <code>True</code> <code>whiten</code> <code>bool</code> <p>If True, whiten the signal before computing the wavelet transform.</p> <code>True</code> <code>whiten_order</code> <code>int</code> <p>Order of the autoregressive model used for whitening.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling rate, in Hz.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to <code>compute_wavelet_transform</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>mwt</code> <code>2d array</code> <p>Time frequency representation of the input signal.</p> <code>sigs</code> <code>1d array</code> <p>Average signal.</p> <code>times</code> <code>1d array</code> <p>Time points for each sample in the output.</p> <code>freqs</code> <code>1d array</code> <p>Frequencies used in the wavelet transform.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.lfp.spectral import event_triggered_wavelet\n</code></pre> <pre><code>&gt;&gt;&gt; basepath = r\"Z:\\Data\\hpc_ctx_project\\HP04\\day_34_20240503\"\n</code></pre> <pre><code>&gt;&gt;&gt; # load lfp\n&gt;&gt;&gt; nChannels, fs, _, _ = loading.loadXML(basepath)\n&gt;&gt;&gt; # Load the LFP data\n&gt;&gt;&gt; lfp, ts = loading.loadLFP(basepath, n_channels=nChannels,\n&gt;&gt;&gt;                channel=23,\n&gt;&gt;&gt;                frequency=fs)\n&gt;&gt;&gt; # load events\n&gt;&gt;&gt; opto = loading.load_events(basepath, epoch_name=\"optoStim\")\n&gt;&gt;&gt; opto = opto.merge(gap=.1)\n</code></pre> <pre><code>&gt;&gt;&gt; # compute event triggered averate\n&gt;&gt;&gt; mwt, sigs, times, freqs = event_triggered_wavelet(\n&gt;&gt;&gt;    lfp,\n&gt;&gt;&gt;    ts,\n&gt;&gt;&gt;    opto.starts,\n&gt;&gt;&gt; )\n</code></pre> <pre><code>&gt;&gt;&gt; # plot\n&gt;&gt;&gt; plt.figure(figsize=set_size(\"thesis\", fraction=1, subplots=(1, 1)))\n</code></pre> <pre><code>&gt;&gt;&gt; im = plt.imshow(\n&gt;&gt;&gt;     abs(mwt),\n&gt;&gt;&gt;     aspect=\"auto\",\n&gt;&gt;&gt;     extent=[times[0], times[-1], freqs[-1], freqs[0]],\n&gt;&gt;&gt;     cmap=\"magma\",\n&gt;&gt;&gt;     vmax=600,\n&gt;&gt;&gt;     vmin=50,\n&gt;&gt;&gt; )\n&gt;&gt;&gt; plt.axhline(23, color=\"orange\", linestyle=\"--\", label=\"23hz\")\n</code></pre> <pre><code>&gt;&gt;&gt; plt.yscale(\"log\")\n&gt;&gt;&gt; # move legend outside of plot\n&gt;&gt;&gt; plt.legend(loc=\"upper right\", bbox_to_anchor=(1.1, 1.1), frameon=False)\n</code></pre> <pre><code>&gt;&gt;&gt; plt.gca().invert_yaxis()\n</code></pre> <pre><code>&gt;&gt;&gt; plt.colorbar(location=\"top\", label=\"Power (uV^2)\")\n&gt;&gt;&gt; # move colorbar more to the left\n&gt;&gt;&gt; plt.gcf().axes[1].set_position([0.5, 0.8, 0.4, 0.6])\n</code></pre> <pre><code>&gt;&gt;&gt; plt.gca().set_ylabel(\"Frequency (Hz)\")\n</code></pre> <pre><code>&gt;&gt;&gt; plt.gca().set_xlabel(\"Time from opto stim (s)\")\n</code></pre> <pre><code>&gt;&gt;&gt; plt.twinx()\n&gt;&gt;&gt; plt.yscale(\"linear\")\n&gt;&gt;&gt; plt.axvline(0, color=\"k\", linestyle=\"--\")\n&gt;&gt;&gt; plt.axvline(0.5, color=\"k\", linestyle=\"--\")\n&gt;&gt;&gt; plt.plot(times, sigs, \"w\", linewidth=0.5)\n</code></pre> <pre><code>&gt;&gt;&gt; # plt.gca().set_xlabel('Time (s)')\n&gt;&gt;&gt; plt.gca().set_ylabel(\"Voltage (uV)\")\n&gt;&gt;&gt; plt.gca().set_title(\"PFC during 23Hz stim in behavior\", y=1)\n</code></pre> Source code in <code>neuro_py/lfp/spectral.py</code> <pre><code>def event_triggered_wavelet(\n    signal: np.ndarray,\n    timestamps: np.ndarray,\n    events: np.ndarray,\n    max_lag: float = 1,\n    freq_min: float = 4,\n    freq_max: float = 100,\n    freq_step: float = 4,\n    return_pandas: bool = False,\n    parallel: bool = True,\n    whiten: bool = True,\n    whiten_order: int = 2,\n    fs: Optional[float] = None,\n    **kwargs,\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n    Tuple[pd.DataFrame, pd.Series],\n]:\n    \"\"\"\n    Compute the event-triggered wavelet transform of a signal.\n\n    Parameters\n    ----------\n    signal : 1d array\n        Time series.\n    timestamps : 1d array\n        Time points for each sample in the signal.\n    events : 1d array\n        Time points of events.\n    max_lag : float\n        Maximum lag to consider, in seconds.\n    freq_min : float\n        Minimum frequency to consider, in Hz.\n    freq_max : float\n        Maximum frequency to consider, in Hz.\n    freq_step : float\n        Step size for frequency range, in Hz.\n    return_pandas : bool\n        If True, return the output as pandas objects.\n    parallel : bool\n        If True, use parallel processing to compute the wavelet transform.\n    whiten : bool\n        If True, whiten the signal before computing the wavelet transform.\n    whiten_order : int\n        Order of the autoregressive model used for whitening.\n    fs : float\n        Sampling rate, in Hz.\n    kwargs\n        Additional keyword arguments to pass to `compute_wavelet_transform`.\n\n    Returns\n    -------\n    mwt : 2d array\n        Time frequency representation of the input signal.\n    sigs : 1d array\n        Average signal.\n    times : 1d array\n        Time points for each sample in the output.\n    freqs : 1d array\n        Frequencies used in the wavelet transform.\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.lfp.spectral import event_triggered_wavelet\n\n    &gt;&gt;&gt; basepath = r\"Z:\\\\Data\\\\hpc_ctx_project\\\\HP04\\\\day_34_20240503\"\n\n    &gt;&gt;&gt; # load lfp\n    &gt;&gt;&gt; nChannels, fs, _, _ = loading.loadXML(basepath)\n    &gt;&gt;&gt; # Load the LFP data\n    &gt;&gt;&gt; lfp, ts = loading.loadLFP(basepath, n_channels=nChannels,\n    &gt;&gt;&gt;                channel=23,\n    &gt;&gt;&gt;                frequency=fs)\n    &gt;&gt;&gt; # load events\n    &gt;&gt;&gt; opto = loading.load_events(basepath, epoch_name=\"optoStim\")\n    &gt;&gt;&gt; opto = opto.merge(gap=.1)\n\n    &gt;&gt;&gt; # compute event triggered averate\n    &gt;&gt;&gt; mwt, sigs, times, freqs = event_triggered_wavelet(\n    &gt;&gt;&gt;    lfp,\n    &gt;&gt;&gt;    ts,\n    &gt;&gt;&gt;    opto.starts,\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; # plot\n    &gt;&gt;&gt; plt.figure(figsize=set_size(\"thesis\", fraction=1, subplots=(1, 1)))\n\n    &gt;&gt;&gt; im = plt.imshow(\n    &gt;&gt;&gt;     abs(mwt),\n    &gt;&gt;&gt;     aspect=\"auto\",\n    &gt;&gt;&gt;     extent=[times[0], times[-1], freqs[-1], freqs[0]],\n    &gt;&gt;&gt;     cmap=\"magma\",\n    &gt;&gt;&gt;     vmax=600,\n    &gt;&gt;&gt;     vmin=50,\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; plt.axhline(23, color=\"orange\", linestyle=\"--\", label=\"23hz\")\n\n    &gt;&gt;&gt; plt.yscale(\"log\")\n    &gt;&gt;&gt; # move legend outside of plot\n    &gt;&gt;&gt; plt.legend(loc=\"upper right\", bbox_to_anchor=(1.1, 1.1), frameon=False)\n\n    &gt;&gt;&gt; plt.gca().invert_yaxis()\n\n    &gt;&gt;&gt; plt.colorbar(location=\"top\", label=\"Power (uV^2)\")\n    &gt;&gt;&gt; # move colorbar more to the left\n    &gt;&gt;&gt; plt.gcf().axes[1].set_position([0.5, 0.8, 0.4, 0.6])\n\n\n    &gt;&gt;&gt; plt.gca().set_ylabel(\"Frequency (Hz)\")\n\n    &gt;&gt;&gt; plt.gca().set_xlabel(\"Time from opto stim (s)\")\n\n    &gt;&gt;&gt; plt.twinx()\n    &gt;&gt;&gt; plt.yscale(\"linear\")\n    &gt;&gt;&gt; plt.axvline(0, color=\"k\", linestyle=\"--\")\n    &gt;&gt;&gt; plt.axvline(0.5, color=\"k\", linestyle=\"--\")\n    &gt;&gt;&gt; plt.plot(times, sigs, \"w\", linewidth=0.5)\n\n\n    &gt;&gt;&gt; # plt.gca().set_xlabel('Time (s)')\n    &gt;&gt;&gt; plt.gca().set_ylabel(\"Voltage (uV)\")\n    &gt;&gt;&gt; plt.gca().set_title(\"PFC during 23Hz stim in behavior\", y=1)\n    \"\"\"\n\n    signal_ = signal.copy()\n    if whiten:\n        signal_ = whiten_lfp(signal, order=whiten_order)\n\n    # set up frequency range\n    freqs = np.arange(freq_min, freq_max, freq_step)\n    # set up time range\n    if fs is None:\n        ds = timestamps[1] - timestamps[0]\n        fs = 1 / ds\n    # Create times array based on the sample rate (fs)\n    times = np.arange(-max_lag, max_lag, 1 / fs)\n    # Number of samples corresponding to the time window around each event\n    n_samples = int(max_lag * 2 * fs)\n\n    # Ensure the length of times matches n_samples\n    if len(times) != n_samples:\n        times = np.linspace(-max_lag, max_lag, n_samples)\n\n    n_freqs = len(freqs)\n    n_samples = len(times)\n\n    # set up mwt and sigs to store results\n    mwt = np.zeros((n_freqs, n_samples))\n    sigs = np.zeros(n_samples)\n\n    event_i = 0\n\n    def process_event(start):\n        nonlocal event_i\n        nonlocal mwt\n        nonlocal sigs\n\n        if start + max_lag &gt; timestamps.max() or start - max_lag &lt; timestamps.min():\n            return None, None\n\n        idx = (timestamps &gt;= start - max_lag) &amp; (timestamps &lt;= start + max_lag)\n\n        mwt_partial = np.abs(\n            compute_wavelet_transform(sig=signal_[idx], fs=fs, freqs=freqs, **kwargs)\n        )\n\n        return mwt_partial, signal[idx]\n\n    if parallel:\n        with ThreadPoolExecutor() as executor:\n            results = list(executor.map(process_event, events))\n\n        for mwt_partial, sig_partial in results:\n            if mwt_partial is not None:\n                # samples might be missing if the event is too close to the edge\n                if mwt_partial.shape[1] != n_samples:\n                    continue\n                mwt += mwt_partial\n                sigs += sig_partial\n                event_i += 1\n    else:\n        for start in events:\n            mwt_partial, sig_partial = process_event(start)\n            if mwt_partial is not None:\n                mwt += mwt_partial\n                sigs += sig_partial\n                event_i += 1\n\n    mwt /= event_i\n    sigs /= event_i\n\n    if return_pandas:\n        mwt = pd.DataFrame(mwt.T, index=times, columns=freqs)\n        sigs = pd.Series(sigs, index=times)\n        return mwt, sigs\n    else:\n        return mwt, sigs, times, freqs\n</code></pre>"},{"location":"reference/neuro_py/lfp/#neuro_py.lfp.filter_signal","title":"<code>filter_signal(sig, fs, pass_type, f_range, filter_type='fir', n_cycles=3, n_seconds=None, butterworth_order=4, remove_edges=True)</code>","text":"<p>Filter a neural signal using an FIR or IIR filter.</p> <p>Parameters:</p> Name Type Description Default <code>sig</code> <code>ndarray</code> <p>Time series to be filtered. N signals x M samples array.</p> required <code>fs</code> <code>float</code> <p>Sampling rate, in Hz.</p> required <code>pass_type</code> <code>(bandpass, bandstop, lowpass, highpass)</code> <p>Type of filter to apply.</p> <code>'bandpass'</code> <code>f_range</code> <code>float or tuple of float</code> <p>Frequency range for filtering. For 'lowpass' and 'highpass', a single float can be provided. For 'bandpass' and 'bandstop', a tuple specifying (f_low, f_high) is required.</p> required <code>filter_type</code> <code>(fir, iir)</code> <p>Type of filter to apply: 'fir' for FIR or 'iir' for IIR (Butterworth). Default is 'fir'.</p> <code>'fir'</code> <code>n_cycles</code> <code>int</code> <p>Number of cycles to define the kernel length for FIR filters. Default is 3.</p> <code>3</code> <code>n_seconds</code> <code>float</code> <p>Length of the FIR filter in seconds. Overrides <code>n_cycles</code> if specified. Ignored for IIR.</p> <code>None</code> <code>butterworth_order</code> <code>int</code> <p>Order of the Butterworth filter. Only applies to IIR filters. Default is 4.</p> <code>4</code> <code>remove_edges</code> <code>bool</code> <p>If True, replace samples within half the kernel length with NaN (FIR filters only). Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series.</p> <p>Examples:</p> <p>Apply a lowpass FIR filter to a signal:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from your_module import filter_signal\n&gt;&gt;&gt; fs = 1000  # Sampling rate (Hz)\n&gt;&gt;&gt; t = np.linspace(0, 1, fs, endpoint=False)\n&gt;&gt;&gt; sig = np.sin(2 * np.pi * 1 * t) + 0.5 * np.sin(2 * np.pi * 50 * t)  # Signal with 1Hz and 50Hz components\n&gt;&gt;&gt; pass_type = 'lowpass'\n&gt;&gt;&gt; f_range = 10  # Lowpass filter at 10 Hz\n&gt;&gt;&gt; filt_sig = filter_signal(sig, fs, pass_type, f_range, filter_type='fir')\n&gt;&gt;&gt; plt.plot(t, sig, label='Original Signal')\n&gt;&gt;&gt; plt.plot(t, filt_sig, label='Filtered Signal')\n&gt;&gt;&gt; plt.legend()\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>neuro_py/lfp/spectral.py</code> <pre><code>def filter_signal(\n    sig: np.ndarray,\n    fs: float,\n    pass_type: str,\n    f_range: Union[float, Tuple[float, float]],\n    filter_type: str = \"fir\",\n    n_cycles: int = 3,\n    n_seconds: Optional[float] = None,\n    butterworth_order: int = 4,\n    remove_edges: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"\n    Filter a neural signal using an FIR or IIR filter.\n\n    Parameters\n    ----------\n    sig : np.ndarray\n        Time series to be filtered. N signals x M samples array.\n    fs : float\n        Sampling rate, in Hz.\n    pass_type : {'bandpass', 'bandstop', 'lowpass', 'highpass'}\n        Type of filter to apply.\n    f_range : float or tuple of float\n        Frequency range for filtering. For 'lowpass' and 'highpass', a single float can be provided.\n        For 'bandpass' and 'bandstop', a tuple specifying (f_low, f_high) is required.\n    filter_type : {'fir', 'iir'}, optional\n        Type of filter to apply: 'fir' for FIR or 'iir' for IIR (Butterworth). Default is 'fir'.\n    n_cycles : int, optional\n        Number of cycles to define the kernel length for FIR filters. Default is 3.\n    n_seconds : float, optional\n        Length of the FIR filter in seconds. Overrides `n_cycles` if specified. Ignored for IIR.\n    butterworth_order : int, optional\n        Order of the Butterworth filter. Only applies to IIR filters. Default is 4.\n    remove_edges : bool, optional\n        If True, replace samples within half the kernel length with NaN (FIR filters only). Default is True.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered time series.\n\n    Examples\n    --------\n    Apply a lowpass FIR filter to a signal:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from your_module import filter_signal\n    &gt;&gt;&gt; fs = 1000  # Sampling rate (Hz)\n    &gt;&gt;&gt; t = np.linspace(0, 1, fs, endpoint=False)\n    &gt;&gt;&gt; sig = np.sin(2 * np.pi * 1 * t) + 0.5 * np.sin(2 * np.pi * 50 * t)  # Signal with 1Hz and 50Hz components\n    &gt;&gt;&gt; pass_type = 'lowpass'\n    &gt;&gt;&gt; f_range = 10  # Lowpass filter at 10 Hz\n    &gt;&gt;&gt; filt_sig = filter_signal(sig, fs, pass_type, f_range, filter_type='fir')\n    &gt;&gt;&gt; plt.plot(t, sig, label='Original Signal')\n    &gt;&gt;&gt; plt.plot(t, filt_sig, label='Filtered Signal')\n    &gt;&gt;&gt; plt.legend()\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n\n    # Validate pass_type\n    if pass_type not in [\"bandpass\", \"bandstop\", \"lowpass\", \"highpass\"]:\n        raise ValueError(\n            \"`pass_type` must be one of: 'bandpass', 'bandstop', 'lowpass', 'highpass'.\"\n        )\n\n    # Ensure `f_range` is properly defined for the filter type\n    if isinstance(f_range, (int, float)):\n        if pass_type == \"lowpass\":\n            f_range = (None, f_range)  # Convert single value to tuple for lowpass\n        elif pass_type == \"highpass\":\n            f_range = (f_range, None)  # Convert single value to tuple for highpass\n        else:\n            raise ValueError(\n                \"`f_range` must be a tuple for 'bandpass' or 'bandstop' filters.\"\n            )\n\n    # Validate bandpass/bandstop filters\n    if pass_type in [\"bandpass\", \"bandstop\"]:\n        if not isinstance(f_range, tuple) or f_range[0] is None or f_range[1] is None:\n            raise ValueError(\n                \"Both frequencies must be specified for 'bandpass' and 'bandstop' filters.\"\n            )\n\n    # Nyquist frequency\n    nyquist = fs / 2\n\n    # FIR filter implementation\n    if filter_type == \"fir\":\n        # Compute filter kernel length\n        if n_seconds is not None:\n            kernel_len = int(n_seconds * fs)\n        else:\n            kernel_len = (\n                int((n_cycles / f_range[0]) * fs)\n                if f_range[0]\n                else int((n_cycles / f_range[1]) * fs)\n            )\n        if kernel_len % 2 == 0:\n            kernel_len += 1  # Ensure kernel length is odd\n\n        # Define FIR filter coefficients\n        if pass_type in [\"bandpass\", \"bandstop\"]:\n            fir_coefs = firwin(\n                kernel_len,\n                [f_range[0] / nyquist, f_range[1] / nyquist],\n                pass_zero=(pass_type == \"bandstop\"),\n            )\n        elif pass_type == \"lowpass\":\n            fir_coefs = firwin(kernel_len, f_range[1] / nyquist, pass_zero=True)\n        elif pass_type == \"highpass\":\n            fir_coefs = firwin(kernel_len, f_range[0] / nyquist, pass_zero=False)\n\n        # Apply the FIR filter\n        if len(sig.shape) == 1:\n            sig_filt = np.convolve(sig, fir_coefs, mode=\"same\")\n        else:\n            sig_filt = np.vstack([np.convolve(sig_, fir_coefs, mode=\"same\") for sig_ in sig])\n\n    # IIR filter implementation\n    elif filter_type == \"iir\":\n        # Design a Butterworth filter\n        if pass_type in [\"bandpass\", \"bandstop\"]:\n            b, a = butter(\n                butterworth_order,\n                [f_range[0] / nyquist, f_range[1] / nyquist],\n                btype=pass_type,\n            )\n        elif pass_type == \"lowpass\":\n            b, a = butter(butterworth_order, f_range[1] / nyquist, btype=\"low\")\n        elif pass_type == \"highpass\":\n            b, a = butter(butterworth_order, f_range[0] / nyquist, btype=\"high\")\n\n        # Apply the IIR filter\n        sig_filt = filtfilt(b, a, sig)\n\n    else:\n        raise ValueError(\"`filter_type` must be 'fir' or 'iir'.\")\n\n    # Optionally remove edges\n    if remove_edges and filter_type == \"fir\":\n        edge_len = kernel_len // 2\n        sig_filt[:edge_len] = np.nan\n        sig_filt[-edge_len:] = np.nan\n\n    return sig_filt\n</code></pre>"},{"location":"reference/neuro_py/lfp/#neuro_py.lfp.get_coords","title":"<code>get_coords(basepath, shank=0)</code>","text":"<p>Get the coordinates of the channels from the probe layout.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the basepath.</p> required <code>shank</code> <code>int</code> <p>Shank to get the coordinates from, by default 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinates of the channels.</p> Source code in <code>neuro_py/lfp/CSD.py</code> <pre><code>def get_coords(basepath: str, shank: int = 0) -&gt; np.ndarray:\n    \"\"\"\n    Get the coordinates of the channels from the probe layout.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the basepath.\n    shank : int, optional\n        Shank to get the coordinates from, by default 0.\n\n    Returns\n    -------\n    np.ndarray\n        Coordinates of the channels.\n    \"\"\"\n    import quantities as pq\n\n    # load the probe layout\n    probe_layout = loading.load_probe_layout(basepath)\n\n    # get the coordinates of the channels\n    coords = probe_layout.loc[shank == probe_layout.shank, \"y\"].values\n\n    # rescale the coordinates so none are negative and in mm\n    rescaled_coords = (coords - coords.min()) * pq.mm\n\n    # add dimension to coords to make it (nchannels,1)\n    rescaled_coords = rescaled_coords[:, np.newaxis]\n\n    return rescaled_coords\n</code></pre>"},{"location":"reference/neuro_py/lfp/#neuro_py.lfp.get_csd","title":"<code>get_csd(basepath, data, shank, fs=1250, diam=0.015, method='DeltaiCSD', channel_offset=0.046)</code>","text":"<p>compute the CSD for a given basepath and data using elephant estimate_csd.</p> <p>Klas H. Pettersen, Anna Devor, Istvan Ulbert, Anders M. Dale, Gaute T. Einevoll, Current-source density estimation based on inversion of electrostatic forward solution: Effects of finite extent of neuronal activity and conductivity discontinuities, Journal of Neuroscience Methods, Volume 154, Issues 1-2, 30 June 2006, Pages 116-133, ISSN 0165-0270, http://dx.doi.org/10.1016/j.jneumeth.2005.12.005.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>path to the basepath</p> required <code>data</code> <code>array</code> <p>data to compute the CSD on [channels x time]</p> required <code>fs</code> <code>int</code> <p>sampling rate of the data, by default 1250 Hz</p> <code>1250</code> <code>diam</code> <code>float</code> <p>diameter of the electrode, by default 0.015 mm</p> <code>0.015</code> <code>method</code> <code>str</code> <p>method to compute the CSD, by default 'DeltaiCSD'</p> <code>'DeltaiCSD'</code> <p>Returns:</p> Type Description <code>AnalogSignal</code> <p>CSD signal</p> Dependencies <p>get_coords, estimate_csd (Elephant), neo, quantities</p> Source code in <code>neuro_py/lfp/CSD.py</code> <pre><code>def get_csd(\n    basepath, data, shank, fs=1250, diam=0.015, method=\"DeltaiCSD\", channel_offset=0.046\n):\n    \"\"\"\n    compute the CSD for a given basepath and data using elephant estimate_csd.\n\n    Klas H. Pettersen, Anna Devor, Istvan Ulbert, Anders M. Dale, Gaute T. Einevoll,\n    Current-source density estimation based on inversion of electrostatic forward\n    solution: Effects of finite extent of neuronal activity and conductivity\n    discontinuities, Journal of Neuroscience Methods, Volume 154, Issues 1-2,\n    30 June 2006, Pages 116-133, ISSN 0165-0270,\n    http://dx.doi.org/10.1016/j.jneumeth.2005.12.005.\n\n    Parameters\n    ----------\n    basepath : str\n        path to the basepath\n    data : np.array\n        data to compute the CSD on [channels x time]\n    fs : int, optional\n        sampling rate of the data, by default 1250 Hz\n    diam : float, optional\n        diameter of the electrode, by default 0.015 mm\n    method : str, optional\n        method to compute the CSD, by default 'DeltaiCSD'\n\n    Returns\n    -------\n    neo.AnalogSignal\n        CSD signal\n\n    Dependencies\n    ------------\n    get_coords, estimate_csd (Elephant), neo, quantities\n\n    \"\"\"\n    import quantities as pq\n    from elephant.current_source_density import estimate_csd\n    from neo import AnalogSignal\n\n\n    coords = get_coords(basepath, shank=shank)\n\n    signal = AnalogSignal(\n        data,\n        units=\"mV\",\n        t_start=0 * pq.s,\n        sampling_rate=fs * pq.Hz,\n        dtype=float,\n    )\n\n    if method == \"DeltaiCSD\":\n        csd = estimate_csd(signal, coordinates=coords, diam=diam * pq.mm, method=method)\n\n    elif method == \"StandardCSD\":\n        # create coordinates for the CSD\n        coords = np.zeros(data.shape[1])\n        for idx, i in enumerate(coords):\n            if idx == 0:\n                coords[idx] = 0\n            else:\n                coords[idx] = coords[idx - 1] + channel_offset\n\n        coords = coords * pq.mm\n\n        # add dimension to coords to make it (64,1)\n        coords = coords[:, np.newaxis]\n\n        csd = estimate_csd(signal, coordinates=coords, method=method)\n\n    elif method == \"KD1CSD\":\n        # create coordinates for the CSD\n        coords = np.zeros(data.shape[1])\n        for idx, i in enumerate(coords):\n            if idx == 0:\n                coords[idx] = 0\n            else:\n                coords[idx] = coords[idx - 1] + channel_offset\n\n        coords = coords * pq.mm\n\n        # add dimension to coords to make it (64,1)\n        coords = coords[:, np.newaxis]\n        csd = estimate_csd(signal, coordinates=coords, method=method)\n\n    return csd\n</code></pre>"},{"location":"reference/neuro_py/lfp/#neuro_py.lfp.get_theta_channel","title":"<code>get_theta_channel(basepath, tag='CA1so')</code>","text":"<p>Get the theta channel for the specified brain region.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path for loading data.</p> required <code>tag</code> <code>str</code> <p>The tag identifying the brain region. Default is \"CA1so\".</p> <code>'CA1so'</code> <p>Returns:</p> Type Description <code>int or None</code> <p>The index of the theta channel (0-based), or None if not found.</p> Source code in <code>neuro_py/lfp/theta_cycles.py</code> <pre><code>def get_theta_channel(basepath: str, tag: str = \"CA1so\") -&gt; Optional[int]:\n    \"\"\"\n    Get the theta channel for the specified brain region.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path for loading data.\n    tag : str, optional\n        The tag identifying the brain region. Default is \"CA1so\".\n\n    Returns\n    -------\n    int or None\n        The index of the theta channel (0-based), or None if not found.\n    \"\"\"\n    brain_region = loading.load_brain_regions(basepath)\n\n    channel_tags = loading.load_channel_tags(basepath)\n\n    if tag in brain_region.keys():\n        theta_chan = brain_region[tag][\"channels\"]\n    else:\n        return None\n\n    bad_ch = channel_tags[\"Bad\"][\"channels\"]\n    for ch in theta_chan:\n        if np.any(ch != bad_ch):\n            theta_chan = ch\n            break\n    return ch - 1  # return in base 0\n</code></pre>"},{"location":"reference/neuro_py/lfp/#neuro_py.lfp.get_theta_cycles","title":"<code>get_theta_cycles(basepath, theta_freq=(6, 10), lowpass=48, detection_params=None, ch=None)</code>","text":"<p>Detect theta cycles in LFP data and save the results.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path for loading LFP data.</p> required <code>theta_freq</code> <code>tuple</code> <p>Frequency range for theta detection (default is (6, 10)).</p> <code>(6, 10)</code> <code>lowpass</code> <code>int</code> <p>Cut-off frequency for low-pass filtering (default is 48).</p> <code>48</code> <code>detection_params</code> <code>dict or None</code> <p>Parameters for theta detection (default is None).</p> <code>None</code> <code>ch</code> <code>int or None</code> <p>Channel used for theta detection (default is None).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/lfp/theta_cycles.py</code> <pre><code>def get_theta_cycles(\n    basepath: str,\n    theta_freq: Tuple[int, int] = (6, 10),\n    lowpass: int = 48,\n    detection_params: Optional[dict] = None,\n    ch: Optional[int] = None,\n) -&gt; Optional[None]:\n    \"\"\"\n    Detect theta cycles in LFP data and save the results.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path for loading LFP data.\n    theta_freq : tuple, optional\n        Frequency range for theta detection (default is (6, 10)).\n    lowpass : int, optional\n        Cut-off frequency for low-pass filtering (default is 48).\n    detection_params : dict or None, optional\n        Parameters for theta detection (default is None).\n    ch : int or None, optional\n        Channel used for theta detection (default is None).\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # import bycycle, hidden import to avoid mandatory dependency\n    from bycycle import Bycycle\n\n    # load lfp as memmap\n    lfp, ts, fs = process_lfp(basepath)\n\n    # get theta channel - default chooses CA1so\n    if ch is None:\n        ch = get_theta_channel(basepath, tag=\"CA1so\")\n\n    if ch is None:\n        ch = get_theta_channel(basepath, tag=\"CA1sp\")\n\n    if ch is None:\n        Warning(\"No theta channel found\")\n        return None\n\n    # per bycycle documentation, low-pass filter signal before running bycycle 4x the frequency of interest\n    filt_sig = filter_signal(lfp[:, ch], fs, \"lowpass\", lowpass, remove_edges=False)\n\n    # for detecting theta epochs\n    if detection_params is None:\n        thresholds = {\n            \"amp_fraction\": 0.1,\n            \"amp_consistency\": 0.4,\n            \"period_consistency\": 0.5,\n            \"monotonicity\": 0.6,\n            \"min_n_cycles\": 3,\n        }\n    else:\n        thresholds = detection_params\n\n    # initialize bycycle object\n    bm = Bycycle(thresholds=thresholds)\n    bm.fit(filt_sig, fs, theta_freq)\n\n    save_theta_cycles(bm.df_features, ts, basepath, detection_params=thresholds, ch=ch)\n</code></pre>"},{"location":"reference/neuro_py/lfp/#neuro_py.lfp.process_lfp","title":"<code>process_lfp(basepath)</code>","text":"<p>Process and load Local Field Potential (LFP) data.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path for loading LFP data.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the LFP data, timestamps, and sampling frequency.</p> Source code in <code>neuro_py/lfp/theta_cycles.py</code> <pre><code>def process_lfp(basepath: str) -&gt; Tuple[np.ndarray, np.ndarray, float]:\n    \"\"\"\n    Process and load Local Field Potential (LFP) data.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path for loading LFP data.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the LFP data, timestamps, and sampling frequency.\n    \"\"\"\n    nChannels, fs, _, _ = loading.loadXML(basepath)\n\n    lfp, ts = loading.loadLFP(\n        basepath, n_channels=nChannels, channel=None, frequency=fs\n    )\n    return lfp, ts, fs\n</code></pre>"},{"location":"reference/neuro_py/lfp/#neuro_py.lfp.save_theta_cycles","title":"<code>save_theta_cycles(df, ts, basepath, detection_params, ch, event_name='thetacycles', detection_name=None)</code>","text":"<p>Save theta cycles detected using bycycle to a .mat file in the cell explorer format.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The bycycle dataframe containing theta cycle features.</p> required <code>ts</code> <code>ndarray</code> <p>Timestamps of the LFP data.</p> required <code>basepath</code> <code>str</code> <p>Base path to save the file to.</p> required <code>detection_params</code> <code>dict</code> <p>Dictionary of detection parameters.</p> required <code>ch</code> <code>int</code> <p>Channel used for theta detection.</p> required <code>event_name</code> <code>str</code> <p>Name of the events (default is \"thetacycles\").</p> <code>'thetacycles'</code> <code>detection_name</code> <code>str or None</code> <p>Name of the detection (default is None).</p> <code>None</code> Source code in <code>neuro_py/lfp/theta_cycles.py</code> <pre><code>def save_theta_cycles(\n    df: pd.DataFrame,\n    ts: np.ndarray,\n    basepath: str,\n    detection_params: dict,\n    ch: int,\n    event_name: str = \"thetacycles\",\n    detection_name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Save theta cycles detected using bycycle to a .mat file in the cell explorer format.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The bycycle dataframe containing theta cycle features.\n    ts : np.ndarray\n        Timestamps of the LFP data.\n    basepath : str\n        Base path to save the file to.\n    detection_params : dict\n        Dictionary of detection parameters.\n    ch : int\n        Channel used for theta detection.\n    event_name : str, optional\n        Name of the events (default is \"thetacycles\").\n    detection_name : str or None, optional\n        Name of the detection (default is None).\n    \"\"\"\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".\" + event_name + \".events.mat\"\n    )\n    data = {}\n    data[event_name] = {}\n\n    # create variables that will be saved\n    timestamps = np.array(\n        [ts[df.sample_peak.values[:-1]], ts[df.sample_peak.values[1:]]]\n    )\n    peaks = ts[df.sample_last_trough.values[1:]]\n    amplitudes = df.band_amp.values[1:]\n    duration = np.diff(\n        np.array([ts[df.sample_peak.values[:-1]], ts[df.sample_peak.values[1:]]]),\n        axis=0,\n    )\n    center = np.median(\n        np.array([ts[df.sample_peak.values[:-1]], ts[df.sample_peak.values[1:]]]),\n        axis=0,\n    )\n\n    # limit to cycles using is_burst\n    timestamps = timestamps[:, df.is_burst.values[1:]]\n    peaks = peaks[df.is_burst.values[1:]]\n    amplitudes = amplitudes[df.is_burst.values[1:]]\n    duration = duration[:, df.is_burst.values[1:]]\n    center = center[df.is_burst.values[1:]]\n\n    # save start_ts and stop_ts as 2d array\n    data[event_name][\"timestamps\"] = timestamps.T\n    data[event_name][\"peaks\"] = peaks.T\n    data[event_name][\"amplitudes\"] = amplitudes.T\n    data[event_name][\"amplitudeUnits\"] = \"mV\"\n    data[event_name][\"eventID\"] = []\n    data[event_name][\"eventIDlabels\"] = []\n    data[event_name][\"eventIDbinary\"] = []\n\n    # check if only single epoch\n    data[event_name][\"duration\"] = duration.T\n\n    data[event_name][\"center\"] = center.T\n    data[event_name][\"detectorinfo\"] = {}\n    if detection_name is None:\n        data[event_name][\"detectorinfo\"][\"detectorname\"] = []\n    else:\n        data[event_name][\"detectorinfo\"][\"detectorname\"] = detection_name\n    data[event_name][\"detectorinfo\"][\"detectionparms\"] = detection_params\n    data[event_name][\"detectorinfo\"][\"detectionintervals\"] = []\n    data[event_name][\"detectorinfo\"][\"theta_channel\"] = ch\n\n    savemat(filename, data, long_field_names=True)\n</code></pre>"},{"location":"reference/neuro_py/lfp/#neuro_py.lfp.whiten_lfp","title":"<code>whiten_lfp(lfp, order=2)</code>","text":"<p>Perform temporal whitening of Local Field Potential (LFP) data using an Autoregressive (AR) model.</p> <p>This function applies temporal whitening to LFP data by fitting an AR model of the specified order and using the model to remove temporal correlations, resulting in a 'whitened' signal.</p> <p>Parameters:</p> Name Type Description Default <code>lfp</code> <code>ndarray</code> <p>A 1D numpy array containing the LFP data.</p> required <code>order</code> <code>(int, optional(default=2))</code> <p>The order of the AR model to be used for whitening the LFP data.</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The temporally whitened LFP data as a 1D numpy array.</p> Source code in <code>neuro_py/lfp/spectral.py</code> <pre><code>def whiten_lfp(lfp: np.ndarray, order: int = 2) -&gt; np.ndarray:\n    \"\"\"\n    Perform temporal whitening of Local Field Potential (LFP) data using an Autoregressive (AR) model.\n\n    This function applies temporal whitening to LFP data by fitting an AR model of the specified order\n    and using the model to remove temporal correlations, resulting in a 'whitened' signal.\n\n    Parameters\n    ----------\n    lfp : ndarray\n        A 1D numpy array containing the LFP data.\n    order : int, optional (default=2)\n        The order of the AR model to be used for whitening the LFP data.\n\n    Returns\n    -------\n    ndarray\n        The temporally whitened LFP data as a 1D numpy array.\n    \"\"\"\n\n    rho, _ = yule_walker(lfp, order=order)\n\n    a = np.concatenate(([1.0], -rho))\n\n    # Apply the whitening filter to the LFP data and return the result as a 1D array\n    return signal.convolve(lfp, a, \"same\")\n</code></pre>"},{"location":"reference/neuro_py/lfp/CSD/","title":"neuro_py.lfp.CSD","text":""},{"location":"reference/neuro_py/lfp/CSD/#neuro_py.lfp.CSD.get_coords","title":"<code>get_coords(basepath, shank=0)</code>","text":"<p>Get the coordinates of the channels from the probe layout.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the basepath.</p> required <code>shank</code> <code>int</code> <p>Shank to get the coordinates from, by default 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinates of the channels.</p> Source code in <code>neuro_py/lfp/CSD.py</code> <pre><code>def get_coords(basepath: str, shank: int = 0) -&gt; np.ndarray:\n    \"\"\"\n    Get the coordinates of the channels from the probe layout.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the basepath.\n    shank : int, optional\n        Shank to get the coordinates from, by default 0.\n\n    Returns\n    -------\n    np.ndarray\n        Coordinates of the channels.\n    \"\"\"\n    import quantities as pq\n\n    # load the probe layout\n    probe_layout = loading.load_probe_layout(basepath)\n\n    # get the coordinates of the channels\n    coords = probe_layout.loc[shank == probe_layout.shank, \"y\"].values\n\n    # rescale the coordinates so none are negative and in mm\n    rescaled_coords = (coords - coords.min()) * pq.mm\n\n    # add dimension to coords to make it (nchannels,1)\n    rescaled_coords = rescaled_coords[:, np.newaxis]\n\n    return rescaled_coords\n</code></pre>"},{"location":"reference/neuro_py/lfp/CSD/#neuro_py.lfp.CSD.get_csd","title":"<code>get_csd(basepath, data, shank, fs=1250, diam=0.015, method='DeltaiCSD', channel_offset=0.046)</code>","text":"<p>compute the CSD for a given basepath and data using elephant estimate_csd.</p> <p>Klas H. Pettersen, Anna Devor, Istvan Ulbert, Anders M. Dale, Gaute T. Einevoll, Current-source density estimation based on inversion of electrostatic forward solution: Effects of finite extent of neuronal activity and conductivity discontinuities, Journal of Neuroscience Methods, Volume 154, Issues 1-2, 30 June 2006, Pages 116-133, ISSN 0165-0270, http://dx.doi.org/10.1016/j.jneumeth.2005.12.005.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>path to the basepath</p> required <code>data</code> <code>array</code> <p>data to compute the CSD on [channels x time]</p> required <code>fs</code> <code>int</code> <p>sampling rate of the data, by default 1250 Hz</p> <code>1250</code> <code>diam</code> <code>float</code> <p>diameter of the electrode, by default 0.015 mm</p> <code>0.015</code> <code>method</code> <code>str</code> <p>method to compute the CSD, by default 'DeltaiCSD'</p> <code>'DeltaiCSD'</code> <p>Returns:</p> Type Description <code>AnalogSignal</code> <p>CSD signal</p> Dependencies <p>get_coords, estimate_csd (Elephant), neo, quantities</p> Source code in <code>neuro_py/lfp/CSD.py</code> <pre><code>def get_csd(\n    basepath, data, shank, fs=1250, diam=0.015, method=\"DeltaiCSD\", channel_offset=0.046\n):\n    \"\"\"\n    compute the CSD for a given basepath and data using elephant estimate_csd.\n\n    Klas H. Pettersen, Anna Devor, Istvan Ulbert, Anders M. Dale, Gaute T. Einevoll,\n    Current-source density estimation based on inversion of electrostatic forward\n    solution: Effects of finite extent of neuronal activity and conductivity\n    discontinuities, Journal of Neuroscience Methods, Volume 154, Issues 1-2,\n    30 June 2006, Pages 116-133, ISSN 0165-0270,\n    http://dx.doi.org/10.1016/j.jneumeth.2005.12.005.\n\n    Parameters\n    ----------\n    basepath : str\n        path to the basepath\n    data : np.array\n        data to compute the CSD on [channels x time]\n    fs : int, optional\n        sampling rate of the data, by default 1250 Hz\n    diam : float, optional\n        diameter of the electrode, by default 0.015 mm\n    method : str, optional\n        method to compute the CSD, by default 'DeltaiCSD'\n\n    Returns\n    -------\n    neo.AnalogSignal\n        CSD signal\n\n    Dependencies\n    ------------\n    get_coords, estimate_csd (Elephant), neo, quantities\n\n    \"\"\"\n    import quantities as pq\n    from elephant.current_source_density import estimate_csd\n    from neo import AnalogSignal\n\n\n    coords = get_coords(basepath, shank=shank)\n\n    signal = AnalogSignal(\n        data,\n        units=\"mV\",\n        t_start=0 * pq.s,\n        sampling_rate=fs * pq.Hz,\n        dtype=float,\n    )\n\n    if method == \"DeltaiCSD\":\n        csd = estimate_csd(signal, coordinates=coords, diam=diam * pq.mm, method=method)\n\n    elif method == \"StandardCSD\":\n        # create coordinates for the CSD\n        coords = np.zeros(data.shape[1])\n        for idx, i in enumerate(coords):\n            if idx == 0:\n                coords[idx] = 0\n            else:\n                coords[idx] = coords[idx - 1] + channel_offset\n\n        coords = coords * pq.mm\n\n        # add dimension to coords to make it (64,1)\n        coords = coords[:, np.newaxis]\n\n        csd = estimate_csd(signal, coordinates=coords, method=method)\n\n    elif method == \"KD1CSD\":\n        # create coordinates for the CSD\n        coords = np.zeros(data.shape[1])\n        for idx, i in enumerate(coords):\n            if idx == 0:\n                coords[idx] = 0\n            else:\n                coords[idx] = coords[idx - 1] + channel_offset\n\n        coords = coords * pq.mm\n\n        # add dimension to coords to make it (64,1)\n        coords = coords[:, np.newaxis]\n        csd = estimate_csd(signal, coordinates=coords, method=method)\n\n    return csd\n</code></pre>"},{"location":"reference/neuro_py/lfp/preprocessing/","title":"neuro_py.lfp.preprocessing","text":""},{"location":"reference/neuro_py/lfp/preprocessing/#neuro_py.lfp.preprocessing.clean_lfp","title":"<code>clean_lfp(lfp, t=None, thresholds=(5, 10), artifact_time_expand=(0.25, 0.1), return_bad_intervals=False)</code>","text":"<p>Remove artefacts and noise from a local field potential (LFP) signal.</p> <p>Parameters:</p> Name Type Description Default <code>lfp</code> <code>AnalogSignalArray</code> <p>The LFP signal to be cleaned. Single signal only.</p> required <code>thresholds</code> <code>tuple of float</code> <p>A tuple of two thresholds for detecting artefacts and noise. The first threshold is used to detect large global artefacts by finding values in the z-scored LFP signal that deviate by more than the threshold number of sigmas from the mean. The second threshold is used to detect noise by finding values in the derivative of the z-scored LFP signal that are greater than the threshold. Default is (5, 10).</p> <code>(5, 10)</code> <code>artifact_time_expand</code> <code>tuple of float</code> <p>A tuple of two time intervals around detected artefacts and noise. The first interval is used to expand the detected large global artefacts. The second interval is used to expand the detected noise. Default is (0.25, 0.1).</p> <code>(0.25, 0.1)</code> <code>return_bad_intervals</code> <code>bool</code> <p>If True, also returns intervals of artefacts and noise as an <code>nel.EpochArray</code>. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, EpochArray]]</code> <p>The cleaned LFP signal. If <code>return_bad_intervals</code> is True, also returns an <code>nel.EpochArray</code> representing the intervals of artefacts and noise.</p> Notes <p>Based on https://github.com/ayalab1/neurocode/blob/master/lfp/CleanLFP.m by Ralitsa Todorova</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lfp = nel.AnalogSignalArray(data=np.random.randn(1250), timestamps=np.arange(1250)/1250)\n&gt;&gt;&gt; clean_lfp(lfp)\narray([-1.73104885,  1.08192036,  1.40332741, ..., -2.78671212,\n    -1.63661574, -1.10868426])\n</code></pre> Source code in <code>neuro_py/lfp/preprocessing.py</code> <pre><code>def clean_lfp(\n    lfp: Union[nel.AnalogSignalArray, np.ndarray],\n    t: np.ndarray = None,\n    thresholds: Tuple[float, float] = (5, 10),\n    artifact_time_expand: Tuple[float, float] = (0.25, 0.1),\n    return_bad_intervals: bool = False,\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, nel.EpochArray]]:\n    \"\"\"\n    Remove artefacts and noise from a local field potential (LFP) signal.\n\n    Parameters\n    ----------\n    lfp : nel.AnalogSignalArray\n        The LFP signal to be cleaned. Single signal only.\n    thresholds : tuple of float, optional\n        A tuple of two thresholds for detecting artefacts and noise. The first threshold is used to detect large global\n        artefacts by finding values in the z-scored LFP signal that deviate by more than the threshold number of sigmas\n        from the mean. The second threshold is used to detect noise by finding values in the derivative of the z-scored\n        LFP signal that are greater than the threshold. Default is (5, 10).\n    artifact_time_expand : tuple of float, optional\n        A tuple of two time intervals around detected artefacts and noise. The first interval is used to expand the detected\n        large global artefacts. The second interval is used to expand the detected noise. Default is (0.25, 0.1).\n    return_bad_intervals : bool, optional\n        If True, also returns intervals of artefacts and noise as an `nel.EpochArray`. Default is False.\n\n    Returns\n    -------\n    Union[np.ndarray, Tuple[np.ndarray, nel.EpochArray]]\n        The cleaned LFP signal. If `return_bad_intervals` is True, also returns an `nel.EpochArray`\n        representing the intervals of artefacts and noise.\n\n    Notes\n    -----\n    Based on https://github.com/ayalab1/neurocode/blob/master/lfp/CleanLFP.m by Ralitsa Todorova\n\n    Examples\n    --------\n    &gt;&gt;&gt; lfp = nel.AnalogSignalArray(data=np.random.randn(1250), timestamps=np.arange(1250)/1250)\n    &gt;&gt;&gt; clean_lfp(lfp)\n    array([-1.73104885,  1.08192036,  1.40332741, ..., -2.78671212,\n        -1.63661574, -1.10868426])\n    \"\"\"\n    threshold1 = thresholds[0]  # in sigmas deviating from the mean\n    aroundArtefact1 = artifact_time_expand[\n        0\n    ]  # interval to expand large global artefacts\n\n    threshold2 = thresholds[1]  # for derivative of z-scored signal\n    aroundArtefact2 = artifact_time_expand[1]  # interval to expand detected noise\n\n    if isinstance(lfp, nel.AnalogSignalArray):\n        t = lfp.time  # time points of LFP signal\n        values = lfp.copy().data.flatten()  # values of LFP signal\n        z = lfp.zscore().data.flatten()  # z-scored values of LFP signal\n    elif isinstance(lfp, np.ndarray):\n        if t is None:\n            raise ValueError(\"t must be provided when lfp is np.ndarray\")\n        values = lfp.flatten()\n        z = (values - np.mean(values)) / np.std(values)\n    else:\n        raise ValueError(\"lfp must be nel.AnalogSignalArray or np.ndarray\")\n\n    d = np.append(np.diff(z), 0)  # derivative of z-scored LFP signal\n\n    # Detect large global artefacts [0]\n    artefactInterval = t[\n        np.array(intervals.find_interval(np.abs(z) &gt; threshold1), dtype=int)\n    ]\n    artefactInterval = nel.EpochArray(artefactInterval)\n    if not artefactInterval.isempty:\n        artefactInterval = artefactInterval.expand(aroundArtefact1)\n\n    # Find noise using the derivative of the z-scored signal [1]\n    noisyInterval = t[\n        np.array(intervals.find_interval(np.abs(d) &gt; threshold2), dtype=int)\n    ]\n    noisyInterval = nel.EpochArray(noisyInterval)\n    if not noisyInterval.isempty:\n        noisyInterval = noisyInterval.expand(aroundArtefact2)\n\n    # Combine intervals for artefacts and noise\n    bad = (artefactInterval | noisyInterval).merge()\n\n    if bad.isempty:\n        return values\n\n    # Find timestamps within intervals for artefacts and noise\n    in_interval = intervals.in_intervals(t, bad.data)\n\n    # Interpolate values for timestamps within intervals for artefacts and noise\n    values[in_interval] = np.interp(\n        t[in_interval], t[~in_interval], values[~in_interval]\n    )\n\n    return (values, bad) if return_bad_intervals else values\n</code></pre>"},{"location":"reference/neuro_py/lfp/spectral/","title":"neuro_py.lfp.spectral","text":""},{"location":"reference/neuro_py/lfp/spectral/#neuro_py.lfp.spectral.compute_wavelet_transform","title":"<code>compute_wavelet_transform(sig, fs, freqs, wavelet='cmor', center_frequency=0.5, bandwidth_frequency=1.5, method='conv')</code>","text":"<p>Compute the time-frequency representation of a signal using Morlet wavelets via PyWavelets.</p> <p>Parameters:</p> Name Type Description Default <code>sig</code> <code>ndarray</code> <p>Time series data (1D array).</p> required <code>fs</code> <code>float</code> <p>Sampling rate, in Hz.</p> required <code>freqs</code> <code>Union[ndarray, List[float], Tuple[float, float, Optional[float]]]</code> <p>Frequencies to analyze with Morlet wavelets. - If an array or list, specifies exact frequency values. - If a tuple, defines a frequency range as <code>(freq_start, freq_stop[, freq_step])</code>.     The <code>freq_step</code> is optional and defaults to 1. Range is inclusive of <code>freq_stop</code>.</p> required <code>wavelet</code> <code>str</code> <p>The name of the wavelet to use for the CWT. Default is 'cmor'. - wavelist = pywt.wavelist(kind='continuous') to get a list of available wavelets.</p> <code>'cmor'</code> <code>center_frequency</code> <code>float</code> <p>The center frequency of the Morlet wavelet.</p> <code>0.5</code> <code>bandwidth_frequency</code> <code>float</code> <p>The bandwidth of the Morlet wavelet.</p> <code>1.5</code> <code>method</code> <code>(conv, fft)</code> <p>The method used to compute the CWT. Can be any of:     - <code>conv</code> uses <code>numpy.convolve</code>.     - <code>fft</code> uses frequency domain convolution.     - <code>auto</code> uses automatic selection based on an estimate of the       computational complexity at each scale.</p> <code>'conv'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The time-frequency representation of the input signal. Shape is <code>(n_freqs, n_time_points)</code>.</p> Notes <p>This function uses <code>pywt.cwt</code> with Morlet wavelets to compute the time-frequency representation.</p> Source code in <code>neuro_py/lfp/spectral.py</code> <pre><code>def compute_wavelet_transform(\n    sig: np.ndarray,\n    fs: float,\n    freqs: Union[np.ndarray, List[float], Tuple[float, float, Optional[float]]],\n    wavelet: str = \"cmor\",\n    center_frequency: float = 0.5,\n    bandwidth_frequency: float = 1.5,\n    method=\"conv\",\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute the time-frequency representation of a signal using Morlet wavelets via PyWavelets.\n\n    Parameters\n    ----------\n    sig : np.ndarray\n        Time series data (1D array).\n    fs : float\n        Sampling rate, in Hz.\n    freqs : Union[np.ndarray, List[float], Tuple[float, float, Optional[float]]]\n        Frequencies to analyze with Morlet wavelets.\n        - If an array or list, specifies exact frequency values.\n        - If a tuple, defines a frequency range as `(freq_start, freq_stop[, freq_step])`.\n            The `freq_step` is optional and defaults to 1. Range is inclusive of `freq_stop`.\n    wavelet : str, optional\n        The name of the wavelet to use for the CWT. Default is 'cmor'.\n        - wavelist = pywt.wavelist(kind='continuous') to get a list of available wavelets.\n    center_frequency : float, optional\n        The center frequency of the Morlet wavelet.\n    bandwidth_frequency : float, optional\n        The bandwidth of the Morlet wavelet.\n    method : {'conv', 'fft'}, optional\n        The method used to compute the CWT. Can be any of:\n            - ``conv`` uses ``numpy.convolve``.\n            - ``fft`` uses frequency domain convolution.\n            - ``auto`` uses automatic selection based on an estimate of the\n              computational complexity at each scale.\n    Returns\n    -------\n    np.ndarray\n        The time-frequency representation of the input signal. Shape is `(n_freqs, n_time_points)`.\n\n    Notes\n    -----\n    This function uses `pywt.cwt` with Morlet wavelets to compute the time-frequency representation.\n    \"\"\"\n\n    # Convert the frequency range to an array if it is given as a list or tuple\n    if isinstance(freqs, (tuple, list)):\n        freqs = (\n            np.arange(*freqs)\n            if len(freqs) == 3\n            else np.linspace(freqs[0], freqs[1], 100)\n        )\n\n    # Time step\n    dt = 1 / fs\n\n    # Define the wavelet name\n    wavelet_name = f\"{wavelet}{center_frequency}-{bandwidth_frequency}\"\n\n    # Convert frequencies to scales\n    scales = pywt.frequency2scale(wavelet_name, freqs / fs)\n\n    # Perform the Continuous Wavelet Transform\n    coefficients, _ = pywt.cwt(\n        sig, scales, wavelet_name, sampling_period=dt, method=method\n    )\n\n    return coefficients\n</code></pre>"},{"location":"reference/neuro_py/lfp/spectral/#neuro_py.lfp.spectral.event_triggered_wavelet","title":"<code>event_triggered_wavelet(signal, timestamps, events, max_lag=1, freq_min=4, freq_max=100, freq_step=4, return_pandas=False, parallel=True, whiten=True, whiten_order=2, fs=None, **kwargs)</code>","text":"<p>Compute the event-triggered wavelet transform of a signal.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>1d array</code> <p>Time series.</p> required <code>timestamps</code> <code>1d array</code> <p>Time points for each sample in the signal.</p> required <code>events</code> <code>1d array</code> <p>Time points of events.</p> required <code>max_lag</code> <code>float</code> <p>Maximum lag to consider, in seconds.</p> <code>1</code> <code>freq_min</code> <code>float</code> <p>Minimum frequency to consider, in Hz.</p> <code>4</code> <code>freq_max</code> <code>float</code> <p>Maximum frequency to consider, in Hz.</p> <code>100</code> <code>freq_step</code> <code>float</code> <p>Step size for frequency range, in Hz.</p> <code>4</code> <code>return_pandas</code> <code>bool</code> <p>If True, return the output as pandas objects.</p> <code>False</code> <code>parallel</code> <code>bool</code> <p>If True, use parallel processing to compute the wavelet transform.</p> <code>True</code> <code>whiten</code> <code>bool</code> <p>If True, whiten the signal before computing the wavelet transform.</p> <code>True</code> <code>whiten_order</code> <code>int</code> <p>Order of the autoregressive model used for whitening.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling rate, in Hz.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to <code>compute_wavelet_transform</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>mwt</code> <code>2d array</code> <p>Time frequency representation of the input signal.</p> <code>sigs</code> <code>1d array</code> <p>Average signal.</p> <code>times</code> <code>1d array</code> <p>Time points for each sample in the output.</p> <code>freqs</code> <code>1d array</code> <p>Frequencies used in the wavelet transform.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.lfp.spectral import event_triggered_wavelet\n</code></pre> <pre><code>&gt;&gt;&gt; basepath = r\"Z:\\Data\\hpc_ctx_project\\HP04\\day_34_20240503\"\n</code></pre> <pre><code>&gt;&gt;&gt; # load lfp\n&gt;&gt;&gt; nChannels, fs, _, _ = loading.loadXML(basepath)\n&gt;&gt;&gt; # Load the LFP data\n&gt;&gt;&gt; lfp, ts = loading.loadLFP(basepath, n_channels=nChannels,\n&gt;&gt;&gt;                channel=23,\n&gt;&gt;&gt;                frequency=fs)\n&gt;&gt;&gt; # load events\n&gt;&gt;&gt; opto = loading.load_events(basepath, epoch_name=\"optoStim\")\n&gt;&gt;&gt; opto = opto.merge(gap=.1)\n</code></pre> <pre><code>&gt;&gt;&gt; # compute event triggered averate\n&gt;&gt;&gt; mwt, sigs, times, freqs = event_triggered_wavelet(\n&gt;&gt;&gt;    lfp,\n&gt;&gt;&gt;    ts,\n&gt;&gt;&gt;    opto.starts,\n&gt;&gt;&gt; )\n</code></pre> <pre><code>&gt;&gt;&gt; # plot\n&gt;&gt;&gt; plt.figure(figsize=set_size(\"thesis\", fraction=1, subplots=(1, 1)))\n</code></pre> <pre><code>&gt;&gt;&gt; im = plt.imshow(\n&gt;&gt;&gt;     abs(mwt),\n&gt;&gt;&gt;     aspect=\"auto\",\n&gt;&gt;&gt;     extent=[times[0], times[-1], freqs[-1], freqs[0]],\n&gt;&gt;&gt;     cmap=\"magma\",\n&gt;&gt;&gt;     vmax=600,\n&gt;&gt;&gt;     vmin=50,\n&gt;&gt;&gt; )\n&gt;&gt;&gt; plt.axhline(23, color=\"orange\", linestyle=\"--\", label=\"23hz\")\n</code></pre> <pre><code>&gt;&gt;&gt; plt.yscale(\"log\")\n&gt;&gt;&gt; # move legend outside of plot\n&gt;&gt;&gt; plt.legend(loc=\"upper right\", bbox_to_anchor=(1.1, 1.1), frameon=False)\n</code></pre> <pre><code>&gt;&gt;&gt; plt.gca().invert_yaxis()\n</code></pre> <pre><code>&gt;&gt;&gt; plt.colorbar(location=\"top\", label=\"Power (uV^2)\")\n&gt;&gt;&gt; # move colorbar more to the left\n&gt;&gt;&gt; plt.gcf().axes[1].set_position([0.5, 0.8, 0.4, 0.6])\n</code></pre> <pre><code>&gt;&gt;&gt; plt.gca().set_ylabel(\"Frequency (Hz)\")\n</code></pre> <pre><code>&gt;&gt;&gt; plt.gca().set_xlabel(\"Time from opto stim (s)\")\n</code></pre> <pre><code>&gt;&gt;&gt; plt.twinx()\n&gt;&gt;&gt; plt.yscale(\"linear\")\n&gt;&gt;&gt; plt.axvline(0, color=\"k\", linestyle=\"--\")\n&gt;&gt;&gt; plt.axvline(0.5, color=\"k\", linestyle=\"--\")\n&gt;&gt;&gt; plt.plot(times, sigs, \"w\", linewidth=0.5)\n</code></pre> <pre><code>&gt;&gt;&gt; # plt.gca().set_xlabel('Time (s)')\n&gt;&gt;&gt; plt.gca().set_ylabel(\"Voltage (uV)\")\n&gt;&gt;&gt; plt.gca().set_title(\"PFC during 23Hz stim in behavior\", y=1)\n</code></pre> Source code in <code>neuro_py/lfp/spectral.py</code> <pre><code>def event_triggered_wavelet(\n    signal: np.ndarray,\n    timestamps: np.ndarray,\n    events: np.ndarray,\n    max_lag: float = 1,\n    freq_min: float = 4,\n    freq_max: float = 100,\n    freq_step: float = 4,\n    return_pandas: bool = False,\n    parallel: bool = True,\n    whiten: bool = True,\n    whiten_order: int = 2,\n    fs: Optional[float] = None,\n    **kwargs,\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n    Tuple[pd.DataFrame, pd.Series],\n]:\n    \"\"\"\n    Compute the event-triggered wavelet transform of a signal.\n\n    Parameters\n    ----------\n    signal : 1d array\n        Time series.\n    timestamps : 1d array\n        Time points for each sample in the signal.\n    events : 1d array\n        Time points of events.\n    max_lag : float\n        Maximum lag to consider, in seconds.\n    freq_min : float\n        Minimum frequency to consider, in Hz.\n    freq_max : float\n        Maximum frequency to consider, in Hz.\n    freq_step : float\n        Step size for frequency range, in Hz.\n    return_pandas : bool\n        If True, return the output as pandas objects.\n    parallel : bool\n        If True, use parallel processing to compute the wavelet transform.\n    whiten : bool\n        If True, whiten the signal before computing the wavelet transform.\n    whiten_order : int\n        Order of the autoregressive model used for whitening.\n    fs : float\n        Sampling rate, in Hz.\n    kwargs\n        Additional keyword arguments to pass to `compute_wavelet_transform`.\n\n    Returns\n    -------\n    mwt : 2d array\n        Time frequency representation of the input signal.\n    sigs : 1d array\n        Average signal.\n    times : 1d array\n        Time points for each sample in the output.\n    freqs : 1d array\n        Frequencies used in the wavelet transform.\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.lfp.spectral import event_triggered_wavelet\n\n    &gt;&gt;&gt; basepath = r\"Z:\\\\Data\\\\hpc_ctx_project\\\\HP04\\\\day_34_20240503\"\n\n    &gt;&gt;&gt; # load lfp\n    &gt;&gt;&gt; nChannels, fs, _, _ = loading.loadXML(basepath)\n    &gt;&gt;&gt; # Load the LFP data\n    &gt;&gt;&gt; lfp, ts = loading.loadLFP(basepath, n_channels=nChannels,\n    &gt;&gt;&gt;                channel=23,\n    &gt;&gt;&gt;                frequency=fs)\n    &gt;&gt;&gt; # load events\n    &gt;&gt;&gt; opto = loading.load_events(basepath, epoch_name=\"optoStim\")\n    &gt;&gt;&gt; opto = opto.merge(gap=.1)\n\n    &gt;&gt;&gt; # compute event triggered averate\n    &gt;&gt;&gt; mwt, sigs, times, freqs = event_triggered_wavelet(\n    &gt;&gt;&gt;    lfp,\n    &gt;&gt;&gt;    ts,\n    &gt;&gt;&gt;    opto.starts,\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; # plot\n    &gt;&gt;&gt; plt.figure(figsize=set_size(\"thesis\", fraction=1, subplots=(1, 1)))\n\n    &gt;&gt;&gt; im = plt.imshow(\n    &gt;&gt;&gt;     abs(mwt),\n    &gt;&gt;&gt;     aspect=\"auto\",\n    &gt;&gt;&gt;     extent=[times[0], times[-1], freqs[-1], freqs[0]],\n    &gt;&gt;&gt;     cmap=\"magma\",\n    &gt;&gt;&gt;     vmax=600,\n    &gt;&gt;&gt;     vmin=50,\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; plt.axhline(23, color=\"orange\", linestyle=\"--\", label=\"23hz\")\n\n    &gt;&gt;&gt; plt.yscale(\"log\")\n    &gt;&gt;&gt; # move legend outside of plot\n    &gt;&gt;&gt; plt.legend(loc=\"upper right\", bbox_to_anchor=(1.1, 1.1), frameon=False)\n\n    &gt;&gt;&gt; plt.gca().invert_yaxis()\n\n    &gt;&gt;&gt; plt.colorbar(location=\"top\", label=\"Power (uV^2)\")\n    &gt;&gt;&gt; # move colorbar more to the left\n    &gt;&gt;&gt; plt.gcf().axes[1].set_position([0.5, 0.8, 0.4, 0.6])\n\n\n    &gt;&gt;&gt; plt.gca().set_ylabel(\"Frequency (Hz)\")\n\n    &gt;&gt;&gt; plt.gca().set_xlabel(\"Time from opto stim (s)\")\n\n    &gt;&gt;&gt; plt.twinx()\n    &gt;&gt;&gt; plt.yscale(\"linear\")\n    &gt;&gt;&gt; plt.axvline(0, color=\"k\", linestyle=\"--\")\n    &gt;&gt;&gt; plt.axvline(0.5, color=\"k\", linestyle=\"--\")\n    &gt;&gt;&gt; plt.plot(times, sigs, \"w\", linewidth=0.5)\n\n\n    &gt;&gt;&gt; # plt.gca().set_xlabel('Time (s)')\n    &gt;&gt;&gt; plt.gca().set_ylabel(\"Voltage (uV)\")\n    &gt;&gt;&gt; plt.gca().set_title(\"PFC during 23Hz stim in behavior\", y=1)\n    \"\"\"\n\n    signal_ = signal.copy()\n    if whiten:\n        signal_ = whiten_lfp(signal, order=whiten_order)\n\n    # set up frequency range\n    freqs = np.arange(freq_min, freq_max, freq_step)\n    # set up time range\n    if fs is None:\n        ds = timestamps[1] - timestamps[0]\n        fs = 1 / ds\n    # Create times array based on the sample rate (fs)\n    times = np.arange(-max_lag, max_lag, 1 / fs)\n    # Number of samples corresponding to the time window around each event\n    n_samples = int(max_lag * 2 * fs)\n\n    # Ensure the length of times matches n_samples\n    if len(times) != n_samples:\n        times = np.linspace(-max_lag, max_lag, n_samples)\n\n    n_freqs = len(freqs)\n    n_samples = len(times)\n\n    # set up mwt and sigs to store results\n    mwt = np.zeros((n_freqs, n_samples))\n    sigs = np.zeros(n_samples)\n\n    event_i = 0\n\n    def process_event(start):\n        nonlocal event_i\n        nonlocal mwt\n        nonlocal sigs\n\n        if start + max_lag &gt; timestamps.max() or start - max_lag &lt; timestamps.min():\n            return None, None\n\n        idx = (timestamps &gt;= start - max_lag) &amp; (timestamps &lt;= start + max_lag)\n\n        mwt_partial = np.abs(\n            compute_wavelet_transform(sig=signal_[idx], fs=fs, freqs=freqs, **kwargs)\n        )\n\n        return mwt_partial, signal[idx]\n\n    if parallel:\n        with ThreadPoolExecutor() as executor:\n            results = list(executor.map(process_event, events))\n\n        for mwt_partial, sig_partial in results:\n            if mwt_partial is not None:\n                # samples might be missing if the event is too close to the edge\n                if mwt_partial.shape[1] != n_samples:\n                    continue\n                mwt += mwt_partial\n                sigs += sig_partial\n                event_i += 1\n    else:\n        for start in events:\n            mwt_partial, sig_partial = process_event(start)\n            if mwt_partial is not None:\n                mwt += mwt_partial\n                sigs += sig_partial\n                event_i += 1\n\n    mwt /= event_i\n    sigs /= event_i\n\n    if return_pandas:\n        mwt = pd.DataFrame(mwt.T, index=times, columns=freqs)\n        sigs = pd.Series(sigs, index=times)\n        return mwt, sigs\n    else:\n        return mwt, sigs, times, freqs\n</code></pre>"},{"location":"reference/neuro_py/lfp/spectral/#neuro_py.lfp.spectral.filter_signal","title":"<code>filter_signal(sig, fs, pass_type, f_range, filter_type='fir', n_cycles=3, n_seconds=None, butterworth_order=4, remove_edges=True)</code>","text":"<p>Filter a neural signal using an FIR or IIR filter.</p> <p>Parameters:</p> Name Type Description Default <code>sig</code> <code>ndarray</code> <p>Time series to be filtered. N signals x M samples array.</p> required <code>fs</code> <code>float</code> <p>Sampling rate, in Hz.</p> required <code>pass_type</code> <code>(bandpass, bandstop, lowpass, highpass)</code> <p>Type of filter to apply.</p> <code>'bandpass'</code> <code>f_range</code> <code>float or tuple of float</code> <p>Frequency range for filtering. For 'lowpass' and 'highpass', a single float can be provided. For 'bandpass' and 'bandstop', a tuple specifying (f_low, f_high) is required.</p> required <code>filter_type</code> <code>(fir, iir)</code> <p>Type of filter to apply: 'fir' for FIR or 'iir' for IIR (Butterworth). Default is 'fir'.</p> <code>'fir'</code> <code>n_cycles</code> <code>int</code> <p>Number of cycles to define the kernel length for FIR filters. Default is 3.</p> <code>3</code> <code>n_seconds</code> <code>float</code> <p>Length of the FIR filter in seconds. Overrides <code>n_cycles</code> if specified. Ignored for IIR.</p> <code>None</code> <code>butterworth_order</code> <code>int</code> <p>Order of the Butterworth filter. Only applies to IIR filters. Default is 4.</p> <code>4</code> <code>remove_edges</code> <code>bool</code> <p>If True, replace samples within half the kernel length with NaN (FIR filters only). Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Filtered time series.</p> <p>Examples:</p> <p>Apply a lowpass FIR filter to a signal:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from your_module import filter_signal\n&gt;&gt;&gt; fs = 1000  # Sampling rate (Hz)\n&gt;&gt;&gt; t = np.linspace(0, 1, fs, endpoint=False)\n&gt;&gt;&gt; sig = np.sin(2 * np.pi * 1 * t) + 0.5 * np.sin(2 * np.pi * 50 * t)  # Signal with 1Hz and 50Hz components\n&gt;&gt;&gt; pass_type = 'lowpass'\n&gt;&gt;&gt; f_range = 10  # Lowpass filter at 10 Hz\n&gt;&gt;&gt; filt_sig = filter_signal(sig, fs, pass_type, f_range, filter_type='fir')\n&gt;&gt;&gt; plt.plot(t, sig, label='Original Signal')\n&gt;&gt;&gt; plt.plot(t, filt_sig, label='Filtered Signal')\n&gt;&gt;&gt; plt.legend()\n&gt;&gt;&gt; plt.show()\n</code></pre> Source code in <code>neuro_py/lfp/spectral.py</code> <pre><code>def filter_signal(\n    sig: np.ndarray,\n    fs: float,\n    pass_type: str,\n    f_range: Union[float, Tuple[float, float]],\n    filter_type: str = \"fir\",\n    n_cycles: int = 3,\n    n_seconds: Optional[float] = None,\n    butterworth_order: int = 4,\n    remove_edges: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"\n    Filter a neural signal using an FIR or IIR filter.\n\n    Parameters\n    ----------\n    sig : np.ndarray\n        Time series to be filtered. N signals x M samples array.\n    fs : float\n        Sampling rate, in Hz.\n    pass_type : {'bandpass', 'bandstop', 'lowpass', 'highpass'}\n        Type of filter to apply.\n    f_range : float or tuple of float\n        Frequency range for filtering. For 'lowpass' and 'highpass', a single float can be provided.\n        For 'bandpass' and 'bandstop', a tuple specifying (f_low, f_high) is required.\n    filter_type : {'fir', 'iir'}, optional\n        Type of filter to apply: 'fir' for FIR or 'iir' for IIR (Butterworth). Default is 'fir'.\n    n_cycles : int, optional\n        Number of cycles to define the kernel length for FIR filters. Default is 3.\n    n_seconds : float, optional\n        Length of the FIR filter in seconds. Overrides `n_cycles` if specified. Ignored for IIR.\n    butterworth_order : int, optional\n        Order of the Butterworth filter. Only applies to IIR filters. Default is 4.\n    remove_edges : bool, optional\n        If True, replace samples within half the kernel length with NaN (FIR filters only). Default is True.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered time series.\n\n    Examples\n    --------\n    Apply a lowpass FIR filter to a signal:\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from your_module import filter_signal\n    &gt;&gt;&gt; fs = 1000  # Sampling rate (Hz)\n    &gt;&gt;&gt; t = np.linspace(0, 1, fs, endpoint=False)\n    &gt;&gt;&gt; sig = np.sin(2 * np.pi * 1 * t) + 0.5 * np.sin(2 * np.pi * 50 * t)  # Signal with 1Hz and 50Hz components\n    &gt;&gt;&gt; pass_type = 'lowpass'\n    &gt;&gt;&gt; f_range = 10  # Lowpass filter at 10 Hz\n    &gt;&gt;&gt; filt_sig = filter_signal(sig, fs, pass_type, f_range, filter_type='fir')\n    &gt;&gt;&gt; plt.plot(t, sig, label='Original Signal')\n    &gt;&gt;&gt; plt.plot(t, filt_sig, label='Filtered Signal')\n    &gt;&gt;&gt; plt.legend()\n    &gt;&gt;&gt; plt.show()\n    \"\"\"\n\n    # Validate pass_type\n    if pass_type not in [\"bandpass\", \"bandstop\", \"lowpass\", \"highpass\"]:\n        raise ValueError(\n            \"`pass_type` must be one of: 'bandpass', 'bandstop', 'lowpass', 'highpass'.\"\n        )\n\n    # Ensure `f_range` is properly defined for the filter type\n    if isinstance(f_range, (int, float)):\n        if pass_type == \"lowpass\":\n            f_range = (None, f_range)  # Convert single value to tuple for lowpass\n        elif pass_type == \"highpass\":\n            f_range = (f_range, None)  # Convert single value to tuple for highpass\n        else:\n            raise ValueError(\n                \"`f_range` must be a tuple for 'bandpass' or 'bandstop' filters.\"\n            )\n\n    # Validate bandpass/bandstop filters\n    if pass_type in [\"bandpass\", \"bandstop\"]:\n        if not isinstance(f_range, tuple) or f_range[0] is None or f_range[1] is None:\n            raise ValueError(\n                \"Both frequencies must be specified for 'bandpass' and 'bandstop' filters.\"\n            )\n\n    # Nyquist frequency\n    nyquist = fs / 2\n\n    # FIR filter implementation\n    if filter_type == \"fir\":\n        # Compute filter kernel length\n        if n_seconds is not None:\n            kernel_len = int(n_seconds * fs)\n        else:\n            kernel_len = (\n                int((n_cycles / f_range[0]) * fs)\n                if f_range[0]\n                else int((n_cycles / f_range[1]) * fs)\n            )\n        if kernel_len % 2 == 0:\n            kernel_len += 1  # Ensure kernel length is odd\n\n        # Define FIR filter coefficients\n        if pass_type in [\"bandpass\", \"bandstop\"]:\n            fir_coefs = firwin(\n                kernel_len,\n                [f_range[0] / nyquist, f_range[1] / nyquist],\n                pass_zero=(pass_type == \"bandstop\"),\n            )\n        elif pass_type == \"lowpass\":\n            fir_coefs = firwin(kernel_len, f_range[1] / nyquist, pass_zero=True)\n        elif pass_type == \"highpass\":\n            fir_coefs = firwin(kernel_len, f_range[0] / nyquist, pass_zero=False)\n\n        # Apply the FIR filter\n        if len(sig.shape) == 1:\n            sig_filt = np.convolve(sig, fir_coefs, mode=\"same\")\n        else:\n            sig_filt = np.vstack([np.convolve(sig_, fir_coefs, mode=\"same\") for sig_ in sig])\n\n    # IIR filter implementation\n    elif filter_type == \"iir\":\n        # Design a Butterworth filter\n        if pass_type in [\"bandpass\", \"bandstop\"]:\n            b, a = butter(\n                butterworth_order,\n                [f_range[0] / nyquist, f_range[1] / nyquist],\n                btype=pass_type,\n            )\n        elif pass_type == \"lowpass\":\n            b, a = butter(butterworth_order, f_range[1] / nyquist, btype=\"low\")\n        elif pass_type == \"highpass\":\n            b, a = butter(butterworth_order, f_range[0] / nyquist, btype=\"high\")\n\n        # Apply the IIR filter\n        sig_filt = filtfilt(b, a, sig)\n\n    else:\n        raise ValueError(\"`filter_type` must be 'fir' or 'iir'.\")\n\n    # Optionally remove edges\n    if remove_edges and filter_type == \"fir\":\n        edge_len = kernel_len // 2\n        sig_filt[:edge_len] = np.nan\n        sig_filt[-edge_len:] = np.nan\n\n    return sig_filt\n</code></pre>"},{"location":"reference/neuro_py/lfp/spectral/#neuro_py.lfp.spectral.whiten_lfp","title":"<code>whiten_lfp(lfp, order=2)</code>","text":"<p>Perform temporal whitening of Local Field Potential (LFP) data using an Autoregressive (AR) model.</p> <p>This function applies temporal whitening to LFP data by fitting an AR model of the specified order and using the model to remove temporal correlations, resulting in a 'whitened' signal.</p> <p>Parameters:</p> Name Type Description Default <code>lfp</code> <code>ndarray</code> <p>A 1D numpy array containing the LFP data.</p> required <code>order</code> <code>(int, optional(default=2))</code> <p>The order of the AR model to be used for whitening the LFP data.</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The temporally whitened LFP data as a 1D numpy array.</p> Source code in <code>neuro_py/lfp/spectral.py</code> <pre><code>def whiten_lfp(lfp: np.ndarray, order: int = 2) -&gt; np.ndarray:\n    \"\"\"\n    Perform temporal whitening of Local Field Potential (LFP) data using an Autoregressive (AR) model.\n\n    This function applies temporal whitening to LFP data by fitting an AR model of the specified order\n    and using the model to remove temporal correlations, resulting in a 'whitened' signal.\n\n    Parameters\n    ----------\n    lfp : ndarray\n        A 1D numpy array containing the LFP data.\n    order : int, optional (default=2)\n        The order of the AR model to be used for whitening the LFP data.\n\n    Returns\n    -------\n    ndarray\n        The temporally whitened LFP data as a 1D numpy array.\n    \"\"\"\n\n    rho, _ = yule_walker(lfp, order=order)\n\n    a = np.concatenate(([1.0], -rho))\n\n    # Apply the whitening filter to the LFP data and return the result as a 1D array\n    return signal.convolve(lfp, a, \"same\")\n</code></pre>"},{"location":"reference/neuro_py/lfp/spectral/#neuro_py.lfp.spectral.yule_walker","title":"<code>yule_walker(x, order=1, method='adjusted', df=None, inv=False, demean=True)</code>","text":"<p>Estimate autoregressive (AR) parameters using the Yule-Walker equations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array_like</code> <p>A 1D array containing the input sequence.</p> required <code>order</code> <code>int</code> <p>The order of the autoregressive process. Default is 1.</p> <code>1</code> <code>method</code> <code>(adjusted, mle)</code> <p>Determines the denominator in the estimate of the autocorrelation function (ACF) at lag <code>k</code>. - If \"mle\", the denominator is <code>n = x.shape[0]</code>. - If \"adjusted\", the denominator is <code>n - k</code>. Default is \"adjusted\".</p> <code>\"adjusted\"</code> <code>df</code> <code>int</code> <p>Specifies the degrees of freedom. If <code>df</code> is supplied, it is used in place of <code>n</code>. Default is None.</p> <code>None</code> <code>inv</code> <code>bool</code> <p>If True, the inverse of the autocorrelation matrix is also returned. Default is False.</p> <code>False</code> <code>demean</code> <code>bool</code> <p>If True, the mean is subtracted from <code>x</code> before estimation. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>rho</code> <code>ndarray</code> <p>AR coefficients of size <code>(order,)</code> computed using the Yule-Walker method.</p> <code>sigma</code> <code>float</code> <p>The estimate of the residual standard deviation.</p> <code>inv_matrix</code> <code>(ndarray, optional)</code> <p>The inverse of the autocorrelation matrix (only returned if <code>inv=True</code>).</p> Notes <p>The function solves the Yule-Walker equations to compute the autoregressive coefficients of an AR process.</p> <ul> <li>The Toeplitz matrix of autocorrelations is constructed for solving the equations.</li> <li>If the autocorrelation matrix is singular, the pseudoinverse (<code>pinv</code>) is used as a fallback.</li> <li>For further details, see the AR model Wikipedia page.</li> </ul> See Also <p>statsmodels.regression.linear_model.yule_walker : Original implementation in statsmodels.</p> <p>Examples:</p> <p>Estimate AR(2) coefficients from a time series:</p> <pre><code>&gt;&gt;&gt; x = np.random.randn(1000)\n&gt;&gt;&gt; rho, sigma = yule_walker(x, order=2)\n</code></pre> <p>Estimate AR(3) coefficients and get the autocorrelation matrix inverse:</p> <pre><code>&gt;&gt;&gt; rho, sigma, inv_matrix = yule_walker(x, order=3, inv=True)\n</code></pre> Source code in <code>neuro_py/lfp/spectral.py</code> <pre><code>def yule_walker(\n    x: Union[np.ndarray, list],\n    order: int = 1,\n    method: str = \"adjusted\",\n    df: Optional[int] = None,\n    inv: bool = False,\n    demean: bool = True,\n) -&gt; Union[Tuple[np.ndarray, float], Tuple[np.ndarray, float, np.ndarray]]:\n    \"\"\"\n    Estimate autoregressive (AR) parameters using the Yule-Walker equations.\n\n    Parameters\n    ----------\n    x : array_like\n        A 1D array containing the input sequence.\n    order : int, optional\n        The order of the autoregressive process. Default is 1.\n    method : {\"adjusted\", \"mle\"}, optional\n        Determines the denominator in the estimate of the autocorrelation function (ACF) at lag `k`.\n        - If \"mle\", the denominator is `n = x.shape[0]`.\n        - If \"adjusted\", the denominator is `n - k`.\n        Default is \"adjusted\".\n    df : int, optional\n        Specifies the degrees of freedom. If `df` is supplied, it is used in place of `n`. Default is None.\n    inv : bool, optional\n        If True, the inverse of the autocorrelation matrix is also returned. Default is False.\n    demean : bool, optional\n        If True, the mean is subtracted from `x` before estimation. Default is True.\n\n    Returns\n    -------\n    rho : np.ndarray\n        AR coefficients of size `(order,)` computed using the Yule-Walker method.\n    sigma : float\n        The estimate of the residual standard deviation.\n    inv_matrix : np.ndarray, optional\n        The inverse of the autocorrelation matrix (only returned if `inv=True`).\n\n    Notes\n    -----\n    The function solves the Yule-Walker equations to compute the autoregressive coefficients of an AR process.\n\n    - The Toeplitz matrix of autocorrelations is constructed for solving the equations.\n    - If the autocorrelation matrix is singular, the pseudoinverse (`pinv`) is used as a fallback.\n    - For further details, see the [AR model Wikipedia page](https://en.wikipedia.org/wiki/Autoregressive_moving_average_model).\n\n    See Also\n    --------\n    statsmodels.regression.linear_model.yule_walker : Original implementation in statsmodels.\n\n    Examples\n    --------\n    Estimate AR(2) coefficients from a time series:\n\n    &gt;&gt;&gt; x = np.random.randn(1000)\n    &gt;&gt;&gt; rho, sigma = yule_walker(x, order=2)\n\n    Estimate AR(3) coefficients and get the autocorrelation matrix inverse:\n\n    &gt;&gt;&gt; rho, sigma, inv_matrix = yule_walker(x, order=3, inv=True)\n    \"\"\"\n\n    if method not in (\"adjusted\", \"mle\"):\n        raise ValueError(\"ACF estimation method must be 'adjusted' or 'MLE'\")\n    x = np.array(x, dtype=np.float64)\n    if demean:\n        x -= x.mean()\n    n = df or x.shape[0]\n\n    # this handles df_resid ie., n - p\n    adj_needed = method == \"adjusted\"\n\n    if x.ndim &gt; 1 and x.shape[1] != 1:\n        raise ValueError(\"expecting a vector to estimate AR parameters\")\n    r = np.zeros(order + 1, np.float64)\n    r[0] = (x**2).sum() / n\n    for k in range(1, order + 1):\n        r[k] = (x[0:-k] * x[k:]).sum() / (n - k * adj_needed)\n    R = toeplitz(r[:-1])\n\n    try:\n        rho = np.linalg.solve(R, r[1:])\n    except np.linalg.LinAlgError as err:\n        if \"Singular matrix\" in str(err):\n            warnings.warn(\"Matrix is singular. Using pinv.\")\n            rho = np.linalg.pinv(R) @ r[1:]\n        else:\n            raise\n\n    sigmasq = r[0] - (r[1:] * rho).sum()\n    if not np.isnan(sigmasq) and sigmasq &gt; 0:\n        sigma = np.sqrt(sigmasq)\n    else:\n        sigma = np.nan\n    if inv:\n        return rho, sigma, np.linalg.inv(R)\n    else:\n        return rho, sigma\n</code></pre>"},{"location":"reference/neuro_py/lfp/theta_cycles/","title":"neuro_py.lfp.theta_cycles","text":""},{"location":"reference/neuro_py/lfp/theta_cycles/#neuro_py.lfp.theta_cycles.get_ep_from_df","title":"<code>get_ep_from_df(df, ts)</code>","text":"<p>Extract epochs of theta oscillations from a bycycle dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing burst detection results.</p> required <code>ts</code> <code>ndarray</code> <p>Timestamps of the LFP data.</p> required <p>Returns:</p> Type Description <code>EpochArray</code> <p>An array of theta epochs.</p> Source code in <code>neuro_py/lfp/theta_cycles.py</code> <pre><code>def get_ep_from_df(df: pd.DataFrame, ts: np.ndarray) -&gt; nel.EpochArray:\n    \"\"\"\n    Extract epochs of theta oscillations from a bycycle dataframe.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The dataframe containing burst detection results.\n    ts : np.ndarray\n        Timestamps of the LFP data.\n\n    Returns\n    -------\n    nel.EpochArray\n        An array of theta epochs.\n    \"\"\"\n    index_for_oscilation_epoch = find_interval(df.is_burst)\n    start = []\n    stop = []\n    for idx in index_for_oscilation_epoch:\n        start.append(df.sample_peak[idx[0]])\n        stop.append(df.sample_peak[idx[1]])\n\n    # convert list to array\n    start = np.array(start)\n    stop = np.array(stop)\n\n    # index ts get get start and end ts for each oscillation epoch\n\n    start_ts = ts[start]\n    stop_ts = ts[stop]\n\n    theta_epoch = nel.EpochArray([np.array([start_ts, stop_ts]).T])\n\n    return theta_epoch\n</code></pre>"},{"location":"reference/neuro_py/lfp/theta_cycles/#neuro_py.lfp.theta_cycles.get_theta_channel","title":"<code>get_theta_channel(basepath, tag='CA1so')</code>","text":"<p>Get the theta channel for the specified brain region.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path for loading data.</p> required <code>tag</code> <code>str</code> <p>The tag identifying the brain region. Default is \"CA1so\".</p> <code>'CA1so'</code> <p>Returns:</p> Type Description <code>int or None</code> <p>The index of the theta channel (0-based), or None if not found.</p> Source code in <code>neuro_py/lfp/theta_cycles.py</code> <pre><code>def get_theta_channel(basepath: str, tag: str = \"CA1so\") -&gt; Optional[int]:\n    \"\"\"\n    Get the theta channel for the specified brain region.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path for loading data.\n    tag : str, optional\n        The tag identifying the brain region. Default is \"CA1so\".\n\n    Returns\n    -------\n    int or None\n        The index of the theta channel (0-based), or None if not found.\n    \"\"\"\n    brain_region = loading.load_brain_regions(basepath)\n\n    channel_tags = loading.load_channel_tags(basepath)\n\n    if tag in brain_region.keys():\n        theta_chan = brain_region[tag][\"channels\"]\n    else:\n        return None\n\n    bad_ch = channel_tags[\"Bad\"][\"channels\"]\n    for ch in theta_chan:\n        if np.any(ch != bad_ch):\n            theta_chan = ch\n            break\n    return ch - 1  # return in base 0\n</code></pre>"},{"location":"reference/neuro_py/lfp/theta_cycles/#neuro_py.lfp.theta_cycles.get_theta_cycles","title":"<code>get_theta_cycles(basepath, theta_freq=(6, 10), lowpass=48, detection_params=None, ch=None)</code>","text":"<p>Detect theta cycles in LFP data and save the results.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path for loading LFP data.</p> required <code>theta_freq</code> <code>tuple</code> <p>Frequency range for theta detection (default is (6, 10)).</p> <code>(6, 10)</code> <code>lowpass</code> <code>int</code> <p>Cut-off frequency for low-pass filtering (default is 48).</p> <code>48</code> <code>detection_params</code> <code>dict or None</code> <p>Parameters for theta detection (default is None).</p> <code>None</code> <code>ch</code> <code>int or None</code> <p>Channel used for theta detection (default is None).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/lfp/theta_cycles.py</code> <pre><code>def get_theta_cycles(\n    basepath: str,\n    theta_freq: Tuple[int, int] = (6, 10),\n    lowpass: int = 48,\n    detection_params: Optional[dict] = None,\n    ch: Optional[int] = None,\n) -&gt; Optional[None]:\n    \"\"\"\n    Detect theta cycles in LFP data and save the results.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path for loading LFP data.\n    theta_freq : tuple, optional\n        Frequency range for theta detection (default is (6, 10)).\n    lowpass : int, optional\n        Cut-off frequency for low-pass filtering (default is 48).\n    detection_params : dict or None, optional\n        Parameters for theta detection (default is None).\n    ch : int or None, optional\n        Channel used for theta detection (default is None).\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # import bycycle, hidden import to avoid mandatory dependency\n    from bycycle import Bycycle\n\n    # load lfp as memmap\n    lfp, ts, fs = process_lfp(basepath)\n\n    # get theta channel - default chooses CA1so\n    if ch is None:\n        ch = get_theta_channel(basepath, tag=\"CA1so\")\n\n    if ch is None:\n        ch = get_theta_channel(basepath, tag=\"CA1sp\")\n\n    if ch is None:\n        Warning(\"No theta channel found\")\n        return None\n\n    # per bycycle documentation, low-pass filter signal before running bycycle 4x the frequency of interest\n    filt_sig = filter_signal(lfp[:, ch], fs, \"lowpass\", lowpass, remove_edges=False)\n\n    # for detecting theta epochs\n    if detection_params is None:\n        thresholds = {\n            \"amp_fraction\": 0.1,\n            \"amp_consistency\": 0.4,\n            \"period_consistency\": 0.5,\n            \"monotonicity\": 0.6,\n            \"min_n_cycles\": 3,\n        }\n    else:\n        thresholds = detection_params\n\n    # initialize bycycle object\n    bm = Bycycle(thresholds=thresholds)\n    bm.fit(filt_sig, fs, theta_freq)\n\n    save_theta_cycles(bm.df_features, ts, basepath, detection_params=thresholds, ch=ch)\n</code></pre>"},{"location":"reference/neuro_py/lfp/theta_cycles/#neuro_py.lfp.theta_cycles.process_lfp","title":"<code>process_lfp(basepath)</code>","text":"<p>Process and load Local Field Potential (LFP) data.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path for loading LFP data.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the LFP data, timestamps, and sampling frequency.</p> Source code in <code>neuro_py/lfp/theta_cycles.py</code> <pre><code>def process_lfp(basepath: str) -&gt; Tuple[np.ndarray, np.ndarray, float]:\n    \"\"\"\n    Process and load Local Field Potential (LFP) data.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path for loading LFP data.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the LFP data, timestamps, and sampling frequency.\n    \"\"\"\n    nChannels, fs, _, _ = loading.loadXML(basepath)\n\n    lfp, ts = loading.loadLFP(\n        basepath, n_channels=nChannels, channel=None, frequency=fs\n    )\n    return lfp, ts, fs\n</code></pre>"},{"location":"reference/neuro_py/lfp/theta_cycles/#neuro_py.lfp.theta_cycles.save_theta_cycles","title":"<code>save_theta_cycles(df, ts, basepath, detection_params, ch, event_name='thetacycles', detection_name=None)</code>","text":"<p>Save theta cycles detected using bycycle to a .mat file in the cell explorer format.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The bycycle dataframe containing theta cycle features.</p> required <code>ts</code> <code>ndarray</code> <p>Timestamps of the LFP data.</p> required <code>basepath</code> <code>str</code> <p>Base path to save the file to.</p> required <code>detection_params</code> <code>dict</code> <p>Dictionary of detection parameters.</p> required <code>ch</code> <code>int</code> <p>Channel used for theta detection.</p> required <code>event_name</code> <code>str</code> <p>Name of the events (default is \"thetacycles\").</p> <code>'thetacycles'</code> <code>detection_name</code> <code>str or None</code> <p>Name of the detection (default is None).</p> <code>None</code> Source code in <code>neuro_py/lfp/theta_cycles.py</code> <pre><code>def save_theta_cycles(\n    df: pd.DataFrame,\n    ts: np.ndarray,\n    basepath: str,\n    detection_params: dict,\n    ch: int,\n    event_name: str = \"thetacycles\",\n    detection_name: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Save theta cycles detected using bycycle to a .mat file in the cell explorer format.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The bycycle dataframe containing theta cycle features.\n    ts : np.ndarray\n        Timestamps of the LFP data.\n    basepath : str\n        Base path to save the file to.\n    detection_params : dict\n        Dictionary of detection parameters.\n    ch : int\n        Channel used for theta detection.\n    event_name : str, optional\n        Name of the events (default is \"thetacycles\").\n    detection_name : str or None, optional\n        Name of the detection (default is None).\n    \"\"\"\n    filename = os.path.join(\n        basepath, os.path.basename(basepath) + \".\" + event_name + \".events.mat\"\n    )\n    data = {}\n    data[event_name] = {}\n\n    # create variables that will be saved\n    timestamps = np.array(\n        [ts[df.sample_peak.values[:-1]], ts[df.sample_peak.values[1:]]]\n    )\n    peaks = ts[df.sample_last_trough.values[1:]]\n    amplitudes = df.band_amp.values[1:]\n    duration = np.diff(\n        np.array([ts[df.sample_peak.values[:-1]], ts[df.sample_peak.values[1:]]]),\n        axis=0,\n    )\n    center = np.median(\n        np.array([ts[df.sample_peak.values[:-1]], ts[df.sample_peak.values[1:]]]),\n        axis=0,\n    )\n\n    # limit to cycles using is_burst\n    timestamps = timestamps[:, df.is_burst.values[1:]]\n    peaks = peaks[df.is_burst.values[1:]]\n    amplitudes = amplitudes[df.is_burst.values[1:]]\n    duration = duration[:, df.is_burst.values[1:]]\n    center = center[df.is_burst.values[1:]]\n\n    # save start_ts and stop_ts as 2d array\n    data[event_name][\"timestamps\"] = timestamps.T\n    data[event_name][\"peaks\"] = peaks.T\n    data[event_name][\"amplitudes\"] = amplitudes.T\n    data[event_name][\"amplitudeUnits\"] = \"mV\"\n    data[event_name][\"eventID\"] = []\n    data[event_name][\"eventIDlabels\"] = []\n    data[event_name][\"eventIDbinary\"] = []\n\n    # check if only single epoch\n    data[event_name][\"duration\"] = duration.T\n\n    data[event_name][\"center\"] = center.T\n    data[event_name][\"detectorinfo\"] = {}\n    if detection_name is None:\n        data[event_name][\"detectorinfo\"][\"detectorname\"] = []\n    else:\n        data[event_name][\"detectorinfo\"][\"detectorname\"] = detection_name\n    data[event_name][\"detectorinfo\"][\"detectionparms\"] = detection_params\n    data[event_name][\"detectorinfo\"][\"detectionintervals\"] = []\n    data[event_name][\"detectorinfo\"][\"theta_channel\"] = ch\n\n    savemat(filename, data, long_field_names=True)\n</code></pre>"},{"location":"reference/neuro_py/plotting/","title":"neuro_py.plotting","text":""},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.AngleAnnotation","title":"<code>AngleAnnotation</code>","text":"<p>               Bases: <code>Arc</code></p> <p>Draws an arc between two vectors which appears circular in display space.</p> Source code in <code>neuro_py/plotting/decorators.py</code> <pre><code>class AngleAnnotation(Arc):\n    \"\"\"\n    Draws an arc between two vectors which appears circular in display space.\n    \"\"\"\n    def __init__(self, xy, p1, p2, size=75, unit=\"points\", ax=None,\n                 text=\"\", textposition=\"inside\", text_kw=None, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        xy, p1, p2 : tuple or array of two floats\n            Center position and two points. Angle annotation is drawn between\n            the two vectors connecting *p1* and *p2* with *xy*, respectively.\n            Units are data coordinates.\n\n        size : float\n            Diameter of the angle annotation in units specified by *unit*.\n\n        unit : str\n            One of the following strings to specify the unit of *size*:\n\n            * \"pixels\": pixels\n            * \"points\": points, use points instead of pixels to not have a\n              dependence on the DPI\n            * \"axes width\", \"axes height\": relative units of Axes width, height\n            * \"axes min\", \"axes max\": minimum or maximum of relative Axes\n              width, height\n\n        ax : `matplotlib.axes.Axes`\n            The Axes to add the angle annotation to.\n\n        text : str\n            The text to mark the angle with.\n\n        textposition : {\"inside\", \"outside\", \"edge\"}\n            Whether to show the text in- or outside the arc. \"edge\" can be used\n            for custom positions anchored at the arc's edge.\n\n        text_kw : dict\n            Dictionary of arguments passed to the Annotation.\n\n        **kwargs\n            Further parameters are passed to `matplotlib.patches.Arc`. Use this\n            to specify, color, linewidth etc. of the arc.\n\n        \"\"\"\n        self.ax = ax or plt.gca()\n        self._xydata = xy  # in data coordinates\n        self.vec1 = p1\n        self.vec2 = p2\n        self.size = size\n        self.unit = unit\n        self.textposition = textposition\n\n        super().__init__(self._xydata, size, size, angle=0.0,\n                         theta1=self.theta1, theta2=self.theta2, **kwargs)\n\n        self.set_transform(IdentityTransform())\n        self.ax.add_patch(self)\n\n        self.kw = dict(ha=\"center\", va=\"center\",\n                       xycoords=IdentityTransform(),\n                       xytext=(0, 0), textcoords=\"offset points\",\n                       annotation_clip=True)\n        self.kw.update(text_kw or {})\n        self.text = ax.annotate(text, xy=self._center, **self.kw)\n\n    def get_size(self):\n        factor = 1.\n        if self.unit == \"points\":\n            factor = self.ax.figure.dpi / 72.\n        elif self.unit[:4] == \"axes\":\n            b = TransformedBbox(Bbox.unit(), self.ax.transAxes)\n            dic = {\"max\": max(b.width, b.height),\n                   \"min\": min(b.width, b.height),\n                   \"width\": b.width, \"height\": b.height}\n            factor = dic[self.unit[5:]]\n        return self.size * factor\n\n    def set_size(self, size):\n        self.size = size\n\n    def get_center_in_pixels(self):\n        \"\"\"return center in pixels\"\"\"\n        return self.ax.transData.transform(self._xydata)\n\n    def set_center(self, xy):\n        \"\"\"set center in data coordinates\"\"\"\n        self._xydata = xy\n\n    def get_theta(self, vec):\n        vec_in_pixels = self.ax.transData.transform(vec) - self._center\n        return np.rad2deg(np.arctan2(vec_in_pixels[1], vec_in_pixels[0]))\n\n    def get_theta1(self):\n        return self.get_theta(self.vec1)\n\n    def get_theta2(self):\n        return self.get_theta(self.vec2)\n\n    def set_theta(self, angle):\n        pass\n\n    # Redefine attributes of the Arc to always give values in pixel space\n    _center = property(get_center_in_pixels, set_center)\n    theta1 = property(get_theta1, set_theta)\n    theta2 = property(get_theta2, set_theta)\n    width = property(get_size, set_size)\n    height = property(get_size, set_size)\n\n    # The following two methods are needed to update the text position.\n    def draw(self, renderer):\n        self.update_text()\n        super().draw(renderer)\n\n    def update_text(self):\n        c = self._center\n        s = self.get_size()\n        angle_span = (self.theta2 - self.theta1) % 360\n        angle = np.deg2rad(self.theta1 + angle_span / 2)\n        r = s / 2\n        if self.textposition == \"inside\":\n            r = s / np.interp(angle_span, [60, 90, 135, 180],\n                                          [3.3, 3.5, 3.8, 4])\n        self.text.xy = c + r * np.array([np.cos(angle), np.sin(angle)])\n        if self.textposition == \"outside\":\n            def R90(a, r, w, h):\n                if a &lt; np.arctan(h/2/(r+w/2)):\n                    return np.sqrt((r+w/2)**2 + (np.tan(a)*(r+w/2))**2)\n                else:\n                    c = np.sqrt((w/2)**2+(h/2)**2)\n                    T = np.arcsin(c * np.cos(np.pi/2 - a + np.arcsin(h/2/c))/r)\n                    xy = r * np.array([np.cos(a + T), np.sin(a + T)])\n                    xy += np.array([w/2, h/2])\n                    return np.sqrt(np.sum(xy**2))\n\n            def R(a, r, w, h):\n                aa = (a % (np.pi/4))*((a % (np.pi/2)) &lt;= np.pi/4) + \\\n                     (np.pi/4 - (a % (np.pi/4)))*((a % (np.pi/2)) &gt;= np.pi/4)\n                return R90(aa, r, *[w, h][::int(np.sign(np.cos(2*a)))])\n\n            bbox = self.text.get_window_extent()\n            X = R(angle, r, bbox.width, bbox.height)\n            trans = self.ax.figure.dpi_scale_trans.inverted()\n            offs = trans.transform(((X-s/2), 0))[0] * 72\n            self.text.set_position([offs*np.cos(angle), offs*np.sin(angle)])\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.AngleAnnotation.get_center_in_pixels","title":"<code>get_center_in_pixels()</code>","text":"<p>return center in pixels</p> Source code in <code>neuro_py/plotting/decorators.py</code> <pre><code>def get_center_in_pixels(self):\n    \"\"\"return center in pixels\"\"\"\n    return self.ax.transData.transform(self._xydata)\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.AngleAnnotation.set_center","title":"<code>set_center(xy)</code>","text":"<p>set center in data coordinates</p> Source code in <code>neuro_py/plotting/decorators.py</code> <pre><code>def set_center(self, xy):\n    \"\"\"set center in data coordinates\"\"\"\n    self._xydata = xy\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.adjust_box_widths","title":"<code>adjust_box_widths(g, fac)</code>","text":"<p>Adjust the widths of boxes in a Seaborn-generated boxplot.</p> <p>This function iterates through the axes of the provided FacetGrid and modifies the widths of the boxplot boxes by a specified factor.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>FacetGrid</code> <p>The FacetGrid object containing the boxplot.</p> required <code>fac</code> <code>float</code> <p>The factor by which to adjust the box widths. A value &lt; 1 will narrow the boxes, while &gt; 1 will widen them.</p> required <p>Returns:</p> Type Description <code>None</code> <p>The function modifies the box widths in place.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import seaborn as sns\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; tips = sns.load_dataset(\"tips\")\n&gt;&gt;&gt; g = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n&gt;&gt;&gt; adjust_box_widths(g, 0.5)  # Narrow the boxes by 50%\n</code></pre> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def adjust_box_widths(g: sns.axisgrid.FacetGrid, fac: float) -&gt; None:\n    \"\"\"\n    Adjust the widths of boxes in a Seaborn-generated boxplot.\n\n    This function iterates through the axes of the provided FacetGrid\n    and modifies the widths of the boxplot boxes by a specified factor.\n\n    Parameters\n    ----------\n    g : seaborn.axisgrid.FacetGrid\n        The FacetGrid object containing the boxplot.\n    fac : float\n        The factor by which to adjust the box widths.\n        A value &lt; 1 will narrow the boxes, while &gt; 1 will widen them.\n\n    Returns\n    -------\n    None\n        The function modifies the box widths in place.\n\n    Examples\n    -------\n    &gt;&gt;&gt; import seaborn as sns\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; tips = sns.load_dataset(\"tips\")\n    &gt;&gt;&gt; g = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n    &gt;&gt;&gt; adjust_box_widths(g, 0.5)  # Narrow the boxes by 50%\n    \"\"\"\n\n    # iterating through Axes instances\n    for ax in g.axes:\n\n        # iterating through axes artists:\n        for c in ax.get_children():\n\n            # searching for PathPatches\n            if isinstance(c, PathPatch):\n                # getting current width of box:\n                p = c.get_path()\n                verts = p.vertices\n                verts_sub = verts[:-1]\n                xmin = np.min(verts_sub[:, 0])\n                xmax = np.max(verts_sub[:, 0])\n                xmid = 0.5 * (xmin + xmax)\n                xhalf = 0.5 * (xmax - xmin)\n\n                # setting new width of box\n                xmin_new = xmid - fac * xhalf\n                xmax_new = xmid + fac * xhalf\n                verts_sub[verts_sub[:, 0] == xmin, 0] = xmin_new\n                verts_sub[verts_sub[:, 0] == xmax, 0] = xmax_new\n\n                # setting new width of median line\n                for line in ax.lines:\n                    if np.all(line.get_xdata() == [xmin, xmax]):\n                        line.set_xdata([xmin_new, xmax_new])\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.lighten_color","title":"<code>lighten_color(color, amount=0.5)</code>","text":"<p>Lightens a hex color by blending it with white by a given percentage.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>str</code> <p>Hex color code (e.g., '#AABBCC').</p> required <code>amount</code> <code>float</code> <p>Fraction of the lightening, where 0 is no change and 1 is white, by default 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>str</code> <p>Lightened hex color code (e.g., '#FFFFFF').</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the color string is not a valid hex code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lighten_color(\"#AABBCC\", 0.3)\n'#c3cfdb'\n</code></pre> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def lighten_color(color: str, amount: float = 0.5) -&gt; str:\n    \"\"\"\n    Lightens a hex color by blending it with white by a given percentage.\n\n    Parameters\n    ----------\n    color : str\n        Hex color code (e.g., '#AABBCC').\n    amount : float, optional\n        Fraction of the lightening, where 0 is no change and 1 is white, by default 0.5.\n\n    Returns\n    -------\n    str\n        Lightened hex color code (e.g., '#FFFFFF').\n\n    Raises\n    ------\n    ValueError\n        If the color string is not a valid hex code.\n\n    Examples\n    -------\n    &gt;&gt;&gt; lighten_color(\"#AABBCC\", 0.3)\n    '#c3cfdb'\n    \"\"\"\n    try:\n        c = color.lstrip(\"#\")\n        c = tuple(int(c[i : i + 2], 16) for i in (0, 2, 4))\n        c = (\n            int((1 - amount) * c[0] + amount * 255),\n            int((1 - amount) * c[1] + amount * 255),\n            int((1 - amount) * c[2] + amount * 255),\n        )\n        return \"#%02x%02x%02x\" % c\n    except ValueError:\n        return color\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.plot_events","title":"<code>plot_events(events, labels, cmap='tab20', gridlines=True, alpha=0.75, ax=None)</code>","text":"<p>Plot multiple event epochs as colored spans on a time axis.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>list of nelpy EpochArray</code> <p>List of EpochArrays representing events.</p> required <code>labels</code> <code>list of str</code> <p>List of labels for each event type.</p> required <code>cmap</code> <code>str</code> <p>Colormap for the event spans, by default 'tab20'.</p> <code>'tab20'</code> <code>gridlines</code> <code>bool</code> <p>Whether to plot horizontal gridlines, by default True.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>Transparency of event spans, by default 0.75.</p> <code>0.75</code> <code>ax</code> <code>Axes or None</code> <p>Matplotlib Axes to plot on. If None, the current axis will be used, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The axis with the plotted events.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # load sleep states\n&gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n&gt;&gt;&gt; # make nelpy epoch arrays\n&gt;&gt;&gt; nrem_epochs = nel.EpochArray(state_dict['NREMstate'])\n&gt;&gt;&gt; wake_epochs = nel.EpochArray(state_dict['WAKEstate'])\n&gt;&gt;&gt; rem_epochs = nel.EpochArray(state_dict['REMstate'])\n&gt;&gt;&gt; # add to list\n&gt;&gt;&gt; events = [nrem_epochs, wake_epochs, rem_epochs]\n&gt;&gt;&gt; # plot\n&gt;&gt;&gt; plt.figure(figsize=(20, 5))\n&gt;&gt;&gt; plot_events(events, ['nrem', 'wake', 'rem'])\n</code></pre> Source code in <code>neuro_py/plotting/events.py</code> <pre><code>def plot_events(\n    events: List[EpochArray],\n    labels: List[str],\n    cmap: str = \"tab20\",\n    gridlines: bool = True,\n    alpha: float = 0.75,\n    ax: Union[plt.Axes, None] = None,\n) -&gt; plt.Axes:\n    \"\"\"\n    Plot multiple event epochs as colored spans on a time axis.\n\n    Parameters\n    ----------\n    events : list of nelpy EpochArray\n        List of EpochArrays representing events.\n    labels : list of str\n        List of labels for each event type.\n    cmap : str, optional\n        Colormap for the event spans, by default 'tab20'.\n    gridlines : bool, optional\n        Whether to plot horizontal gridlines, by default True.\n    alpha : float, optional\n        Transparency of event spans, by default 0.75.\n    ax : plt.Axes or None, optional\n        Matplotlib Axes to plot on. If None, the current axis will be used, by default None.\n\n    Returns\n    -------\n    plt.Axes\n        The axis with the plotted events.\n\n    Examples\n    -------\n    &gt;&gt;&gt; # load sleep states\n    &gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n    &gt;&gt;&gt; # make nelpy epoch arrays\n    &gt;&gt;&gt; nrem_epochs = nel.EpochArray(state_dict['NREMstate'])\n    &gt;&gt;&gt; wake_epochs = nel.EpochArray(state_dict['WAKEstate'])\n    &gt;&gt;&gt; rem_epochs = nel.EpochArray(state_dict['REMstate'])\n    &gt;&gt;&gt; # add to list\n    &gt;&gt;&gt; events = [nrem_epochs, wake_epochs, rem_epochs]\n    &gt;&gt;&gt; # plot\n    &gt;&gt;&gt; plt.figure(figsize=(20, 5))\n    &gt;&gt;&gt; plot_events(events, ['nrem', 'wake', 'rem'])\n    \"\"\"\n    # get colormap\n    cmap = matplotlib.cm.get_cmap(cmap)\n\n    # set up y axis\n    y = np.linspace(0, 1, len(events) + 1)\n\n    # set up ax if not provided\n    if ax is None:\n        ax = plt.gca()\n\n    # iter over each event\n    for i, evt in enumerate(events):\n        # add horizontal line underneath\n        if gridlines:\n            ax.axhline(y[i] + np.diff(y)[0] / 2, color=\"k\", zorder=-100, alpha=0.1)\n\n        # plot events\n        for pair in range(evt.n_intervals):\n            ax.axvspan(\n                evt.starts[pair],\n                evt.stops[pair],\n                y[i],\n                y[i + 1],\n                alpha=alpha,\n                color=cmap(i * 0.1),\n            )\n\n    ax.set_yticks(y[:-1] + np.diff(y)[0] / 2)\n    ax.set_yticklabels(labels)\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.plot_joint_peth","title":"<code>plot_joint_peth(peth_1, peth_2, ts, smooth_std=2, labels=['peth_1', 'peth_2', 'event'])</code>","text":"<p>Plot joint peri-event time histograms (PETHs) and the difference between the observed and expected responses.</p> <p>Parameters:</p> Name Type Description Default <code>peth_1</code> <code>ndarray</code> <p>Peri-event time histogram (PETH) for the first event. Shape: (n_events, n_time_points).</p> required <code>peth_2</code> <code>ndarray</code> <p>Peri-event time histogram (PETH) for the second event. Shape: (n_events, n_time_points).</p> required <code>ts</code> <code>ndarray</code> <p>Time vector for the PETHs.</p> required <code>smooth_std</code> <code>float</code> <p>Standard deviation of the Gaussian kernel used to smooth the PETHs. Default is 2.</p> <code>2</code> <code>labels</code> <code>List[str]</code> <p>Labels for the PETHs. Default is [\"peth_1\", \"peth_2\", \"event\"].</p> <code>['peth_1', 'peth_2', 'event']</code> <p>Returns:</p> Type Description <code>Tuple[Figure, ndarray]</code> <p>Figure and axes objects for the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; peth_1 = np.random.rand(10, 100)  # Example data for peth_1\n&gt;&gt;&gt; peth_2 = np.random.rand(10, 100)  # Example data for peth_2\n&gt;&gt;&gt; ts = np.linspace(-1, 1, 100)  # Example time vector\n&gt;&gt;&gt; plot_joint_peth(peth_1, peth_2, ts)\n</code></pre> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def plot_joint_peth(\n    peth_1: np.ndarray,\n    peth_2: np.ndarray,\n    ts: np.ndarray,\n    smooth_std: float = 2,\n    labels: list = [\"peth_1\", \"peth_2\", \"event\"],\n) -&gt; Tuple[plt.Figure, np.ndarray]:\n    \"\"\"\n    Plot joint peri-event time histograms (PETHs) and the difference between the observed and expected responses.\n\n    Parameters\n    ----------\n    peth_1 : np.ndarray\n        Peri-event time histogram (PETH) for the first event. Shape: (n_events, n_time_points).\n    peth_2 : np.ndarray\n        Peri-event time histogram (PETH) for the second event. Shape: (n_events, n_time_points).\n    ts : np.ndarray\n        Time vector for the PETHs.\n    smooth_std : float, optional\n        Standard deviation of the Gaussian kernel used to smooth the PETHs. Default is 2.\n    labels : List[str], optional\n        Labels for the PETHs. Default is [\"peth_1\", \"peth_2\", \"event\"].\n\n    Returns\n    -------\n    Tuple[plt.Figure, np.ndarray]\n        Figure and axes objects for the plot.\n\n    Examples\n    -------\n    &gt;&gt;&gt; peth_1 = np.random.rand(10, 100)  # Example data for peth_1\n    &gt;&gt;&gt; peth_2 = np.random.rand(10, 100)  # Example data for peth_2\n    &gt;&gt;&gt; ts = np.linspace(-1, 1, 100)  # Example time vector\n    &gt;&gt;&gt; plot_joint_peth(peth_1, peth_2, ts)\n\n    \"\"\"\n\n    window = [ts[0], ts[-1]]\n\n    joint, expected, difference = joint_peth(peth_1, peth_2, smooth_std=smooth_std)\n\n    # get average of diagonals\n    corrected = average_diagonal(difference.T)\n    # get center values of corrected_2\n    corrected = corrected[\n        difference.shape[1] // 2 : (difference.shape[1] // 2) + difference.shape[1]\n    ]\n\n    fig, ax = plt.subplots(\n        2,\n        4,\n        figsize=(12, 4),\n        gridspec_kw={\"width_ratios\": [0.25, 1, 1, 1], \"height_ratios\": [0.25, 1]},\n    )\n    # space between panels\n    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n    ax[1, 1].imshow(\n        joint,\n        aspect=\"auto\",\n        interpolation=\"nearest\",\n        origin=\"lower\",\n        extent=[window[0], window[-1], window[0], window[-1]],\n    )\n\n    ax[0, 1].plot(\n        np.linspace(window[0], window[-1], len(joint)), joint.mean(axis=0), color=\"k\"\n    )\n    ax[0, 1].set_ylabel(f\"{labels[1]} rate\")\n    ax[0, 1].axvline(0, ls=\"--\", color=\"k\")\n\n    ax[1, 0].plot(\n        joint.mean(axis=1), np.linspace(window[0], window[-1], len(joint)), color=\"k\"\n    )\n    ax[1, 0].axhline(0, ls=\"--\", color=\"k\")\n    ax[1, 0].set_xlabel(f\"{labels[0]} rate\")\n\n    # plt.colorbar(f)\n    ax[1, 2].imshow(\n        expected,\n        aspect=\"auto\",\n        interpolation=\"nearest\",\n        origin=\"lower\",\n        extent=[window[0], window[-1], window[0], window[-1]],\n    )\n    ax[1, 2].set_title(\"Expected\")\n\n    ax[1, 3].imshow(\n        difference,\n        aspect=\"auto\",\n        interpolation=\"nearest\",\n        origin=\"lower\",\n        extent=[window[0], window[-1], window[0], window[-1]],\n    )\n\n    ax[0, 3].set_title(f\"corrected {labels[0]} response to {labels[1]}\")\n    ax[0, 3].plot(\n        np.linspace(window[0], window[-1], len(corrected)),\n        corrected,\n        color=\"k\",\n    )\n    ax[0, 3].set_xlim(window[0], window[-1])\n    ax[0, 3].axvline(0, ls=\"--\", color=\"k\")\n\n    for a in ax[1, 1:].ravel():\n        a.plot([-1, 1], [-1, 1], \"k--\")\n        a.axvline(0, c=\"w\", ls=\"--\")\n        a.axhline(0, c=\"w\", ls=\"--\")\n        a.set_xlim(window[0], window[-1])\n        a.set_ylim(window[0], window[-1])\n    ax[0, 0].axis(\"off\")\n    ax[0, 2].axis(\"off\")\n\n    ax[1, 1].set_xlabel(f\"{labels[1]} time from {labels[-1]} (s)\")\n    ax[1, 2].set_xlabel(f\"{labels[1]} time from {labels[-1]} (s)\")\n    ax[1, 3].set_xlabel(f\"{labels[1]} time from {labels[-1]} (s)\")\n\n    ax[1, 0].set_ylabel(f\"{labels[0]} time from {labels[-1]} (s)\")\n\n    # turn off x ticsk\n    ax[0, 1].set_xticks([])\n    ax[0, 3].set_xticks([])\n\n    ax[1, 1].set_yticks([])\n    ax[1, 2].set_yticks([])\n    ax[1, 3].set_yticks([])\n\n    ax[0, 3].set_xlabel(\"obs - expected\")\n\n    sns.despine()\n\n    return fig, ax\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.plot_peth","title":"<code>plot_peth(peth, ax=None, smooth=False, smooth_window=0.3, smooth_std=5, smooth_win_type='gaussian', **kwargs)</code>","text":"<p>Plot a peri-event time histogram (PETH).  Assumes that the index is time and the columns are trials/cells/etc.</p> <p>Parameters:</p> Name Type Description Default <code>peth</code> <code>DataFrame</code> <p>Peri-event time histogram to plot.</p> required <code>ax</code> <code>Axes</code> <p>Axis to plot on, by default None.</p> <code>None</code> <code>smooth</code> <code>bool</code> <p>Whether to apply smoothing to the data, by default False.</p> <code>False</code> <code>smooth_window</code> <code>float</code> <p>Window size for smoothing (in the same units as the index), by default 0.30.</p> <code>0.3</code> <code>smooth_std</code> <code>int</code> <p>Standard deviation of the smoothing window, by default 5.</p> <code>5</code> <code>smooth_win_type</code> <code>str</code> <p>The type of smoothing window to use, by default 'gaussian'.</p> <code>'gaussian'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to seaborn.lineplot.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Axis with the plotted PETH.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If peth is not a pandas DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.plotting.events import plot_peth\n&gt;&gt;&gt; from neuro_py.process import peri_event\n&gt;&gt;&gt; from neuro_py.io import loading\n&gt;&gt;&gt; st, cm = loading.load_spikes(basepath)\n&gt;&gt;&gt; ripple_epochs = loading.load_ripples_events(basepath, return_epoch_array=True)\n&gt;&gt;&gt; ripple_peth = peri_event.compute_psth(st.data, ripple_epochs.starts)\n&gt;&gt;&gt; plot_peth(ripple_peth)\n</code></pre> Source code in <code>neuro_py/plotting/events.py</code> <pre><code>def plot_peth(\n    peth: pd.DataFrame,\n    ax: Optional[plt.Axes] = None,\n    smooth: bool = False,\n    smooth_window: float = 0.30,\n    smooth_std: int = 5,\n    smooth_win_type: str = \"gaussian\",\n    **kwargs,\n) -&gt; plt.Axes:\n    \"\"\"\n    Plot a peri-event time histogram (PETH). \n    Assumes that the index is time and the columns are trials/cells/etc.\n\n    Parameters\n    ----------\n    peth : pd.DataFrame\n        Peri-event time histogram to plot.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on, by default None.\n    smooth : bool, optional\n        Whether to apply smoothing to the data, by default False.\n    smooth_window : float, optional\n        Window size for smoothing (in the same units as the index), by default 0.30.\n    smooth_std : int, optional\n        Standard deviation of the smoothing window, by default 5.\n    smooth_win_type : str, optional\n        The type of smoothing window to use, by default 'gaussian'.\n    **kwargs\n        Additional keyword arguments to pass to seaborn.lineplot.\n\n    Returns\n    -------\n    matplotlib.axes.Axes\n        Axis with the plotted PETH.\n\n    Raises\n    ------\n    TypeError\n        If peth is not a pandas DataFrame.\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.plotting.events import plot_peth\n    &gt;&gt;&gt; from neuro_py.process import peri_event\n    &gt;&gt;&gt; from neuro_py.io import loading\n    &gt;&gt;&gt; st, cm = loading.load_spikes(basepath)\n    &gt;&gt;&gt; ripple_epochs = loading.load_ripples_events(basepath, return_epoch_array=True)\n    &gt;&gt;&gt; ripple_peth = peri_event.compute_psth(st.data, ripple_epochs.starts)\n    &gt;&gt;&gt; plot_peth(ripple_peth)\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # verify peth is a dataframe\n    if not isinstance(peth, pd.DataFrame):\n        raise TypeError(\"peth must be a pandas dataframe\")\n\n    if smooth:\n        # convert window to samples\n        smooth_window = int(smooth_window / np.diff(peth.index)[0])\n        # smooth the peth\n        peth = (\n            peth.rolling(\n                window=smooth_window,\n                win_type=smooth_win_type,\n                center=True,\n                min_periods=1,\n            )\n            .mean(std=smooth_std)\n            .copy()\n        )\n\n    # melt the dataframe so that the index is time and there is a column for each trial/cell/etc.\n    peth_long = pd.melt(peth.reset_index(), id_vars=[\"index\"], value_name=\"peth\")\n\n    # plot the peth as a lineplot with seaborn\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=FutureWarning)\n        lineplot_ax = sns.lineplot(data=peth_long, x=\"index\", y=\"peth\", ax=ax, **kwargs)\n\n    ax.set_xlabel(\"Time (s)\")\n    sns.despine(ax=ax)\n    return lineplot_ax\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.plot_peth_fast","title":"<code>plot_peth_fast(peth, ts=None, ax=None, ci=0.95, smooth=False, smooth_window=0.3, smooth_std=5, smooth_win_type='gaussian', alpha=0.2, **kwargs)</code>","text":"<p>Plot a peth. Assumes that the index is time and the columns are trials/cells/etc.</p> <p>Less flexible, but faster version of plot_peth</p> <p>Parameters:</p> Name Type Description Default <code>peth</code> <code>(DataFrame, ndarray)</code> <p>Peth to plot</p> required <code>ts</code> <code>ndarray</code> <p>Time points to plot, by default None</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on, by default None</p> <code>None</code> <code>ci</code> <code>float</code> <p>Confidence interval to plot, by default 0.95</p> <code>0.95</code> <code>smooth</code> <code>bool</code> <p>Whether to smooth the peth, by default False</p> <code>False</code> <code>smooth_window</code> <code>float</code> <p>Window to smooth the peth, by default 0.30</p> <code>0.3</code> <code>smooth_std</code> <code>int</code> <p>Standard deviation of the smoothing window, by default 5</p> <code>5</code> <code>smooth_win_type</code> <code>str</code> <p>Type of smoothing window, by default \"gaussian\"</p> <code>'gaussian'</code> <code>alpha</code> <code>float</code> <p>Transparency of the confidence interval, by default 0.2</p> <code>0.2</code> <code>**kwargs</code> <p>Keyword arguments to pass to ax.plot</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Axis with plot</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If peth is not a pandas dataframe</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.plotting.events import plot_peth\n&gt;&gt;&gt; from neuro_py.process import peri_event\n&gt;&gt;&gt; from neuro_py.io import loading\n</code></pre> <pre><code>&gt;&gt;&gt; st, cm = loading.load_spikes(basepath)\n&gt;&gt;&gt; rippple_epochs = loading.load_ripples_events(basepath, return_epoch_array=True)\n</code></pre> <pre><code>&gt;&gt;&gt; ripple_peth = peri_event.compute_psth(st.data, rippple_epochs.starts)\n&gt;&gt;&gt; plot_peth_fast(ripple_peth)\n</code></pre> Source code in <code>neuro_py/plotting/events.py</code> <pre><code>def plot_peth_fast(\n    peth: Union[pd.DataFrame, np.ndarray],\n    ts: Union[np.ndarray, None] = None,\n    ax: Union[plt.Axes, None] = None,\n    ci: float = 0.95,\n    smooth: bool = False,\n    smooth_window: float = 0.30,\n    smooth_std: int = 5,\n    smooth_win_type: str = \"gaussian\",\n    alpha: float = 0.2,\n    **kwargs,\n) -&gt; plt.Axes:\n    \"\"\"\n    Plot a peth. Assumes that the index is time and the columns are trials/cells/etc.\n\n    Less flexible, but faster version of plot_peth\n\n    Parameters\n    ----------\n    peth : pd.DataFrame, np.ndarray\n        Peth to plot\n    ts : np.ndarray, optional\n        Time points to plot, by default None\n    ax : plt.Axes, optional\n        Axis to plot on, by default None\n    ci : float, optional\n        Confidence interval to plot, by default 0.95\n    smooth : bool, optional\n        Whether to smooth the peth, by default False\n    smooth_window : float, optional\n        Window to smooth the peth, by default 0.30\n    smooth_std : int, optional\n        Standard deviation of the smoothing window, by default 5\n    smooth_win_type : str, optional\n        Type of smoothing window, by default \"gaussian\"\n    alpha : float, optional\n        Transparency of the confidence interval, by default 0.2\n\n    **kwargs\n        Keyword arguments to pass to ax.plot\n\n    Returns\n    -------\n    plt.Axes\n        Axis with plot\n\n    Raises\n    ------\n    TypeError\n        If peth is not a pandas dataframe\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.plotting.events import plot_peth\n    &gt;&gt;&gt; from neuro_py.process import peri_event\n    &gt;&gt;&gt; from neuro_py.io import loading\n\n    &gt;&gt;&gt; st, cm = loading.load_spikes(basepath)\n    &gt;&gt;&gt; rippple_epochs = loading.load_ripples_events(basepath, return_epoch_array=True)\n\n    &gt;&gt;&gt; ripple_peth = peri_event.compute_psth(st.data, rippple_epochs.starts)\n    &gt;&gt;&gt; plot_peth_fast(ripple_peth)\n\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # verify peth is a dataframe, if not convert it\n    if not isinstance(peth, pd.DataFrame):\n\n        # if ts is not provided, create a range of time points\n        if ts is None:\n            ts = np.arange(peth.shape[0])\n\n        # transpose peth so that time is rows and trials are columns\n        if len(ts) == peth.shape[1]:\n            peth = peth.T\n\n        peth = pd.DataFrame(\n            index=ts,\n            columns=np.arange(peth.shape[1]),\n            data=peth,\n        )\n        # raise TypeError(\"peth must be a pandas dataframe\")\n\n    if smooth:\n        # convert window to samples\n        smooth_window = int(smooth_window / np.diff(peth.index)[0])\n        # smooth the peth\n        peth = (\n            peth.rolling(\n                window=smooth_window,\n                win_type=smooth_win_type,\n                center=True,\n                min_periods=1,\n            )\n            .mean(std=smooth_std)\n            .copy()\n        )\n\n    # plot the peth as a lineplot with matplotlib\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n        ax.plot(peth.index, np.nanmean(peth, axis=1), **kwargs)\n\n    # drop label from kwargs, as it was already used in the plot\n    kwargs.pop(\"label\", None)\n\n    lower, upper = confidence_intervals(peth.values.T, conf=ci)\n    ax.fill_between(peth.index, lower, upper, alpha=alpha, **kwargs)\n\n    ax.set_xlabel(\"Time (s)\")\n    sns.despine(ax=ax)\n\n    return ax\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.restore_natural_scale","title":"<code>restore_natural_scale(ax, min_, max_, n_steps=4, x_axis=True, y_axis=True)</code>","text":"<p>Converts logarithmic scale ticks to natural scale (base 10) for the specified axes.</p> <p>This function sets the ticks on the specified axes to be evenly spaced in the logarithmic scale and converts them back to the natural scale for display.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axis to modify.</p> required <code>min_</code> <code>float</code> <p>The minimum value for the ticks in logarithmic scale.</p> required <code>max_</code> <code>float</code> <p>The maximum value for the ticks in logarithmic scale.</p> required <code>n_steps</code> <code>int</code> <p>The number of ticks to create, by default 4.</p> <code>4</code> <code>x_axis</code> <code>bool</code> <p>If True, adjust the x-axis, by default True.</p> <code>True</code> <code>y_axis</code> <code>bool</code> <p>If True, adjust the y-axis, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>This function modifies the axis ticks in place.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; ax.set_xscale('log10')\n&gt;&gt;&gt; ax.plot(np.log10([1, 10, 100]), np.log10([1, 10, 100]))\n&gt;&gt;&gt; restore_natural_scale(ax, 0, 2)\n</code></pre> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def restore_natural_scale(\n    ax: matplotlib.axes.Axes,\n    min_: float,\n    max_: float,\n    n_steps: int = 4,\n    x_axis: bool = True,\n    y_axis: bool = True,\n) -&gt; None:\n    \"\"\"\n    Converts logarithmic scale ticks to natural scale (base 10) for the specified axes.\n\n    This function sets the ticks on the specified axes to be evenly spaced\n    in the logarithmic scale and converts them back to the natural scale\n    for display.\n\n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes\n        The axis to modify.\n    min_ : float\n        The minimum value for the ticks in logarithmic scale.\n    max_ : float\n        The maximum value for the ticks in logarithmic scale.\n    n_steps : int, optional\n        The number of ticks to create, by default 4.\n    x_axis : bool, optional\n        If True, adjust the x-axis, by default True.\n    y_axis : bool, optional\n        If True, adjust the y-axis, by default True.\n\n    Returns\n    -------\n    None\n        This function modifies the axis ticks in place.\n\n    Examples\n    -------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; ax.set_xscale('log10')\n    &gt;&gt;&gt; ax.plot(np.log10([1, 10, 100]), np.log10([1, 10, 100]))\n    &gt;&gt;&gt; restore_natural_scale(ax, 0, 2)\n    \"\"\"\n    ticks = np.linspace(min_, max_, n_steps)\n\n    if x_axis:\n        ax.set_xticks(ticks)\n        ax.set_xticklabels(np.round(10**ticks, 3))\n\n    if y_axis:\n        ax.set_yticks(ticks)\n        ax.set_yticklabels(np.round(10**ticks, 3))\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.set_equal_axis_range","title":"<code>set_equal_axis_range(ax1, ax2)</code>","text":"<p>Synchronizes the x and y axis ranges between two matplotlib axes.</p> <p>Parameters:</p> Name Type Description Default <code>ax1</code> <code>Axes</code> <p>The first axis to synchronize.</p> required <code>ax2</code> <code>Axes</code> <p>The second axis to synchronize.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, (ax1, ax2) = plt.subplots(1, 2)\n&gt;&gt;&gt; ax1.plot([1, 2, 3], [4, 5, 6])\n&gt;&gt;&gt; ax2.plot([1, 2, 3], [2, 3, 4])\n&gt;&gt;&gt; set_equal_axis_range(ax1, ax2)\n</code></pre> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def set_equal_axis_range(ax1: plt.Axes, ax2: plt.Axes) -&gt; None:\n    \"\"\"\n    Synchronizes the x and y axis ranges between two matplotlib axes.\n\n    Parameters\n    ----------\n    ax1 : matplotlib.axes.Axes\n        The first axis to synchronize.\n    ax2 : matplotlib.axes.Axes\n        The second axis to synchronize.\n\n    Examples\n    -------\n    &gt;&gt;&gt; fig, (ax1, ax2) = plt.subplots(1, 2)\n    &gt;&gt;&gt; ax1.plot([1, 2, 3], [4, 5, 6])\n    &gt;&gt;&gt; ax2.plot([1, 2, 3], [2, 3, 4])\n    &gt;&gt;&gt; set_equal_axis_range(ax1, ax2)\n    \"\"\"\n    # Get x and y axis limits for both axes\n    axis_x_values = np.hstack(np.array((ax1.get_xlim(), ax2.get_xlim())))\n    axis_y_values = np.hstack(np.array((ax1.get_ylim(), ax2.get_ylim())))\n\n    ax1.set_xlim(axis_x_values.min(), axis_x_values.max())\n    ax1.set_ylim(axis_y_values.min(), axis_y_values.max())\n    ax2.set_xlim(axis_x_values.min(), axis_x_values.max())\n    ax2.set_ylim(axis_y_values.min(), axis_y_values.max())\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.set_plotting_defaults","title":"<code>set_plotting_defaults()</code>","text":"<p>Set default plotting parameters for matplotlib with LaTeX-style fonts.</p> <p>This function updates matplotlib's plotting style to use serif fonts, sets font sizes for various elements, and ensures that SVG output uses non-embedded fonts for better compatibility.</p> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def set_plotting_defaults() -&gt; None:\n    \"\"\"\n    Set default plotting parameters for matplotlib with LaTeX-style fonts.\n\n    This function updates matplotlib's plotting style to use serif fonts,\n    sets font sizes for various elements, and ensures that SVG output uses\n    non-embedded fonts for better compatibility.\n    \"\"\"\n    tex_fonts = {\n        \"font.family\": \"serif\",\n        \"axes.labelsize\": 10,\n        \"font.size\": 10,\n        \"legend.fontsize\": 8,\n        \"xtick.labelsize\": 8,\n        \"ytick.labelsize\": 8,\n        \"svg.fonttype\": \"none\",\n    }\n\n    plt.style.use(\"default\")\n    plt.rcParams.update(tex_fonts)\n</code></pre>"},{"location":"reference/neuro_py/plotting/#neuro_py.plotting.set_size","title":"<code>set_size(width, fraction=1, subplots=(1, 1))</code>","text":"<p>Set figure dimensions to avoid scaling in LaTeX.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>float or str</code> <p>Document width in points (float) or predefined document type (str). Supported types: 'thesis', 'beamer', 'paper'.</p> required <code>fraction</code> <code>float</code> <p>Fraction of the width which you wish the figure to occupy, by default 1.</p> <code>1</code> <code>subplots</code> <code>tuple of int</code> <p>Number of rows and columns of subplots, by default (1, 1).</p> <code>(1, 1)</code> <p>Returns:</p> Type Description <code>tuple of float</code> <p>Dimensions of the figure in inches (width, height).</p> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def set_size(\n    width: Union[float, str], fraction: float = 1, subplots: Tuple[int, int] = (1, 1)\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Set figure dimensions to avoid scaling in LaTeX.\n\n    Parameters\n    ----------\n    width : float or str\n        Document width in points (float) or predefined document type (str).\n        Supported types: 'thesis', 'beamer', 'paper'.\n    fraction : float, optional\n        Fraction of the width which you wish the figure to occupy, by default 1.\n    subplots : tuple of int, optional\n        Number of rows and columns of subplots, by default (1, 1).\n\n    Returns\n    -------\n    tuple of float\n        Dimensions of the figure in inches (width, height).\n    \"\"\"\n    if width == \"thesis\":\n        width_pt = 426.79135\n    elif width == \"beamer\":\n        width_pt = 307.28987\n    elif width == \"paper\":\n        width_pt = 595.276\n    else:\n        width_pt = width\n\n    # Width of figure (in pts)\n    fig_width_pt = width_pt * fraction\n    # Convert from pt to inches\n    inches_per_pt = 1 / 72.27\n\n    # Golden ratio to set aesthetic figure height\n    # https://disq.us/p/2940ij3\n    golden_ratio = (5**0.5 - 1) / 2\n\n    # Figure width in inches\n    fig_width_in = fig_width_pt * inches_per_pt\n    # Figure height in inches\n    fig_height_in = fig_width_in * golden_ratio * (subplots[0] / subplots[1])\n\n    return (fig_width_in, fig_height_in)\n</code></pre>"},{"location":"reference/neuro_py/plotting/decorators/","title":"neuro_py.plotting.decorators","text":""},{"location":"reference/neuro_py/plotting/decorators/#neuro_py.plotting.decorators.AngleAnnotation","title":"<code>AngleAnnotation</code>","text":"<p>               Bases: <code>Arc</code></p> <p>Draws an arc between two vectors which appears circular in display space.</p> Source code in <code>neuro_py/plotting/decorators.py</code> <pre><code>class AngleAnnotation(Arc):\n    \"\"\"\n    Draws an arc between two vectors which appears circular in display space.\n    \"\"\"\n    def __init__(self, xy, p1, p2, size=75, unit=\"points\", ax=None,\n                 text=\"\", textposition=\"inside\", text_kw=None, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        xy, p1, p2 : tuple or array of two floats\n            Center position and two points. Angle annotation is drawn between\n            the two vectors connecting *p1* and *p2* with *xy*, respectively.\n            Units are data coordinates.\n\n        size : float\n            Diameter of the angle annotation in units specified by *unit*.\n\n        unit : str\n            One of the following strings to specify the unit of *size*:\n\n            * \"pixels\": pixels\n            * \"points\": points, use points instead of pixels to not have a\n              dependence on the DPI\n            * \"axes width\", \"axes height\": relative units of Axes width, height\n            * \"axes min\", \"axes max\": minimum or maximum of relative Axes\n              width, height\n\n        ax : `matplotlib.axes.Axes`\n            The Axes to add the angle annotation to.\n\n        text : str\n            The text to mark the angle with.\n\n        textposition : {\"inside\", \"outside\", \"edge\"}\n            Whether to show the text in- or outside the arc. \"edge\" can be used\n            for custom positions anchored at the arc's edge.\n\n        text_kw : dict\n            Dictionary of arguments passed to the Annotation.\n\n        **kwargs\n            Further parameters are passed to `matplotlib.patches.Arc`. Use this\n            to specify, color, linewidth etc. of the arc.\n\n        \"\"\"\n        self.ax = ax or plt.gca()\n        self._xydata = xy  # in data coordinates\n        self.vec1 = p1\n        self.vec2 = p2\n        self.size = size\n        self.unit = unit\n        self.textposition = textposition\n\n        super().__init__(self._xydata, size, size, angle=0.0,\n                         theta1=self.theta1, theta2=self.theta2, **kwargs)\n\n        self.set_transform(IdentityTransform())\n        self.ax.add_patch(self)\n\n        self.kw = dict(ha=\"center\", va=\"center\",\n                       xycoords=IdentityTransform(),\n                       xytext=(0, 0), textcoords=\"offset points\",\n                       annotation_clip=True)\n        self.kw.update(text_kw or {})\n        self.text = ax.annotate(text, xy=self._center, **self.kw)\n\n    def get_size(self):\n        factor = 1.\n        if self.unit == \"points\":\n            factor = self.ax.figure.dpi / 72.\n        elif self.unit[:4] == \"axes\":\n            b = TransformedBbox(Bbox.unit(), self.ax.transAxes)\n            dic = {\"max\": max(b.width, b.height),\n                   \"min\": min(b.width, b.height),\n                   \"width\": b.width, \"height\": b.height}\n            factor = dic[self.unit[5:]]\n        return self.size * factor\n\n    def set_size(self, size):\n        self.size = size\n\n    def get_center_in_pixels(self):\n        \"\"\"return center in pixels\"\"\"\n        return self.ax.transData.transform(self._xydata)\n\n    def set_center(self, xy):\n        \"\"\"set center in data coordinates\"\"\"\n        self._xydata = xy\n\n    def get_theta(self, vec):\n        vec_in_pixels = self.ax.transData.transform(vec) - self._center\n        return np.rad2deg(np.arctan2(vec_in_pixels[1], vec_in_pixels[0]))\n\n    def get_theta1(self):\n        return self.get_theta(self.vec1)\n\n    def get_theta2(self):\n        return self.get_theta(self.vec2)\n\n    def set_theta(self, angle):\n        pass\n\n    # Redefine attributes of the Arc to always give values in pixel space\n    _center = property(get_center_in_pixels, set_center)\n    theta1 = property(get_theta1, set_theta)\n    theta2 = property(get_theta2, set_theta)\n    width = property(get_size, set_size)\n    height = property(get_size, set_size)\n\n    # The following two methods are needed to update the text position.\n    def draw(self, renderer):\n        self.update_text()\n        super().draw(renderer)\n\n    def update_text(self):\n        c = self._center\n        s = self.get_size()\n        angle_span = (self.theta2 - self.theta1) % 360\n        angle = np.deg2rad(self.theta1 + angle_span / 2)\n        r = s / 2\n        if self.textposition == \"inside\":\n            r = s / np.interp(angle_span, [60, 90, 135, 180],\n                                          [3.3, 3.5, 3.8, 4])\n        self.text.xy = c + r * np.array([np.cos(angle), np.sin(angle)])\n        if self.textposition == \"outside\":\n            def R90(a, r, w, h):\n                if a &lt; np.arctan(h/2/(r+w/2)):\n                    return np.sqrt((r+w/2)**2 + (np.tan(a)*(r+w/2))**2)\n                else:\n                    c = np.sqrt((w/2)**2+(h/2)**2)\n                    T = np.arcsin(c * np.cos(np.pi/2 - a + np.arcsin(h/2/c))/r)\n                    xy = r * np.array([np.cos(a + T), np.sin(a + T)])\n                    xy += np.array([w/2, h/2])\n                    return np.sqrt(np.sum(xy**2))\n\n            def R(a, r, w, h):\n                aa = (a % (np.pi/4))*((a % (np.pi/2)) &lt;= np.pi/4) + \\\n                     (np.pi/4 - (a % (np.pi/4)))*((a % (np.pi/2)) &gt;= np.pi/4)\n                return R90(aa, r, *[w, h][::int(np.sign(np.cos(2*a)))])\n\n            bbox = self.text.get_window_extent()\n            X = R(angle, r, bbox.width, bbox.height)\n            trans = self.ax.figure.dpi_scale_trans.inverted()\n            offs = trans.transform(((X-s/2), 0))[0] * 72\n            self.text.set_position([offs*np.cos(angle), offs*np.sin(angle)])\n</code></pre>"},{"location":"reference/neuro_py/plotting/decorators/#neuro_py.plotting.decorators.AngleAnnotation.get_center_in_pixels","title":"<code>get_center_in_pixels()</code>","text":"<p>return center in pixels</p> Source code in <code>neuro_py/plotting/decorators.py</code> <pre><code>def get_center_in_pixels(self):\n    \"\"\"return center in pixels\"\"\"\n    return self.ax.transData.transform(self._xydata)\n</code></pre>"},{"location":"reference/neuro_py/plotting/decorators/#neuro_py.plotting.decorators.AngleAnnotation.set_center","title":"<code>set_center(xy)</code>","text":"<p>set center in data coordinates</p> Source code in <code>neuro_py/plotting/decorators.py</code> <pre><code>def set_center(self, xy):\n    \"\"\"set center in data coordinates\"\"\"\n    self._xydata = xy\n</code></pre>"},{"location":"reference/neuro_py/plotting/events/","title":"neuro_py.plotting.events","text":""},{"location":"reference/neuro_py/plotting/events/#neuro_py.plotting.events.plot_events","title":"<code>plot_events(events, labels, cmap='tab20', gridlines=True, alpha=0.75, ax=None)</code>","text":"<p>Plot multiple event epochs as colored spans on a time axis.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>list of nelpy EpochArray</code> <p>List of EpochArrays representing events.</p> required <code>labels</code> <code>list of str</code> <p>List of labels for each event type.</p> required <code>cmap</code> <code>str</code> <p>Colormap for the event spans, by default 'tab20'.</p> <code>'tab20'</code> <code>gridlines</code> <code>bool</code> <p>Whether to plot horizontal gridlines, by default True.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>Transparency of event spans, by default 0.75.</p> <code>0.75</code> <code>ax</code> <code>Axes or None</code> <p>Matplotlib Axes to plot on. If None, the current axis will be used, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The axis with the plotted events.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # load sleep states\n&gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n&gt;&gt;&gt; # make nelpy epoch arrays\n&gt;&gt;&gt; nrem_epochs = nel.EpochArray(state_dict['NREMstate'])\n&gt;&gt;&gt; wake_epochs = nel.EpochArray(state_dict['WAKEstate'])\n&gt;&gt;&gt; rem_epochs = nel.EpochArray(state_dict['REMstate'])\n&gt;&gt;&gt; # add to list\n&gt;&gt;&gt; events = [nrem_epochs, wake_epochs, rem_epochs]\n&gt;&gt;&gt; # plot\n&gt;&gt;&gt; plt.figure(figsize=(20, 5))\n&gt;&gt;&gt; plot_events(events, ['nrem', 'wake', 'rem'])\n</code></pre> Source code in <code>neuro_py/plotting/events.py</code> <pre><code>def plot_events(\n    events: List[EpochArray],\n    labels: List[str],\n    cmap: str = \"tab20\",\n    gridlines: bool = True,\n    alpha: float = 0.75,\n    ax: Union[plt.Axes, None] = None,\n) -&gt; plt.Axes:\n    \"\"\"\n    Plot multiple event epochs as colored spans on a time axis.\n\n    Parameters\n    ----------\n    events : list of nelpy EpochArray\n        List of EpochArrays representing events.\n    labels : list of str\n        List of labels for each event type.\n    cmap : str, optional\n        Colormap for the event spans, by default 'tab20'.\n    gridlines : bool, optional\n        Whether to plot horizontal gridlines, by default True.\n    alpha : float, optional\n        Transparency of event spans, by default 0.75.\n    ax : plt.Axes or None, optional\n        Matplotlib Axes to plot on. If None, the current axis will be used, by default None.\n\n    Returns\n    -------\n    plt.Axes\n        The axis with the plotted events.\n\n    Examples\n    -------\n    &gt;&gt;&gt; # load sleep states\n    &gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n    &gt;&gt;&gt; # make nelpy epoch arrays\n    &gt;&gt;&gt; nrem_epochs = nel.EpochArray(state_dict['NREMstate'])\n    &gt;&gt;&gt; wake_epochs = nel.EpochArray(state_dict['WAKEstate'])\n    &gt;&gt;&gt; rem_epochs = nel.EpochArray(state_dict['REMstate'])\n    &gt;&gt;&gt; # add to list\n    &gt;&gt;&gt; events = [nrem_epochs, wake_epochs, rem_epochs]\n    &gt;&gt;&gt; # plot\n    &gt;&gt;&gt; plt.figure(figsize=(20, 5))\n    &gt;&gt;&gt; plot_events(events, ['nrem', 'wake', 'rem'])\n    \"\"\"\n    # get colormap\n    cmap = matplotlib.cm.get_cmap(cmap)\n\n    # set up y axis\n    y = np.linspace(0, 1, len(events) + 1)\n\n    # set up ax if not provided\n    if ax is None:\n        ax = plt.gca()\n\n    # iter over each event\n    for i, evt in enumerate(events):\n        # add horizontal line underneath\n        if gridlines:\n            ax.axhline(y[i] + np.diff(y)[0] / 2, color=\"k\", zorder=-100, alpha=0.1)\n\n        # plot events\n        for pair in range(evt.n_intervals):\n            ax.axvspan(\n                evt.starts[pair],\n                evt.stops[pair],\n                y[i],\n                y[i + 1],\n                alpha=alpha,\n                color=cmap(i * 0.1),\n            )\n\n    ax.set_yticks(y[:-1] + np.diff(y)[0] / 2)\n    ax.set_yticklabels(labels)\n</code></pre>"},{"location":"reference/neuro_py/plotting/events/#neuro_py.plotting.events.plot_peth","title":"<code>plot_peth(peth, ax=None, smooth=False, smooth_window=0.3, smooth_std=5, smooth_win_type='gaussian', **kwargs)</code>","text":"<p>Plot a peri-event time histogram (PETH).  Assumes that the index is time and the columns are trials/cells/etc.</p> <p>Parameters:</p> Name Type Description Default <code>peth</code> <code>DataFrame</code> <p>Peri-event time histogram to plot.</p> required <code>ax</code> <code>Axes</code> <p>Axis to plot on, by default None.</p> <code>None</code> <code>smooth</code> <code>bool</code> <p>Whether to apply smoothing to the data, by default False.</p> <code>False</code> <code>smooth_window</code> <code>float</code> <p>Window size for smoothing (in the same units as the index), by default 0.30.</p> <code>0.3</code> <code>smooth_std</code> <code>int</code> <p>Standard deviation of the smoothing window, by default 5.</p> <code>5</code> <code>smooth_win_type</code> <code>str</code> <p>The type of smoothing window to use, by default 'gaussian'.</p> <code>'gaussian'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to seaborn.lineplot.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Axis with the plotted PETH.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If peth is not a pandas DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.plotting.events import plot_peth\n&gt;&gt;&gt; from neuro_py.process import peri_event\n&gt;&gt;&gt; from neuro_py.io import loading\n&gt;&gt;&gt; st, cm = loading.load_spikes(basepath)\n&gt;&gt;&gt; ripple_epochs = loading.load_ripples_events(basepath, return_epoch_array=True)\n&gt;&gt;&gt; ripple_peth = peri_event.compute_psth(st.data, ripple_epochs.starts)\n&gt;&gt;&gt; plot_peth(ripple_peth)\n</code></pre> Source code in <code>neuro_py/plotting/events.py</code> <pre><code>def plot_peth(\n    peth: pd.DataFrame,\n    ax: Optional[plt.Axes] = None,\n    smooth: bool = False,\n    smooth_window: float = 0.30,\n    smooth_std: int = 5,\n    smooth_win_type: str = \"gaussian\",\n    **kwargs,\n) -&gt; plt.Axes:\n    \"\"\"\n    Plot a peri-event time histogram (PETH). \n    Assumes that the index is time and the columns are trials/cells/etc.\n\n    Parameters\n    ----------\n    peth : pd.DataFrame\n        Peri-event time histogram to plot.\n    ax : matplotlib.axes.Axes, optional\n        Axis to plot on, by default None.\n    smooth : bool, optional\n        Whether to apply smoothing to the data, by default False.\n    smooth_window : float, optional\n        Window size for smoothing (in the same units as the index), by default 0.30.\n    smooth_std : int, optional\n        Standard deviation of the smoothing window, by default 5.\n    smooth_win_type : str, optional\n        The type of smoothing window to use, by default 'gaussian'.\n    **kwargs\n        Additional keyword arguments to pass to seaborn.lineplot.\n\n    Returns\n    -------\n    matplotlib.axes.Axes\n        Axis with the plotted PETH.\n\n    Raises\n    ------\n    TypeError\n        If peth is not a pandas DataFrame.\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.plotting.events import plot_peth\n    &gt;&gt;&gt; from neuro_py.process import peri_event\n    &gt;&gt;&gt; from neuro_py.io import loading\n    &gt;&gt;&gt; st, cm = loading.load_spikes(basepath)\n    &gt;&gt;&gt; ripple_epochs = loading.load_ripples_events(basepath, return_epoch_array=True)\n    &gt;&gt;&gt; ripple_peth = peri_event.compute_psth(st.data, ripple_epochs.starts)\n    &gt;&gt;&gt; plot_peth(ripple_peth)\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # verify peth is a dataframe\n    if not isinstance(peth, pd.DataFrame):\n        raise TypeError(\"peth must be a pandas dataframe\")\n\n    if smooth:\n        # convert window to samples\n        smooth_window = int(smooth_window / np.diff(peth.index)[0])\n        # smooth the peth\n        peth = (\n            peth.rolling(\n                window=smooth_window,\n                win_type=smooth_win_type,\n                center=True,\n                min_periods=1,\n            )\n            .mean(std=smooth_std)\n            .copy()\n        )\n\n    # melt the dataframe so that the index is time and there is a column for each trial/cell/etc.\n    peth_long = pd.melt(peth.reset_index(), id_vars=[\"index\"], value_name=\"peth\")\n\n    # plot the peth as a lineplot with seaborn\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=FutureWarning)\n        lineplot_ax = sns.lineplot(data=peth_long, x=\"index\", y=\"peth\", ax=ax, **kwargs)\n\n    ax.set_xlabel(\"Time (s)\")\n    sns.despine(ax=ax)\n    return lineplot_ax\n</code></pre>"},{"location":"reference/neuro_py/plotting/events/#neuro_py.plotting.events.plot_peth_fast","title":"<code>plot_peth_fast(peth, ts=None, ax=None, ci=0.95, smooth=False, smooth_window=0.3, smooth_std=5, smooth_win_type='gaussian', alpha=0.2, **kwargs)</code>","text":"<p>Plot a peth. Assumes that the index is time and the columns are trials/cells/etc.</p> <p>Less flexible, but faster version of plot_peth</p> <p>Parameters:</p> Name Type Description Default <code>peth</code> <code>(DataFrame, ndarray)</code> <p>Peth to plot</p> required <code>ts</code> <code>ndarray</code> <p>Time points to plot, by default None</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Axis to plot on, by default None</p> <code>None</code> <code>ci</code> <code>float</code> <p>Confidence interval to plot, by default 0.95</p> <code>0.95</code> <code>smooth</code> <code>bool</code> <p>Whether to smooth the peth, by default False</p> <code>False</code> <code>smooth_window</code> <code>float</code> <p>Window to smooth the peth, by default 0.30</p> <code>0.3</code> <code>smooth_std</code> <code>int</code> <p>Standard deviation of the smoothing window, by default 5</p> <code>5</code> <code>smooth_win_type</code> <code>str</code> <p>Type of smoothing window, by default \"gaussian\"</p> <code>'gaussian'</code> <code>alpha</code> <code>float</code> <p>Transparency of the confidence interval, by default 0.2</p> <code>0.2</code> <code>**kwargs</code> <p>Keyword arguments to pass to ax.plot</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Axis with plot</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If peth is not a pandas dataframe</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.plotting.events import plot_peth\n&gt;&gt;&gt; from neuro_py.process import peri_event\n&gt;&gt;&gt; from neuro_py.io import loading\n</code></pre> <pre><code>&gt;&gt;&gt; st, cm = loading.load_spikes(basepath)\n&gt;&gt;&gt; rippple_epochs = loading.load_ripples_events(basepath, return_epoch_array=True)\n</code></pre> <pre><code>&gt;&gt;&gt; ripple_peth = peri_event.compute_psth(st.data, rippple_epochs.starts)\n&gt;&gt;&gt; plot_peth_fast(ripple_peth)\n</code></pre> Source code in <code>neuro_py/plotting/events.py</code> <pre><code>def plot_peth_fast(\n    peth: Union[pd.DataFrame, np.ndarray],\n    ts: Union[np.ndarray, None] = None,\n    ax: Union[plt.Axes, None] = None,\n    ci: float = 0.95,\n    smooth: bool = False,\n    smooth_window: float = 0.30,\n    smooth_std: int = 5,\n    smooth_win_type: str = \"gaussian\",\n    alpha: float = 0.2,\n    **kwargs,\n) -&gt; plt.Axes:\n    \"\"\"\n    Plot a peth. Assumes that the index is time and the columns are trials/cells/etc.\n\n    Less flexible, but faster version of plot_peth\n\n    Parameters\n    ----------\n    peth : pd.DataFrame, np.ndarray\n        Peth to plot\n    ts : np.ndarray, optional\n        Time points to plot, by default None\n    ax : plt.Axes, optional\n        Axis to plot on, by default None\n    ci : float, optional\n        Confidence interval to plot, by default 0.95\n    smooth : bool, optional\n        Whether to smooth the peth, by default False\n    smooth_window : float, optional\n        Window to smooth the peth, by default 0.30\n    smooth_std : int, optional\n        Standard deviation of the smoothing window, by default 5\n    smooth_win_type : str, optional\n        Type of smoothing window, by default \"gaussian\"\n    alpha : float, optional\n        Transparency of the confidence interval, by default 0.2\n\n    **kwargs\n        Keyword arguments to pass to ax.plot\n\n    Returns\n    -------\n    plt.Axes\n        Axis with plot\n\n    Raises\n    ------\n    TypeError\n        If peth is not a pandas dataframe\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.plotting.events import plot_peth\n    &gt;&gt;&gt; from neuro_py.process import peri_event\n    &gt;&gt;&gt; from neuro_py.io import loading\n\n    &gt;&gt;&gt; st, cm = loading.load_spikes(basepath)\n    &gt;&gt;&gt; rippple_epochs = loading.load_ripples_events(basepath, return_epoch_array=True)\n\n    &gt;&gt;&gt; ripple_peth = peri_event.compute_psth(st.data, rippple_epochs.starts)\n    &gt;&gt;&gt; plot_peth_fast(ripple_peth)\n\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # verify peth is a dataframe, if not convert it\n    if not isinstance(peth, pd.DataFrame):\n\n        # if ts is not provided, create a range of time points\n        if ts is None:\n            ts = np.arange(peth.shape[0])\n\n        # transpose peth so that time is rows and trials are columns\n        if len(ts) == peth.shape[1]:\n            peth = peth.T\n\n        peth = pd.DataFrame(\n            index=ts,\n            columns=np.arange(peth.shape[1]),\n            data=peth,\n        )\n        # raise TypeError(\"peth must be a pandas dataframe\")\n\n    if smooth:\n        # convert window to samples\n        smooth_window = int(smooth_window / np.diff(peth.index)[0])\n        # smooth the peth\n        peth = (\n            peth.rolling(\n                window=smooth_window,\n                win_type=smooth_win_type,\n                center=True,\n                min_periods=1,\n            )\n            .mean(std=smooth_std)\n            .copy()\n        )\n\n    # plot the peth as a lineplot with matplotlib\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n        ax.plot(peth.index, np.nanmean(peth, axis=1), **kwargs)\n\n    # drop label from kwargs, as it was already used in the plot\n    kwargs.pop(\"label\", None)\n\n    lower, upper = confidence_intervals(peth.values.T, conf=ci)\n    ax.fill_between(peth.index, lower, upper, alpha=alpha, **kwargs)\n\n    ax.set_xlabel(\"Time (s)\")\n    sns.despine(ax=ax)\n\n    return ax\n</code></pre>"},{"location":"reference/neuro_py/plotting/figure_helpers/","title":"neuro_py.plotting.figure_helpers","text":""},{"location":"reference/neuro_py/plotting/figure_helpers/#neuro_py.plotting.figure_helpers.adjust_box_widths","title":"<code>adjust_box_widths(g, fac)</code>","text":"<p>Adjust the widths of boxes in a Seaborn-generated boxplot.</p> <p>This function iterates through the axes of the provided FacetGrid and modifies the widths of the boxplot boxes by a specified factor.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>FacetGrid</code> <p>The FacetGrid object containing the boxplot.</p> required <code>fac</code> <code>float</code> <p>The factor by which to adjust the box widths. A value &lt; 1 will narrow the boxes, while &gt; 1 will widen them.</p> required <p>Returns:</p> Type Description <code>None</code> <p>The function modifies the box widths in place.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import seaborn as sns\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; tips = sns.load_dataset(\"tips\")\n&gt;&gt;&gt; g = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n&gt;&gt;&gt; adjust_box_widths(g, 0.5)  # Narrow the boxes by 50%\n</code></pre> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def adjust_box_widths(g: sns.axisgrid.FacetGrid, fac: float) -&gt; None:\n    \"\"\"\n    Adjust the widths of boxes in a Seaborn-generated boxplot.\n\n    This function iterates through the axes of the provided FacetGrid\n    and modifies the widths of the boxplot boxes by a specified factor.\n\n    Parameters\n    ----------\n    g : seaborn.axisgrid.FacetGrid\n        The FacetGrid object containing the boxplot.\n    fac : float\n        The factor by which to adjust the box widths.\n        A value &lt; 1 will narrow the boxes, while &gt; 1 will widen them.\n\n    Returns\n    -------\n    None\n        The function modifies the box widths in place.\n\n    Examples\n    -------\n    &gt;&gt;&gt; import seaborn as sns\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; tips = sns.load_dataset(\"tips\")\n    &gt;&gt;&gt; g = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n    &gt;&gt;&gt; adjust_box_widths(g, 0.5)  # Narrow the boxes by 50%\n    \"\"\"\n\n    # iterating through Axes instances\n    for ax in g.axes:\n\n        # iterating through axes artists:\n        for c in ax.get_children():\n\n            # searching for PathPatches\n            if isinstance(c, PathPatch):\n                # getting current width of box:\n                p = c.get_path()\n                verts = p.vertices\n                verts_sub = verts[:-1]\n                xmin = np.min(verts_sub[:, 0])\n                xmax = np.max(verts_sub[:, 0])\n                xmid = 0.5 * (xmin + xmax)\n                xhalf = 0.5 * (xmax - xmin)\n\n                # setting new width of box\n                xmin_new = xmid - fac * xhalf\n                xmax_new = xmid + fac * xhalf\n                verts_sub[verts_sub[:, 0] == xmin, 0] = xmin_new\n                verts_sub[verts_sub[:, 0] == xmax, 0] = xmax_new\n\n                # setting new width of median line\n                for line in ax.lines:\n                    if np.all(line.get_xdata() == [xmin, xmax]):\n                        line.set_xdata([xmin_new, xmax_new])\n</code></pre>"},{"location":"reference/neuro_py/plotting/figure_helpers/#neuro_py.plotting.figure_helpers.lighten_color","title":"<code>lighten_color(color, amount=0.5)</code>","text":"<p>Lightens a hex color by blending it with white by a given percentage.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>str</code> <p>Hex color code (e.g., '#AABBCC').</p> required <code>amount</code> <code>float</code> <p>Fraction of the lightening, where 0 is no change and 1 is white, by default 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>str</code> <p>Lightened hex color code (e.g., '#FFFFFF').</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the color string is not a valid hex code.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; lighten_color(\"#AABBCC\", 0.3)\n'#c3cfdb'\n</code></pre> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def lighten_color(color: str, amount: float = 0.5) -&gt; str:\n    \"\"\"\n    Lightens a hex color by blending it with white by a given percentage.\n\n    Parameters\n    ----------\n    color : str\n        Hex color code (e.g., '#AABBCC').\n    amount : float, optional\n        Fraction of the lightening, where 0 is no change and 1 is white, by default 0.5.\n\n    Returns\n    -------\n    str\n        Lightened hex color code (e.g., '#FFFFFF').\n\n    Raises\n    ------\n    ValueError\n        If the color string is not a valid hex code.\n\n    Examples\n    -------\n    &gt;&gt;&gt; lighten_color(\"#AABBCC\", 0.3)\n    '#c3cfdb'\n    \"\"\"\n    try:\n        c = color.lstrip(\"#\")\n        c = tuple(int(c[i : i + 2], 16) for i in (0, 2, 4))\n        c = (\n            int((1 - amount) * c[0] + amount * 255),\n            int((1 - amount) * c[1] + amount * 255),\n            int((1 - amount) * c[2] + amount * 255),\n        )\n        return \"#%02x%02x%02x\" % c\n    except ValueError:\n        return color\n</code></pre>"},{"location":"reference/neuro_py/plotting/figure_helpers/#neuro_py.plotting.figure_helpers.plot_joint_peth","title":"<code>plot_joint_peth(peth_1, peth_2, ts, smooth_std=2, labels=['peth_1', 'peth_2', 'event'])</code>","text":"<p>Plot joint peri-event time histograms (PETHs) and the difference between the observed and expected responses.</p> <p>Parameters:</p> Name Type Description Default <code>peth_1</code> <code>ndarray</code> <p>Peri-event time histogram (PETH) for the first event. Shape: (n_events, n_time_points).</p> required <code>peth_2</code> <code>ndarray</code> <p>Peri-event time histogram (PETH) for the second event. Shape: (n_events, n_time_points).</p> required <code>ts</code> <code>ndarray</code> <p>Time vector for the PETHs.</p> required <code>smooth_std</code> <code>float</code> <p>Standard deviation of the Gaussian kernel used to smooth the PETHs. Default is 2.</p> <code>2</code> <code>labels</code> <code>List[str]</code> <p>Labels for the PETHs. Default is [\"peth_1\", \"peth_2\", \"event\"].</p> <code>['peth_1', 'peth_2', 'event']</code> <p>Returns:</p> Type Description <code>Tuple[Figure, ndarray]</code> <p>Figure and axes objects for the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; peth_1 = np.random.rand(10, 100)  # Example data for peth_1\n&gt;&gt;&gt; peth_2 = np.random.rand(10, 100)  # Example data for peth_2\n&gt;&gt;&gt; ts = np.linspace(-1, 1, 100)  # Example time vector\n&gt;&gt;&gt; plot_joint_peth(peth_1, peth_2, ts)\n</code></pre> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def plot_joint_peth(\n    peth_1: np.ndarray,\n    peth_2: np.ndarray,\n    ts: np.ndarray,\n    smooth_std: float = 2,\n    labels: list = [\"peth_1\", \"peth_2\", \"event\"],\n) -&gt; Tuple[plt.Figure, np.ndarray]:\n    \"\"\"\n    Plot joint peri-event time histograms (PETHs) and the difference between the observed and expected responses.\n\n    Parameters\n    ----------\n    peth_1 : np.ndarray\n        Peri-event time histogram (PETH) for the first event. Shape: (n_events, n_time_points).\n    peth_2 : np.ndarray\n        Peri-event time histogram (PETH) for the second event. Shape: (n_events, n_time_points).\n    ts : np.ndarray\n        Time vector for the PETHs.\n    smooth_std : float, optional\n        Standard deviation of the Gaussian kernel used to smooth the PETHs. Default is 2.\n    labels : List[str], optional\n        Labels for the PETHs. Default is [\"peth_1\", \"peth_2\", \"event\"].\n\n    Returns\n    -------\n    Tuple[plt.Figure, np.ndarray]\n        Figure and axes objects for the plot.\n\n    Examples\n    -------\n    &gt;&gt;&gt; peth_1 = np.random.rand(10, 100)  # Example data for peth_1\n    &gt;&gt;&gt; peth_2 = np.random.rand(10, 100)  # Example data for peth_2\n    &gt;&gt;&gt; ts = np.linspace(-1, 1, 100)  # Example time vector\n    &gt;&gt;&gt; plot_joint_peth(peth_1, peth_2, ts)\n\n    \"\"\"\n\n    window = [ts[0], ts[-1]]\n\n    joint, expected, difference = joint_peth(peth_1, peth_2, smooth_std=smooth_std)\n\n    # get average of diagonals\n    corrected = average_diagonal(difference.T)\n    # get center values of corrected_2\n    corrected = corrected[\n        difference.shape[1] // 2 : (difference.shape[1] // 2) + difference.shape[1]\n    ]\n\n    fig, ax = plt.subplots(\n        2,\n        4,\n        figsize=(12, 4),\n        gridspec_kw={\"width_ratios\": [0.25, 1, 1, 1], \"height_ratios\": [0.25, 1]},\n    )\n    # space between panels\n    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n    ax[1, 1].imshow(\n        joint,\n        aspect=\"auto\",\n        interpolation=\"nearest\",\n        origin=\"lower\",\n        extent=[window[0], window[-1], window[0], window[-1]],\n    )\n\n    ax[0, 1].plot(\n        np.linspace(window[0], window[-1], len(joint)), joint.mean(axis=0), color=\"k\"\n    )\n    ax[0, 1].set_ylabel(f\"{labels[1]} rate\")\n    ax[0, 1].axvline(0, ls=\"--\", color=\"k\")\n\n    ax[1, 0].plot(\n        joint.mean(axis=1), np.linspace(window[0], window[-1], len(joint)), color=\"k\"\n    )\n    ax[1, 0].axhline(0, ls=\"--\", color=\"k\")\n    ax[1, 0].set_xlabel(f\"{labels[0]} rate\")\n\n    # plt.colorbar(f)\n    ax[1, 2].imshow(\n        expected,\n        aspect=\"auto\",\n        interpolation=\"nearest\",\n        origin=\"lower\",\n        extent=[window[0], window[-1], window[0], window[-1]],\n    )\n    ax[1, 2].set_title(\"Expected\")\n\n    ax[1, 3].imshow(\n        difference,\n        aspect=\"auto\",\n        interpolation=\"nearest\",\n        origin=\"lower\",\n        extent=[window[0], window[-1], window[0], window[-1]],\n    )\n\n    ax[0, 3].set_title(f\"corrected {labels[0]} response to {labels[1]}\")\n    ax[0, 3].plot(\n        np.linspace(window[0], window[-1], len(corrected)),\n        corrected,\n        color=\"k\",\n    )\n    ax[0, 3].set_xlim(window[0], window[-1])\n    ax[0, 3].axvline(0, ls=\"--\", color=\"k\")\n\n    for a in ax[1, 1:].ravel():\n        a.plot([-1, 1], [-1, 1], \"k--\")\n        a.axvline(0, c=\"w\", ls=\"--\")\n        a.axhline(0, c=\"w\", ls=\"--\")\n        a.set_xlim(window[0], window[-1])\n        a.set_ylim(window[0], window[-1])\n    ax[0, 0].axis(\"off\")\n    ax[0, 2].axis(\"off\")\n\n    ax[1, 1].set_xlabel(f\"{labels[1]} time from {labels[-1]} (s)\")\n    ax[1, 2].set_xlabel(f\"{labels[1]} time from {labels[-1]} (s)\")\n    ax[1, 3].set_xlabel(f\"{labels[1]} time from {labels[-1]} (s)\")\n\n    ax[1, 0].set_ylabel(f\"{labels[0]} time from {labels[-1]} (s)\")\n\n    # turn off x ticsk\n    ax[0, 1].set_xticks([])\n    ax[0, 3].set_xticks([])\n\n    ax[1, 1].set_yticks([])\n    ax[1, 2].set_yticks([])\n    ax[1, 3].set_yticks([])\n\n    ax[0, 3].set_xlabel(\"obs - expected\")\n\n    sns.despine()\n\n    return fig, ax\n</code></pre>"},{"location":"reference/neuro_py/plotting/figure_helpers/#neuro_py.plotting.figure_helpers.restore_natural_scale","title":"<code>restore_natural_scale(ax, min_, max_, n_steps=4, x_axis=True, y_axis=True)</code>","text":"<p>Converts logarithmic scale ticks to natural scale (base 10) for the specified axes.</p> <p>This function sets the ticks on the specified axes to be evenly spaced in the logarithmic scale and converts them back to the natural scale for display.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axis to modify.</p> required <code>min_</code> <code>float</code> <p>The minimum value for the ticks in logarithmic scale.</p> required <code>max_</code> <code>float</code> <p>The maximum value for the ticks in logarithmic scale.</p> required <code>n_steps</code> <code>int</code> <p>The number of ticks to create, by default 4.</p> <code>4</code> <code>x_axis</code> <code>bool</code> <p>If True, adjust the x-axis, by default True.</p> <code>True</code> <code>y_axis</code> <code>bool</code> <p>If True, adjust the y-axis, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>This function modifies the axis ticks in place.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; ax.set_xscale('log10')\n&gt;&gt;&gt; ax.plot(np.log10([1, 10, 100]), np.log10([1, 10, 100]))\n&gt;&gt;&gt; restore_natural_scale(ax, 0, 2)\n</code></pre> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def restore_natural_scale(\n    ax: matplotlib.axes.Axes,\n    min_: float,\n    max_: float,\n    n_steps: int = 4,\n    x_axis: bool = True,\n    y_axis: bool = True,\n) -&gt; None:\n    \"\"\"\n    Converts logarithmic scale ticks to natural scale (base 10) for the specified axes.\n\n    This function sets the ticks on the specified axes to be evenly spaced\n    in the logarithmic scale and converts them back to the natural scale\n    for display.\n\n    Parameters\n    ----------\n    ax : matplotlib.axes.Axes\n        The axis to modify.\n    min_ : float\n        The minimum value for the ticks in logarithmic scale.\n    max_ : float\n        The maximum value for the ticks in logarithmic scale.\n    n_steps : int, optional\n        The number of ticks to create, by default 4.\n    x_axis : bool, optional\n        If True, adjust the x-axis, by default True.\n    y_axis : bool, optional\n        If True, adjust the y-axis, by default True.\n\n    Returns\n    -------\n    None\n        This function modifies the axis ticks in place.\n\n    Examples\n    -------\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; ax.set_xscale('log10')\n    &gt;&gt;&gt; ax.plot(np.log10([1, 10, 100]), np.log10([1, 10, 100]))\n    &gt;&gt;&gt; restore_natural_scale(ax, 0, 2)\n    \"\"\"\n    ticks = np.linspace(min_, max_, n_steps)\n\n    if x_axis:\n        ax.set_xticks(ticks)\n        ax.set_xticklabels(np.round(10**ticks, 3))\n\n    if y_axis:\n        ax.set_yticks(ticks)\n        ax.set_yticklabels(np.round(10**ticks, 3))\n</code></pre>"},{"location":"reference/neuro_py/plotting/figure_helpers/#neuro_py.plotting.figure_helpers.set_equal_axis_range","title":"<code>set_equal_axis_range(ax1, ax2)</code>","text":"<p>Synchronizes the x and y axis ranges between two matplotlib axes.</p> <p>Parameters:</p> Name Type Description Default <code>ax1</code> <code>Axes</code> <p>The first axis to synchronize.</p> required <code>ax2</code> <code>Axes</code> <p>The second axis to synchronize.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, (ax1, ax2) = plt.subplots(1, 2)\n&gt;&gt;&gt; ax1.plot([1, 2, 3], [4, 5, 6])\n&gt;&gt;&gt; ax2.plot([1, 2, 3], [2, 3, 4])\n&gt;&gt;&gt; set_equal_axis_range(ax1, ax2)\n</code></pre> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def set_equal_axis_range(ax1: plt.Axes, ax2: plt.Axes) -&gt; None:\n    \"\"\"\n    Synchronizes the x and y axis ranges between two matplotlib axes.\n\n    Parameters\n    ----------\n    ax1 : matplotlib.axes.Axes\n        The first axis to synchronize.\n    ax2 : matplotlib.axes.Axes\n        The second axis to synchronize.\n\n    Examples\n    -------\n    &gt;&gt;&gt; fig, (ax1, ax2) = plt.subplots(1, 2)\n    &gt;&gt;&gt; ax1.plot([1, 2, 3], [4, 5, 6])\n    &gt;&gt;&gt; ax2.plot([1, 2, 3], [2, 3, 4])\n    &gt;&gt;&gt; set_equal_axis_range(ax1, ax2)\n    \"\"\"\n    # Get x and y axis limits for both axes\n    axis_x_values = np.hstack(np.array((ax1.get_xlim(), ax2.get_xlim())))\n    axis_y_values = np.hstack(np.array((ax1.get_ylim(), ax2.get_ylim())))\n\n    ax1.set_xlim(axis_x_values.min(), axis_x_values.max())\n    ax1.set_ylim(axis_y_values.min(), axis_y_values.max())\n    ax2.set_xlim(axis_x_values.min(), axis_x_values.max())\n    ax2.set_ylim(axis_y_values.min(), axis_y_values.max())\n</code></pre>"},{"location":"reference/neuro_py/plotting/figure_helpers/#neuro_py.plotting.figure_helpers.set_plotting_defaults","title":"<code>set_plotting_defaults()</code>","text":"<p>Set default plotting parameters for matplotlib with LaTeX-style fonts.</p> <p>This function updates matplotlib's plotting style to use serif fonts, sets font sizes for various elements, and ensures that SVG output uses non-embedded fonts for better compatibility.</p> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def set_plotting_defaults() -&gt; None:\n    \"\"\"\n    Set default plotting parameters for matplotlib with LaTeX-style fonts.\n\n    This function updates matplotlib's plotting style to use serif fonts,\n    sets font sizes for various elements, and ensures that SVG output uses\n    non-embedded fonts for better compatibility.\n    \"\"\"\n    tex_fonts = {\n        \"font.family\": \"serif\",\n        \"axes.labelsize\": 10,\n        \"font.size\": 10,\n        \"legend.fontsize\": 8,\n        \"xtick.labelsize\": 8,\n        \"ytick.labelsize\": 8,\n        \"svg.fonttype\": \"none\",\n    }\n\n    plt.style.use(\"default\")\n    plt.rcParams.update(tex_fonts)\n</code></pre>"},{"location":"reference/neuro_py/plotting/figure_helpers/#neuro_py.plotting.figure_helpers.set_size","title":"<code>set_size(width, fraction=1, subplots=(1, 1))</code>","text":"<p>Set figure dimensions to avoid scaling in LaTeX.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>float or str</code> <p>Document width in points (float) or predefined document type (str). Supported types: 'thesis', 'beamer', 'paper'.</p> required <code>fraction</code> <code>float</code> <p>Fraction of the width which you wish the figure to occupy, by default 1.</p> <code>1</code> <code>subplots</code> <code>tuple of int</code> <p>Number of rows and columns of subplots, by default (1, 1).</p> <code>(1, 1)</code> <p>Returns:</p> Type Description <code>tuple of float</code> <p>Dimensions of the figure in inches (width, height).</p> Source code in <code>neuro_py/plotting/figure_helpers.py</code> <pre><code>def set_size(\n    width: Union[float, str], fraction: float = 1, subplots: Tuple[int, int] = (1, 1)\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Set figure dimensions to avoid scaling in LaTeX.\n\n    Parameters\n    ----------\n    width : float or str\n        Document width in points (float) or predefined document type (str).\n        Supported types: 'thesis', 'beamer', 'paper'.\n    fraction : float, optional\n        Fraction of the width which you wish the figure to occupy, by default 1.\n    subplots : tuple of int, optional\n        Number of rows and columns of subplots, by default (1, 1).\n\n    Returns\n    -------\n    tuple of float\n        Dimensions of the figure in inches (width, height).\n    \"\"\"\n    if width == \"thesis\":\n        width_pt = 426.79135\n    elif width == \"beamer\":\n        width_pt = 307.28987\n    elif width == \"paper\":\n        width_pt = 595.276\n    else:\n        width_pt = width\n\n    # Width of figure (in pts)\n    fig_width_pt = width_pt * fraction\n    # Convert from pt to inches\n    inches_per_pt = 1 / 72.27\n\n    # Golden ratio to set aesthetic figure height\n    # https://disq.us/p/2940ij3\n    golden_ratio = (5**0.5 - 1) / 2\n\n    # Figure width in inches\n    fig_width_in = fig_width_pt * inches_per_pt\n    # Figure height in inches\n    fig_height_in = fig_width_in * golden_ratio * (subplots[0] / subplots[1])\n\n    return (fig_width_in, fig_height_in)\n</code></pre>"},{"location":"reference/neuro_py/process/","title":"neuro_py.process","text":""},{"location":"reference/neuro_py/process/#neuro_py.process.acf_power","title":"<code>acf_power(acf, norm=True)</code>","text":"<p>Compute the power spectrum of the signal by calculating the FFT of the autocorrelation function (ACF).</p> <p>Parameters:</p> Name Type Description Default <code>acf</code> <code>ndarray</code> <p>1D array of counts for the ACF.</p> required <code>norm</code> <code>bool</code> <p>If True, normalize the power spectrum. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>psd</code> <code>ndarray</code> <p>1D array representing the power spectrum of the signal.</p> Notes <p>The power spectrum is computed by taking the Fourier Transform of the ACF, then squaring the absolute values of the FFT result. The Nyquist frequency is accounted for by returning only the first half of the spectrum.</p> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def acf_power(acf: np.ndarray, norm: Optional[bool] = True) -&gt; np.ndarray:\n    \"\"\"\n    Compute the power spectrum of the signal by calculating the FFT of the autocorrelation function (ACF).\n\n    Parameters\n    ----------\n    acf : np.ndarray\n        1D array of counts for the ACF.\n    norm : bool, optional\n        If True, normalize the power spectrum. Default is True.\n\n    Returns\n    -------\n    psd : np.ndarray\n        1D array representing the power spectrum of the signal.\n\n    Notes\n    -----\n    The power spectrum is computed by taking the Fourier Transform of the ACF,\n    then squaring the absolute values of the FFT result.\n    The Nyquist frequency is accounted for by returning only the first half of the spectrum.\n    \"\"\"\n\n    # Take the FFT\n    fft = np.fft.fft(acf)\n\n    # Compute the power spectrum\n    pow = np.abs(fft) ** 2\n\n    # Account for Nyquist frequency\n    psd = pow[: pow.shape[0] // 2]\n\n    # Normalize if required\n    if norm:\n        psd = psd / np.trapz(psd)\n\n    return psd\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.average_diagonal","title":"<code>average_diagonal(mat)</code>","text":"<p>Average values over all offset diagonals of a 2D array.</p> <p>Parameters:</p> Name Type Description Default <code>mat</code> <code>ndarray</code> <p>2D array from which to compute the average values over diagonals.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>ndarray</code> <p>1D array containing the average values over all offset diagonals.</p> Notes <p>The method used for computing averages is based on the concept of accumulating values along each diagonal offset and then dividing by the number of elements in each diagonal.</p> Reference <p>https://stackoverflow.com/questions/71362928/average-values-over-all-offset-diagonals</p> Source code in <code>neuro_py/process/utils.py</code> <pre><code>def average_diagonal(mat: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Average values over all offset diagonals of a 2D array.\n\n    Parameters\n    ----------\n    mat : np.ndarray\n        2D array from which to compute the average values over diagonals.\n\n    Returns\n    -------\n    output : np.ndarray\n        1D array containing the average values over all offset diagonals.\n\n    Notes\n    -----\n    The method used for computing averages is based on the concept of\n    accumulating values along each diagonal offset and then dividing by\n    the number of elements in each diagonal.\n\n    Reference\n    ---------\n    https://stackoverflow.com/questions/71362928/average-values-over-all-offset-diagonals\n    \"\"\"\n    n = mat.shape[0]\n    output = np.zeros(n * 2 - 1, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        output[i : i + n] += mat[n - 1 - i]\n    output[0:n] /= np.arange(1, n + 1, 1, dtype=np.float64)\n    output[n:] /= np.arange(n - 1, 0, -1, dtype=np.float64)\n    return output\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.circular_shift","title":"<code>circular_shift(m, s)</code>","text":"<p>Circularly shift matrix rows or columns by specified amounts.</p> <p>Each matrix row (or column) is circularly shifted by a different amount.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>ndarray</code> <p>Matrix to rotate. Should be a 2D array.</p> required <code>s</code> <code>ndarray</code> <p>Shift amounts for each row (horizontal vector) or column (vertical vector). Should be a 1D array.</p> required <p>Returns:</p> Name Type Description <code>shifted</code> <code>ndarray</code> <p>Matrix <code>m</code> with rows (or columns) circularly shifted by the amounts in <code>s</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>s</code> is not a vector of integers or if <code>m</code> is not a 2D matrix. If the sizes of <code>m</code> and <code>s</code> are incompatible.</p> Notes <p>This function is adapted from CircularShift.m, Copyright (C) 2012 by Micha\u00ebl Zugaro.</p> Source code in <code>neuro_py/process/utils.py</code> <pre><code>def circular_shift(m: np.ndarray, s: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Circularly shift matrix rows or columns by specified amounts.\n\n    Each matrix row (or column) is circularly shifted by a different amount.\n\n    Parameters\n    ----------\n    m : np.ndarray\n        Matrix to rotate. Should be a 2D array.\n    s : np.ndarray\n        Shift amounts for each row (horizontal vector) or column (vertical vector).\n        Should be a 1D array.\n\n    Returns\n    -------\n    shifted : np.ndarray\n        Matrix `m` with rows (or columns) circularly shifted by the amounts in `s`.\n\n    Raises\n    ------\n    ValueError\n        If `s` is not a vector of integers or if `m` is not a 2D matrix.\n        If the sizes of `m` and `s` are incompatible.\n\n    Notes\n    -----\n    This function is adapted from CircularShift.m, Copyright (C) 2012 by Micha\u00ebl Zugaro.\n    \"\"\"\n    # Check number of parameters\n    if len(s.shape) != 1:\n        raise ValueError(\"Second parameter is not a vector of integers.\")\n    if len(m.shape) != 2:\n        raise ValueError(\"First parameter is not a 2D matrix.\")\n\n    mm, nm = m.shape\n    # if s is 1d array, add dimension\n    if len(s.shape) == 1:\n        s = s[np.newaxis, :]\n    ms, ns = s.shape\n\n    # Check parameter sizes\n    if mm != ms and nm != ns:\n        raise ValueError(\"Incompatible parameter sizes.\")\n\n    # The algorithm below works along columns; transpose if necessary\n    s = -np.ravel(s)\n    if ns == 1:\n        m = m.T\n        mm, nm = m.shape\n\n    # Shift matrix S, where Sij is the vertical shift for element ij\n    shift = np.tile(s, (mm, 1))\n\n    # Before we start, each element Mij has a linear index Aij.\n    # After circularly shifting the rows, it will have a linear index Bij.\n    # We now construct Bij.\n\n    # First, create matrix C where each item Cij = i (row number)\n    lines = np.tile(np.arange(mm)[:, np.newaxis], (1, nm))\n    # Next, update C so that Cij becomes the target row number (after circular shift)\n    lines = np.mod(lines + shift, mm)\n    # lines[lines == 0] = mm\n    # Finally, transform Cij into a linear index, yielding Bij\n    indices = lines + np.tile(np.arange(nm) * mm, (mm, 1))\n\n    # Circular shift (reshape so that it is not transformed into a vector)\n    shifted = m.ravel()[(indices.flatten() - 1).astype(int)].reshape(mm, nm)\n\n    # flip matrix right to left\n    # shifted = np.fliplr(shifted)\n\n    shifted = np.flipud(shifted)\n\n    # Transpose back if necessary\n    if ns == 1:\n        shifted = shifted.T\n\n    return shifted\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.compute_AutoCorrs","title":"<code>compute_AutoCorrs(spks, binsize=0.001, nbins=100)</code>","text":"<p>Compute autocorrelations for spike trains.</p> <p>Parameters:</p> Name Type Description Default <code>spks</code> <code>ndarray</code> <p>Nested ndarrays where each array contains the spike times for one neuron.</p> required <code>binsize</code> <code>float</code> <p>The size of each bin in seconds, by default 0.001 (1 ms).</p> <code>0.001</code> <code>nbins</code> <code>int</code> <p>The number of bins for the autocorrelation, by default 100.</p> <code>100</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame where each column represents the autocorrelation of the corresponding neuron. The index is the time lag, and the values are the autocorrelations.</p> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def compute_AutoCorrs(\n    spks: np.ndarray, binsize: float = 0.001, nbins: int = 100\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute autocorrelations for spike trains.\n\n    Parameters\n    ----------\n    spks : np.ndarray\n        Nested ndarrays where each array contains the spike times for one neuron.\n    binsize : float, optional\n        The size of each bin in seconds, by default 0.001 (1 ms).\n    nbins : int, optional\n        The number of bins for the autocorrelation, by default 100.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame where each column represents the autocorrelation of the corresponding neuron.\n        The index is the time lag, and the values are the autocorrelations.\n    \"\"\"\n    # First let's prepare a pandas dataframe to receive the data\n    times = np.arange(0, binsize * (nbins + 1), binsize) - (nbins * binsize) / 2\n    autocorrs = pd.DataFrame(index=times, columns=np.arange(len(spks)))\n\n    # Now we can iterate over the dictionnary of spikes\n    for i, s in enumerate(spks):\n        if len(s) == 0:\n            continue\n        # Calling the crossCorr function\n        autocorrs[i] = crossCorr(s, s, binsize, nbins)\n\n    # And don't forget to replace the 0 ms for 0\n    autocorrs.loc[0] = 0.0\n    return autocorrs\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.compute_cross_correlogram","title":"<code>compute_cross_correlogram(X, dt=1.0, window=0.5)</code>","text":"<p>Compute pairwise cross-correlograms between signals in an array.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>N-dimensional array of shape (n_signals, n_timepoints) representing the signals.</p> required <code>dt</code> <code>float</code> <p>Time step between samples in seconds, default is 1.0.</p> <code>1.0</code> <code>window</code> <code>float</code> <p>Window size in seconds for the cross-correlogram. The output will include values within +/- window from the center. If None, returns the full correlogram.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pairwise cross-correlogram with time lags as the index and signal pairs as columns.</p> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def compute_cross_correlogram(\n    X: np.ndarray, dt: float = 1.0, window: float = 0.5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute pairwise cross-correlograms between signals in an array.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        N-dimensional array of shape (n_signals, n_timepoints) representing the signals.\n    dt : float, optional\n        Time step between samples in seconds, default is 1.0.\n    window : float, optional\n        Window size in seconds for the cross-correlogram. The output will include values\n        within +/- window from the center. If None, returns the full correlogram.\n\n    Returns\n    -------\n    pd.DataFrame\n        Pairwise cross-correlogram with time lags as the index and signal pairs as columns.\n    \"\"\"\n\n    crosscorrs = {}\n    pairs = list(itertools.combinations(np.arange(X.shape[0]), 2))\n    for i, j in pairs:\n        auc = signal.correlate(X[i], X[j])\n        times = signal.correlation_lags(len(X[i]), len(X[j])) * dt\n        # normalize by coeff\n        normalizer = np.sqrt((X[i] ** 2).sum(axis=0) * (X[j] ** 2).sum(axis=0))\n        auc /= normalizer\n\n        crosscorrs[(i, j)] = pd.Series(index=times, data=auc, dtype=\"float32\")\n    crosscorrs = pd.DataFrame.from_dict(crosscorrs)\n\n    if window is None:\n        return crosscorrs\n    else:\n        return crosscorrs[(crosscorrs.index &gt;= -window) &amp; (crosscorrs.index &lt;= window)]\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.compute_image_spread","title":"<code>compute_image_spread(X, exponent=2, normalize=True)</code>","text":"<p>Compute the spread of an image using the square root of a weighted moment.</p> <p>The spread is calculated as the square root of a weighted moment of the image, where the weights are derived from the deviations of each pixel from the center of mass (COM) of the image.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D numpy array of shape (numBinsY, numBinsX). If <code>normalize</code> is True, the input is assumed to represent a probability distribution.</p> required <code>exponent</code> <code>float</code> <p>The exponent used in the moment calculation. Default is 2.</p> <code>2</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the input array so that its sum is 1. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>spread</code> <code>float</code> <p>The computed spread, defined as the square root of the weighted moment.</p> <code>image_moment</code> <code>float</code> <p>The raw weighted moment of the image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n&gt;&gt;&gt; spread, image_moment = compute_image_spread(X, exponent=2)\n&gt;&gt;&gt; print(spread)\n0.5704157028642128\n&gt;&gt;&gt; print(image_moment)\n0.325374074074074\n</code></pre> References <p>Widloski &amp; Foster, 2022</p> Source code in <code>neuro_py/process/utils.py</code> <pre><code>def compute_image_spread(\n    X: np.ndarray, exponent: float = 2, normalize: bool = True\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Compute the spread of an image using the square root of a weighted moment.\n\n    The spread is calculated as the square root of a weighted moment of the image,\n    where the weights are derived from the deviations of each pixel from the\n    center of mass (COM) of the image.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        A 2D numpy array of shape (numBinsY, numBinsX). If `normalize` is True,\n        the input is assumed to represent a probability distribution.\n    exponent : float, optional\n        The exponent used in the moment calculation. Default is 2.\n    normalize : bool, optional\n        If True, normalize the input array so that its sum is 1. Default is True.\n\n    Returns\n    -------\n    spread : float\n        The computed spread, defined as the square root of the weighted moment.\n    image_moment : float\n        The raw weighted moment of the image.\n\n    Examples\n    --------\n    &gt;&gt;&gt; X = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n    &gt;&gt;&gt; spread, image_moment = compute_image_spread(X, exponent=2)\n    &gt;&gt;&gt; print(spread)\n    0.5704157028642128\n    &gt;&gt;&gt; print(image_moment)\n    0.325374074074074\n\n    References\n    ----------\n    Widloski &amp; Foster, 2022\n    \"\"\"\n    if np.allclose(X, 0):\n        return np.nan, np.nan  # Return NaN if the input is all zero\n\n    if normalize:\n        X = X / np.nansum(X)  # Normalize the input\n\n    numBinsY, numBinsX = X.shape\n\n    # Compute center of mass (COM) for the X (columns) direction.\n    cols = np.arange(1, numBinsX + 1)\n    sumX = np.nansum(X, axis=0)  # sum over rows, shape: (numBinsX,)\n    totalX = np.nansum(sumX)\n    # Add a small correction term\n    comX = np.nansum(sumX * cols) / totalX + 0.5 / numBinsX\n\n    # Compute center of mass for the Y (rows) direction.\n    rows = np.arange(1, numBinsY + 1)\n    sumY = np.nansum(X, axis=1)  # sum over columns, shape: (numBinsY,)\n    totalY = np.nansum(sumY)\n    comY = np.nansum(sumY * rows) / totalY + 0.5 / numBinsY\n\n    # Create a meshgrid for the bin indices (using 1-indexing like MATLAB)\n    XX, YY = np.meshgrid(np.arange(1, numBinsX + 1), np.arange(1, numBinsY + 1))\n\n    # Compute the weighted moment using the product of the deviations raised to the given exponent.\n    # For each bin, we compute:\n    #     |XX - comX|^exponent * |YY - comY|^exponent * X(i,j)\n    moment = np.nansum(\n        (np.abs(XX - comX) ** exponent) * (np.abs(YY - comY) ** exponent) * X\n    )\n\n    # Normalize by the total probability.\n    image_moment = moment / np.nansum(X)\n\n    # The spread is the square root of the image moment.\n    spread = np.sqrt(image_moment)\n\n    return spread, image_moment\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.compute_psth","title":"<code>compute_psth(spikes, event, bin_width=0.002, n_bins=100, window=None)</code>","text":"<p>Compute the Peri-Stimulus Time Histogram (PSTH) from spike trains.</p> <p>This function calculates the PSTH for a given set of spike times aligned to specific events. The PSTH provides a histogram of spike counts in response to the events over a defined time window.</p> <p>Parameters:</p> Name Type Description Default <code>spikes</code> <code>ndarray</code> <p>An array of spike times for multiple trials, with each trial in a separate row.</p> required <code>event</code> <code>ndarray</code> <p>An array of event times to which the spikes are aligned.</p> required <code>bin_width</code> <code>float</code> <p>Width of each time bin in seconds (default is 0.002 seconds).</p> <code>0.002</code> <code>n_bins</code> <code>int</code> <p>Number of bins to create for the histogram (default is 100).</p> <code>100</code> <code>window</code> <code>list</code> <p>Time window around each event to consider for the PSTH. If None, a symmetric window is created based on <code>n_bins</code> and <code>bin_width</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing the PSTH, indexed by time bins and columns representing each trial's PSTH.</p> Notes <p>If the specified window is not symmetric around 0, it is adjusted to be symmetric.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spikes = np.array([[0.1, 0.15, 0.2], [0.1, 0.12, 0.13]])\n&gt;&gt;&gt; event = np.array([0.1, 0.3])\n&gt;&gt;&gt; psth = compute_psth(spikes, event)\n</code></pre> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def compute_psth(\n    spikes: np.ndarray,\n    event: np.ndarray,\n    bin_width: float = 0.002,\n    n_bins: int = 100,\n    window: list = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the Peri-Stimulus Time Histogram (PSTH) from spike trains.\n\n    This function calculates the PSTH for a given set of spike times aligned to specific events.\n    The PSTH provides a histogram of spike counts in response to the events over a defined time window.\n\n    Parameters\n    ----------\n    spikes : np.ndarray\n        An array of spike times for multiple trials, with each trial in a separate row.\n    event : np.ndarray\n        An array of event times to which the spikes are aligned.\n    bin_width : float, optional\n        Width of each time bin in seconds (default is 0.002 seconds).\n    n_bins : int, optional\n        Number of bins to create for the histogram (default is 100).\n    window : list, optional\n        Time window around each event to consider for the PSTH. If None, a symmetric window is created based on `n_bins` and `bin_width`.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing the PSTH, indexed by time bins and columns representing each trial's PSTH.\n\n    Notes\n    -----\n    If the specified window is not symmetric around 0, it is adjusted to be symmetric.\n\n    Examples\n    -------\n    &gt;&gt;&gt; spikes = np.array([[0.1, 0.15, 0.2], [0.1, 0.12, 0.13]])\n    &gt;&gt;&gt; event = np.array([0.1, 0.3])\n    &gt;&gt;&gt; psth = compute_psth(spikes, event)\n    \"\"\"\n    if window is not None:\n        window_original = None\n        # check if window is symmetric around 0, if not make it so\n        if ((window[1] - window[0]) / 2 != window[1]) | (\n            (window[1] - window[0]) / -2 != window[0]\n        ):\n            window_original = np.array(window)\n            window = [-np.max(np.abs(window)), np.max(np.abs(window))]\n\n        times = np.arange(window[0], window[1] + bin_width / 2, bin_width)\n        n_bins = len(times) - 1\n    else:\n        times = np.linspace(\n            -(n_bins * bin_width) / 2, (n_bins * bin_width) / 2, n_bins + 1\n        )\n\n    ccg = pd.DataFrame(index=times, columns=np.arange(len(spikes)))\n    # Now we can iterate over spikes\n    for i, s in enumerate(spikes):\n        ccg[i] = crossCorr(event, s, bin_width, n_bins)\n\n    # if window was not symmetric, remove the extra bins\n    if window is not None:\n        if window_original is not None:\n            ccg = ccg.loc[window_original[0] : window_original[1], :]\n    return ccg\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.corrcc","title":"<code>corrcc(alpha1, alpha2, axis=None)</code>","text":"<p>Circular correlation coefficient for two circular random variables.</p> <p>Parameters:</p> Name Type Description Default <code>alpha1</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>alpha2</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>axis</code> <code>Optional[int]</code> <p>The axis along which to compute the correlation coefficient. If None, compute over the entire array (default is None).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rho</code> <code>float</code> <p>Circular-circular correlation coefficient.</p> <code>pval</code> <code>float</code> <p>p-value for testing the significance of the correlation coefficient.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; alpha1 = np.array([0.1, 0.2, 0.4, 0.5])\n&gt;&gt;&gt; alpha2 = np.array([0.3, 0.6, 0.2, 0.8])\n&gt;&gt;&gt; rho, pval = corrcc(alpha1, alpha2)\n&gt;&gt;&gt; print(f\"Circular correlation: {rho}, p-value: {pval}\")\n</code></pre> Notes <p>The function computes the correlation between two sets of angles using a method that adjusts for circular data. The significance of the correlation coefficient is tested using the fact that the test statistic is approximately normally distributed.</p> References <p>Jammalamadaka et al (2001)</p> <p>Original code: https://github.com/circstat/pycircstat Modified by: Salman Qasim, 11/12/2018</p> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def corrcc(\n    alpha1: np.ndarray, alpha2: np.ndarray, axis: Optional[int] = None\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Circular correlation coefficient for two circular random variables.\n\n    Parameters\n    ----------\n    alpha1 : np.ndarray\n        Sample of angles in radians.\n    alpha2 : np.ndarray\n        Sample of angles in radians.\n    axis : Optional[int], optional\n        The axis along which to compute the correlation coefficient.\n        If None, compute over the entire array (default is None).\n\n    Returns\n    -------\n    rho : float\n        Circular-circular correlation coefficient.\n    pval : float\n        p-value for testing the significance of the correlation coefficient.\n\n    Examples\n    --------\n    &gt;&gt;&gt; alpha1 = np.array([0.1, 0.2, 0.4, 0.5])\n    &gt;&gt;&gt; alpha2 = np.array([0.3, 0.6, 0.2, 0.8])\n    &gt;&gt;&gt; rho, pval = corrcc(alpha1, alpha2)\n    &gt;&gt;&gt; print(f\"Circular correlation: {rho}, p-value: {pval}\")\n\n    Notes\n    -----\n    The function computes the correlation between two sets of angles using a\n    method that adjusts for circular data. The significance of the correlation\n    coefficient is tested using the fact that the test statistic is approximately\n    normally distributed.\n\n    References\n    ----------\n    Jammalamadaka et al (2001)\n\n    Original code: https://github.com/circstat/pycircstat\n    Modified by: Salman Qasim, 11/12/2018\n    \"\"\"\n    assert alpha1.shape == alpha2.shape, \"Input dimensions do not match.\"\n\n    n = len(alpha1)\n\n    # center data on circular mean\n    alpha1_centered, alpha2_centered = pcs.center(alpha1, alpha2, axis=axis)\n\n    num = np.sum(np.sin(alpha1_centered) * np.sin(alpha2_centered), axis=axis)\n    den = np.sqrt(\n        np.sum(np.sin(alpha1_centered) ** 2, axis=axis)\n        * np.sum(np.sin(alpha2_centered) ** 2, axis=axis)\n    )\n    # compute correlation coefficient from p. 176\n    rho = num / den\n\n    # Modification:\n    # significance of this correlation coefficient can be tested using the fact that Z is approx. normal\n\n    l20 = np.mean(np.sin(alpha1_centered) ** 2)\n    l02 = np.mean(np.sin(alpha2_centered) ** 2)\n    l22 = np.mean((np.sin(alpha1_centered) ** 2) * (np.sin(alpha2_centered) ** 2))\n    z = np.sqrt((n * l20 * l02) / l22) * rho\n    pval = 2 * (1 - sp.stats.norm.cdf(np.abs(z)))  # two-sided test\n\n    return rho, pval\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.corrcc_uniform","title":"<code>corrcc_uniform(alpha1, alpha2, axis=None)</code>","text":"<p>Circular correlation coefficient for two circular random variables. Use this function if at least one of the variables may follow a uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>alpha1</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>alpha2</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>axis</code> <code>Optional[int]</code> <p>The axis along which to compute the correlation coefficient. If None, compute over the entire array (default is None).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rho</code> <code>float</code> <p>Circular-circular correlation coefficient.</p> <code>pval</code> <code>float</code> <p>p-value for testing the significance of the correlation coefficient.</p> Notes <p>This method accounts for cases where one or both of the circular variables may follow a uniform distribution. The significance of the correlation coefficient is tested using a normal approximation of the Z statistic.</p> References <p>Jammalamadaka, et al (2001).</p> <p>Original code: https://github.com/circstat/pycircstat Modified by: Salman Qasim, 11/12/2018 https://github.com/HoniSanders/measure_phaseprec/blob/master/cl_corr.m</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; alpha1 = np.array([0.1, 0.2, 0.4, 0.5])\n&gt;&gt;&gt; alpha2 = np.array([0.3, 0.6, 0.2, 0.8])\n&gt;&gt;&gt; rho, pval = corrcc_uniform(alpha1, alpha2)\n&gt;&gt;&gt; print(f\"Circular correlation: {rho}, p-value: {pval}\")\n</code></pre> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def corrcc_uniform(\n    alpha1: np.ndarray, alpha2: np.ndarray, axis: Optional[int] = None\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Circular correlation coefficient for two circular random variables.\n    Use this function if at least one of the variables may follow a uniform distribution.\n\n    Parameters\n    ----------\n    alpha1 : np.ndarray\n        Sample of angles in radians.\n    alpha2 : np.ndarray\n        Sample of angles in radians.\n    axis : Optional[int], optional\n        The axis along which to compute the correlation coefficient.\n        If None, compute over the entire array (default is None).\n\n    Returns\n    -------\n    rho : float\n        Circular-circular correlation coefficient.\n    pval : float\n        p-value for testing the significance of the correlation coefficient.\n\n    Notes\n    -----\n    This method accounts for cases where one or both of the circular variables\n    may follow a uniform distribution. The significance of the correlation coefficient\n    is tested using a normal approximation of the Z statistic.\n\n    References\n    ----------\n    Jammalamadaka, et al (2001).\n\n    Original code: https://github.com/circstat/pycircstat\n    Modified by: Salman Qasim, 11/12/2018\n    https://github.com/HoniSanders/measure_phaseprec/blob/master/cl_corr.m\n\n    Examples\n    --------\n    &gt;&gt;&gt; alpha1 = np.array([0.1, 0.2, 0.4, 0.5])\n    &gt;&gt;&gt; alpha2 = np.array([0.3, 0.6, 0.2, 0.8])\n    &gt;&gt;&gt; rho, pval = corrcc_uniform(alpha1, alpha2)\n    &gt;&gt;&gt; print(f\"Circular correlation: {rho}, p-value: {pval}\")\n    \"\"\"\n\n    assert alpha1.shape == alpha2.shape, \"Input dimensions do not match.\"\n\n    n = len(alpha1)\n\n    # center data on circular mean\n    alpha1_centered, alpha2_centered = pcs.center(alpha1, alpha2, axis=axis)\n\n    # One of the sample means is not well defined due to uniform distribution of data\n    # so take the difference of the resultant vector length for the sum and difference\n    # of the alphas\n    num = pcs.resultant_vector_length(alpha1 - alpha2) - pcs.resultant_vector_length(\n        alpha1 + alpha2\n    )\n    den = 2 * np.sqrt(\n        np.sum(np.sin(alpha1_centered) ** 2, axis=axis)\n        * np.sum(np.sin(alpha2_centered) ** 2, axis=axis)\n    )\n    rho = n * num / den\n    # significance of this correlation coefficient can be tested using the fact that Z\n    # is approx. normal\n\n    l20 = np.mean(np.sin(alpha1_centered) ** 2)\n    l02 = np.mean(np.sin(alpha2_centered) ** 2)\n    l22 = np.mean((np.sin(alpha1_centered) ** 2) * (np.sin(alpha2_centered) ** 2))\n    z = np.sqrt((n * l20 * l02) / l22) * rho\n    pval = 2 * (1 - sp.stats.norm.cdf(np.abs(z)))  # two-sided test\n\n    return rho, pval\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.count_events","title":"<code>count_events(events, time_ref, time_range)</code>","text":"<p>Count the number of events that occur within a given time range after each reference event.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>ndarray</code> <p>A 1D array of event times.</p> required <code>time_ref</code> <code>ndarray</code> <p>A 1D array of reference times.</p> required <code>time_range</code> <code>tuple of (float, float)</code> <p>A tuple containing the start and end times of the time range.</p> required <p>Returns:</p> Name Type Description <code>counts</code> <code>ndarray</code> <p>A 1D array of event counts, one for each reference time (same length as time_ref).</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def count_events(\n    events: np.ndarray, time_ref: np.ndarray, time_range: Tuple[float, float]\n) -&gt; np.ndarray:\n    \"\"\"\n    Count the number of events that occur within a given time range after each reference event.\n\n    Parameters\n    ----------\n    events : np.ndarray\n        A 1D array of event times.\n    time_ref : np.ndarray\n        A 1D array of reference times.\n    time_range : tuple of (float, float)\n        A tuple containing the start and end times of the time range.\n\n    Returns\n    -------\n    counts : np.ndarray\n        A 1D array of event counts, one for each reference time (same length as time_ref).\n    \"\"\"\n    # Initialize an array to store the event counts\n    counts = np.zeros_like(time_ref)\n\n    # Iterate over the reference times\n    for i, r in enumerate(time_ref):\n        # Check if any events occur within the time range\n        idx = (events &gt; r + time_range[0]) &amp; (events &lt; r + time_range[1])\n        # Increment the event count if any events are found\n        counts[i] = len(events[idx])\n\n    return counts\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.count_in_interval","title":"<code>count_in_interval(st, event_starts, event_stops, par_type='counts')</code>","text":"<p>Count timestamps in specified intervals and return a matrix where each column represents the counts for each spike train over given event epochs.</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>ndarray</code> <p>A 1D array where each element is a spike train for a unit.</p> required <code>event_starts</code> <code>ndarray</code> <p>A 1D array containing the start times of events.</p> required <code>event_stops</code> <code>ndarray</code> <p>A 1D array containing the stop times of events.</p> required <code>par_type</code> <code>str</code> <p>The type of count calculation to perform: - 'counts': returns raw counts of spikes in the intervals. - 'binary': returns a binary matrix indicating presence (1) or absence (0) of spikes. - 'firing_rate': returns the firing rate calculated as counts divided by the interval duration. Defaults to 'binary'.</p> <code>'counts'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D array (n units x n epochs) where each column shows the counts (or binary values or firing rates) per unit for each epoch.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def count_in_interval(\n    st: np.ndarray,\n    event_starts: np.ndarray,\n    event_stops: np.ndarray,\n    par_type: str = \"counts\",\n) -&gt; np.ndarray:\n    \"\"\"\n    Count timestamps in specified intervals and return a matrix where each\n    column represents the counts for each spike train over given event epochs.\n\n    Parameters\n    ----------\n    st : np.ndarray\n        A 1D array where each element is a spike train for a unit.\n\n    event_starts : np.ndarray\n        A 1D array containing the start times of events.\n\n    event_stops : np.ndarray\n        A 1D array containing the stop times of events.\n\n    par_type : str, optional\n        The type of count calculation to perform:\n        - 'counts': returns raw counts of spikes in the intervals.\n        - 'binary': returns a binary matrix indicating presence (1) or absence (0) of spikes.\n        - 'firing_rate': returns the firing rate calculated as counts divided by the interval duration.\n        Defaults to 'binary'.\n\n    Returns\n    -------\n    np.ndarray\n        A 2D array (n units x n epochs) where each column shows the counts (or binary values or firing rates)\n        per unit for each epoch.\n    \"\"\"\n    # convert to numpy array\n    event_starts, event_stops = np.array(event_starts), np.array(event_stops)\n\n    # initialize matrix\n    unit_mat = np.zeros((len(st), (len(event_starts))))\n\n    # loop over units and bin spikes into epochs\n    for i, s in enumerate(st):\n        idx1 = np.searchsorted(s, event_starts, \"right\")\n        idx2 = np.searchsorted(s, event_stops, \"left\")\n        unit_mat[i, :] = idx2 - idx1\n\n    par_type_funcs = {\n        \"counts\": lambda x: x,\n        \"binary\": lambda x: (x &gt; 0) * 1,\n        \"firing_rate\": lambda x: x / (event_stops - event_starts),\n    }\n    calc_func = par_type_funcs[par_type]\n    unit_mat = calc_func(unit_mat)\n\n    return unit_mat\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.crossCorr","title":"<code>crossCorr(t1, t2, binsize, nbins)</code>","text":"<p>Perform the discrete cross-correlogram of two time series.</p> <p>This function calculates the firing rate of the series 't2' relative to the timings of 't1'. The units should be in seconds for all arguments.</p> <p>Parameters:</p> Name Type Description Default <code>t1</code> <code>ndarray</code> <p>First time series.</p> required <code>t2</code> <code>ndarray</code> <p>Second time series.</p> required <code>binsize</code> <code>float</code> <p>Size of the bin in seconds.</p> required <code>nbins</code> <code>int</code> <p>Number of bins.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Cross-correlogram of the two time series.</p> Notes <p>This implementation is based on the work of Guillaume Viejo. References: - https://github.com/PeyracheLab/StarterPack/blob/master/python/main6_autocorr.py - https://github.com/pynapple-org/pynapple/blob/main/pynapple/process/correlograms.py</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>@jit(nopython=True)\ndef crossCorr(\n    t1: np.ndarray,\n    t2: np.ndarray,\n    binsize: float,\n    nbins: int,\n) -&gt; np.ndarray:\n    \"\"\"\n    Perform the discrete cross-correlogram of two time series.\n\n    This function calculates the firing rate of the series 't2' relative to the timings of 't1'.\n    The units should be in seconds for all arguments.\n\n    Parameters\n    ----------\n    t1 : np.ndarray\n        First time series.\n    t2 : np.ndarray\n        Second time series.\n    binsize : float\n        Size of the bin in seconds.\n    nbins : int\n        Number of bins.\n\n    Returns\n    -------\n    np.ndarray\n        Cross-correlogram of the two time series.\n\n    Notes\n    -----\n    This implementation is based on the work of Guillaume Viejo.\n    References:\n    - https://github.com/PeyracheLab/StarterPack/blob/master/python/main6_autocorr.py\n    - https://github.com/pynapple-org/pynapple/blob/main/pynapple/process/correlograms.py\n    \"\"\"\n    # Calculate the length of the input time series\n    nt1 = len(t1)\n    nt2 = len(t2)\n\n    # Ensure that 'nbins' is an odd number\n    if np.floor(nbins / 2) * 2 == nbins:\n        nbins = nbins + 1\n\n    # Calculate the half-width of the cross-correlogram window\n    w = (nbins / 2) * binsize\n    C = np.zeros(nbins)\n    i2 = 1\n\n    # Iterate through the first time series\n    for i1 in range(nt1):\n        lbound = t1[i1] - w\n\n        # Find the index of the first element in 't2' that is within 'lbound'\n        while i2 &lt; nt2 and t2[i2] &lt; lbound:\n            i2 = i2 + 1\n\n        # Find the index of the last element in 't2' that is within 'lbound'\n        while i2 &gt; 1 and t2[i2 - 1] &gt; lbound:\n            i2 = i2 - 1\n\n        rbound = lbound\n        last_index = i2\n\n        # Calculate the cross-correlogram values for each bin\n        for j in range(nbins):\n            k = 0\n            rbound = rbound + binsize\n\n            # Count the number of elements in 't2' that fall within the bin\n            while last_index &lt; nt2 and t2[last_index] &lt; rbound:\n                last_index = last_index + 1\n                k = k + 1\n\n            C[j] += k\n\n    # Normalize the cross-correlogram by dividing by the total observation time and bin size\n    C = C / (nt1 * binsize)\n\n    return C\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.decode_file_path","title":"<code>decode_file_path(save_file)</code>","text":"<p>Decode an encoded file path to retrieve the original session path.</p> <p>Parameters:</p> Name Type Description Default <code>save_file</code> <code>str</code> <p>Encoded file path that includes the original session path.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Original session path before encoding.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; save_file = r\"Z:\\home\\ryanh\\projects\\ripple_heterogeneity\\replay_02_17_23\\Z---___Data___AYAold___AB3___AB3_38_41.pkl\"\n&gt;&gt;&gt; decode_file_path(save_file)\n\"Z:\\Data\\AYAold\\AB3\\AB3_38_41\"\n</code></pre> Source code in <code>neuro_py/process/batch_analysis.py</code> <pre><code>def decode_file_path(save_file: str) -&gt; str:\n    \"\"\"\n    Decode an encoded file path to retrieve the original session path.\n\n    Parameters\n    ----------\n    save_file : str\n        Encoded file path that includes the original session path.\n\n    Returns\n    -------\n    str\n        Original session path before encoding.\n\n    Examples\n    -------\n    &gt;&gt;&gt; save_file = r\"Z:\\\\home\\\\ryanh\\\\projects\\\\ripple_heterogeneity\\\\replay_02_17_23\\\\Z---___Data___AYAold___AB3___AB3_38_41.pkl\"\n    &gt;&gt;&gt; decode_file_path(save_file)\n    \"Z:\\\\Data\\\\AYAold\\\\AB3\\\\AB3_38_41\"\n    \"\"\"\n\n    # get basepath from save_file\n    basepath = os.path.basename(save_file).replace(\"___\", os.sep).replace(\"---\", \":\")\n    # also remove file extension\n    basepath = os.path.splitext(basepath)[0]\n\n    return basepath\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.deconvolve_peth","title":"<code>deconvolve_peth(signal, events, bin_width=0.002, n_bins=100)</code>","text":"<p>Perform deconvolution of a peri-event time histogram (PETH) signal.</p> <p>This function calculates the deconvolved signal based on the input signal and events.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>ndarray</code> <p>An array representing the discrete events.</p> required <code>events</code> <code>ndarray</code> <p>An array representing the discrete events.</p> required <code>bin_width</code> <code>float</code> <p>The width of a time bin in seconds (default is 0.002 seconds).</p> <code>0.002</code> <code>n_bins</code> <code>int</code> <p>The number of bins to use in the PETH (default is 100 bins).</p> <code>100</code> <p>Returns:</p> Name Type Description <code>deconvolved</code> <code>ndarray</code> <p>An array representing the deconvolved signal.</p> <code>times</code> <code>ndarray</code> <p>An array representing the time points corresponding to the bins.</p> Notes <p>Based on DeconvolvePETH.m from https://github.com/ayalab1/neurocode/blob/master/spikes/DeconvolvePETH.m</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def deconvolve_peth(\n    signal: np.ndarray, events: np.ndarray, bin_width: float = 0.002, n_bins: int = 100\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Perform deconvolution of a peri-event time histogram (PETH) signal.\n\n    This function calculates the deconvolved signal based on the input signal and events.\n\n    Parameters\n    ----------\n    signal : np.ndarray\n        An array representing the discrete events.\n    events : np.ndarray\n        An array representing the discrete events.\n    bin_width : float, optional\n        The width of a time bin in seconds (default is 0.002 seconds).\n    n_bins : int, optional\n        The number of bins to use in the PETH (default is 100 bins).\n\n    Returns\n    -------\n    deconvolved : np.ndarray\n        An array representing the deconvolved signal.\n    times : np.ndarray\n        An array representing the time points corresponding to the bins.\n\n    Notes\n    -----\n    Based on DeconvolvePETH.m from https://github.com/ayalab1/neurocode/blob/master/spikes/DeconvolvePETH.m\n    \"\"\"\n\n    # calculate time lags for peth\n    times = np.linspace(-(n_bins * bin_width) / 2, (n_bins * bin_width) / 2, n_bins + 1)\n\n    # Calculate the autocorrelogram of the signal and the PETH of the events and the signal\n    autocorrelogram = crossCorr(signal, signal, bin_width, n_bins * 2)\n    raw_peth = crossCorr(events, signal, bin_width, n_bins * 2)\n\n    # If raw_peth all zeros, return zeros\n    if not raw_peth.any():\n        return np.zeros(len(times)), times\n\n    # Subtract the mean value from the raw_peth\n    const = np.mean(raw_peth)\n    raw_peth = raw_peth - const\n\n    # Calculate the Toeplitz matrix using the autocorrelogram and\n    #   the cross-correlation of the autocorrelogram\n    T0 = toeplitz(\n        autocorrelogram,\n        np.hstack([autocorrelogram[0], np.zeros(len(autocorrelogram) - 1)]),\n    )\n    T = T0[n_bins:, : n_bins + 1]\n\n    # Calculate the deconvolved signal by solving a linear equation\n    deconvolved = np.linalg.solve(\n        T, raw_peth[int(n_bins / 2) : int(n_bins / 2 * 3 + 1)].T + const / len(events)\n    )\n\n    return deconvolved, times\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.dpsschk","title":"<code>dpsschk(tapers, N, Fs)</code>","text":"<p>Check and generate DPSS tapers.</p> <p>Parameters:</p> Name Type Description Default <code>tapers</code> <code>Union[ndarray, Tuple[float, int]]</code> <p>Input can be either an array representing [NW, K] or a tuple with the number of tapers and the maximum number of tapers.</p> required <code>N</code> <code>int</code> <p>Number of points for FFT.</p> required <code>Fs</code> <code>float</code> <p>Sampling frequency.</p> required <p>Returns:</p> Name Type Description <code>tapers</code> <code>ndarray</code> <p>Tapers matrix, shape [tapers, eigenvalues].</p> Notes <p>The function computes DPSS (Discrete Prolate Spheroidal Sequences) tapers and scales them by the square root of the sampling frequency.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def dpsschk(\n    tapers: Union[np.ndarray, Tuple[float, int]], N: int, Fs: float\n) -&gt; np.ndarray:\n    \"\"\"\n    Check and generate DPSS tapers.\n\n    Parameters\n    ----------\n    tapers : Union[np.ndarray, Tuple[float, int]]\n        Input can be either an array representing [NW, K] or a tuple with\n        the number of tapers and the maximum number of tapers.\n    N : int\n        Number of points for FFT.\n    Fs : float\n        Sampling frequency.\n\n    Returns\n    -------\n    tapers : np.ndarray\n        Tapers matrix, shape [tapers, eigenvalues].\n\n    Notes\n    -----\n    The function computes DPSS (Discrete Prolate Spheroidal Sequences) tapers\n    and scales them by the square root of the sampling frequency.\n    \"\"\"\n    tapers, eigs = dpss(N, NW=tapers[0], Kmax=tapers[1], sym=False, return_ratios=True)\n    tapers = tapers * np.sqrt(Fs)\n    tapers = tapers.T\n    return tapers\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.encode_file_path","title":"<code>encode_file_path(basepath, save_path)</code>","text":"<p>Encode file path to be used as a filename.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session to be encoded.</p> required <code>save_path</code> <code>str</code> <p>Directory where the encoded file will be saved.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Encoded file path suitable for use as a filename.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; basepath = r\"Z:\\Data\\AYAold\\AB3\\AB3_38_41\"\n&gt;&gt;&gt; save_path = r\"Z:\\home\\ryanh\\projects\\ripple_heterogeneity\\replay_02_17_23\"\n&gt;&gt;&gt; encode_file_path(basepath, save_path)\n\"Z:\\home\\ryanh\\projects\\ripple_heterogeneity\\replay_02_17_23\\Z---___Data___AYAold___AB3___AB3_38_41.pkl\"\n</code></pre> Source code in <code>neuro_py/process/batch_analysis.py</code> <pre><code>def encode_file_path(basepath: str, save_path: str) -&gt; str:\n    \"\"\"\n    Encode file path to be used as a filename.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session to be encoded.\n    save_path : str\n        Directory where the encoded file will be saved.\n\n    Returns\n    -------\n    str\n        Encoded file path suitable for use as a filename.\n\n    Examples\n    -------\n    &gt;&gt;&gt; basepath = r\"Z:\\\\Data\\\\AYAold\\\\AB3\\\\AB3_38_41\"\n    &gt;&gt;&gt; save_path = r\"Z:\\\\home\\\\ryanh\\\\projects\\\\ripple_heterogeneity\\\\replay_02_17_23\"\n    &gt;&gt;&gt; encode_file_path(basepath, save_path)\n    \"Z:\\\\home\\\\ryanh\\\\projects\\\\ripple_heterogeneity\\\\replay_02_17_23\\\\Z---___Data___AYAold___AB3___AB3_38_41.pkl\"\n    \"\"\"\n    # normalize paths\n    basepath = os.path.normpath(basepath)\n    save_path = os.path.normpath(save_path)\n    # encode file path with unlikely characters\n    save_file = os.path.join(\n        save_path, basepath.replace(os.sep, \"___\").replace(\":\", \"---\") + \".pkl\"\n    )\n    return save_file\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.event_spiking_threshold","title":"<code>event_spiking_threshold(spikes, events, window=[-0.5, 0.5], event_size=0.1, spiking_thres=0, binsize=0.01, sigma=0.02, min_units=6, show_fig=False)</code>","text":"<p>event_spiking_threshold: filter events based on spiking threshold</p> <p>Parameters:</p> Name Type Description Default <code>spikes</code> <code>SpikeTrainArray</code> <p>Spike train array of neurons.</p> required <code>events</code> <code>ndarray</code> <p>Event times in seconds.</p> required <code>window</code> <code>list of float</code> <p>Time window (in seconds) to compute event-triggered average, by default [-0.5, 0.5].</p> <code>[-0.5, 0.5]</code> <code>event_size</code> <code>float</code> <p>Time window (in seconds) around event to measure firing response, by default 0.1.</p> <code>0.1</code> <code>spiking_thres</code> <code>float</code> <p>Spiking threshold in z-score units, by default 0.</p> <code>0</code> <code>binsize</code> <code>float</code> <p>Bin size (in seconds) for time-binning the spike trains, by default 0.01.</p> <code>0.01</code> <code>sigma</code> <code>float</code> <p>Standard deviation (in seconds) for Gaussian smoothing of spike counts, by default 0.02.</p> <code>0.02</code> <code>min_units</code> <code>int</code> <p>Minimum number of units required to compute event-triggered average, by default 6.</p> <code>6</code> <code>show_fig</code> <code>bool</code> <p>If True, plots the figure of event-triggered spiking activity, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean array indicating valid events that meet the spiking threshold.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; basepath = r\"U:\\data\\hpc_ctx_project\\HP04\\day_32_20240430\"\n&gt;&gt;&gt; ripples = loading.load_ripples_events(basepath, return_epoch_array=False)\n&gt;&gt;&gt; st, cell_metrics = loading.load_spikes(\n        basepath,\n        brainRegion=\"CA1\",\n        support=nel.EpochArray([0, loading.load_epoch(basepath).iloc[-1].stopTime])\n    )\n&gt;&gt;&gt; idx = event_spiking_threshold(st, ripples.peaks.values, show_fig=True)\n&gt;&gt;&gt; print(f\"Number of valid ripples: {idx.sum()} out of {len(ripples)}\")\nNumber of valid ripples: 9244 out of 12655\n</code></pre> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def event_spiking_threshold(\n    spikes: SpikeTrainArray,\n    events: np.ndarray,\n    window: list = [-0.5, 0.5],\n    event_size: float = 0.1,\n    spiking_thres: float = 0,\n    binsize: float = 0.01,\n    sigma: float = 0.02,\n    min_units: int = 6,\n    show_fig: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    event_spiking_threshold: filter events based on spiking threshold\n\n    Parameters\n    ----------\n    spikes : nel.SpikeTrainArray\n        Spike train array of neurons.\n    events : np.ndarray\n        Event times in seconds.\n    window : list of float, optional\n        Time window (in seconds) to compute event-triggered average, by default [-0.5, 0.5].\n    event_size : float, optional\n        Time window (in seconds) around event to measure firing response, by default 0.1.\n    spiking_thres : float, optional\n        Spiking threshold in z-score units, by default 0.\n    binsize : float, optional\n        Bin size (in seconds) for time-binning the spike trains, by default 0.01.\n    sigma : float, optional\n        Standard deviation (in seconds) for Gaussian smoothing of spike counts, by default 0.02.\n    min_units : int, optional\n        Minimum number of units required to compute event-triggered average, by default 6.\n    show_fig : bool, optional\n        If True, plots the figure of event-triggered spiking activity, by default False.\n\n    Returns\n    -------\n    np.ndarray\n        Boolean array indicating valid events that meet the spiking threshold.\n\n    Examples\n    -------\n    &gt;&gt;&gt; basepath = r\"U:\\\\data\\\\hpc_ctx_project\\\\HP04\\\\day_32_20240430\"\n    &gt;&gt;&gt; ripples = loading.load_ripples_events(basepath, return_epoch_array=False)\n    &gt;&gt;&gt; st, cell_metrics = loading.load_spikes(\n            basepath,\n            brainRegion=\"CA1\",\n            support=nel.EpochArray([0, loading.load_epoch(basepath).iloc[-1].stopTime])\n        )\n    &gt;&gt;&gt; idx = event_spiking_threshold(st, ripples.peaks.values, show_fig=True)\n    &gt;&gt;&gt; print(f\"Number of valid ripples: {idx.sum()} out of {len(ripples)}\")\n    Number of valid ripples: 9244 out of 12655\n\n    \"\"\"\n\n    # check if there are enough units to compute a confident event triggered average\n    if spikes.n_active &lt; min_units:\n        return np.ones(len(events), dtype=bool)\n\n    # bin spikes\n    bst = spikes.bin(ds=binsize).smooth(sigma=sigma)\n    # sum over all neurons and zscore\n    bst = bst.data.sum(axis=0)\n    bst = (bst - bst.mean()) / bst.std()\n    # get event triggered average\n    avg_signal, time_lags = event_triggered_average_fast(\n        bst[np.newaxis, :],\n        events,\n        sampling_rate=int(1 / binsize),\n        window=window,\n        return_average=False,\n    )\n    # get the event response within the event size\n    idx = (time_lags &gt;= -event_size) &amp; (time_lags &lt;= event_size)\n    event_response = avg_signal[0, idx, :].mean(axis=0)\n\n    # get events that are above threshold\n    valid_events = event_response &gt; spiking_thres\n\n    if show_fig:\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        sorted_idx = np.argsort(event_response)\n\n        fig, ax = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n        ax[0].imshow(\n            avg_signal[0, :, sorted_idx],\n            aspect=\"auto\",\n            extent=[time_lags[0], time_lags[-1], 0, len(event_response)],\n            vmin=-2,\n            vmax=2,\n            origin=\"lower\",\n            interpolation=\"nearest\",\n        )\n        ax[0].axhline(\n            np.where(event_response[sorted_idx] &gt; spiking_thres)[0][0],\n            color=\"r\",\n            linestyle=\"--\",\n        )\n        ax[1].plot(event_response[sorted_idx], np.arange(len(event_response)))\n        ax[1].axvline(spiking_thres, color=\"r\", linestyle=\"--\")\n        ax[0].set_xlabel(\"Time from event (s)\")\n        ax[0].set_ylabel(\"Event index\")\n        ax[1].set_xlabel(\"Average response\")\n        ax[1].set_ylabel(\"Event index\")\n        sns.despine()\n\n    return valid_events\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.event_triggered_average","title":"<code>event_triggered_average(timestamps, signal, events, sampling_rate=None, window=[-0.5, 0.5], return_pandas=False)</code>","text":"<p>Calculates the spike-triggered averages of signals in a time window relative to the event times of corresponding events for multiple signals.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>ndarray</code> <p>A 1D array of timestamps corresponding to the signal samples.</p> required <code>signal</code> <code>ndarray</code> <p>A 2D array of shape (n_samples, n_signals) containing the signal values.</p> required <code>events</code> <code>Union[ndarray, List[ndarray]]</code> <p>One or more 1D arrays of event times. If a single array is provided, it will be multiplied n-fold to match the number of signals.</p> required <code>sampling_rate</code> <code>Union[float, None]</code> <p>The sampling rate of the signal. If not provided, it will be calculated based on the timestamps.</p> <code>None</code> <code>window</code> <code>List[float]</code> <p>A list containing two elements: the start and stop times relative to an event for the time interval of signal averaging. Default is [-0.5, 0.5].</p> <code>[-0.5, 0.5]</code> <code>return_pandas</code> <code>bool</code> <p>If True, return the result as a Pandas DataFrame. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple containing the event-triggered averages of the signals and the corresponding time lags.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m1 = assembly_reactivation.AssemblyReact(basepath=r\"Z:\\Data\\HMC2\\day5\")\n</code></pre> <pre><code>&gt;&gt;&gt; m1.load_data()\n&gt;&gt;&gt; m1.get_weights(epoch=m1.epochs[1])\n&gt;&gt;&gt; assembly_act = m1.get_assembly_act()\n</code></pre> <pre><code>&gt;&gt;&gt; peth_avg, time_lags = event_triggered_average(\n...    assembly_act.abscissa_vals, assembly_act.data.T, m1.ripples.starts, window=[-0.5, 0.5]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; plt.plot(time_lags,peth_avg)\n&gt;&gt;&gt; plt.show()\n</code></pre> Notes <p>The function is adapted from elephant.sta.spike_triggered_average to be used with ndarray.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def event_triggered_average(\n    timestamps: np.ndarray,\n    signal: np.ndarray,\n    events: Union[np.ndarray, List[np.ndarray]],\n    sampling_rate: Union[float, None] = None,\n    window: List[float] = [-0.5, 0.5],\n    return_pandas: bool = False,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculates the spike-triggered averages of signals in a time window\n    relative to the event times of corresponding events for multiple signals.\n\n    Parameters\n    ----------\n    timestamps : np.ndarray\n        A 1D array of timestamps corresponding to the signal samples.\n\n    signal : np.ndarray\n        A 2D array of shape (n_samples, n_signals) containing the signal values.\n\n    events : Union[np.ndarray, List[np.ndarray]]\n        One or more 1D arrays of event times. If a single array is provided,\n        it will be multiplied n-fold to match the number of signals.\n\n    sampling_rate : Union[float, None], optional\n        The sampling rate of the signal. If not provided, it will be calculated\n        based on the timestamps.\n\n    window : List[float], optional\n        A list containing two elements: the start and stop times relative to an event\n        for the time interval of signal averaging. Default is [-0.5, 0.5].\n\n    return_pandas : bool, optional\n        If True, return the result as a Pandas DataFrame. Default is False.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing the event-triggered averages of the signals and the\n        corresponding time lags.\n\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; m1 = assembly_reactivation.AssemblyReact(basepath=r\"Z:\\\\Data\\\\HMC2\\\\day5\")\n\n    &gt;&gt;&gt; m1.load_data()\n    &gt;&gt;&gt; m1.get_weights(epoch=m1.epochs[1])\n    &gt;&gt;&gt; assembly_act = m1.get_assembly_act()\n\n    &gt;&gt;&gt; peth_avg, time_lags = event_triggered_average(\n    ...    assembly_act.abscissa_vals, assembly_act.data.T, m1.ripples.starts, window=[-0.5, 0.5]\n    ... )\n\n    &gt;&gt;&gt; plt.plot(time_lags,peth_avg)\n    &gt;&gt;&gt; plt.show()\n\n    Notes\n    -----\n    The function is adapted from elephant.sta.spike_triggered_average to be used with ndarray.\n    \"\"\"\n\n    # check inputs\n    if len(window) != 2:\n        raise ValueError(\n            \"'window' must be a tuple of 2 elements, not {}\".format(len(window))\n        )\n\n    if window[0] &gt; window[1]:\n        raise ValueError(\n            \"'window' first value must be less than second value, not {}\".format(\n                len(window)\n            )\n        )\n\n    if not isinstance(timestamps, np.ndarray):\n        raise ValueError(\n            \"'timestamps' must be a numpy ndarray, not {}\".format(type(timestamps))\n        )\n\n    if not isinstance(signal, np.ndarray):\n        raise ValueError(\n            \"'signal' must be a numpy ndarray, not {}\".format(type(signal))\n        )\n\n    if not isinstance(events, (list, np.ndarray)):\n        raise ValueError(\n            \"'events' must be a numpy ndarray or list, not {}\".format(type(events))\n        )\n\n    if signal.shape[0] != timestamps.shape[0]:\n        raise ValueError(\"'signal' and 'timestamps' must have the same number of rows\")\n\n    if len(timestamps.shape) &gt; 1:\n        raise ValueError(\n            \"'timestamps' must be a 1D array, not {}\".format(len(timestamps.shape))\n        )\n\n    window_starttime, window_stoptime = window\n\n    if len(signal.shape) == 1:\n        signal = np.expand_dims(signal, -1)\n\n    _, num_signals = signal.shape\n\n    if sampling_rate is None:\n        sampling_rate = 1 / stats.mode(np.diff(timestamps), keepdims=True)[0][0]\n\n    # window_bins: number of bins of the chosen averaging interval\n    window_bins = int(np.ceil(((window_stoptime - window_starttime) * sampling_rate)))\n    # result_sta: array containing finally the spike-triggered averaged signal\n    result_sta = np.zeros((window_bins, num_signals))\n    # setting of correct times of the spike-triggered average\n    # relative to the spike\n    time_lags = np.linspace(window_starttime, window_stoptime, window_bins)\n\n    used_events = np.zeros(num_signals, dtype=int)\n    total_used_events = 0\n\n    for i in range(num_signals):\n        # summing over all respective signal intervals around spiketimes\n        for event in events:\n            # locate signal in time range\n            idx = (timestamps &gt;= event + window_starttime) &amp; (\n                timestamps &lt;= event + window_stoptime\n            )\n\n            # for speed, instead of checking if we have enough time each iteration, just skip if we don't\n            try:\n                result_sta[:, i] += signal[idx, i]\n            except Exception:\n                continue\n            # counting of the used event\n            used_events[i] += 1\n\n        # normalization\n        result_sta[:, i] = result_sta[:, i] / used_events[i]\n\n        total_used_events += used_events[i]\n\n    if total_used_events == 0:\n        warnings.warn(\"No events at all was either found or used for averaging\")\n\n    if return_pandas:\n        return pd.DataFrame(\n            index=time_lags, columns=np.arange(result_sta.shape[1]), data=result_sta\n        )\n\n    return result_sta, time_lags\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.event_triggered_average_fast","title":"<code>event_triggered_average_fast(signal, events, sampling_rate, window=[-0.5, 0.5], return_average=True, return_pandas=False)</code>","text":"<p>Calculate the event-triggered average of a signal.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>ndarray</code> <p>A 2D array of signal data with shape (channels, timepoints).</p> required <code>events</code> <code>ndarray</code> <p>A 1D array of event times.</p> required <code>sampling_rate</code> <code>int</code> <p>The sampling rate of the signal in Hz.</p> required <code>window</code> <code>Union[list, Tuple[float, float]]</code> <p>A list or tuple specifying the time window (in seconds) to average the signal around each event. Defaults to [-0.5, 0.5].</p> <code>[-0.5, 0.5]</code> <code>return_average</code> <code>bool</code> <p>Whether to return the average of the event-triggered average. Defaults to True. If False, returns the full event-triggered average matrix (channels x timepoints x events).</p> <code>True</code> <code>return_pandas</code> <code>bool</code> <p>If True, returns the average as a Pandas DataFrame. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, DataFrame]</code> <p>If <code>return_average</code> is True, returns the event-triggered average of the signal (channels x timepoints) or a Pandas DataFrame if <code>return_pandas</code> is True. If <code>return_average</code> is False, returns the full event-triggered average matrix.</p> <code>ndarray</code> <p>An array of time lags corresponding to the event-triggered averages.</p> Notes <ul> <li>The function filters out events that do not fit within the valid range of the signal   considering the specified window size.</li> </ul> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def event_triggered_average_fast(\n    signal: np.ndarray,\n    events: np.ndarray,\n    sampling_rate: int,\n    window: Union[list, Tuple[float, float]] = [-0.5, 0.5],\n    return_average: bool = True,\n    return_pandas: bool = False,\n) -&gt; Tuple[Union[np.ndarray, pd.DataFrame], np.ndarray]:\n    \"\"\"\n    Calculate the event-triggered average of a signal.\n\n    Parameters\n    ----------\n    signal : np.ndarray\n        A 2D array of signal data with shape (channels, timepoints).\n\n    events : np.ndarray\n        A 1D array of event times.\n\n    sampling_rate : int\n        The sampling rate of the signal in Hz.\n\n    window : Union[list, Tuple[float, float]], optional\n        A list or tuple specifying the time window (in seconds) to average the signal\n        around each event. Defaults to [-0.5, 0.5].\n\n    return_average : bool, optional\n        Whether to return the average of the event-triggered average. Defaults to True.\n        If False, returns the full event-triggered average matrix (channels x timepoints x events).\n\n    return_pandas : bool, optional\n        If True, returns the average as a Pandas DataFrame. Defaults to False.\n\n    Returns\n    -------\n    Union[np.ndarray, pd.DataFrame]\n        If `return_average` is True, returns the event-triggered average of the signal\n        (channels x timepoints) or a Pandas DataFrame if `return_pandas` is True.\n        If `return_average` is False, returns the full event-triggered average matrix.\n\n    np.ndarray\n        An array of time lags corresponding to the event-triggered averages.\n\n    Notes\n    -----\n    - The function filters out events that do not fit within the valid range of the signal\n      considering the specified window size.\n    \"\"\"\n\n    window_starttime, window_stoptime = window\n    window_bins = int(np.ceil(((window_stoptime - window_starttime) * sampling_rate)))\n    time_lags = np.linspace(window_starttime, window_stoptime, window_bins)\n\n    events = events[\n        (events * sampling_rate &gt; len(time_lags) / 2 + 1)\n        &amp; (events * sampling_rate &lt; signal.shape[1] - len(time_lags) / 2 + 1)\n    ]\n\n    avg_signal = np.zeros(\n        [signal.shape[0], len(time_lags), len(events)], dtype=signal.dtype\n    )\n\n    for i, event in enumerate(events):\n        ts_idx = np.arange(\n            np.round(event * sampling_rate) - len(time_lags) / 2,\n            np.round(event * sampling_rate) + len(time_lags) / 2,\n        ).astype(int)\n        avg_signal[:, :, i] = signal[:, ts_idx]\n\n    if return_pandas and return_average:\n        return pd.DataFrame(\n            index=time_lags,\n            columns=np.arange(signal.shape[0]),\n            data=bn.nanmean(avg_signal, axis=2).T,\n        )\n\n    if return_average:\n        return bn.nanmean(avg_signal, axis=2), time_lags\n    else:\n        return avg_signal, time_lags\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.event_triggered_average_irregular_sample","title":"<code>event_triggered_average_irregular_sample(timestamps, data, time_ref, bin_width=0.002, n_bins=100, window=None)</code>","text":"<p>Compute the average and standard deviation of data values within a window around each reference time, specifically for irregularly sampled data.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>ndarray</code> <p>A 1D array of times associated with data.</p> required <code>data</code> <code>ndarray</code> <p>A 1D array of data values.</p> required <code>time_ref</code> <code>ndarray</code> <p>A 1D array of reference times.</p> required <code>bin_width</code> <code>float</code> <p>The width of each bin in the window, in seconds. Default is 0.002 seconds.</p> <code>0.002</code> <code>n_bins</code> <code>int</code> <p>The number of bins in the window. Default is 100.</p> <code>100</code> <code>window</code> <code>Union[tuple, None]</code> <p>A tuple containing the start and end times of the window to be plotted around each reference time. If not provided, the window will be centered around each reference time and have a width of <code>n_bins * bin_width</code> seconds.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Two DataFrames: the first containing the average values, the second the standard deviation of data values within the window around each reference time.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def event_triggered_average_irregular_sample(\n    timestamps: np.ndarray,\n    data: np.ndarray,\n    time_ref: np.ndarray,\n    bin_width: float = 0.002,\n    n_bins: int = 100,\n    window: Union[tuple, None] = None,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Compute the average and standard deviation of data values within a window around\n    each reference time, specifically for irregularly sampled data.\n\n    Parameters\n    ----------\n    timestamps : np.ndarray\n        A 1D array of times associated with data.\n    data : np.ndarray\n        A 1D array of data values.\n    time_ref : np.ndarray\n        A 1D array of reference times.\n    bin_width : float, optional\n        The width of each bin in the window, in seconds. Default is 0.002 seconds.\n    n_bins : int, optional\n        The number of bins in the window. Default is 100.\n    window : Union[tuple, None], optional\n        A tuple containing the start and end times of the window to be plotted around each reference time.\n        If not provided, the window will be centered around each reference time and have a\n        width of `n_bins * bin_width` seconds.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        Two DataFrames: the first containing the average values, the second the\n        standard deviation of data values within the window around each reference time.\n    \"\"\"\n\n    if window is not None:\n        times = np.arange(window[0], window[1] + bin_width, bin_width)\n    else:\n        times = np.linspace(\n            -(n_bins * bin_width) / 2, (n_bins * bin_width) / 2, n_bins + 1\n        )\n    x = []\n    y = []\n    for i, r in enumerate(time_ref):\n        idx = (timestamps &gt; r + times.min()) &amp; (timestamps &lt; r + times.max())\n        x.append((timestamps - r)[idx])\n        y.append(data[idx])\n\n    temp_df = pd.DataFrame()\n    if len(x) == 0:\n        return temp_df, temp_df\n    temp_df[\"time\"] = np.hstack(x)\n    temp_df[\"data\"] = np.hstack(y)\n    temp_df = temp_df.sort_values(by=\"time\", ascending=True)\n\n    average_val = np.zeros(len(times) - 1)\n    std_val = np.zeros(len(times) - 1)\n    for i in range(len(times) - 1):\n        average_val[i] = temp_df[\n            temp_df.time.between(times[i], times[i + 1])\n        ].data.mean()\n        std_val[i] = temp_df[temp_df.time.between(times[i], times[i + 1])].data.std()\n\n    avg = pd.DataFrame(index=times[:-1] + bin_width / 2)\n    avg[0] = average_val\n\n    std = pd.DataFrame(index=times[:-1] + bin_width / 2)\n    std[0] = std_val\n\n    return avg, std\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.fast_acf","title":"<code>fast_acf(counts, width, bin_width, cut_peak=True)</code>","text":"<p>Compute the Auto-Correlation Function (ACF) in a fast manner using Numba.</p> <p>This function calculates the ACF of a given variable of interest, such as spike times or spike phases, leveraging the <code>pcorrelate</code> function for efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>ndarray</code> <p>1D array of the variable of interest (e.g., spike times or spike phases).</p> required <code>width</code> <code>float</code> <p>Time window for the ACF computation.</p> required <code>bin_width</code> <code>float</code> <p>Width of the bins for the ACF.</p> required <code>cut_peak</code> <code>bool</code> <p>If True, the largest central peak will be replaced for subsequent fitting. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>acf</code> <code>ndarray</code> <p>1D array of counts for the ACF.</p> <code>bins</code> <code>ndarray</code> <p>1D array of lag bins for the ACF.</p> Notes <ul> <li>The ACF is calculated over a specified time window and returns the   counts of the ACF along with the corresponding bins.</li> <li>The <code>cut_peak</code> parameter allows for the adjustment of the ACF peak, which   can be useful for fitting processes.</li> </ul> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def fast_acf(\n    counts: np.ndarray, width: float, bin_width: float, cut_peak: bool = True\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the Auto-Correlation Function (ACF) in a fast manner using Numba.\n\n    This function calculates the ACF of a given variable of interest, such as\n    spike times or spike phases, leveraging the `pcorrelate` function for efficiency.\n\n    Parameters\n    ----------\n    counts : np.ndarray\n        1D array of the variable of interest (e.g., spike times or spike phases).\n    width : float\n        Time window for the ACF computation.\n    bin_width : float\n        Width of the bins for the ACF.\n    cut_peak : bool, optional\n        If True, the largest central peak will be replaced for subsequent fitting. Default is True.\n\n    Returns\n    -------\n    acf : np.ndarray\n        1D array of counts for the ACF.\n    bins : np.ndarray\n        1D array of lag bins for the ACF.\n\n    Notes\n    -----\n    - The ACF is calculated over a specified time window and returns the\n      counts of the ACF along with the corresponding bins.\n    - The `cut_peak` parameter allows for the adjustment of the ACF peak, which\n      can be useful for fitting processes.\n    \"\"\"\n\n    n_b = int(np.ceil(width / bin_width))  # Num. edges per side\n    # Define the edges of the bins (including rightmost bin)\n    bins = np.linspace(-width, width, 2 * n_b, endpoint=True)\n    temp = pcorrelate(counts, counts, np.split(bins, 2)[1])\n    acf = np.ones(bins.shape[0] - 1)\n    acf[0 : temp.shape[0]] = np.flip(temp)\n    acf[temp.shape[0]] = temp[0]\n    acf[temp.shape[0] + 1 :] = temp\n\n    if cut_peak:\n        acf[np.nanargmax(acf)] = np.sort(acf)[-2]\n\n    return acf, bins\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.find_intersecting_intervals","title":"<code>find_intersecting_intervals(set1, set2, return_indices=True)</code>","text":"<p>Find the amount of time two sets of intervals are intersecting each other for each intersection.</p> <p>Parameters:</p> Name Type Description Default <code>set1</code> <code>nelpy EpochArray</code> <p>The first set of intervals to check for intersections.</p> required <code>set2</code> <code>nelpy EpochArray</code> <p>The second set of intervals to check for intersections.</p> required <code>return_indices</code> <code>bool</code> <p>If True, return the indices of the intervals in set2 that intersect with each interval in set1. If False, return the amount of time each interval in set1 intersects with any interval in set2.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, List[bool]]</code> <p>If return_indices is True, returns a boolean array indicating whether each interval in set1 intersects with any interval in set2. If return_indices is False, returns a NumPy array with the amount of time each interval in set1 intersects with any interval in set2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; set1 = nel.EpochArray([(1, 3), (5, 7), (9, 10)])\n&gt;&gt;&gt; set2 = nel.EpochArray([(2, 4), (6, 8)])\n&gt;&gt;&gt; find_intersecting_intervals(set1, set2)\n[True, True, False]\n&gt;&gt;&gt; find_intersecting_intervals(set1, set2, return_indices=False)\n[1, 2, 0]\n</code></pre> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def find_intersecting_intervals(\n    set1: nel.EpochArray, set2: nel.EpochArray, return_indices: bool = True\n) -&gt; Union[np.ndarray, List[bool]]:\n    \"\"\"\n    Find the amount of time two sets of intervals are intersecting each other for each intersection.\n\n    Parameters\n    ----------\n    set1 : nelpy EpochArray\n        The first set of intervals to check for intersections.\n    set2 : nelpy EpochArray\n        The second set of intervals to check for intersections.\n    return_indices : bool, optional\n        If True, return the indices of the intervals in set2 that intersect with each interval in set1.\n        If False, return the amount of time each interval in set1 intersects with any interval in set2.\n\n    Returns\n    -------\n    Union[np.ndarray, List[bool]]\n        If return_indices is True, returns a boolean array indicating whether each interval in set1 intersects with any interval in set2.\n        If return_indices is False, returns a NumPy array with the amount of time each interval in set1 intersects with any interval in set2.\n\n    Examples\n    --------\n    &gt;&gt;&gt; set1 = nel.EpochArray([(1, 3), (5, 7), (9, 10)])\n    &gt;&gt;&gt; set2 = nel.EpochArray([(2, 4), (6, 8)])\n    &gt;&gt;&gt; find_intersecting_intervals(set1, set2)\n    [True, True, False]\n    &gt;&gt;&gt; find_intersecting_intervals(set1, set2, return_indices=False)\n    [1, 2, 0]\n    \"\"\"\n    if not isinstance(set1, core.IntervalArray) &amp; isinstance(set2, core.IntervalArray):\n        raise ValueError(\"only EpochArrays are supported\")\n\n    intersection = np.array(_find_intersecting_intervals(set1.data, set2.data))\n    if return_indices:\n        return intersection &gt; 0\n    return intersection\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.find_interval","title":"<code>find_interval(logical)</code>","text":"<p>Find consecutive intervals of True values in a list of boolean values.</p> <p>Parameters:</p> Name Type Description Default <code>logical</code> <code>List[bool]</code> <p>The list of boolean values.</p> required <p>Returns:</p> Type Description <code>List[Tuple[int, int]]</code> <p>A list of tuples representing the start and end indices of each consecutive interval of True values in the logical list.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; find_interval([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1])\n[(2, 4), (6, 7), (10, 11)]\n&gt;&gt;&gt; find_interval([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1])\n[(0, 2), (4, 5), (9, 10)]\n</code></pre> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def find_interval(logical: List[bool]) -&gt; List[Tuple[int, int]]:\n    \"\"\"\n    Find consecutive intervals of True values in a list of boolean values.\n\n    Parameters\n    ----------\n    logical : List[bool]\n        The list of boolean values.\n\n    Returns\n    -------\n    List[Tuple[int, int]]\n        A list of tuples representing the start and end indices of each consecutive interval of True values in the logical list.\n\n    Examples\n    --------\n    &gt;&gt;&gt; find_interval([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1])\n    [(2, 4), (6, 7), (10, 11)]\n    &gt;&gt;&gt; find_interval([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1])\n    [(0, 2), (4, 5), (9, 10)]\n    \"\"\"\n    intervals = []\n    start = None\n    for i, value in enumerate(logical):\n        if value and start is None:\n            start = i\n        elif not value and start is not None:\n            intervals.append((start, i - 1))\n            start = None\n    if start is not None:\n        intervals.append((start, len(logical) - 1))\n    return intervals\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.get_rank_order","title":"<code>get_rank_order(st, epochs, method='peak_fr', ref='cells', padding=0.05, dt=0.001, sigma=0.01, min_units=5)</code>","text":"<p>Calculate the rank order of spike trains within specified epochs.</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>ndarray or array</code> <p>Spike train data. Can be a nelpy array containing spike times.</p> required <code>epochs</code> <code>EpochArray</code> <p>An object containing the epochs (windows) in which to calculate the rank order.</p> required <code>method</code> <code>str</code> <p>Method to calculate rank order. Choices are 'first_spike' or 'peak_fr'. Defaults to 'peak_fr'.</p> <code>'peak_fr'</code> <code>ref</code> <code>str</code> <p>Reference frame for rank order. Choices are 'cells' or 'epoch'. Defaults to 'cells'.</p> <code>'cells'</code> <code>padding</code> <code>float</code> <p>Padding (in seconds) to apply to the epochs. Defaults to 0.05 seconds.</p> <code>0.05</code> <code>dt</code> <code>float</code> <p>Bin width (in seconds) for finding relative time in the epoch reference. Defaults to 0.001 seconds.</p> <code>0.001</code> <code>sigma</code> <code>float</code> <p>Smoothing sigma (in seconds) for the 'peak_fr' method. Defaults to 0.01 seconds.</p> <code>0.01</code> <code>min_units</code> <code>int</code> <p>Minimum number of active units required to compute the rank order. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>median_rank</code> <code>ndarray</code> <p>The median rank order across all epochs, normalized between 0 and 1.</p> <code>rank_order</code> <code>ndarray</code> <p>A 2D array of rank orders, where each column corresponds to an epoch, and each row corresponds to a cell, normalized between 0 and 1.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; st, _ = loading.load_spikes(basepath, putativeCellType='Pyr')\n&gt;&gt;&gt; forward_replay = nel.EpochArray(np.array([starts, stops]).T)\n&gt;&gt;&gt; median_rank, rank_order = get_rank_order(st, forward_replay)\n</code></pre> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def get_rank_order(\n    st: SpikeTrainArray,  # Assuming 'nelpy.array' is a custom type\n    epochs: EpochArray,\n    method: str = \"peak_fr\",  # 'first_spike' or 'peak_fr'\n    ref: str = \"cells\",  # 'cells' or 'epoch'\n    padding: float = 0.05,\n    dt: float = 0.001,\n    sigma: float = 0.01,\n    min_units: int = 5,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate the rank order of spike trains within specified epochs.\n\n    Parameters\n    ----------\n    st : np.ndarray or nelpy.array\n        Spike train data. Can be a nelpy array containing spike times.\n\n    epochs : nelpy.EpochArray\n        An object containing the epochs (windows) in which to calculate the rank order.\n\n    method : str, optional\n        Method to calculate rank order. Choices are 'first_spike' or 'peak_fr'.\n        Defaults to 'peak_fr'.\n\n    ref : str, optional\n        Reference frame for rank order. Choices are 'cells' or 'epoch'.\n        Defaults to 'cells'.\n\n    padding : float, optional\n        Padding (in seconds) to apply to the epochs. Defaults to 0.05 seconds.\n\n    dt : float, optional\n        Bin width (in seconds) for finding relative time in the epoch reference.\n        Defaults to 0.001 seconds.\n\n    sigma : float, optional\n        Smoothing sigma (in seconds) for the 'peak_fr' method. Defaults to 0.01 seconds.\n\n    min_units : int, optional\n        Minimum number of active units required to compute the rank order. Defaults to 5.\n\n    Returns\n    -------\n    median_rank : np.ndarray\n        The median rank order across all epochs, normalized between 0 and 1.\n\n    rank_order : np.ndarray\n        A 2D array of rank orders, where each column corresponds to an epoch,\n        and each row corresponds to a cell, normalized between 0 and 1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; st, _ = loading.load_spikes(basepath, putativeCellType='Pyr')\n    &gt;&gt;&gt; forward_replay = nel.EpochArray(np.array([starts, stops]).T)\n    &gt;&gt;&gt; median_rank, rank_order = get_rank_order(st, forward_replay)\n    \"\"\"\n    # filter out specific warnings\n    warnings.filterwarnings(\n        \"ignore\", message=\"ignoring events outside of eventarray support\"\n    )\n    warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\")\n\n    if method not in [\"first_spike\", \"peak_fr\"]:\n        raise Exception(\"method \" + method + \" not implemented\")\n    if ref not in [\"cells\", \"epoch\"]:\n        raise Exception(\"ref \" + ref + \" not implemented\")\n\n    def get_min_ts(st_temp):\n        min_ts = []\n        for ts in st_temp.data:\n            # nan if no spikes\n            if len(ts) == 0:\n                min_ts.append(np.nan)\n            else:\n                min_ts.append(np.nanmin(ts))\n        return min_ts\n\n    def rank_order_first_spike(st_epoch, epochs, dt, min_units, ref):\n        # set up empty matrix for rank order\n        rank_order = np.ones([st_epoch.data.shape[0], epochs.n_intervals]) * np.nan\n\n        unit_id = np.arange(st_epoch.data.shape[0])\n        st_epoch._abscissa.support = epochs\n\n        # iter over every event\n        for event_i, st_temp in enumerate(st_epoch):\n            if ref == \"cells\":\n                # get firing order\n                idx = np.array(st_temp.get_event_firing_order()) - 1\n                # reorder unit ids by order and remove non-active\n                units = unit_id[idx][st_temp.n_events[idx] &gt; 0]\n                # how many are left?\n                nUnits = len(units)\n\n                if nUnits &lt; min_units:\n                    rank_order[:, event_i] = np.nan\n                else:\n                    # arange 1 to n units in order of units\n                    rank_order[units, event_i] = np.arange(nUnits)\n                    # normalize by n units\n                    rank_order[units, event_i] = rank_order[units, event_i] / nUnits\n            elif ref == \"epoch\":\n                # find first spike time for each cell\n                min_ts = get_min_ts(st_temp)\n                # make time stamps for interpolation\n                epoch_ts = np.arange(epochs[event_i].start, epochs[event_i].stop, dt)\n                # make normalized range 0-1\n                norm_range = np.linspace(0, 1, len(epoch_ts))\n                # get spike order relative to normalized range\n                if len(min_ts) &lt; min_units:\n                    rank_order[:, event_i] = np.nan\n                else:\n                    rank_order[:, event_i] = np.interp(min_ts, epoch_ts, norm_range)\n        return rank_order\n\n    def rank_order_fr(st, epochs, dt, sigma, min_units, ref):\n        # set up empty matrix for rank order\n        rank_order = np.zeros([st.data.shape[0], epochs.n_intervals]) * np.nan\n\n        unit_id = np.arange(st.data.shape[0])\n\n        edges = split_epoch_by_width(epochs.data, dt)\n\n        z_t = count_in_interval(st.data, edges[:, 0], edges[:, 1], par_type=\"counts\")\n        _, interval_id = in_intervals(edges[:, 0], epochs.data, return_interval=True)\n\n        # iter over epochs\n        for event_i, epochs_temp in enumerate(epochs):\n            # smooth spike train in order to estimate peak\n            # z_t_temp.smooth(sigma=sigma, inplace=True)\n            z_t_temp = z_t[:, interval_id == event_i]\n            # smooth spike train in order to estimate peak\n            z_t_temp = gaussian_filter1d(z_t_temp, sigma / dt, axis=1)\n            if ref == \"cells\":\n                # find loc of each peak and get sorted idx of active units\n                idx = np.argsort(np.argmax(z_t_temp, axis=1))\n                # reorder unit ids by order and remove non-active\n                units = unit_id[idx][np.sum(z_t_temp[idx, :] &gt; 0, axis=1) &gt; 0]\n\n                nUnits = len(units)\n\n                if nUnits &lt; min_units:\n                    rank_order[:, event_i] = np.nan\n                else:\n                    # arange 1 to n units in order of units\n                    rank_order[units, event_i] = np.arange(nUnits)\n                    # normalize by n units\n                    rank_order[units, event_i] = rank_order[units, event_i] / nUnits\n            elif ref == \"epoch\":\n                # iterate over each cell\n                for cell_i, unit in enumerate(z_t_temp):\n                    # if the cell is not active apply nan\n                    if not np.any(unit &gt; 0):\n                        rank_order[cell_i, event_i] = np.nan\n                    else:\n                        # calculate normalized rank order (0-1)\n                        rank_order[cell_i, event_i] = np.argmax(unit) / len(unit)\n        return rank_order\n\n    # expand epochs by padding amount\n    epochs = epochs.expand(padding)\n\n    # check if no active cells\n    if st.n_active == 0:\n        return np.tile(np.nan, st.data.shape), np.tile(\n            np.nan, (st.data.shape[0], epochs.n_intervals)\n        )\n\n    # check if there are any spikes in the epoch\n    st_epoch = count_in_interval(\n        st.data, epochs.starts, epochs.stops, par_type=\"counts\"\n    )\n\n    # if no spikes in epoch, break out\n    if (st_epoch == 0).all():\n        return np.tile(np.nan, st.data.shape), np.tile(\n            np.nan, (st.data.shape[0], epochs.n_intervals)\n        )\n\n    # set up empty matrix for rank order\n    if method == \"peak_fr\":\n        rank_order = rank_order_fr(st, epochs, dt, sigma, min_units, ref)\n    elif method == \"first_spike\":\n        rank_order = rank_order_first_spike(st[epochs], epochs, dt, min_units, ref)\n    else:\n        raise Exception(\"method \" + method + \" not implemented\")\n\n    return np.nanmedian(rank_order, axis=1), rank_order\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.get_raster_points","title":"<code>get_raster_points(data, time_ref, bin_width=0.002, n_bins=100, window=None)</code>","text":"<p>Generate points for a raster plot centered around each reference time in the <code>time_ref</code> array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A 1D array of time values.</p> required <code>time_ref</code> <code>ndarray</code> <p>A 1D array of reference times.</p> required <code>bin_width</code> <code>float</code> <p>The width of each bin in the raster plot, in seconds. Default is 0.002 seconds.</p> <code>0.002</code> <code>n_bins</code> <code>int</code> <p>The number of bins in the raster plot. Default is 100.</p> <code>100</code> <code>window</code> <code>tuple</code> <p>A tuple containing the start and end times of the window to be plotted around each reference time. If not provided, the window will be centered around each reference time and have a width of <code>n_bins * bin_width</code> seconds.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>A 1D array of x values representing the time offsets of each data point relative to the corresponding reference time.</p> <code>y</code> <code>ndarray</code> <p>A 1D array of y values representing the reference times.</p> <code>times</code> <code>ndarray</code> <p>A 1D array of time values corresponding to the bins in the raster plot.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>@jit(nopython=True)\ndef get_raster_points(\n    data: np.ndarray,\n    time_ref: np.ndarray,\n    bin_width: float = 0.002,\n    n_bins: int = 100,\n    window: Optional[Tuple[float, float]] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate points for a raster plot centered around each reference time in the `time_ref` array.\n\n    Parameters\n    ----------\n    data : ndarray\n        A 1D array of time values.\n    time_ref : ndarray\n        A 1D array of reference times.\n    bin_width : float, optional\n        The width of each bin in the raster plot, in seconds. Default is 0.002 seconds.\n    n_bins : int, optional\n        The number of bins in the raster plot. Default is 100.\n    window : tuple, optional\n        A tuple containing the start and end times of the window to be plotted around each reference time.\n        If not provided, the window will be centered around each reference time and have a width of `n_bins * bin_width` seconds.\n\n    Returns\n    -------\n    x : ndarray\n        A 1D array of x values representing the time offsets of each data point relative to the corresponding reference time.\n    y : ndarray\n        A 1D array of y values representing the reference times.\n    times : ndarray\n        A 1D array of time values corresponding to the bins in the raster plot.\n    \"\"\"\n    if window is not None:\n        times = np.arange(window[0], window[1] + bin_width / 2, bin_width)\n    else:\n        times = np.linspace(\n            -(n_bins * bin_width) / 2, (n_bins * bin_width) / 2, n_bins + 1\n        )\n\n    x = np.empty(0)\n    y = np.empty(0)\n    for i, r in enumerate(time_ref):\n        idx = (data &gt; r + times.min()) &amp; (data &lt; r + times.max())\n        cur_data = data[idx]\n        x = np.concatenate((x, cur_data - r))\n        y = np.concatenate((y, np.ones_like(cur_data) * i))\n\n    return x, y, times\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.get_tapers","title":"<code>get_tapers(N, bandwidth, *, fs=1.0, min_lambda=0.95, n_tapers=None)</code>","text":"<p>Compute tapers and associated energy concentrations for the Thomson multitaper method.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>int</code> <p>Length of taper.</p> required <code>bandwidth</code> <code>float</code> <p>Bandwidth of taper, in Hz.</p> required <code>fs</code> <code>float</code> <p>Sampling rate, in Hz. Default is 1 Hz.</p> <code>1.0</code> <code>min_lambda</code> <code>float</code> <p>Minimum energy concentration that each taper must satisfy. Default is 0.95.</p> <code>0.95</code> <code>n_tapers</code> <code>Optional[int]</code> <p>Number of tapers to compute. Default is to use all tapers that satisfy 'min_lambda'.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tapers</code> <code>ndarray</code> <p>Array of tapers with shape (n_tapers, N).</p> <code>lambdas</code> <code>ndarray</code> <p>Energy concentrations for each taper with shape (n_tapers,).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If not enough tapers are available or if none of the tapers satisfy the minimum energy concentration criteria.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def get_tapers(\n    N: int,\n    bandwidth: float,\n    *,\n    fs: float = 1.0,\n    min_lambda: float = 0.95,\n    n_tapers: Optional[int] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute tapers and associated energy concentrations for the Thomson\n    multitaper method.\n\n    Parameters\n    ----------\n    N : int\n        Length of taper.\n    bandwidth : float\n        Bandwidth of taper, in Hz.\n    fs : float, optional\n        Sampling rate, in Hz. Default is 1 Hz.\n    min_lambda : float, optional\n        Minimum energy concentration that each taper must satisfy. Default is 0.95.\n    n_tapers : Optional[int], optional\n        Number of tapers to compute. Default is to use all tapers that satisfy 'min_lambda'.\n\n    Returns\n    -------\n    tapers : np.ndarray\n        Array of tapers with shape (n_tapers, N).\n    lambdas : np.ndarray\n        Energy concentrations for each taper with shape (n_tapers,).\n\n    Raises\n    ------\n    ValueError\n        If not enough tapers are available or if none of the tapers satisfy the\n        minimum energy concentration criteria.\n    \"\"\"\n\n    NW = bandwidth * N / fs\n    K = int(np.ceil(2 * NW)) - 1\n    if n_tapers is not None:\n        K = min(K, n_tapers)\n    if K &lt; 1:\n        raise ValueError(\n            f\"Not enough tapers, with 'NW' of {NW}. Increase the bandwidth or \"\n            \"use more data points\"\n        )\n\n    tapers, lambdas = dpss(N, NW=NW, Kmax=K, sym=False, norm=2, return_ratios=True)\n    mask = lambdas &gt; min_lambda\n    if not np.sum(mask) &gt; 0:\n        raise ValueError(\n            \"None of the tapers satisfied the minimum energy concentration\"\n            f\" criteria of {min_lambda}\"\n        )\n    tapers = tapers[mask]\n    lambdas = lambdas[mask]\n\n    if n_tapers is not None:\n        if n_tapers &gt; tapers.shape[0]:\n            raise ValueError(\n                f\"'n_tapers' of {n_tapers} is greater than the {tapers.shape[0]}\"\n                f\" that satisfied the minimum energy concentration criteria of {min_lambda}\"\n            )\n        tapers = tapers[:n_tapers]\n        lambdas = lambdas[:n_tapers]\n\n    return tapers, lambdas\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.getfgrid","title":"<code>getfgrid(Fs, nfft, fpass)</code>","text":"<p>Get frequency grid for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>Fs</code> <code>int</code> <p>Sampling frequency.</p> required <code>nfft</code> <code>int</code> <p>Number of points for FFT.</p> required <code>fpass</code> <code>List[float]</code> <p>Frequency range to evaluate (as [fmin, fmax]).</p> required <p>Returns:</p> Name Type Description <code>f</code> <code>ndarray</code> <p>Frequency vector within the specified range.</p> <code>findx</code> <code>ndarray</code> <p>Boolean array indicating the indices of the frequency vector that fall within the specified range.</p> Notes <p>The frequency vector is computed based on the sampling frequency and the number of FFT points. Only frequencies within the range defined by <code>fpass</code> are returned.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def getfgrid(Fs: int, nfft: int, fpass: List[float]) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Get frequency grid for evaluation.\n\n    Parameters\n    ----------\n    Fs : int\n        Sampling frequency.\n    nfft : int\n        Number of points for FFT.\n    fpass : List[float]\n        Frequency range to evaluate (as [fmin, fmax]).\n\n    Returns\n    -------\n    f : np.ndarray\n        Frequency vector within the specified range.\n    findx : np.ndarray\n        Boolean array indicating the indices of the frequency vector that fall within the specified range.\n\n    Notes\n    -----\n    The frequency vector is computed based on the sampling frequency and the number of FFT points.\n    Only frequencies within the range defined by `fpass` are returned.\n    \"\"\"\n    df = Fs / nfft\n    f = np.arange(0, Fs + df, df)\n    f = f[0:nfft]\n    findx = (f &gt;= fpass[0]) &amp; (f &lt;= fpass[-1])\n    f = f[findx]\n    return f, findx\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.in_intervals","title":"<code>in_intervals(timestamps, intervals, return_interval=False, shift=False)</code>","text":"<p>Find which timestamps fall within the given intervals.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>ndarray</code> <p>An array of timestamp values. Assumes sorted.</p> required <code>intervals</code> <code>ndarray</code> <p>An array of time intervals, represented as pairs of start and end times.</p> required <code>return_interval</code> <code>(bool, optional(default=False))</code> <p>If True, return the index of the interval to which each timestamp belongs.</p> <code>False</code> <code>shift</code> <code>(bool, optional(default=False))</code> <p>If True, return the shifted timestamps</p> <code>False</code> <p>Returns:</p> Name Type Description <code>in_interval</code> <code>ndarray</code> <p>A logical index indicating which timestamps fall within the intervals.</p> <code>interval</code> <code>(ndarray, optional)</code> <p>A ndarray indicating for each timestamps which interval it was within.</p> <code>shifted_timestamps</code> <code>(ndarray, optional)</code> <p>The shifted timestamps</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; timestamps = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n&gt;&gt;&gt; intervals = np.array([[2, 4], [5, 7]])\n&gt;&gt;&gt; in_intervals(timestamps, intervals)\narray([False,  True,  True,  True,  True,  True,  True, False])\n</code></pre> <pre><code>&gt;&gt;&gt; in_intervals(timestamps, intervals, return_interval=True)\n(array([False,  True,  True,  True,  True,  True,  True, False]),\narray([nan,  0.,  0.,  0.,  1.,  1.,  1., nan]))\n</code></pre> <pre><code>&gt;&gt;&gt; in_intervals(timestamps, intervals, shift=True)\n(array([False,  True,  True,  True,  True,  True,  True, False]),\narray([0, 1, 2, 2, 3, 4]))\n</code></pre> <pre><code>&gt;&gt;&gt; in_intervals(timestamps, intervals, return_interval=True, shift=True)\n(array([False,  True,  True,  True,  True,  True,  True, False]),\narray([0, 0, 0, 1, 1, 1]),\narray([0, 1, 2, 2, 3, 4]))\n</code></pre> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def in_intervals(\n    timestamps: np.ndarray,\n    intervals: np.ndarray,\n    return_interval: bool = False,\n    shift: bool = False,\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n    \"\"\"\n    Find which timestamps fall within the given intervals.\n\n    Parameters\n    ----------\n    timestamps : ndarray\n        An array of timestamp values. Assumes sorted.\n    intervals : ndarray\n        An array of time intervals, represented as pairs of start and end times.\n    return_interval : bool, optional (default=False)\n        If True, return the index of the interval to which each timestamp belongs.\n    shift : bool, optional (default=False)\n        If True, return the shifted timestamps\n\n    Returns\n    -------\n    in_interval : ndarray\n        A logical index indicating which timestamps fall within the intervals.\n    interval : ndarray, optional\n        A ndarray indicating for each timestamps which interval it was within.\n    shifted_timestamps : ndarray, optional\n        The shifted timestamps\n\n    Examples\n    --------\n    &gt;&gt;&gt; timestamps = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n    &gt;&gt;&gt; intervals = np.array([[2, 4], [5, 7]])\n    &gt;&gt;&gt; in_intervals(timestamps, intervals)\n    array([False,  True,  True,  True,  True,  True,  True, False])\n\n    &gt;&gt;&gt; in_intervals(timestamps, intervals, return_interval=True)\n    (array([False,  True,  True,  True,  True,  True,  True, False]),\n    array([nan,  0.,  0.,  0.,  1.,  1.,  1., nan]))\n\n    &gt;&gt;&gt; in_intervals(timestamps, intervals, shift=True)\n    (array([False,  True,  True,  True,  True,  True,  True, False]),\n    array([0, 1, 2, 2, 3, 4]))\n\n    &gt;&gt;&gt; in_intervals(timestamps, intervals, return_interval=True, shift=True)\n    (array([False,  True,  True,  True,  True,  True,  True, False]),\n    array([0, 0, 0, 1, 1, 1]),\n    array([0, 1, 2, 2, 3, 4]))\n    \"\"\"\n    in_interval = np.zeros(timestamps.shape, dtype=np.bool_)\n    interval = np.full(timestamps.shape, np.nan)\n\n    for i, (start, end) in enumerate(intervals):\n        # Find the leftmost index of a timestamp that is &gt;= start\n        left = np.searchsorted(timestamps, start, side=\"left\")\n        if left == len(timestamps):\n            # If start is greater than all timestamps, skip this interval\n            continue\n        # Find the rightmost index of a timestamp that is &lt;= end\n        right = np.searchsorted(timestamps, end, side=\"right\")\n        if right == left:\n            # If there are no timestamps in the interval, skip it\n            continue\n        # Mark the timestamps in the interval\n        in_interval[left:right] = True\n        interval[left:right] = i\n\n    if shift:\n        # Restrict to the timestamps that fall within the intervals\n        interval = interval[in_interval].astype(int)\n\n        # Calculate shifts based on intervals\n        shifts = np.insert(np.cumsum(intervals[1:, 0] - intervals[:-1, 1]), 0, 0)[\n            interval\n        ]\n\n        # Apply shifts to timestamps\n        shifted_timestamps = timestamps[in_interval] - shifts - intervals[0, 0]\n\n    if return_interval and shift:\n        return in_interval, interval, shifted_timestamps\n\n    if return_interval:\n        return in_interval, interval\n\n    if shift:\n        return in_interval, shifted_timestamps\n\n    return in_interval\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.in_intervals_interval","title":"<code>in_intervals_interval(timestamps, intervals)</code>","text":"<p>for each timestamps value, the index of the interval to which it belongs (nan = none)</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>ndarray</code> <p>An array of timestamp values. assumes sorted</p> required <code>intervals</code> <code>ndarray</code> <p>An array of time intervals, represented as pairs of start and end times.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <p>A ndarray indicating for each timestamps which interval it was within.</p> <code>Note</code> <code>produces same result as in_intervals with return_interval=True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; timestamps = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n&gt;&gt;&gt; intervals = np.array([[2, 4], [5, 7]])\n&gt;&gt;&gt; in_intervals_interval(timestamps, intervals)\narray([nan,  0,  0,  0,  1,  1,  1, nan])\n</code></pre> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>@jit(nopython=True, parallel=True)\ndef in_intervals_interval(timestamps: np.ndarray, intervals: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    for each timestamps value, the index of the interval to which it belongs (nan = none)\n\n    Parameters\n    ----------\n    timestamps : ndarray\n        An array of timestamp values. assumes sorted\n    intervals : ndarray\n        An array of time intervals, represented as pairs of start and end times.\n\n    Returns\n    -------\n    ndarray\n        A ndarray indicating for each timestamps which interval it was within.\n\n    Note: produces same result as in_intervals with return_interval=True\n\n    Examples\n    --------\n    &gt;&gt;&gt; timestamps = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n    &gt;&gt;&gt; intervals = np.array([[2, 4], [5, 7]])\n    &gt;&gt;&gt; in_intervals_interval(timestamps, intervals)\n    array([nan,  0,  0,  0,  1,  1,  1, nan])\n    \"\"\"\n    in_interval = np.full(timestamps.shape, np.nan)\n    for i in numba.prange(intervals.shape[0]):\n        start, end = intervals[i]\n        mask = (timestamps &gt;= start) &amp; (timestamps &lt;= end)\n        in_interval[mask] = i\n\n    return in_interval\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.joint_peth","title":"<code>joint_peth(peth_1, peth_2, smooth_std=2)</code>","text":"<p>joint_peth - produce a joint histogram for the co-occurrence of two sets of signals around events.</p> <p>This analysis tests for interactions. For example, the interaction of ripples and spindles around the occurrence of delta waves. It is a good way to control whether the relationships between two variables is entirely explained by a third variable (the events serving as basis for the PETHs).</p> <p>Parameters:</p> Name Type Description Default <code>peth_1</code> <code>ndarray</code> <p>The first peri-event time histogram (PETH) signal, shape (n_events, n_time).</p> required <code>peth_2</code> <code>ndarray</code> <p>The second peri-event time histogram (PETH) signal, shape (n_events, n_time).</p> required <code>smooth_std</code> <code>float</code> <p>The standard deviation of the Gaussian smoothing kernel (default is 2).</p> <code>2</code> <p>Returns:</p> Name Type Description <code>joint</code> <code>ndarray</code> <p>The joint histogram of the two PETH signals (n_time, n_time).</p> <code>expected</code> <code>ndarray</code> <p>The expected histogram of the two PETH signals (n_time, n_time).</p> <code>difference</code> <code>ndarray</code> <p>The difference between the joint and expected histograms of the two PETH signals (n_time, n_time).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.process.peri_event import joint_peth, peth_matrix, joint_peth\n&gt;&gt;&gt; from neuro_py.spikes.spike_tools import get_spindices\n&gt;&gt;&gt; from neuro_py.io import loading\n</code></pre> <pre><code>&gt;&gt;&gt; # load ripples, delta waves, and PFC pyramidal cell spikes from basepath\n</code></pre> <pre><code>&gt;&gt;&gt; basepath = r\"Z:\\Data\\HMC1\\day8\"\n</code></pre> <pre><code>&gt;&gt;&gt; ripples = loading.load_ripples_events(basepath, return_epoch_array=True)\n&gt;&gt;&gt; delta_waves = loading.load_events(basepath, epoch_name=\"deltaWaves\")\n&gt;&gt;&gt; st,cm = loading.load_spikes(basepath,brainRegion=\"PFC\",putativeCellType=\"Pyr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # flatten spikes (nelpy has .flatten(), but get_spindices is much faster)\n&gt;&gt;&gt; spikes = get_spindices(st.data)\n</code></pre> <pre><code>&gt;&gt;&gt; # create peri-event time histograms (PETHs) for the three signals\n&gt;&gt;&gt; window=[-1,1]\n&gt;&gt;&gt; labels = [\"spikes\", \"ripple\", \"delta\"]\n&gt;&gt;&gt; peth_1,ts = peth_matrix(spikes.spike_times.values, delta_waves.starts, bin_width=0.02, n_bins=101)\n&gt;&gt;&gt; peth_2,ts = peth_matrix(ripples.starts, delta_waves.starts, bin_width=0.02, n_bins=101)\n</code></pre> <pre><code>&gt;&gt;&gt; # calculate the joint, expected, and difference histograms\n&gt;&gt;&gt; joint, expected, difference = joint_peth(peth_1.T, peth_2.T, smooth_std=2)\n</code></pre> Notes <p>Note: sometimes the difference between \"joint\" and \"expected\" may be dominated due to brain state effects (e.g. if both ripples are spindles are more common around delta waves taking place in early SWS and have decreased rates around delta waves in late SWS, then all the values of \"joint\" would be larger than the value of \"expected\". In such a case, to investigate the timing effects in particular and ignore such global changes (correlations across the rows of \"PETH1\" and \"PETH2\"), consider normalizing the rows of the PETHs before calling joint_peth.</p> <p>See Sirota et al. (2003)</p> <p>Adapted from JointPETH.m, Copyright (C) 2018-2022 by Ralitsa Todorova</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def joint_peth(\n    peth_1: np.ndarray, peth_2: np.ndarray, smooth_std: float = 2\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    joint_peth - produce a joint histogram for the co-occurrence of two sets of signals around events.\n\n    This analysis tests for interactions. For example, the interaction of\n    ripples and spindles around the occurrence of delta waves. It is a good way\n    to control whether the relationships between two variables is entirely explained\n    by a third variable (the events serving as basis for the PETHs).\n\n    Parameters\n    ----------\n    peth_1 : np.ndarray\n        The first peri-event time histogram (PETH) signal, shape (n_events, n_time).\n    peth_2 : np.ndarray\n        The second peri-event time histogram (PETH) signal, shape (n_events, n_time).\n    smooth_std : float, optional\n        The standard deviation of the Gaussian smoothing kernel (default is 2).\n\n    Returns\n    -------\n    joint : np.ndarray\n        The joint histogram of the two PETH signals (n_time, n_time).\n    expected : np.ndarray\n        The expected histogram of the two PETH signals (n_time, n_time).\n    difference : np.ndarray\n        The difference between the joint and expected histograms of the two PETH signals (n_time, n_time).\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.process.peri_event import joint_peth, peth_matrix, joint_peth\n    &gt;&gt;&gt; from neuro_py.spikes.spike_tools import get_spindices\n    &gt;&gt;&gt; from neuro_py.io import loading\n\n    &gt;&gt;&gt; # load ripples, delta waves, and PFC pyramidal cell spikes from basepath\n\n    &gt;&gt;&gt; basepath = r\"Z:\\\\Data\\\\HMC1\\\\day8\"\n\n    &gt;&gt;&gt; ripples = loading.load_ripples_events(basepath, return_epoch_array=True)\n    &gt;&gt;&gt; delta_waves = loading.load_events(basepath, epoch_name=\"deltaWaves\")\n    &gt;&gt;&gt; st,cm = loading.load_spikes(basepath,brainRegion=\"PFC\",putativeCellType=\"Pyr\")\n\n    &gt;&gt;&gt; # flatten spikes (nelpy has .flatten(), but get_spindices is much faster)\n    &gt;&gt;&gt; spikes = get_spindices(st.data)\n\n    &gt;&gt;&gt; # create peri-event time histograms (PETHs) for the three signals\n    &gt;&gt;&gt; window=[-1,1]\n    &gt;&gt;&gt; labels = [\"spikes\", \"ripple\", \"delta\"]\n    &gt;&gt;&gt; peth_1,ts = peth_matrix(spikes.spike_times.values, delta_waves.starts, bin_width=0.02, n_bins=101)\n    &gt;&gt;&gt; peth_2,ts = peth_matrix(ripples.starts, delta_waves.starts, bin_width=0.02, n_bins=101)\n\n    &gt;&gt;&gt; # calculate the joint, expected, and difference histograms\n    &gt;&gt;&gt; joint, expected, difference = joint_peth(peth_1.T, peth_2.T, smooth_std=2)\n\n    Notes\n    -----\n    Note: sometimes the difference between \"joint\" and \"expected\" may be dominated due to\n    brain state effects (e.g. if both ripples are spindles are more common around delta\n    waves taking place in early SWS and have decreased rates around delta waves in late\n    SWS, then all the values of \"joint\" would be larger than the value of \"expected\".\n    In such a case, to investigate the timing effects in particular and ignore such\n    global changes (correlations across the rows of \"PETH1\" and \"PETH2\"), consider\n    normalizing the rows of the PETHs before calling joint_peth.\n\n    See Sirota et al. (2003)\n\n    Adapted from JointPETH.m, Copyright (C) 2018-2022 by Ralitsa Todorova\n    \"\"\"\n    from scipy.ndimage import gaussian_filter\n\n    # make inputs np.ndarrays\n    peth_1 = np.array(peth_1)\n    peth_2 = np.array(peth_2)\n\n    # calculate the joint histogram\n    joint = peth_1.T @ peth_2\n\n    # smooth the 2d joint histogram\n    joint = gaussian_filter(joint, smooth_std)\n\n    # calculate the expected histogram\n    expected = np.tile(np.nanmean(peth_1, axis=0), [peth_1.shape[0], 1]).T @ np.tile(\n        np.nanmean(peth_2, axis=0), [peth_2.shape[0], 1]\n    )\n\n    # smooth the 2d expected histogram\n    expected = gaussian_filter(expected, smooth_std)\n\n    # normalize the joint and expected histograms\n    joint = joint / peth_1.shape[0]\n    expected = expected / peth_1.shape[0]\n\n    # square root the joint and expected histograms so result is Hz\n    joint = np.sqrt(joint)\n    expected = np.sqrt(expected)\n\n    # calculate the difference between the joint and expected histograms\n    difference = joint - expected\n\n    return joint, expected, difference\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.load_results","title":"<code>load_results(save_path, verbose=False, add_save_file_name=False)</code>","text":"<p>Load results from pickled pandas DataFrames in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to the folder containing pickled results.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print progress for each file. Defaults to False.</p> <code>False</code> <code>add_save_file_name</code> <code>bool</code> <p>Whether to add a column with the name of the save file. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Concatenated pandas DataFrame with all results.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified folder does not exist or is empty.</p> Source code in <code>neuro_py/process/batch_analysis.py</code> <pre><code>def load_results(\n    save_path: str, verbose: bool = False, add_save_file_name: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load results from pickled pandas DataFrames in the specified directory.\n\n    Parameters\n    ----------\n    save_path : str\n        Path to the folder containing pickled results.\n    verbose : bool, optional\n        Whether to print progress for each file. Defaults to False.\n    add_save_file_name : bool, optional\n        Whether to add a column with the name of the save file. Defaults to False.\n\n    Returns\n    -------\n    pd.DataFrame\n        Concatenated pandas DataFrame with all results.\n\n    Raises\n    ------\n    ValueError\n        If the specified folder does not exist or is empty.\n    \"\"\"\n\n    if not os.path.exists(save_path):\n        raise ValueError(f\"folder {save_path} does not exist\")\n\n    sessions = glob.glob(os.path.join(save_path, \"*.pkl\"))\n\n    results = []\n\n    for session in sessions:\n        if verbose:\n            print(session)\n        with open(session, \"rb\") as f:\n            results_ = pickle.load(f)\n        if results_ is None:\n            continue\n\n        if add_save_file_name:\n            results_[\"save_file_name\"] = os.path.basename(session)\n\n        results.append(results_)\n\n    results = pd.concat(results, ignore_index=True, axis=0)\n\n    return results\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.mtcoherencept","title":"<code>mtcoherencept(data1, data2, Fs, fpass, NW=2.5, n_tapers=4, time_support=None, tapers=None, tapers_ts=None, nfft=None)</code>","text":"<p>Multitaper coherence for point processes.</p> <p>Parameters:</p> Name Type Description Default <code>data1</code> <code>ndarray</code> <p>Array of spike times for the first signal (in seconds).</p> required <code>data2</code> <code>ndarray</code> <p>Array of spike times for the second signal (in seconds).</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency.</p> required <code>fpass</code> <code>list</code> <p>Frequency range to evaluate as [min_freq, max_freq].</p> required <code>NW</code> <code>Union[int, float]</code> <p>Time-bandwidth product, by default 2.5.</p> <code>2.5</code> <code>n_tapers</code> <code>int</code> <p>Number of tapers, by default 4.</p> <code>4</code> <code>time_support</code> <code>Union[list, None]</code> <p>Time range to evaluate, by default None.</p> <code>None</code> <code>tapers</code> <code>Union[ndarray, None]</code> <p>Precomputed tapers, given as [NW, K] or [tapers, eigenvalues], by default None.</p> <code>None</code> <code>tapers_ts</code> <code>Union[ndarray, None]</code> <p>Taper time series, by default None.</p> <code>None</code> <code>nfft</code> <code>Optional[int]</code> <p>Number of points for FFT, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Coherence between the two point processes.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtcoherencept(\n    data1: np.ndarray,\n    data2: np.ndarray,\n    Fs: int,\n    fpass: list,\n    NW: Union[int, float] = 2.5,\n    n_tapers: int = 4,\n    time_support: Union[list, None] = None,\n    tapers: Union[np.ndarray, None] = None,\n    tapers_ts: Union[np.ndarray, None] = None,\n    nfft: Optional[int] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Multitaper coherence for point processes.\n\n    Parameters\n    ----------\n    data1 : np.ndarray\n        Array of spike times for the first signal (in seconds).\n    data2 : np.ndarray\n        Array of spike times for the second signal (in seconds).\n    Fs : int\n        Sampling frequency.\n    fpass : list\n        Frequency range to evaluate as [min_freq, max_freq].\n    NW : Union[int, float], optional\n        Time-bandwidth product, by default 2.5.\n    n_tapers : int, optional\n        Number of tapers, by default 4.\n    time_support : Union[list, None], optional\n        Time range to evaluate, by default None.\n    tapers : Union[np.ndarray, None], optional\n        Precomputed tapers, given as [NW, K] or [tapers, eigenvalues], by default None.\n    tapers_ts : Union[np.ndarray, None], optional\n        Taper time series, by default None.\n    nfft : Optional[int], optional\n        Number of points for FFT, by default None.\n\n    Returns\n    -------\n    pd.DataFrame\n        Coherence between the two point processes.\n    \"\"\"\n    # Check if data is a single unit and put in array\n    if isinstance(data1, np.ndarray):\n        data1 = np.array([data1])\n    if isinstance(data2, np.ndarray):\n        data2 = np.array([data2])\n\n    # Compute power spectral densities (PSD) for both spike trains\n    psd1 = mtspectrumpt(\n        data1, Fs, fpass, NW, n_tapers, time_support, tapers, tapers_ts, nfft\n    )\n    psd2 = mtspectrumpt(\n        data2, Fs, fpass, NW, n_tapers, time_support, tapers, tapers_ts, nfft\n    )\n\n    # Compute cross-spectral density (CSD) between the two spike trains\n    csd = mtcsdpt(\n        data1, data2, Fs, fpass, NW, n_tapers, time_support, tapers, tapers_ts, nfft\n    )\n\n    # Calculate coherence: |Sxy(f)|^2 / (Sxx(f) * Syy(f))\n    coherence = np.abs(csd[\"CSD\"].values) ** 2 / (psd1.values * psd2.values).flatten()\n\n    # Return coherence as a pandas DataFrame\n    coherence_df = pd.DataFrame(index=csd.index, data=coherence, columns=[\"Coherence\"])\n    return coherence_df\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.mtcsdpt","title":"<code>mtcsdpt(data1, data2, Fs, fpass, NW=2.5, n_tapers=4, time_support=None, tapers=None, tapers_ts=None, nfft=None)</code>","text":"<p>Multitaper cross-spectral density (CSD) for point processes.</p> <p>Parameters:</p> Name Type Description Default <code>data1</code> <code>ndarray</code> <p>Array of spike times for the first signal (in seconds).</p> required <code>data2</code> <code>ndarray</code> <p>Array of spike times for the second signal (in seconds).</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency.</p> required <code>fpass</code> <code>list</code> <p>Frequency range to evaluate as [min_freq, max_freq].</p> required <code>NW</code> <code>Union[int, float]</code> <p>Time-bandwidth product, by default 2.5.</p> <code>2.5</code> <code>n_tapers</code> <code>int</code> <p>Number of tapers, by default 4.</p> <code>4</code> <code>time_support</code> <code>Union[list, None]</code> <p>Time range to evaluate, by default None.</p> <code>None</code> <code>tapers</code> <code>Union[ndarray, None]</code> <p>Precomputed tapers, given as [NW, K] or [tapers, eigenvalues], by default None.</p> <code>None</code> <code>tapers_ts</code> <code>Union[ndarray, None]</code> <p>Taper time series, by default None.</p> <code>None</code> <code>nfft</code> <code>Optional[int]</code> <p>Number of points for FFT, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Cross-spectral density between the two point processes.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtcsdpt(\n    data1: np.ndarray,\n    data2: np.ndarray,\n    Fs: int,\n    fpass: list,\n    NW: Union[int, float] = 2.5,\n    n_tapers: int = 4,\n    time_support: Union[list, None] = None,\n    tapers: Union[np.ndarray, None] = None,\n    tapers_ts: Union[np.ndarray, None] = None,\n    nfft: Optional[int] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Multitaper cross-spectral density (CSD) for point processes.\n\n    Parameters\n    ----------\n    data1 : np.ndarray\n        Array of spike times for the first signal (in seconds).\n    data2 : np.ndarray\n        Array of spike times for the second signal (in seconds).\n    Fs : int\n        Sampling frequency.\n    fpass : list\n        Frequency range to evaluate as [min_freq, max_freq].\n    NW : Union[int, float], optional\n        Time-bandwidth product, by default 2.5.\n    n_tapers : int, optional\n        Number of tapers, by default 4.\n    time_support : Union[list, None], optional\n        Time range to evaluate, by default None.\n    tapers : Union[np.ndarray, None], optional\n        Precomputed tapers, given as [NW, K] or [tapers, eigenvalues], by default None.\n    tapers_ts : Union[np.ndarray, None], optional\n        Taper time series, by default None.\n    nfft : Optional[int], optional\n        Number of points for FFT, by default None.\n\n    Returns\n    -------\n    pd.DataFrame\n        Cross-spectral density between the two point processes.\n    \"\"\"\n    if time_support is not None:\n        mintime, maxtime = time_support\n    else:\n        mintime = min(np.min(data1), np.min(data2))\n        maxtime = max(np.max(data1), np.max(data2))\n    dt = 1 / Fs\n\n    # Create tapers if not provided\n    if tapers is None:\n        tapers_ts = np.arange(mintime - dt, maxtime + dt, dt)\n        N = len(tapers_ts)\n        tapers, eigens = dpss(N, NW, n_tapers, return_ratios=True)\n\n    tapers = tapers.T\n    N = len(tapers_ts)\n\n    # Number of points in FFT\n    if nfft is None:\n        nfft = np.max([int(2 ** np.ceil(np.log2(N))), N])\n    f, findx = getfgrid(Fs, nfft, fpass)\n\n    # Compute the multitaper Fourier transforms of both spike trains\n    J1, Msp1, Nsp1 = mtfftpt(data1, tapers, nfft, tapers_ts, f, findx)\n    J2, Msp2, Nsp2 = mtfftpt(data2, tapers, nfft, tapers_ts, f, findx)\n\n    # Cross-spectral density: Sxy = mean(conjugate(J1) * J2)\n    csd = np.real(np.mean(np.conj(J1) * J2, axis=1))\n\n    csd_df = pd.DataFrame(index=f, data=csd, columns=[\"CSD\"])\n    return csd_df\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.mtfftc","title":"<code>mtfftc(data, tapers, nfft, Fs)</code>","text":"<p>Multi-taper Fourier Transform - Continuous Data (Single Signal)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>1D array of data (samples).</p> required <code>tapers</code> <code>ndarray</code> <p>Precomputed DPSS tapers with shape (samples, tapers).</p> required <code>nfft</code> <code>int</code> <p>Length of padded data for FFT.</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency.</p> required <p>Returns:</p> Name Type Description <code>J</code> <code>ndarray</code> <p>FFT in the form (nfft, K), where K is the number of tapers.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtfftc(data: np.ndarray, tapers: np.ndarray, nfft: int, Fs: int) -&gt; np.ndarray:\n    \"\"\"\n    Multi-taper Fourier Transform - Continuous Data (Single Signal)\n\n    Parameters\n    ----------\n    data : np.ndarray\n        1D array of data (samples).\n    tapers : np.ndarray\n        Precomputed DPSS tapers with shape (samples, tapers).\n    nfft : int\n        Length of padded data for FFT.\n    Fs : int\n        Sampling frequency.\n\n    Returns\n    -------\n    J : np.ndarray\n        FFT in the form (nfft, K), where K is the number of tapers.\n    \"\"\"\n    # Ensure data is 1D\n    if data.ndim != 1:\n        raise ValueError(\"Input data must be a 1D array.\")\n\n    NC = data.shape[0]  # Number of samples in data\n    NK, K = tapers.shape  # Number of samples and tapers\n\n    if NK != NC:\n        raise ValueError(\"Length of tapers is incompatible with length of data.\")\n\n    # Project data onto tapers\n    data_proj = data[:, np.newaxis] * tapers  # Shape: (samples, tapers)\n\n    # Compute FFT for each taper\n    J = np.fft.fft(data_proj, n=nfft, axis=0) / Fs  # Shape: (nfft, K)\n\n    return J\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.mtfftpt","title":"<code>mtfftpt(data, tapers, nfft, t, f, findx)</code>","text":"<p>Multitaper FFT for point process times.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>1D array of spike times (in seconds).</p> required <code>tapers</code> <code>ndarray</code> <p>Tapers from the DPSS method.</p> required <code>nfft</code> <code>int</code> <p>Number of points for FFT.</p> required <code>t</code> <code>ndarray</code> <p>Time vector.</p> required <code>f</code> <code>ndarray</code> <p>Frequency vector.</p> required <code>findx</code> <code>list of bool</code> <p>Frequency index.</p> required <p>Returns:</p> Name Type Description <code>J</code> <code>ndarray</code> <p>FFT of the data.</p> <code>Msp</code> <code>float</code> <p>Mean spikes per time.</p> <code>Nsp</code> <code>float</code> <p>Total number of spikes in data.</p> Notes <p>The function computes the multitaper FFT of spike times using the specified tapers and returns the FFT result, mean spikes, and total spike count.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtfftpt(\n    data: np.ndarray,\n    tapers: np.ndarray,\n    nfft: int,\n    t: np.ndarray,\n    f: np.ndarray,\n    findx: List[bool],\n) -&gt; Tuple[np.ndarray, float, float]:\n    \"\"\"\n    Multitaper FFT for point process times.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        1D array of spike times (in seconds).\n    tapers : np.ndarray\n        Tapers from the DPSS method.\n    nfft : int\n        Number of points for FFT.\n    t : np.ndarray\n        Time vector.\n    f : np.ndarray\n        Frequency vector.\n    findx : list of bool\n        Frequency index.\n\n    Returns\n    -------\n    J : np.ndarray\n        FFT of the data.\n    Msp : float\n        Mean spikes per time.\n    Nsp : float\n        Total number of spikes in data.\n\n    Notes\n    -----\n    The function computes the multitaper FFT of spike times using\n    the specified tapers and returns the FFT result, mean spikes,\n    and total spike count.\n    \"\"\"\n    K = tapers.shape[1]\n    nfreq = len(f)\n\n    # get the FFT of the tapers\n    H = np.zeros((nfft, K), dtype=np.complex128)\n    for i in np.arange(K):\n        H[:, i] = np.fft.fft(tapers[:, i], nfft, axis=0)\n\n    H = H[findx, :]\n    w = 2 * np.pi * f\n    dtmp = data\n    indx = np.logical_and(dtmp &gt;= np.min(t), dtmp &lt;= np.max(t))\n    if len(indx):\n        dtmp = dtmp[indx]\n    Nsp = len(dtmp)\n\n    # get the mean spike rate\n    Msp = Nsp / len(t)\n\n    if Msp != 0:\n        # Interpolate spike times for each taper\n        data_proj = np.empty((len(dtmp), K))\n        for i in range(K):\n            data_proj[:, i] = np.interp(dtmp, t, tapers[:, i])\n\n        def compute_J(k):\n            J_k = np.zeros(nfreq, dtype=np.complex128)\n            for i, freq in enumerate(w):\n                phase = -1j * freq * (dtmp - t[0])\n                J_k[i] = np.sum(np.exp(phase) * data_proj[:, k])\n            return J_k\n\n        J = np.array(Parallel(n_jobs=-1)(delayed(compute_J)(k) for k in range(K))).T\n\n        J -= H * Msp\n    else:\n        # No spikes: return zeros\n        J = np.zeros((nfreq, K), dtype=np.complex128)\n\n    return J, Msp, Nsp\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.mtspectrumc","title":"<code>mtspectrumc(data, Fs, fpass, tapers)</code>","text":"<p>Compute the multitaper power spectrum for continuous data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>1D array of continuous data (e.g., LFP).</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency in Hz.</p> required <code>fpass</code> <code>list</code> <p>Frequency range to evaluate as [min_freq, max_freq].</p> required <code>tapers</code> <code>ndarray</code> <p>Tapers array with shape [NW, K] or [tapers, eigenvalues].</p> required <p>Returns:</p> Name Type Description <code>S</code> <code>Series</code> <p>Power spectrum with frequencies as the index.</p> Notes <p>This function utilizes the multitaper method for spectral estimation and returns the power spectrum as a pandas Series.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtspectrumc(\n    data: np.ndarray, Fs: int, fpass: list, tapers: np.ndarray\n) -&gt; pd.Series:\n    \"\"\"\n    Compute the multitaper power spectrum for continuous data.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        1D array of continuous data (e.g., LFP).\n    Fs : int\n        Sampling frequency in Hz.\n    fpass : list\n        Frequency range to evaluate as [min_freq, max_freq].\n    tapers : np.ndarray\n        Tapers array with shape [NW, K] or [tapers, eigenvalues].\n\n    Returns\n    -------\n    S : pd.Series\n        Power spectrum with frequencies as the index.\n\n    Notes\n    -----\n    This function utilizes the multitaper method for spectral estimation\n    and returns the power spectrum as a pandas Series.\n    \"\"\"\n    N = len(data)\n    nfft = np.max(\n        [int(2 ** np.ceil(np.log2(N))), N]\n    )  # number of points in fft of prolates\n    # get the frequency grid\n    f, findx = getfgrid(Fs, nfft, fpass)\n    # get the fft of the tapers\n    tapers = dpsschk(tapers, N, Fs)\n    # get the fft of the data\n    J = mtfftc(data, tapers, nfft, Fs)\n    # restrict fft of tapers to required frequencies\n    J = J[findx, :]\n    # get the power spectrum\n    S = np.real(np.mean(np.conj(J) * J, 1))\n    # return the power spectrum\n    return pd.Series(index=f, data=S)\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.mtspectrumpt","title":"<code>mtspectrumpt(data, Fs, fpass, NW=2.5, n_tapers=4, time_support=None, tapers=None, tapers_ts=None, nfft=None)</code>","text":"<p>Multitaper power spectrum estimation for point process data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Array of spike times (in seconds).</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency.</p> required <code>fpass</code> <code>list of float</code> <p>Frequency range to evaluate.</p> required <code>NW</code> <code>Union[int, float]</code> <p>Time-bandwidth product (default is 2.5).</p> <code>2.5</code> <code>n_tapers</code> <code>int</code> <p>Number of tapers (default is 4).</p> <code>4</code> <code>time_support</code> <code>Union[list, None]</code> <p>Time range to evaluate (default is None).</p> <code>None</code> <code>tapers</code> <code>Union[ndarray, None]</code> <p>Precomputed tapers, given as [NW, K] or [tapers, eigenvalues] (default is None).</p> <code>None</code> <code>tapers_ts</code> <code>Union[ndarray, None]</code> <p>Taper time series (default is None).</p> <code>None</code> <code>nfft</code> <code>Optional[int]</code> <p>Number of points for FFT (default is None).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the power spectrum.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spec = pychronux.mtspectrumpt(\n&gt;&gt;&gt;    st.data,\n&gt;&gt;&gt;    100,\n&gt;&gt;&gt;    [1, 20],\n&gt;&gt;&gt;    NW=3,\n&gt;&gt;&gt;    n_tapers=5,\n&gt;&gt;&gt;    time_support=[st.support.start, st.support.stop],\n&gt;&gt;&gt;    nfft=500,\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtspectrumpt(\n    data: np.ndarray,\n    Fs: int,\n    fpass: list,\n    NW: Union[int, float] = 2.5,\n    n_tapers: int = 4,\n    time_support: Union[list, None] = None,\n    tapers: Union[np.ndarray, None] = None,\n    tapers_ts: Union[np.ndarray, None] = None,\n    nfft: Optional[int] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Multitaper power spectrum estimation for point process data.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Array of spike times (in seconds).\n    Fs : int\n        Sampling frequency.\n    fpass : list of float\n        Frequency range to evaluate.\n    NW : Union[int, float], optional\n        Time-bandwidth product (default is 2.5).\n    n_tapers : int, optional\n        Number of tapers (default is 4).\n    time_support : Union[list, None], optional\n        Time range to evaluate (default is None).\n    tapers : Union[np.ndarray, None], optional\n        Precomputed tapers, given as [NW, K] or [tapers, eigenvalues] (default is None).\n    tapers_ts : Union[np.ndarray, None], optional\n        Taper time series (default is None).\n    nfft : Optional[int], optional\n        Number of points for FFT (default is None).\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing the power spectrum.\n\n\n    Examples\n    -------\n    &gt;&gt;&gt; spec = pychronux.mtspectrumpt(\n    &gt;&gt;&gt;    st.data,\n    &gt;&gt;&gt;    100,\n    &gt;&gt;&gt;    [1, 20],\n    &gt;&gt;&gt;    NW=3,\n    &gt;&gt;&gt;    n_tapers=5,\n    &gt;&gt;&gt;    time_support=[st.support.start, st.support.stop],\n    &gt;&gt;&gt;    nfft=500,\n    &gt;&gt;&gt; )\n    \"\"\"\n\n    # check data\n    if len(data) == 0:\n        return pd.DataFrame()\n\n    # check frequency range\n    if fpass[0] &gt; fpass[1]:\n        raise ValueError(\n            \"Invalid frequency range: fpass[0] should be less than fpass[1].\"\n        )\n\n    if time_support is not None:\n        mintime, maxtime = time_support\n    else:\n        if data.dtype == np.object_:\n            mintime = np.min(np.concatenate(data))\n            maxtime = np.max(np.concatenate(data))\n        else:\n            mintime = np.min(data)\n            maxtime = np.max(data)\n\n    dt = 1 / Fs\n\n    if tapers is None:\n        tapers_ts = np.arange(mintime - dt, maxtime + dt, dt)\n        N = len(tapers_ts)\n        tapers, eigens = dpss(N, NW, n_tapers, return_ratios=True)\n        tapers = tapers.T\n\n    if tapers_ts is None:\n        tapers_ts = np.arange(mintime - dt, maxtime + dt, dt)\n\n    N = len(tapers_ts)\n    # number of points in fft of prolates\n    if nfft is None:\n        nfft = np.max([int(2 ** np.ceil(np.log2(N))), N])\n    f, findx = getfgrid(Fs, nfft, fpass)\n\n    spec = np.zeros((len(f), len(data)))\n    for i, d in enumerate(data):\n        J, _, _ = mtfftpt(d, tapers, nfft, tapers_ts, f, findx)\n        spec[:, i] = np.real(np.mean(np.conj(J) * J, 1))\n\n    spectrum_df = pd.DataFrame(index=f, columns=np.arange(len(data)), dtype=np.float64)\n    spectrum_df[:] = spec\n    return spectrum_df\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.nearest_event_delay","title":"<code>nearest_event_delay(ts_1, ts_2)</code>","text":"<p>Return for each timestamp in ts_1 the nearest timestamp in ts_2 and the delay between the two.</p> <p>Parameters:</p> Name Type Description Default <code>ts_1</code> <code>ndarray</code> <p>1D array of timestamps.</p> required <code>ts_2</code> <code>ndarray</code> <p>1D array of timestamps (must be monotonically increasing).</p> required <p>Returns:</p> Name Type Description <code>nearest_ts</code> <code>ndarray</code> <p>Nearest timestamps in ts_2 for each timestamp in ts_1.</p> <code>delays</code> <code>ndarray</code> <p>Delays between ts_1 and nearest_ts.</p> <code>nearest_index</code> <code>ndarray</code> <p>Index of nearest_ts in ts_2.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If ts_1 or ts_2 are empty or not monotonically increasing.</p> Notes <p>Both ts_1 and ts_2 must be monotonically increasing arrays of timestamps.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def nearest_event_delay(\n    ts_1: np.ndarray, ts_2: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Return for each timestamp in ts_1 the nearest timestamp in ts_2 and the delay between the two.\n\n    Parameters\n    ----------\n    ts_1 : np.ndarray\n        1D array of timestamps.\n    ts_2 : np.ndarray\n        1D array of timestamps (must be monotonically increasing).\n\n    Returns\n    -------\n    nearest_ts : np.ndarray\n        Nearest timestamps in ts_2 for each timestamp in ts_1.\n    delays : np.ndarray\n        Delays between ts_1 and nearest_ts.\n    nearest_index : np.ndarray\n        Index of nearest_ts in ts_2.\n\n    Raises\n    ------\n    ValueError\n        If ts_1 or ts_2 are empty or not monotonically increasing.\n\n    Notes\n    -----\n    Both ts_1 and ts_2 must be monotonically increasing arrays of timestamps.\n    \"\"\"\n    ts_1, ts_2 = np.array(ts_1), np.array(ts_2)\n\n    if not np.all(np.diff(ts_2) &gt; 0):\n        raise ValueError(\"ts_2 must be monotonically increasing\")\n\n    if not np.all(np.diff(ts_1) &gt; 0):\n        raise ValueError(\"ts_1 must be monotonically increasing\")\n    # check if empty\n    if len(ts_1) == 0:\n        raise ValueError(\"ts_1 is empty\")\n    if len(ts_2) == 0:\n        raise ValueError(\"ts_2 is empty\")\n\n    # Use searchsorted to find the indices where elements of ts_1 should be inserted\n    nearest_indices = np.searchsorted(ts_2, ts_1, side=\"left\")\n\n    # Calculate indices for the elements before and after the insertion points\n    before = np.maximum(nearest_indices - 1, 0)\n    after = np.minimum(nearest_indices, len(ts_2) - 1)\n\n    # Determine the nearest timestamp for each element in ts_1\n    nearest_ts = np.where(\n        np.abs(ts_1 - ts_2[before]) &lt; np.abs(ts_1 - ts_2[after]),\n        ts_2[before],\n        ts_2[after],\n    )\n\n    # Calculate delays between ts_1 and nearest_ts\n    delays = ts_1 - nearest_ts\n\n    # Find the nearest_index using the absolute difference\n    absolute_diff_before = np.abs(ts_1 - ts_2[before])\n    absolute_diff_after = np.abs(ts_1 - ts_2[after])\n    nearest_index = np.where(absolute_diff_before &lt; absolute_diff_after, before, after)\n\n    return nearest_ts, delays, nearest_index\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.nonspatial_phase_precession","title":"<code>nonspatial_phase_precession(unwrapped_spike_phases, width=4 * 2 * np.pi, bin_width=np.pi / 3, cut_peak=True, norm=True, psd_lims=[0.65, 1.55], upsample=4, smooth_sigma=1)</code>","text":"<p>Compute the nonspatial spike-LFP relationship modulation index.</p> <p>Parameters:</p> Name Type Description Default <code>unwrapped_spike_phases</code> <code>ndarray</code> <p>1D array of spike phases that have been linearly unwrapped.</p> required <code>width</code> <code>float</code> <p>Time window for ACF in cycles (default = 4 cycles).</p> <code>4 * 2 * pi</code> <code>bin_width</code> <code>float</code> <p>Width of bins in radians (default = pi/3 radians).</p> <code>pi / 3</code> <code>cut_peak</code> <code>bool</code> <p>Whether or not the largest central peak should be replaced for subsequent fitting.</p> <code>True</code> <code>norm</code> <code>bool</code> <p>To normalize the ACF or not.</p> <code>True</code> <code>psd_lims</code> <code>List[float]</code> <p>Limits of the PSD to consider for peak finding (default = [0.65, 1.55]).</p> <code>[0.65, 1.55]</code> <code>upsample</code> <code>int</code> <p>Upsampling factor (default = 4).</p> <code>4</code> <code>smooth_sigma</code> <code>float</code> <p>Standard deviation for Gaussian smoothing of the PSD (default = 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>max_freq</code> <code>float</code> <p>Relative spike-LFP frequency of the PSD peak.</p> <code>MI</code> <code>float</code> <p>Modulation index of non-spatial phase relationship.</p> <code>psd</code> <code>ndarray</code> <p>Power spectral density of interest.</p> <code>frequencies</code> <code>ndarray</code> <p>Frequencies corresponding to the PSD.</p> <code>acf</code> <code>ndarray</code> <p>Autocorrelation function.</p> Notes <p>The modulation index (MI) is computed based on the maximum peak of the power spectral density (PSD) within specified frequency limits.</p> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def nonspatial_phase_precession(\n    unwrapped_spike_phases: np.ndarray,\n    width: float = 4 * 2 * np.pi,\n    bin_width: float = np.pi / 3,\n    cut_peak: bool = True,\n    norm: bool = True,\n    psd_lims: List[float] = [0.65, 1.55],\n    upsample: int = 4,\n    smooth_sigma: float = 1,\n) -&gt; Tuple[float, float, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the nonspatial spike-LFP relationship modulation index.\n\n    Parameters\n    ----------\n    unwrapped_spike_phases : np.ndarray\n        1D array of spike phases that have been linearly unwrapped.\n    width : float\n        Time window for ACF in cycles (default = 4 cycles).\n    bin_width : float\n        Width of bins in radians (default = pi/3 radians).\n    cut_peak : bool\n        Whether or not the largest central peak should be replaced for subsequent fitting.\n    norm : bool\n        To normalize the ACF or not.\n    psd_lims : List[float]\n        Limits of the PSD to consider for peak finding (default = [0.65, 1.55]).\n    upsample : int\n        Upsampling factor (default = 4).\n    smooth_sigma : float\n        Standard deviation for Gaussian smoothing of the PSD (default = 1).\n\n    Returns\n    -------\n    max_freq : float\n        Relative spike-LFP frequency of the PSD peak.\n    MI : float\n        Modulation index of non-spatial phase relationship.\n    psd : np.ndarray\n        Power spectral density of interest.\n    frequencies : np.ndarray\n        Frequencies corresponding to the PSD.\n    acf : np.ndarray\n        Autocorrelation function.\n\n    Notes\n    -----\n    The modulation index (MI) is computed based on the maximum peak of the power\n    spectral density (PSD) within specified frequency limits.\n    \"\"\"\n\n    frequencies = (\n        (np.arange(2 * (width // bin_width) - 1))\n        * (2 * np.pi)\n        / (2 * width - bin_width)\n    )\n\n    frequencies = np.interp(\n        np.arange(0, len(frequencies), 1 / upsample),\n        np.arange(0, len(frequencies)),\n        frequencies,\n    )\n\n    freqs_of_interest = np.intersect1d(\n        np.where(frequencies &gt; psd_lims[0]), np.where(frequencies &lt; psd_lims[1])\n    )\n\n    acf, _ = fast_acf(unwrapped_spike_phases, width, bin_width, cut_peak=cut_peak)\n    psd = acf_power(acf, norm=norm)\n\n    # upsample 2x psd\n    psd = np.interp(np.arange(0, len(psd), 1 / upsample), np.arange(0, len(psd)), psd)\n    # smooth psd with gaussian filter\n    psd = gaussian_filter1d(psd, smooth_sigma)\n\n    # FIND ALL LOCAL MAXIMA IN WINDOW OF INTEREST\n    all_peaks = find_peaks(psd[freqs_of_interest], None)[0]\n\n    # make sure there is a peak\n    if ~np.any(all_peaks):\n        return (\n            np.nan,\n            np.nan,\n            psd[freqs_of_interest],\n            frequencies[freqs_of_interest],\n            acf,\n        )\n\n    max_peak = np.max(psd[freqs_of_interest][all_peaks])\n    max_idx = [all_peaks[np.argmax(psd[freqs_of_interest][all_peaks])]]\n    max_freq = frequencies[freqs_of_interest][max_idx]\n    MI = max_peak / np.trapz(psd[freqs_of_interest])\n\n    return max_freq, MI, psd[freqs_of_interest], frequencies[freqs_of_interest], acf\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.overlap_intersect","title":"<code>overlap_intersect(epoch, interval, return_indices=True)</code>","text":"<p>Returns the epochs with overlap with the given interval.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The epochs to check.</p> required <code>interval</code> <code>IntervalArray</code> <p>The interval to check for overlap.</p> required <code>return_indices</code> <code>bool</code> <p>If True, returns the indices of the overlapping epochs. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>EpochArray</code> <p>The epochs with overlap with the interval.</p> <code>(Tuple[EpochArray, ndarray], optional)</code> <p>If <code>return_indices</code> is True, also returns the indices of the overlapping epochs.</p> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def overlap_intersect(\n    epoch: nel.EpochArray, interval: nel.IntervalArray, return_indices: bool = True\n) -&gt; Union[nel.EpochArray, Tuple[nel.EpochArray, np.ndarray]]:\n    \"\"\"\n    Returns the epochs with overlap with the given interval.\n\n    Parameters\n    ----------\n    epoch : nelpy.EpochArray\n        The epochs to check.\n    interval : nelpy.IntervalArray\n        The interval to check for overlap.\n    return_indices : bool, optional\n        If True, returns the indices of the overlapping epochs. Default is True.\n\n    Returns\n    -------\n    nelpy.EpochArray\n        The epochs with overlap with the interval.\n    Tuple[nelpy.EpochArray, np.ndarray], optional\n        If `return_indices` is True, also returns the indices of the overlapping epochs.\n    \"\"\"\n    new_intervals = []\n    indices = []\n    for epa in epoch:\n        if any((interval.starts &lt; epa.stop) &amp; (interval.stops &gt; epa.start)):\n            new_intervals.append([epa.start, epa.stop])\n            cand_ep_idx = np.where(\n                (interval.starts &lt; epa.stop) &amp; (interval.stops &gt; epa.start)\n            )\n            indices.append(cand_ep_idx[0][0])\n    out = type(epoch)(new_intervals)\n    out._domain = epoch.domain\n    if return_indices:\n        return out, indices\n    return out\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.pairwise_corr","title":"<code>pairwise_corr(X, method='pearson', pairs=None)</code>","text":"<p>Compute pairwise correlations between all rows of a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D numpy array of shape (n, p), where n is the number of rows (variables) and p is the number of columns (features).</p> required <code>method</code> <code>str</code> <p>Correlation method to use ('pearson', 'spearman', or 'kendall'), by default \"pearson\".</p> <code>'pearson'</code> <code>pairs</code> <code>ndarray</code> <p>Array of shape (m, 2) specifying the pairs of rows to compute correlations between. If None, computes correlations for all unique row pairs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rho</code> <code>ndarray</code> <p>Array of correlation coefficients.</p> <code>pval</code> <code>ndarray</code> <p>Array of p-values for the correlation tests.</p> <code>pairs</code> <code>ndarray</code> <p>Array of pairs (indices) for which correlations were computed.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the method is not 'pearson', 'spearman', or 'kendall'.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.random.rand(10, 5)\n&gt;&gt;&gt; rho, pval, pairs = pairwise_corr(X, method=\"spearman\")\n</code></pre> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def pairwise_corr(\n    X: np.ndarray, method: str = \"pearson\", pairs: Optional[np.ndarray] = None\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute pairwise correlations between all rows of a matrix.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2D numpy array of shape (n, p), where n is the number of rows (variables) and p is the number of columns (features).\n    method : str, optional\n        Correlation method to use ('pearson', 'spearman', or 'kendall'), by default \"pearson\".\n    pairs : np.ndarray, optional\n        Array of shape (m, 2) specifying the pairs of rows to compute correlations between.\n        If None, computes correlations for all unique row pairs.\n\n    Returns\n    -------\n    rho : np.ndarray\n        Array of correlation coefficients.\n    pval : np.ndarray\n        Array of p-values for the correlation tests.\n    pairs : np.ndarray\n        Array of pairs (indices) for which correlations were computed.\n\n    Raises\n    ------\n    ValueError\n        If the method is not 'pearson', 'spearman', or 'kendall'.\n\n    Examples\n    -------\n    &gt;&gt;&gt; X = np.random.rand(10, 5)\n    &gt;&gt;&gt; rho, pval, pairs = pairwise_corr(X, method=\"spearman\")\n    \"\"\"\n    if pairs is None:\n        x = np.arange(0, X.shape[0])\n        pairs = np.array(list(itertools.combinations(x, 2)))\n\n    rho = []\n    pval = []\n    for i, s in enumerate(pairs):\n        if method == \"pearson\":\n            rho_, pval_ = stats.pearsonr(X[s[0], :], X[s[1], :])\n        elif method == \"spearman\":\n            rho_, pval_ = stats.spearmanr(X[s[0], :], X[s[1], :])\n        elif method == \"kendall\":\n            rho_, pval_ = stats.kendalltau(X[s[0], :], X[s[1], :])\n        else:\n            raise ValueError(\"method must be pearson, spearman or kendall\")\n        rho.append(rho_)\n        pval.append(pval_)\n    return rho, pval, pairs\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.pairwise_cross_corr","title":"<code>pairwise_cross_corr(spks, binsize=0.001, nbins=100, return_index=False, pairs=None, deconvolve=False)</code>","text":"<p>Compute pairwise time-lagged cross-correlations between spike trains of different cells.</p> <p>Parameters:</p> Name Type Description Default <code>spks</code> <code>ndarray</code> <p>Nested numpy arrays, where each array contains the spike times for a cell.</p> required <code>binsize</code> <code>float</code> <p>The size of time bins in seconds. Default is 0.001 (1 ms).</p> <code>0.001</code> <code>nbins</code> <code>int</code> <p>Number of bins to use for the correlation window. Default is 100.</p> <code>100</code> <code>return_index</code> <code>bool</code> <p>Whether to return the index (pairs) of cells used for the correlation. Default is False.</p> <code>False</code> <code>pairs</code> <code>ndarray</code> <p>Precomputed list of pairs of cells (indices) to compute the cross-correlation for. If None, all unique pairs will be computed. Default is None.</p> <code>None</code> <code>deconvolve</code> <code>bool</code> <p>Whether to apply deconvolution when computing the cross-correlation. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>crosscorrs</code> <code>DataFrame</code> <p>A pandas DataFrame of shape (t, n_pairs), where t is the time axis and n_pairs are the pairs of cells.</p> <code>pairs</code> <code>(ndarray, optional)</code> <p>The pairs of cells for which cross-correlations were computed. Returned only if <code>return_index</code> is True.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spks = np.array([np.random.rand(100), np.random.rand(100)])\n&gt;&gt;&gt; crosscorrs, pairs = pairwise_cross_corr(spks, binsize=0.01, nbins=50, return_index=True)\n</code></pre> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def pairwise_cross_corr(\n    spks: np.ndarray,\n    binsize: float = 0.001,\n    nbins: int = 100,\n    return_index: bool = False,\n    pairs: Optional[np.ndarray] = None,\n    deconvolve: bool = False,\n) -&gt; Tuple[pd.DataFrame, Optional[np.ndarray]]:\n    \"\"\"\n    Compute pairwise time-lagged cross-correlations between spike trains of different cells.\n\n    Parameters\n    ----------\n    spks : np.ndarray\n        Nested numpy arrays, where each array contains the spike times for a cell.\n    binsize : float, optional\n        The size of time bins in seconds. Default is 0.001 (1 ms).\n    nbins : int, optional\n        Number of bins to use for the correlation window. Default is 100.\n    return_index : bool, optional\n        Whether to return the index (pairs) of cells used for the correlation. Default is False.\n    pairs : np.ndarray, optional\n        Precomputed list of pairs of cells (indices) to compute the cross-correlation for.\n        If None, all unique pairs will be computed. Default is None.\n    deconvolve : bool, optional\n        Whether to apply deconvolution when computing the cross-correlation. Default is False.\n\n    Returns\n    -------\n    crosscorrs : pd.DataFrame\n        A pandas DataFrame of shape (t, n_pairs), where t is the time axis and n_pairs are the pairs of cells.\n    pairs : np.ndarray, optional\n        The pairs of cells for which cross-correlations were computed. Returned only if `return_index` is True.\n\n    Examples\n    -------\n    &gt;&gt;&gt; spks = np.array([np.random.rand(100), np.random.rand(100)])\n    &gt;&gt;&gt; crosscorrs, pairs = pairwise_cross_corr(spks, binsize=0.01, nbins=50, return_index=True)\n    \"\"\"\n    # Get unique combo without repeats\n    if pairs is None:\n        x = np.arange(0, spks.shape[0])\n        pairs = np.array(list(itertools.combinations(x, 2)))\n\n    # prepare a pandas dataframe to receive the data\n    times = np.linspace(-(nbins * binsize) / 2, (nbins * binsize) / 2, nbins + 1)\n\n    def compute_crosscorr(pair):\n        i, j = pair\n        crosscorr = crossCorr(spks[i], spks[j], binsize, nbins)\n        return crosscorr\n\n    def compute_crosscorr_deconvolve(pair):\n        i, j = pair\n        crosscorr, _ = deconvolve_peth(spks[i], spks[j], binsize, nbins)\n        return crosscorr\n\n    if deconvolve:\n        crosscorrs = [compute_crosscorr_deconvolve(pair) for pair in pairs]\n    else:\n        crosscorrs = [compute_crosscorr(pair) for pair in pairs]\n\n    crosscorrs = pd.DataFrame(\n        index=times,\n        data=np.array(crosscorrs).T,\n        columns=np.arange(len(pairs)),\n    )\n\n    if return_index:\n        return crosscorrs, pairs\n    else:\n        return crosscorrs\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.pairwise_spatial_corr","title":"<code>pairwise_spatial_corr(X, return_index=False, pairs=None)</code>","text":"<p>Compute pairwise spatial correlations between cells' spatial maps.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 3D numpy array of shape (n_cells, n_space, n_space) representing the spatial maps of cells.</p> required <code>return_index</code> <code>bool</code> <p>If True, returns the indices of the cell pairs used for the correlation.</p> <code>False</code> <code>pairs</code> <code>ndarray</code> <p>Array of cell pairs for which to compute the correlation. If not provided, all unique pairs are used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>spatial_corr</code> <code>ndarray</code> <p>Array containing the Pearson correlation coefficients for each pair of cells.</p> <code>pairs</code> <code>(ndarray, optional)</code> <p>Array of cell pairs used for the correlation (if return_index is True).</p> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def pairwise_spatial_corr(\n    X: np.ndarray, return_index: bool = False, pairs: np.ndarray = None\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Compute pairwise spatial correlations between cells' spatial maps.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        A 3D numpy array of shape (n_cells, n_space, n_space) representing the spatial maps of cells.\n    return_index : bool, optional\n        If True, returns the indices of the cell pairs used for the correlation.\n    pairs : np.ndarray, optional\n        Array of cell pairs for which to compute the correlation. If not provided, all unique pairs are used.\n\n    Returns\n    -------\n    spatial_corr : np.ndarray\n        Array containing the Pearson correlation coefficients for each pair of cells.\n    pairs : np.ndarray, optional\n        Array of cell pairs used for the correlation (if return_index is True).\n    \"\"\"\n    # Get unique combo without repeats\n    if pairs is None:\n        x = np.arange(0, X.shape[0])\n        pairs = np.array(list(itertools.combinations(x, 2)))\n\n    spatial_corr = []\n    # Now we can iterate over spikes\n    for i, s in enumerate(pairs):\n        # Calling the crossCorr function\n        x1 = X[s[0], :, :].flatten()\n        x2 = X[s[1], :, :].flatten()\n        bad_idx = np.isnan(x1) | np.isnan(x2)\n        spatial_corr.append(np.corrcoef(x1[~bad_idx], x2[~bad_idx])[0, 1])\n\n    if return_index:\n        return np.array(spatial_corr), pairs\n    else:\n        return np.array(spatial_corr)\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.peth_matrix","title":"<code>peth_matrix(data, time_ref, bin_width=0.002, n_bins=100, window=None)</code>","text":"<p>Generate a peri-event time histogram (PETH) matrix.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A 1D array of time values.</p> required <code>time_ref</code> <code>ndarray</code> <p>A 1D array of reference times.</p> required <code>bin_width</code> <code>float</code> <p>The width of each bin in the PETH matrix, in seconds. Default is 0.002 seconds.</p> <code>0.002</code> <code>n_bins</code> <code>int</code> <p>The number of bins in the PETH matrix. Default is 100.</p> <code>100</code> <code>window</code> <code>tuple</code> <p>A tuple containing the start and end times of the window to be plotted around each reference time. If not provided, the window will be centered around each reference time and have a width of <code>n_bins * bin_width</code> seconds.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>H</code> <code>ndarray</code> <p>A 2D array representing the PETH matrix.</p> <code>t</code> <code>ndarray</code> <p>A 1D array of time values corresponding to the bins in the PETH matrix.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>@jit(nopython=True, parallel=True)\ndef peth_matrix(\n    data: np.ndarray,\n    time_ref: np.ndarray,\n    bin_width: float = 0.002,\n    n_bins: int = 100,\n    window: Union[list, None] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate a peri-event time histogram (PETH) matrix.\n\n    Parameters\n    ----------\n    data : ndarray\n        A 1D array of time values.\n    time_ref : ndarray\n        A 1D array of reference times.\n    bin_width : float, optional\n        The width of each bin in the PETH matrix, in seconds. Default is 0.002 seconds.\n    n_bins : int, optional\n        The number of bins in the PETH matrix. Default is 100.\n    window : tuple, optional\n        A tuple containing the start and end times of the window to be plotted around each reference time.\n        If not provided, the window will be centered around each reference time and have a width of `n_bins * bin_width` seconds.\n\n    Returns\n    -------\n    H : ndarray\n        A 2D array representing the PETH matrix.\n    t : ndarray\n        A 1D array of time values corresponding to the bins in the PETH matrix.\n\n    \"\"\"\n    if window is not None:\n        times = np.arange(window[0], window[1] + bin_width / 2, bin_width)\n        n_bins = len(times) - 1\n    else:\n        times = (\n            np.arange(0, bin_width * n_bins, bin_width)\n            - (bin_width * n_bins) / 2\n            + bin_width / 2\n        )\n\n    H = np.zeros((len(times), len(time_ref)))\n\n    for event_i in prange(len(time_ref)):\n        H[:, event_i] = crossCorr([time_ref[event_i]], data, bin_width, n_bins)\n\n    return H * bin_width, times\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.point_spectra","title":"<code>point_spectra(times, Fs=1250, freq_range=[1, 20], tapers0=[3, 5], pad=0, nfft=None)</code>","text":"<p>Compute multitaper power spectrum for point processes.</p> <p>Parameters:</p> Name Type Description Default <code>times</code> <code>ndarray</code> <p>Array of spike times (in seconds).</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency (default is 1250 Hz).</p> <code>1250</code> <code>freq_range</code> <code>List[float]</code> <p>Frequency range to evaluate (default is [1, 20] Hz).</p> <code>[1, 20]</code> <code>tapers0</code> <code>List[int]</code> <p>Time-bandwidth product and number of tapers (default is [3, 5]). The time-bandwidth product is used to compute the tapers.</p> <code>[3, 5]</code> <code>pad</code> <code>int</code> <p>Padding for the FFT (default is 0).</p> <code>0</code> <code>nfft</code> <code>Optional[int]</code> <p>Number of points for FFT (default is None).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>spectra</code> <code>ndarray</code> <p>Power spectrum.</p> <code>f</code> <code>ndarray</code> <p>Frequency vector.</p> Notes <p>Alternative function to <code>mtspectrumpt</code> for computing the power spectrum</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def point_spectra(\n    times: np.ndarray,\n    Fs: int = 1250,\n    freq_range: List[float] = [1, 20],\n    tapers0: List[int] = [3, 5],\n    pad: int = 0,\n    nfft: Optional[int] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute multitaper power spectrum for point processes.\n\n    Parameters\n    ----------\n    times : np.ndarray\n        Array of spike times (in seconds).\n    Fs : int, optional\n        Sampling frequency (default is 1250 Hz).\n    freq_range : List[float], optional\n        Frequency range to evaluate (default is [1, 20] Hz).\n    tapers0 : List[int], optional\n        Time-bandwidth product and number of tapers (default is [3, 5]).\n        The time-bandwidth product is used to compute the tapers.\n    pad : int, optional\n        Padding for the FFT (default is 0).\n    nfft : Optional[int], optional\n        Number of points for FFT (default is None).\n\n    Returns\n    -------\n    spectra : np.ndarray\n        Power spectrum.\n    f : np.ndarray\n        Frequency vector.\n\n    Notes\n    -----\n    Alternative function to `mtspectrumpt` for computing the power spectrum\n    \"\"\"\n\n    # generate frequency grid\n    timesRange = [min(times), max(times)]\n    window = np.floor(np.diff(timesRange))\n    nSamplesPerWindow = int(np.round(Fs * window[0]))\n    if nfft is None:\n        nfft = np.max(\n            [(int(2 ** np.ceil(np.log2(nSamplesPerWindow))) + pad), nSamplesPerWindow]\n        )\n    fAll = np.linspace(0, Fs, int(nfft))\n    frequency_ind = (fAll &gt;= freq_range[0]) &amp; (fAll &lt;= freq_range[1])\n\n    # Generate tapers\n    tapers, _ = dpss(nSamplesPerWindow, tapers0[0], tapers0[1], return_ratios=True)\n    tapers = tapers * np.sqrt(Fs)\n\n    # Compute FFT of tapers and restrict to required frequencies\n    H = np.fft.fft(tapers, n=nfft, axis=1)  # Shape: (K, nfft)\n    H = H[:, frequency_ind]  # Shape: (K, Nf)\n\n    # Angular frequencies\n    f = fAll[frequency_ind]\n    w = 2 * np.pi * f\n\n    # Time grid\n    timegrid = np.linspace(timesRange[0], timesRange[1], nSamplesPerWindow)\n\n    # Ensure times are within range\n    data = times[(times &gt;= timegrid[0]) &amp; (times &lt;= timegrid[-1])]\n\n    # Project spike times onto tapers\n    data_proj = [np.interp(data, timegrid, taper) for taper in tapers]\n    data_proj = np.vstack(data_proj)  # Shape: (K, len(data))\n\n    # Compute multitaper spectrum\n    exponential = np.exp(\n        np.outer(-1j * w, (data - timegrid[0]))\n    )  # Shape: (Nf, len(data))\n    J = exponential @ data_proj.T - H.T * len(data) / len(timegrid)  # Shape: (Nf, K)\n    spectra = np.squeeze(np.mean(np.real(np.conj(J) * J), axis=1))  # Mean across tapers\n\n    return spectra, f\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.randomize_epochs","title":"<code>randomize_epochs(epoch, randomize_each=True, start_stop=None)</code>","text":"<p>Randomly shifts the epochs of a EpochArray object and wraps them around the original time boundaries.</p> <p>This method takes a EpochArray object as input, and can either randomly shift each epoch by a different amount (if <code>randomize_each</code> is True) or shift all the epochs by the same amount (if <code>randomize_each</code> is False). In either case, the method wraps the shifted epochs around the original time boundaries to make sure they remain within the original time range. It then returns the modified EpochArray object.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The EpochArray object whose epochs should be shifted and wrapped.</p> required <code>randomize_each</code> <code>bool</code> <p>If True, each epoch will be shifted by a different random amount. If False, all the epochs will be shifted by the same random amount. Defaults to True.</p> <code>True</code> <code>start_stop</code> <code>array</code> <p>If not None, time support will be taken from start_stop</p> <code>None</code> <p>Returns:</p> Name Type Description <code>new_epochs</code> <code>EpochArray</code> <p>The modified EpochArray object with the shifted and wrapped epochs.</p> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def randomize_epochs(\n    epoch: EpochArray,\n    randomize_each: bool = True,\n    start_stop: Optional[np.ndarray] = None,\n) -&gt; EpochArray:\n    \"\"\"\n    Randomly shifts the epochs of a EpochArray object and wraps them around the original time boundaries.\n\n    This method takes a EpochArray object as input, and can either randomly shift each epoch by a different amount\n    (if `randomize_each` is True) or shift all the epochs by the same amount (if `randomize_each` is False).\n    In either case, the method wraps the shifted epochs around the original time boundaries to make sure they remain\n    within the original time range. It then returns the modified EpochArray object.\n\n    Parameters\n    ----------\n    epoch : EpochArray\n        The EpochArray object whose epochs should be shifted and wrapped.\n    randomize_each : bool, optional\n        If True, each epoch will be shifted by a different random amount.\n        If False, all the epochs will be shifted by the same random amount. Defaults to True.\n    start_stop : array, optional\n        If not None, time support will be taken from start_stop\n\n    Returns\n    -------\n    new_epochs : EpochArray\n        The modified EpochArray object with the shifted and wrapped epochs.\n    \"\"\"\n\n    def wrap_intervals(intervals, start, stop):\n        idx = np.any(intervals &gt; stop, axis=1)\n        intervals[idx] = intervals[idx] - stop + start\n\n        idx = np.any(intervals &lt; start, axis=1)\n        intervals[idx] = intervals[idx] - start + stop\n        return intervals\n\n    new_epochs = epoch.copy()\n\n    if start_stop is None:\n        start = new_epochs.start\n        stop = new_epochs.stop\n    else:\n        start, stop = start_stop\n\n    ts_range = stop - start\n\n    if randomize_each:\n        # Randomly shift each epoch by a different amount\n        random_order = random.sample(\n            range(-int(ts_range), int(ts_range)), new_epochs.n_intervals\n        )\n\n        new_intervals = new_epochs.data + np.expand_dims(random_order, axis=1)\n        new_epochs._data = wrap_intervals(new_intervals, start, stop)\n    else:\n        # Shift all the epochs by the same amount\n        random_shift = random.randint(-int(ts_range), int(ts_range))\n        new_epochs._data = wrap_intervals((new_epochs.data + random_shift), start, stop)\n\n    if not new_epochs.isempty:\n        if np.any(new_epochs.data[:, 1] - new_epochs.data[:, 0] &lt; 0):\n            raise ValueError(\"start must be less than or equal to stop\")\n\n    new_epochs._sort()\n\n    return new_epochs\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.relative_times","title":"<code>relative_times(t, intervals, values=np.array([0, 1]))</code>","text":"<p>Calculate relative times and interval IDs for a set of time points.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray</code> <p>An array of time points.</p> required <code>intervals</code> <code>ndarray</code> <p>An array of time intervals, represented as pairs of start and end times.</p> required <code>values</code> <code>ndarray</code> <p>An array of values to assign to interval bounds. The default is [0,1].</p> <code>array([0, 1])</code> <p>Returns:</p> Name Type Description <code>rt</code> <code>ndarray</code> <p>An array of relative times, one for each time point (same len as t).</p> <code>intervalID</code> <code>ndarray</code> <p>An array of interval IDs, one for each time point (same len as t).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n&gt;&gt;&gt; intervals = np.array([[1, 3], [4, 6], [7, 9]])\n&gt;&gt;&gt; relative_times(t, intervals)\n    (array([nan, 0. , 0.5, 1. , 0. , 0.5, 1. , 0. , 0.5, 1. ]),\n    array([nan,  0.,  0.,  0.,  1.,  1.,  1.,  2.,  2.,  2.]))\n</code></pre> <pre><code>&gt;&gt;&gt; t = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n&gt;&gt;&gt; intervals = np.array([[1, 3], [4, 6], [7, 9]])\n&gt;&gt;&gt; values = np.array([0, 2*np.pi])\n&gt;&gt;&gt; relative_times(t, intervals, values)\n    (array([       nan, 0.        , 3.14159265, 6.28318531, 0.        ,\n            3.14159265, 6.28318531, 0.        , 3.14159265, 6.28318531]),\n    array([nan,  0.,  0.,  0.,  1.,  1.,  1.,  2.,  2.,  2.]))\n</code></pre> Notes <p>Intervals are defined as pairs of start and end times. The relative time is the time within the interval, normalized to the interval duration. The interval ID is the index of the interval in the intervals array. The values array can be used to assign a value to each interval.</p> <p>By Ryan H, based on RelativeTimes.m by Ralitsa Todorova</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>@jit(nopython=True)\ndef relative_times(\n    t: np.ndarray, intervals: np.ndarray, values: np.ndarray = np.array([0, 1])\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate relative times and interval IDs for a set of time points.\n\n    Parameters\n    ----------\n    t : np.ndarray\n        An array of time points.\n    intervals : np.ndarray\n        An array of time intervals, represented as pairs of start and end times.\n    values : np.ndarray, optional\n        An array of values to assign to interval bounds. The default is [0,1].\n\n    Returns\n    -------\n    rt : np.ndarray\n        An array of relative times, one for each time point (same len as t).\n    intervalID : np.ndarray\n        An array of interval IDs, one for each time point (same len as t).\n\n    Examples\n    --------\n    &gt;&gt;&gt; t = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    &gt;&gt;&gt; intervals = np.array([[1, 3], [4, 6], [7, 9]])\n    &gt;&gt;&gt; relative_times(t, intervals)\n        (array([nan, 0. , 0.5, 1. , 0. , 0.5, 1. , 0. , 0.5, 1. ]),\n        array([nan,  0.,  0.,  0.,  1.,  1.,  1.,  2.,  2.,  2.]))\n\n    &gt;&gt;&gt; t = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    &gt;&gt;&gt; intervals = np.array([[1, 3], [4, 6], [7, 9]])\n    &gt;&gt;&gt; values = np.array([0, 2*np.pi])\n    &gt;&gt;&gt; relative_times(t, intervals, values)\n        (array([       nan, 0.        , 3.14159265, 6.28318531, 0.        ,\n                3.14159265, 6.28318531, 0.        , 3.14159265, 6.28318531]),\n        array([nan,  0.,  0.,  0.,  1.,  1.,  1.,  2.,  2.,  2.]))\n\n    Notes\n    -----\n    Intervals are defined as pairs of start and end times. The relative time is the time\n    within the interval, normalized to the interval duration. The interval ID is the index\n    of the interval in the intervals array. The values array can be used to assign a value\n    to each interval.\n\n    By Ryan H, based on RelativeTimes.m by Ralitsa Todorova\n\n    \"\"\"\n\n    rt = np.zeros(len(t), dtype=np.float64) * np.nan\n    intervalID = np.zeros(len(t), dtype=np.float64) * np.nan\n\n    start_times = intervals[:, 0]\n    end_times = intervals[:, 1]\n    values_diff = values[1] - values[0]\n    intervals_diff = end_times - start_times\n    intervals_scale = values_diff / intervals_diff\n\n    for i in range(len(t)):\n        idx = np.searchsorted(start_times, t[i])\n        if idx &gt; 0 and t[i] &lt;= end_times[idx - 1]:\n            interval_i = idx - 1\n        elif idx &lt; len(start_times) and t[i] == start_times[idx]:\n            interval_i = idx\n        else:\n            continue\n\n        scale = intervals_scale[interval_i]\n        rt[i] = ((t[i] - start_times[interval_i]) * scale) + values[0]\n        intervalID[i] = interval_i\n\n    return rt, intervalID\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.remove_inactive_cells","title":"<code>remove_inactive_cells(st, cell_metrics=None, epochs=None, min_spikes=100)</code>","text":"<p>remove_inactive_cells: Remove cells with fewer than min_spikes spikes per sub-epoch</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>SpikeTrainArray</code> <p>SpikeTrainArray object containing spike times for multiple cells.</p> required <code>cell_metrics</code> <code>DataFrame</code> <p>DataFrame containing metrics for each cell (e.g., quality metrics).</p> <code>None</code> <code>epochs</code> <code>EpochArray or list of EpochArray</code> <p>If a list of EpochArray objects is provided, each EpochArray object is treated as a sub-epoch. If a single EpochArray object is provided, each interval in the EpochArray object is treated as a sub-epoch.</p> <code>None</code> <code>min_spikes</code> <code>int</code> <p>Minimum number of spikes required per sub-epoch to retain a cell. Default is 100.</p> <code>100</code> <p>Returns:</p> Type Description <code>Tuple[SpikeTrainArray, Union[DataFrame, None]]</code> <p>A tuple containing: - SpikeTrainArray object with inactive cells removed. - DataFrame containing cell metrics with inactive cells removed (if provided).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.process.intervals import truncate_epoch\n&gt;&gt;&gt; from neuro_py.session.locate_epochs import (\n&gt;&gt;&gt;     find_multitask_pre_post,\n&gt;&gt;&gt;     compress_repeated_epochs,\n&gt;&gt;&gt; )\n&gt;&gt;&gt; from neuro_py.io import loading\n&gt;&gt;&gt; import nelpy as nel\n&gt;&gt;&gt; from neuro_py.process.utils import remove_inactive_cells\n</code></pre> <pre><code>&gt;&gt;&gt; # load data from session\n&gt;&gt;&gt; basepath = r\"Z:\\Data\\hpc_ctx_project\\HP04\\day_1_20240320\"\n</code></pre> <pre><code>&gt;&gt;&gt; # load spikes and cell metrics (cm)\n&gt;&gt;&gt; st, cm = loading.load_spikes(basepath, brainRegion=\"CA1\", putativeCellType=\"Pyr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # load epochs and apply multitask epoch restrictions\n&gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n&gt;&gt;&gt; epoch_df = compress_repeated_epochs(epoch_df)\n&gt;&gt;&gt; pre_task_post = find_multitask_pre_post(\n&gt;&gt;&gt;     epoch_df.environment, post_sleep_flank=True, pre_sleep_common=True\n&gt;&gt;&gt; )\n</code></pre> <pre><code>&gt;&gt;&gt; beh_epochs = nel.EpochArray(\n&gt;&gt;&gt;     epoch_df.iloc[pre_task_post[0]][[\"startTime\", \"stopTime\"]].values\n&gt;&gt;&gt; )\n&gt;&gt;&gt; # load sleep states to restrict to NREM and theta\n&gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n&gt;&gt;&gt; nrem_epochs = nel.EpochArray(\n&gt;&gt;&gt;     state_dict[\"NREMstate\"],\n&gt;&gt;&gt; )\n&gt;&gt;&gt; theta_epochs = nel.EpochArray(\n&gt;&gt;&gt;     state_dict[\"THETA\"],\n&gt;&gt;&gt; )\n&gt;&gt;&gt; # create list of restricted epochs\n&gt;&gt;&gt; restict_epochs = []\n&gt;&gt;&gt; for epoch, epoch_label in zip(beh_epochs, [\"pre\", \"task\", \"post\"]):\n&gt;&gt;&gt;     if epoch_label in \"pre\":\n&gt;&gt;&gt;         # get cumulative hours of sleep\n&gt;&gt;&gt;         epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=3600)\n&gt;&gt;&gt;     elif epoch_label in \"post\":\n&gt;&gt;&gt;         # get cumulative hours of sleep\n&gt;&gt;&gt;         epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=3600)\n&gt;&gt;&gt;     else:\n&gt;&gt;&gt;         # get theta during task\n&gt;&gt;&gt;         epoch_restrict = epoch &amp; theta_epochs\n&gt;&gt;&gt;     restict_epochs.append(epoch_restrict)\n</code></pre> <pre><code>&gt;&gt;&gt; # remove inactive cells\n&gt;&gt;&gt; st, cm = remove_inactive_cells(st, cm, restict_epochs)\n</code></pre> Source code in <code>neuro_py/process/utils.py</code> <pre><code>def remove_inactive_cells(\n    st: nel.core._eventarray.SpikeTrainArray,\n    cell_metrics: Union[pd.DataFrame, None] = None,\n    epochs: Union[\n        List[nel.core._intervalarray.EpochArray],\n        nel.core._intervalarray.EpochArray,\n        None,\n    ] = None,\n    min_spikes: int = 100,\n) -&gt; Tuple[nel.core._eventarray.SpikeTrainArray, Union[pd.DataFrame, None]]:\n    \"\"\"\n    remove_inactive_cells: Remove cells with fewer than min_spikes spikes per sub-epoch\n\n    Parameters\n    ----------\n    st : SpikeTrainArray\n        SpikeTrainArray object containing spike times for multiple cells.\n\n    cell_metrics : pd.DataFrame, optional\n        DataFrame containing metrics for each cell (e.g., quality metrics).\n\n    epochs : EpochArray or list of EpochArray, optional\n        If a list of EpochArray objects is provided, each EpochArray object\n        is treated as a sub-epoch. If a single EpochArray object is provided,\n        each interval in the EpochArray object is treated as a sub-epoch.\n\n    min_spikes : int, optional\n        Minimum number of spikes required per sub-epoch to retain a cell.\n        Default is 100.\n\n    Returns\n    -------\n    Tuple[SpikeTrainArray, Union[pd.DataFrame, None]]\n        A tuple containing:\n        - SpikeTrainArray object with inactive cells removed.\n        - DataFrame containing cell metrics with inactive cells removed (if provided).\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.process.intervals import truncate_epoch\n    &gt;&gt;&gt; from neuro_py.session.locate_epochs import (\n    &gt;&gt;&gt;     find_multitask_pre_post,\n    &gt;&gt;&gt;     compress_repeated_epochs,\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; from neuro_py.io import loading\n    &gt;&gt;&gt; import nelpy as nel\n    &gt;&gt;&gt; from neuro_py.process.utils import remove_inactive_cells\n\n    &gt;&gt;&gt; # load data from session\n    &gt;&gt;&gt; basepath = r\"Z:\\Data\\hpc_ctx_project\\HP04\\day_1_20240320\"\n\n    &gt;&gt;&gt; # load spikes and cell metrics (cm)\n    &gt;&gt;&gt; st, cm = loading.load_spikes(basepath, brainRegion=\"CA1\", putativeCellType=\"Pyr\")\n\n    &gt;&gt;&gt; # load epochs and apply multitask epoch restrictions\n    &gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n    &gt;&gt;&gt; epoch_df = compress_repeated_epochs(epoch_df)\n    &gt;&gt;&gt; pre_task_post = find_multitask_pre_post(\n    &gt;&gt;&gt;     epoch_df.environment, post_sleep_flank=True, pre_sleep_common=True\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; beh_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     epoch_df.iloc[pre_task_post[0]][[\"startTime\", \"stopTime\"]].values\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; # load sleep states to restrict to NREM and theta\n    &gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n    &gt;&gt;&gt; nrem_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     state_dict[\"NREMstate\"],\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; theta_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     state_dict[\"THETA\"],\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; # create list of restricted epochs\n    &gt;&gt;&gt; restict_epochs = []\n    &gt;&gt;&gt; for epoch, epoch_label in zip(beh_epochs, [\"pre\", \"task\", \"post\"]):\n    &gt;&gt;&gt;     if epoch_label in \"pre\":\n    &gt;&gt;&gt;         # get cumulative hours of sleep\n    &gt;&gt;&gt;         epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=3600)\n    &gt;&gt;&gt;     elif epoch_label in \"post\":\n    &gt;&gt;&gt;         # get cumulative hours of sleep\n    &gt;&gt;&gt;         epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=3600)\n    &gt;&gt;&gt;     else:\n    &gt;&gt;&gt;         # get theta during task\n    &gt;&gt;&gt;         epoch_restrict = epoch &amp; theta_epochs\n    &gt;&gt;&gt;     restict_epochs.append(epoch_restrict)\n\n    &gt;&gt;&gt; # remove inactive cells\n    &gt;&gt;&gt; st, cm = remove_inactive_cells(st, cm, restict_epochs)\n    \"\"\"\n\n    def return_results(st, cell_metrics):\n        if cell_metrics is None:\n            return st\n        else:\n            return st, cell_metrics\n\n    # check data types\n    if not isinstance(st, nel.core._eventarray.SpikeTrainArray):\n        raise ValueError(\"st must be a SpikeTrainArray object\")\n\n    if not isinstance(cell_metrics, (pd.core.frame.DataFrame, type(None))):\n        raise ValueError(\"cell_metrics must be a DataFrame object\")\n\n    if not isinstance(epochs, (nel.core._intervalarray.EpochArray, list)):\n        raise ValueError(\n            \"epochs must be an EpochArray object or a list of EpochArray objects\"\n        )\n\n    if isinstance(epochs, list):\n        for epoch in epochs:\n            if not isinstance(epoch, nel.core._intervalarray.EpochArray):\n                raise ValueError(\"list of epochs must contain EpochArray objects\")\n\n    # check if st is empty\n    if st.isempty:\n        return return_results(st, cell_metrics)\n\n    # check if epochs is empty\n    if isinstance(epochs, nel.core._intervalarray.EpochArray):\n        if epochs.isempty:\n            return return_results(st, cell_metrics)\n\n    # check if cell_metrics is empty\n    if cell_metrics is not None and cell_metrics.empty:\n        return return_results(st, cell_metrics)\n\n    # check if min_spikes is less than 1\n    if min_spikes &lt; 1:\n        return return_results(st, cell_metrics)\n\n    # check if st and cell_metrics have the same number of units\n    if cell_metrics is not None and st.n_units != cell_metrics.shape[0]:\n        # assert error message\n        raise ValueError(\"st and cell_metrics must have the same number of units\")\n\n    spk_thres_met = []\n    # check if each cell has at least min_spikes spikes in each epoch\n    for epoch_restrict in epochs:\n        if st[epoch_restrict].isempty:\n            spk_thres_met.append([False] * st.n_units)\n            continue\n        spk_thres_met.append(st[epoch_restrict].n_events &gt;= min_spikes)\n\n    good_idx = np.vstack(spk_thres_met).all(axis=0)\n\n    # remove inactive cells\n    st = st.iloc[:, good_idx]\n    if cell_metrics is not None:\n        cell_metrics = cell_metrics[good_idx]\n\n    return return_results(st, cell_metrics)\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.remove_inactive_cells_pre_task_post","title":"<code>remove_inactive_cells_pre_task_post(st, cell_metrics=None, beh_epochs=None, nrem_epochs=None, theta_epochs=None, min_spikes=100, nrem_time=3600)</code>","text":"<p>remove_inactive_cells_pre_task_post: Remove cells with fewer than min_spikes spikes per pre/task/post</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>SpikeTrainArray</code> <p>SpikeTrainArray object containing spike times for multiple cells.</p> required <code>cell_metrics</code> <code>DataFrame</code> <p>DataFrame containing metrics for each cell (e.g., quality metrics).</p> <code>None</code> <code>beh_epochs</code> <code>EpochArray</code> <p>EpochArray object containing pre/task/post epochs.</p> <code>None</code> <code>nrem_epochs</code> <code>EpochArray</code> <p>EpochArray object containing NREM epochs.</p> <code>None</code> <code>theta_epochs</code> <code>EpochArray</code> <p>EpochArray object containing theta epochs.</p> <code>None</code> <code>min_spikes</code> <code>int</code> <p>Minimum number of spikes required per pre/task/post. Default is 100.</p> <code>100</code> <code>nrem_time</code> <code>int or float</code> <p>Time in seconds to truncate NREM epochs. Default is 3600 seconds.</p> <code>3600</code> <p>Returns:</p> Type Description <code>Tuple[SpikeTrainArray, Union[DataFrame, None]]</code> <p>A tuple containing: - SpikeTrainArray object with inactive cells removed. - DataFrame containing cell metrics with inactive cells removed (if provided).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.process.utils import remove_inactive_cells_pre_task_post\n&gt;&gt;&gt; from neuro_py.io import loading\n&gt;&gt;&gt; from neuro_py.session.locate_epochs import (\n&gt;&gt;&gt;     find_multitask_pre_post,\n&gt;&gt;&gt;     compress_repeated_epochs,\n&gt;&gt;&gt; )\n&gt;&gt;&gt; mport nelpy as nel\n</code></pre> <pre><code>&gt;&gt;&gt; # load data from session\n&gt;&gt;&gt; basepath = r\"Z:\\Data\\hpc_ctx_project\\HP04\\day_1_20240320\"\n</code></pre> <pre><code>&gt;&gt;&gt; # load spikes and cell metrics (cm)\n&gt;&gt;&gt; st, cm = loading.load_spikes(basepath, brainRegion=\"CA1\", putativeCellType=\"Pyr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # load epochs and apply multitask epoch restrictions\n&gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n&gt;&gt;&gt; epoch_df = compress_repeated_epochs(epoch_df)\n&gt;&gt;&gt; pre_task_post = find_multitask_pre_post(\n&gt;&gt;&gt;     epoch_df.environment, post_sleep_flank=True, pre_sleep_common=True\n&gt;&gt;&gt; )\n</code></pre> <pre><code>&gt;&gt;&gt; beh_epochs = nel.EpochArray(\n&gt;&gt;&gt;     epoch_df.iloc[pre_task_post[0]][[\"startTime\", \"stopTime\"]].values\n&gt;&gt;&gt; )\n</code></pre> <pre><code>&gt;&gt;&gt; # load sleep states to restrict to NREM and theta\n&gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n&gt;&gt;&gt; nrem_epochs = nel.EpochArray(\n&gt;&gt;&gt;     state_dict[\"NREMstate\"],\n&gt;&gt;&gt; )\n&gt;&gt;&gt; theta_epochs = nel.EpochArray(\n&gt;&gt;&gt;     state_dict[\"THETA\"],\n&gt;&gt;&gt; )\n</code></pre> <pre><code>&gt;&gt;&gt; st,cm = remove_inactive_cells_pre_task_post(st,cm,beh_epochs,nrem_epochs,theta_epochs)\n</code></pre> Source code in <code>neuro_py/process/utils.py</code> <pre><code>def remove_inactive_cells_pre_task_post(\n    st: nel.core._eventarray.SpikeTrainArray,\n    cell_metrics: Union[pd.core.frame.DataFrame, None] = None,\n    beh_epochs: nel.core._intervalarray.EpochArray = None,\n    nrem_epochs: nel.core._intervalarray.EpochArray = None,\n    theta_epochs: nel.core._intervalarray.EpochArray = None,\n    min_spikes: int = 100,\n    nrem_time: Union[int, float] = 3600,\n) -&gt; tuple:\n    \"\"\"\n    remove_inactive_cells_pre_task_post: Remove cells with fewer than min_spikes spikes per pre/task/post\n\n    Parameters\n    ----------\n    st : SpikeTrainArray\n        SpikeTrainArray object containing spike times for multiple cells.\n\n    cell_metrics : pd.DataFrame, optional\n        DataFrame containing metrics for each cell (e.g., quality metrics).\n\n    beh_epochs : EpochArray\n        EpochArray object containing pre/task/post epochs.\n\n    nrem_epochs : EpochArray\n        EpochArray object containing NREM epochs.\n\n    theta_epochs : EpochArray\n        EpochArray object containing theta epochs.\n\n    min_spikes : int, optional\n        Minimum number of spikes required per pre/task/post. Default is 100.\n\n    nrem_time : int or float, optional\n        Time in seconds to truncate NREM epochs. Default is 3600 seconds.\n\n    Returns\n    -------\n    Tuple[nel.core._eventarray.SpikeTrainArray, Union[pd.DataFrame, None]]\n        A tuple containing:\n        - SpikeTrainArray object with inactive cells removed.\n        - DataFrame containing cell metrics with inactive cells removed (if provided).\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.process.utils import remove_inactive_cells_pre_task_post\n    &gt;&gt;&gt; from neuro_py.io import loading\n    &gt;&gt;&gt; from neuro_py.session.locate_epochs import (\n    &gt;&gt;&gt;     find_multitask_pre_post,\n    &gt;&gt;&gt;     compress_repeated_epochs,\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; mport nelpy as nel\n\n    &gt;&gt;&gt; # load data from session\n    &gt;&gt;&gt; basepath = r\"Z:\\Data\\hpc_ctx_project\\HP04\\day_1_20240320\"\n\n    &gt;&gt;&gt; # load spikes and cell metrics (cm)\n    &gt;&gt;&gt; st, cm = loading.load_spikes(basepath, brainRegion=\"CA1\", putativeCellType=\"Pyr\")\n\n    &gt;&gt;&gt; # load epochs and apply multitask epoch restrictions\n    &gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n    &gt;&gt;&gt; epoch_df = compress_repeated_epochs(epoch_df)\n    &gt;&gt;&gt; pre_task_post = find_multitask_pre_post(\n    &gt;&gt;&gt;     epoch_df.environment, post_sleep_flank=True, pre_sleep_common=True\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; beh_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     epoch_df.iloc[pre_task_post[0]][[\"startTime\", \"stopTime\"]].values\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; # load sleep states to restrict to NREM and theta\n    &gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n    &gt;&gt;&gt; nrem_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     state_dict[\"NREMstate\"],\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; theta_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     state_dict[\"THETA\"],\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; st,cm = remove_inactive_cells_pre_task_post(st,cm,beh_epochs,nrem_epochs,theta_epochs)\n    \"\"\"\n\n    # check data types (further checks are done in remove_inactive_cells)\n    if not isinstance(beh_epochs, nel.core._intervalarray.EpochArray):\n        raise ValueError(\"beh_epochs must be an EpochArray object\")\n\n    if not isinstance(nrem_epochs, nel.core._intervalarray.EpochArray):\n        raise ValueError(\"nrem_epochs must be an EpochArray object\")\n\n    if not isinstance(theta_epochs, nel.core._intervalarray.EpochArray):\n        raise ValueError(\"theta_epochs must be an EpochArray object\")\n\n    # create list of restricted epochs\n    restict_epochs = []\n    for epoch, epoch_label in zip(beh_epochs, [\"pre\", \"task\", \"post\"]):\n        if epoch_label in \"pre\":\n            # get cumulative hours of sleep\n            epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=nrem_time)\n        elif epoch_label in \"post\":\n            # get cumulative hours of sleep\n            epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=nrem_time)\n        else:\n            # get theta during task\n            epoch_restrict = epoch &amp; theta_epochs\n        restict_epochs.append(epoch_restrict)\n\n    return remove_inactive_cells(\n        st, cell_metrics, restict_epochs, min_spikes=min_spikes\n    )\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.shift_epoch_array","title":"<code>shift_epoch_array(epoch, epoch_shift)</code>","text":"<p>Shift an EpochArray by another EpochArray.</p> <p>Shifting means that intervals in 'epoch' will be relative to intervals in 'epoch_shift' as if 'epoch_shift' intervals were without gaps.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The intervals to shift.</p> required <code>epoch_shift</code> <code>EpochArray</code> <p>The intervals to shift by.</p> required <p>Returns:</p> Type Description <code>EpochArray</code> <p>The shifted EpochArray.</p> Notes <p>This function restricts 'epoch' to those within 'epoch_shift' as epochs between 'epoch_shift' intervals would result in a duration of 0.</p> <p>Visual representation: inputs:     epoch       =   [  ]   [  ][]  []     epoch_shift =   [    ][]   [    ] becomes:     epoch       =   [  ]  [  ]    []     epoch_shift =   [    ][][    ]</p> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def shift_epoch_array(\n    epoch: nel.EpochArray, epoch_shift: nel.EpochArray\n) -&gt; nel.EpochArray:\n    \"\"\"\n    Shift an EpochArray by another EpochArray.\n\n    Shifting means that intervals in 'epoch' will be relative to\n    intervals in 'epoch_shift' as if 'epoch_shift' intervals were without gaps.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray\n        The intervals to shift.\n    epoch_shift : nel.EpochArray\n        The intervals to shift by.\n\n    Returns\n    -------\n    nel.EpochArray\n        The shifted EpochArray.\n\n    Notes\n    -----\n    This function restricts 'epoch' to those within 'epoch_shift' as\n    epochs between 'epoch_shift' intervals would result in a duration of 0.\n\n    Visual representation:\n    inputs:\n        epoch       =   [  ]   [  ] [  ]  []\n        epoch_shift =   [    ] [    ]   [    ]\n    becomes:\n        epoch       =   [  ]  [  ]    []\n        epoch_shift =   [    ][    ][    ]\n    \"\"\"\n    # input validation\n    if not isinstance(epoch, nel.EpochArray):\n        raise TypeError(\"epoch must be a nelpy EpochArray\")\n    if not isinstance(epoch_shift, nel.EpochArray):\n        raise TypeError(\"epoch_shift must be a nelpy EpochArray\")\n\n    # restrict epoch to epoch_shift and extract starts and stops\n    epoch_starts, epoch_stops = epoch[epoch_shift].data.T\n\n    # shift starts and stops by epoch_shift\n    _, epoch_starts_shifted = in_intervals(epoch_starts, epoch_shift.data, shift=True)\n    _, epoch_stops_shifted = in_intervals(epoch_stops, epoch_shift.data, shift=True)\n\n    # shift time support as well, if one exists\n    support_starts_shifted, support_stops_shifted = -np.inf, np.inf\n    if epoch.domain.start != -np.inf:\n        _, support_starts_shifted = in_intervals(\n            epoch.domain.start, epoch_shift.data, shift=True\n        )\n    if epoch.domain.stop != np.inf:\n        _, support_stops_shifted = in_intervals(\n            epoch.domain.stop, epoch_shift.data, shift=True\n        )\n\n    session_domain = nel.EpochArray([support_starts_shifted, support_stops_shifted])\n\n    # package shifted intervals into epoch array with shifted time support\n    return nel.EpochArray(\n        np.array([epoch_starts_shifted, epoch_stops_shifted]).T, domain=session_domain\n    )\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.spatial_phase_precession","title":"<code>spatial_phase_precession(circ, lin, slope_bounds=[-3 * np.pi, 3 * np.pi])</code>","text":"<p>Compute the circular-linear correlation as described in https://pubmed.ncbi.nlm.nih.gov/22487609/.</p> <p>Parameters:</p> Name Type Description Default <code>circ</code> <code>ndarray</code> <p>Circular data in radians (e.g., spike phases).</p> required <code>lin</code> <code>ndarray</code> <p>Linear data (e.g., spike positions).</p> required <code>slope_bounds</code> <code>Union[List[float], Tuple[float, float]]</code> <p>The slope range for optimization (default is [-3 * np.pi, 3 * np.pi]).</p> <code>[-3 * pi, 3 * pi]</code> <p>Returns:</p> Name Type Description <code>rho</code> <code>float</code> <p>Circular-linear correlation coefficient.</p> <code>pval</code> <code>float</code> <p>p-value for testing the significance of the correlation coefficient.</p> <code>sl</code> <code>float</code> <p>Slope of the circular-linear correlation.</p> <code>offs</code> <code>float</code> <p>Offset of the circular-linear correlation.</p> Notes <p>This method computes a circular-linear correlation and can handle cases where one or both variables may follow a uniform distribution. It differs from the linear-circular correlation used in other studies (e.g., https://science.sciencemag.org/content/340/6138/1342).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; circ = np.random.uniform(0, 2 * np.pi, 100)\n&gt;&gt;&gt; lin = np.random.uniform(0, 1, 100)\n&gt;&gt;&gt; rho, pval, sl, offs = spatial_phase_precession(circ, lin)\n&gt;&gt;&gt; print(f\"Correlation: {rho}, p-value: {pval}, slope: {sl}, offset: {offs}\")\n</code></pre> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def spatial_phase_precession(\n    circ: np.ndarray,\n    lin: np.ndarray,\n    slope_bounds: Union[List[float], Tuple[float, float]] = [-3 * np.pi, 3 * np.pi],\n) -&gt; Tuple[float, float, float, float]:\n    \"\"\"\n    Compute the circular-linear correlation as described in https://pubmed.ncbi.nlm.nih.gov/22487609/.\n\n    Parameters\n    ----------\n    circ : np.ndarray\n        Circular data in radians (e.g., spike phases).\n    lin : np.ndarray\n        Linear data (e.g., spike positions).\n    slope_bounds : Union[List[float], Tuple[float, float]], optional\n        The slope range for optimization (default is [-3 * np.pi, 3 * np.pi]).\n\n    Returns\n    -------\n    rho : float\n        Circular-linear correlation coefficient.\n    pval : float\n        p-value for testing the significance of the correlation coefficient.\n    sl : float\n        Slope of the circular-linear correlation.\n    offs : float\n        Offset of the circular-linear correlation.\n\n    Notes\n    -----\n    This method computes a circular-linear correlation and can handle cases\n    where one or both variables may follow a uniform distribution. It differs from\n    the linear-circular correlation used in other studies (e.g., https://science.sciencemag.org/content/340/6138/1342).\n\n    Examples\n    -------\n    &gt;&gt;&gt; circ = np.random.uniform(0, 2 * np.pi, 100)\n    &gt;&gt;&gt; lin = np.random.uniform(0, 1, 100)\n    &gt;&gt;&gt; rho, pval, sl, offs = spatial_phase_precession(circ, lin)\n    &gt;&gt;&gt; print(f\"Correlation: {rho}, p-value: {pval}, slope: {sl}, offset: {offs}\")\n    \"\"\"\n\n    # Get rid of all the nans in this data\n    nan_index = np.logical_or(np.isnan(circ), np.isnan(lin))\n    circ = circ[~nan_index]\n    lin = lin[~nan_index]\n\n    # Make sure there are still valid data\n    if np.size(lin) == 0:\n        return np.nan, np.nan, np.nan, np.nan\n\n    def myfun1(p):\n        return -np.sqrt(\n            (np.sum(np.cos(circ - (p * lin))) / len(circ)) ** 2\n            + (np.sum(np.sin(circ - (p * lin))) / len(circ)) ** 2\n        )\n\n    # finding the optimal slope, note that we have to restrict the range of slopes\n\n    sl = sp.optimize.fminbound(\n        myfun1,\n        slope_bounds[0] / (np.max(lin) - np.min(lin)),\n        slope_bounds[1] / (np.max(lin) - np.min(lin)),\n    )\n\n    # calculate offset\n    offs = np.arctan2(\n        np.sum(np.sin(circ - (sl * lin))), np.sum(np.cos(circ - (sl * lin)))\n    )\n    # offs = (offs + np.pi) % (2 * np.pi) - np.pi\n    offs = np.arctan2(np.sin(offs), np.cos(offs))\n\n    # circular variable derived from the linearization\n    linear_circ = np.mod(abs(sl) * lin, 2 * np.pi)\n\n    # # marginal distributions:\n    p1, z1 = pcs.rayleigh(circ)\n    p2, z2 = pcs.rayleigh(linear_circ)\n\n    # circular-linear correlation:\n    if (p1 &gt; 0.5) | (p2 &gt; 0.5):\n        # This means at least one of our variables may be a uniform distribution\n        rho, pval = corrcc_uniform(circ, linear_circ)\n    else:\n        rho, pval = corrcc(circ, linear_circ)\n\n    # Assign the correct sign to rho\n    if sl &lt; 0:\n        rho = -np.abs(rho)\n    else:\n        rho = np.abs(rho)\n\n    # if offs &lt; 0:\n    #     offs = offs + 2 * np.pi\n    # if offs &gt; np.pi:\n    #     offs = offs - 2 * np.pi\n\n    return rho, pval, sl, offs\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.split_epoch_by_width","title":"<code>split_epoch_by_width(intervals, bin_width=0.001)</code>","text":"<p>Generate combined intervals (start, stop) at a specified width within given intervals.</p> <p>Parameters:</p> Name Type Description Default <code>intervals</code> <code>List[Tuple[float, float]]</code> <p>A list of (start, end) tuples representing intervals.</p> required <code>bin_width</code> <code>float</code> <p>The width of each bin in seconds. Default is 0.001 (1 ms).</p> <code>0.001</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D array containing (start, stop) pairs for all bins across intervals.</p> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def split_epoch_by_width(\n    intervals: List[Tuple[float, float]], bin_width: float = 0.001\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate combined intervals (start, stop) at a specified width within given intervals.\n\n    Parameters\n    ----------\n    intervals : List[Tuple[float, float]]\n        A list of (start, end) tuples representing intervals.\n    bin_width : float\n        The width of each bin in seconds. Default is 0.001 (1 ms).\n\n    Returns\n    -------\n    np.ndarray\n        A 2D array containing (start, stop) pairs for all bins across intervals.\n    \"\"\"\n    bin_intervals = []\n    for start, end in intervals:\n        # Generate bin edges\n        edges = np.arange(start, end, bin_width)\n        edges = np.append(edges, end)  # Ensure the final end is included\n        # Generate intervals (start, stop) for each bin\n        intervals = np.stack((edges[:-1], edges[1:]), axis=1)\n        bin_intervals.append(intervals)\n    return np.vstack(bin_intervals)\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.split_epoch_equal_parts","title":"<code>split_epoch_equal_parts(intervals, n_parts, return_epoch_array=True)</code>","text":"<p>Split multiple intervals into equal parts.</p> <p>Parameters:</p> Name Type Description Default <code>intervals</code> <code>(array - like, shape(n_intervals, 2))</code> <p>The intervals to split.</p> required <code>n_parts</code> <code>int</code> <p>The number of parts to split each interval into.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, returns the intervals as a nelpy.EpochArray object. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>split_intervals</code> <code>(array - like, shape(n_intervals * n_parts, 2) or EpochArray)</code> <p>The split intervals.</p> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def split_epoch_equal_parts(\n    intervals: np.ndarray, n_parts: int, return_epoch_array: bool = True\n) -&gt; Union[np.ndarray, nel.EpochArray]:\n    \"\"\"\n    Split multiple intervals into equal parts.\n\n    Parameters\n    ----------\n    intervals : array-like, shape (n_intervals, 2)\n        The intervals to split.\n    n_parts : int\n        The number of parts to split each interval into.\n    return_epoch_array : bool, optional\n        If True, returns the intervals as a nelpy.EpochArray object. Defaults to True.\n\n    Returns\n    -------\n    split_intervals : array-like, shape (n_intervals * n_parts, 2) or nelpy.EpochArray\n        The split intervals.\n    \"\"\"\n    # Ensure intervals is a numpy array\n    intervals = np.asarray(intervals)\n\n    # Number of intervals\n    n_intervals = intervals.shape[0]\n\n    # Preallocate the output array\n    split_intervals = np.zeros((n_intervals * n_parts, 2))\n\n    for i, interval in enumerate(intervals):\n        start, end = interval\n        epoch_parts = np.linspace(start, end, n_parts + 1)\n        epoch_parts = np.vstack((epoch_parts[:-1], epoch_parts[1:])).T\n        split_intervals[i * n_parts : (i + 1) * n_parts] = epoch_parts\n\n    if return_epoch_array:\n        return nel.EpochArray(split_intervals)\n    return split_intervals\n</code></pre>"},{"location":"reference/neuro_py/process/#neuro_py.process.truncate_epoch","title":"<code>truncate_epoch(epoch, time=3600)</code>","text":"<p>Truncates an EpochArray to achieve a specified cumulative time duration.</p> <p>This function takes an input EpochArray 'epoch' and a 'time' value representing the desired cumulative time duration in seconds. It returns a new EpochArray containing intervals that cumulatively match the specified time.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The input EpochArray containing intervals to be truncated.</p> required <code>time</code> <code>Union[int, float]</code> <p>The desired cumulative time in seconds (default is 3600).</p> <code>3600</code> <p>Returns:</p> Type Description <code>EpochArray</code> <p>A new EpochArray containing intervals that cumulatively match the specified time.</p> Algorithm <ol> <li>Calculate the cumulative lengths of intervals in the 'epoch'.</li> <li>If the cumulative time of the 'epoch' is already less than or equal to 'time',     return the original 'epoch'.</li> <li>Find the last interval that fits within the specified 'time' and create a new EpochArray     'truncated_intervals' with intervals up to that point.</li> <li>To achieve the desired cumulative time, calculate the remaining time needed to reach 'time'.</li> <li>Add portions of the next interval to 'truncated_intervals' until the desired 'time' is reached     or all intervals are used.</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_data = [(0, 2), (3, 6), (8, 10)]\n&gt;&gt;&gt; epoch = nel.EpochArray(epoch_data)\n&gt;&gt;&gt; truncated_epoch = truncate_epoch(epoch, time=7)\n</code></pre> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def truncate_epoch(\n    epoch: nel.EpochArray, time: Union[int, float] = 3600\n) -&gt; nel.EpochArray:\n    \"\"\"\n    Truncates an EpochArray to achieve a specified cumulative time duration.\n\n    This function takes an input EpochArray 'epoch' and a 'time' value representing\n    the desired cumulative time duration in seconds. It returns a new EpochArray\n    containing intervals that cumulatively match the specified time.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray\n        The input EpochArray containing intervals to be truncated.\n    time : Union[int, float], optional\n        The desired cumulative time in seconds (default is 3600).\n\n    Returns\n    -------\n    nel.EpochArray\n        A new EpochArray containing intervals that cumulatively match\n        the specified time.\n\n    Algorithm\n    ---------\n    1. Calculate the cumulative lengths of intervals in the 'epoch'.\n    2. If the cumulative time of the 'epoch' is already less than or equal to 'time',\n        return the original 'epoch'.\n    3. Find the last interval that fits within the specified 'time' and create a new EpochArray\n        'truncated_intervals' with intervals up to that point.\n    4. To achieve the desired cumulative time, calculate the remaining time needed to reach 'time'.\n    5. Add portions of the next interval to 'truncated_intervals' until the desired 'time' is reached\n        or all intervals are used.\n\n    Examples\n    --------\n    &gt;&gt;&gt; epoch_data = [(0, 2), (3, 6), (8, 10)]\n    &gt;&gt;&gt; epoch = nel.EpochArray(epoch_data)\n    &gt;&gt;&gt; truncated_epoch = truncate_epoch(epoch, time=7)\n    \"\"\"\n\n    if epoch.isempty:\n        return epoch\n\n    # calcuate cumulative lengths\n    cumulative_lengths = epoch.lengths.cumsum()\n\n    # No truncation needed\n    if cumulative_lengths[-1] &lt;= time:\n        return epoch\n\n    # Find the last interval that fits within the time and make new epoch\n    idx = cumulative_lengths &lt;= time\n    truncated_intervals = nel.EpochArray(epoch.data[idx])\n\n    # It's unlikely that the last interval will fit perfectly, so add the remainder from the next interval\n    #   until the epoch is the desired length\n    interval_i = 0\n    while (time - truncated_intervals.duration) &gt; 1e-10 or interval_i &gt; len(epoch):\n        # Add the last interval\n        next_interval = int(np.where(cumulative_lengths &gt;= time)[0][interval_i])\n\n        remainder = (\n            nel.EpochArray(\n                [\n                    epoch[next_interval].start,\n                    epoch[next_interval].start + (time - truncated_intervals.duration),\n                ]\n            )\n            &amp; epoch[next_interval]\n        )\n        truncated_intervals = truncated_intervals | remainder\n        interval_i += 1\n\n    return truncated_intervals\n</code></pre>"},{"location":"reference/neuro_py/process/batch_analysis/","title":"neuro_py.process.batch_analysis","text":""},{"location":"reference/neuro_py/process/batch_analysis/#neuro_py.process.batch_analysis.decode_file_path","title":"<code>decode_file_path(save_file)</code>","text":"<p>Decode an encoded file path to retrieve the original session path.</p> <p>Parameters:</p> Name Type Description Default <code>save_file</code> <code>str</code> <p>Encoded file path that includes the original session path.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Original session path before encoding.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; save_file = r\"Z:\\home\\ryanh\\projects\\ripple_heterogeneity\\replay_02_17_23\\Z---___Data___AYAold___AB3___AB3_38_41.pkl\"\n&gt;&gt;&gt; decode_file_path(save_file)\n\"Z:\\Data\\AYAold\\AB3\\AB3_38_41\"\n</code></pre> Source code in <code>neuro_py/process/batch_analysis.py</code> <pre><code>def decode_file_path(save_file: str) -&gt; str:\n    \"\"\"\n    Decode an encoded file path to retrieve the original session path.\n\n    Parameters\n    ----------\n    save_file : str\n        Encoded file path that includes the original session path.\n\n    Returns\n    -------\n    str\n        Original session path before encoding.\n\n    Examples\n    -------\n    &gt;&gt;&gt; save_file = r\"Z:\\\\home\\\\ryanh\\\\projects\\\\ripple_heterogeneity\\\\replay_02_17_23\\\\Z---___Data___AYAold___AB3___AB3_38_41.pkl\"\n    &gt;&gt;&gt; decode_file_path(save_file)\n    \"Z:\\\\Data\\\\AYAold\\\\AB3\\\\AB3_38_41\"\n    \"\"\"\n\n    # get basepath from save_file\n    basepath = os.path.basename(save_file).replace(\"___\", os.sep).replace(\"---\", \":\")\n    # also remove file extension\n    basepath = os.path.splitext(basepath)[0]\n\n    return basepath\n</code></pre>"},{"location":"reference/neuro_py/process/batch_analysis/#neuro_py.process.batch_analysis.encode_file_path","title":"<code>encode_file_path(basepath, save_path)</code>","text":"<p>Encode file path to be used as a filename.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the session to be encoded.</p> required <code>save_path</code> <code>str</code> <p>Directory where the encoded file will be saved.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Encoded file path suitable for use as a filename.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; basepath = r\"Z:\\Data\\AYAold\\AB3\\AB3_38_41\"\n&gt;&gt;&gt; save_path = r\"Z:\\home\\ryanh\\projects\\ripple_heterogeneity\\replay_02_17_23\"\n&gt;&gt;&gt; encode_file_path(basepath, save_path)\n\"Z:\\home\\ryanh\\projects\\ripple_heterogeneity\\replay_02_17_23\\Z---___Data___AYAold___AB3___AB3_38_41.pkl\"\n</code></pre> Source code in <code>neuro_py/process/batch_analysis.py</code> <pre><code>def encode_file_path(basepath: str, save_path: str) -&gt; str:\n    \"\"\"\n    Encode file path to be used as a filename.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the session to be encoded.\n    save_path : str\n        Directory where the encoded file will be saved.\n\n    Returns\n    -------\n    str\n        Encoded file path suitable for use as a filename.\n\n    Examples\n    -------\n    &gt;&gt;&gt; basepath = r\"Z:\\\\Data\\\\AYAold\\\\AB3\\\\AB3_38_41\"\n    &gt;&gt;&gt; save_path = r\"Z:\\\\home\\\\ryanh\\\\projects\\\\ripple_heterogeneity\\\\replay_02_17_23\"\n    &gt;&gt;&gt; encode_file_path(basepath, save_path)\n    \"Z:\\\\home\\\\ryanh\\\\projects\\\\ripple_heterogeneity\\\\replay_02_17_23\\\\Z---___Data___AYAold___AB3___AB3_38_41.pkl\"\n    \"\"\"\n    # normalize paths\n    basepath = os.path.normpath(basepath)\n    save_path = os.path.normpath(save_path)\n    # encode file path with unlikely characters\n    save_file = os.path.join(\n        save_path, basepath.replace(os.sep, \"___\").replace(\":\", \"---\") + \".pkl\"\n    )\n    return save_file\n</code></pre>"},{"location":"reference/neuro_py/process/batch_analysis/#neuro_py.process.batch_analysis.load_results","title":"<code>load_results(save_path, verbose=False, add_save_file_name=False)</code>","text":"<p>Load results from pickled pandas DataFrames in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to the folder containing pickled results.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print progress for each file. Defaults to False.</p> <code>False</code> <code>add_save_file_name</code> <code>bool</code> <p>Whether to add a column with the name of the save file. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Concatenated pandas DataFrame with all results.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified folder does not exist or is empty.</p> Source code in <code>neuro_py/process/batch_analysis.py</code> <pre><code>def load_results(\n    save_path: str, verbose: bool = False, add_save_file_name: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load results from pickled pandas DataFrames in the specified directory.\n\n    Parameters\n    ----------\n    save_path : str\n        Path to the folder containing pickled results.\n    verbose : bool, optional\n        Whether to print progress for each file. Defaults to False.\n    add_save_file_name : bool, optional\n        Whether to add a column with the name of the save file. Defaults to False.\n\n    Returns\n    -------\n    pd.DataFrame\n        Concatenated pandas DataFrame with all results.\n\n    Raises\n    ------\n    ValueError\n        If the specified folder does not exist or is empty.\n    \"\"\"\n\n    if not os.path.exists(save_path):\n        raise ValueError(f\"folder {save_path} does not exist\")\n\n    sessions = glob.glob(os.path.join(save_path, \"*.pkl\"))\n\n    results = []\n\n    for session in sessions:\n        if verbose:\n            print(session)\n        with open(session, \"rb\") as f:\n            results_ = pickle.load(f)\n        if results_ is None:\n            continue\n\n        if add_save_file_name:\n            results_[\"save_file_name\"] = os.path.basename(session)\n\n        results.append(results_)\n\n    results = pd.concat(results, ignore_index=True, axis=0)\n\n    return results\n</code></pre>"},{"location":"reference/neuro_py/process/batch_analysis/#neuro_py.process.batch_analysis.main_loop","title":"<code>main_loop(basepath, save_path, func, overwrite=False, skip_if_error=False, **kwargs)</code>","text":"<p>main_loop: file management &amp; run function</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to session.</p> required <code>save_path</code> <code>str</code> <p>Path to save results to (will be created if it doesn't exist).</p> required <code>func</code> <code>Callable</code> <p>Function to run on each basepath in df (see run).</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing files in save_path. Defaults to False.</p> <code>False</code> <code>skip_if_error</code> <code>bool</code> <p>Whether to skip if an error occurs. Defaults to False.</p> <code>False</code> <code>kwargs</code> <code>dict</code> <p>Keyword arguments to pass to func (see run).</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/process/batch_analysis.py</code> <pre><code>def main_loop(\n    basepath: str,\n    save_path: str,\n    func: Callable,\n    overwrite: bool = False,\n    skip_if_error: bool = False,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    main_loop: file management &amp; run function\n\n    Parameters\n    ----------\n    basepath : str\n        Path to session.\n    save_path : str\n        Path to save results to (will be created if it doesn't exist).\n    func : Callable\n        Function to run on each basepath in df (see run).\n    overwrite : bool, optional\n        Whether to overwrite existing files in save_path. Defaults to False.\n    skip_if_error : bool, optional\n        Whether to skip if an error occurs. Defaults to False.\n    kwargs : dict\n        Keyword arguments to pass to func (see run).\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # get file name from basepath\n    save_file = encode_file_path(basepath, save_path)\n\n    # if file exists and overwrite is False, skip\n    if os.path.exists(save_file) and not overwrite:\n        return\n\n    # calc some features\n    if skip_if_error:\n        try:\n            results = func(basepath, **kwargs)\n        except Exception:\n            traceback.print_exc()\n            print(f\"Error in {basepath}\")\n            return\n    else:\n        results = func(basepath, **kwargs)\n\n    # save file\n    with open(save_file, \"wb\") as f:\n        pickle.dump(results, f)\n</code></pre>"},{"location":"reference/neuro_py/process/batch_analysis/#neuro_py.process.batch_analysis.run","title":"<code>run(df, save_path, func, parallel=True, verbose=False, overwrite=False, skip_if_error=False, num_cores=None, **kwargs)</code>","text":"<p>Run a function on each basepath in the DataFrame and save results.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing a 'basepath' column.</p> required <code>save_path</code> <code>str</code> <p>Path to save results to (will be created if it doesn't exist).</p> required <code>func</code> <code>Callable</code> <p>Function to run on each basepath (see main_loop).</p> required <code>parallel</code> <code>bool</code> <p>Whether to run in parallel. Defaults to True.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress. Defaults to False.</p> <code>False</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing files in save_path. Defaults to False.</p> <code>False</code> <code>skip_if_error</code> <code>bool</code> <p>Whether to skip processing if an error occurs. Defaults to False.</p> <code>False</code> <code>num_cores</code> <code>int</code> <p>Number of CPU cores to use (if None, will use all available cores). Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to func.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/process/batch_analysis.py</code> <pre><code>def run(\n    df: pd.DataFrame,\n    save_path: str,\n    func: Callable,\n    parallel: bool = True,\n    verbose: bool = False,\n    overwrite: bool = False,\n    skip_if_error: bool = False,\n    num_cores: int = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Run a function on each basepath in the DataFrame and save results.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing a 'basepath' column.\n    save_path : str\n        Path to save results to (will be created if it doesn't exist).\n    func : Callable\n        Function to run on each basepath (see main_loop).\n    parallel : bool, optional\n        Whether to run in parallel. Defaults to True.\n    verbose : bool, optional\n        Whether to print progress. Defaults to False.\n    overwrite : bool, optional\n        Whether to overwrite existing files in save_path. Defaults to False.\n    skip_if_error : bool, optional\n        Whether to skip processing if an error occurs. Defaults to False.\n    num_cores : int, optional\n        Number of CPU cores to use (if None, will use all available cores). Defaults to None.\n    kwargs : dict\n        Additional keyword arguments to pass to func.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # find sessions to run\n    basepaths = pd.unique(df.basepath)\n    # create save_path if it doesn't exist\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    # run in parallel if parallel is True\n    if parallel:\n        # get number of cores\n        if num_cores is None:\n            num_cores = multiprocessing.cpu_count()\n        # run in parallel\n        Parallel(n_jobs=num_cores)(\n            delayed(main_loop)(\n                basepath, save_path, func, overwrite, skip_if_error, **kwargs\n            )\n            for basepath in tqdm(basepaths)\n        )\n    else:\n        # run in serial\n        for basepath in tqdm(basepaths):\n            if verbose:\n                print(basepath)\n            # run main_loop on each basepath in df\n            main_loop(basepath, save_path, func, overwrite, skip_if_error, **kwargs)\n</code></pre>"},{"location":"reference/neuro_py/process/correlations/","title":"neuro_py.process.correlations","text":""},{"location":"reference/neuro_py/process/correlations/#neuro_py.process.correlations.cch_conv","title":"<code>cch_conv(cch, W=30, HF=0.6)</code>","text":"<p>Convolve the cross-correlogram with a Gaussian window and calculate p-values.</p> <p>Parameters:</p> Name Type Description Default <code>cch</code> <code>ndarray</code> <p>The cross-correlogram data (1D array).</p> required <code>W</code> <code>int</code> <p>The width of the Gaussian window (default is 30).</p> <code>30</code> <code>HF</code> <code>float</code> <p>The height factor to modify the Gaussian peak (default is 0.6).</p> <code>0.6</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray]</code> <ul> <li>pvals : array of p-values.</li> <li>pred : predicted values after convolution.</li> <li>qvals : q-values (1 - pvals).</li> </ul> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def cch_conv(\n    cch: np.ndarray, W: int = 30, HF: float = 0.6\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Convolve the cross-correlogram with a Gaussian window and calculate p-values.\n\n    Parameters\n    ----------\n    cch : np.ndarray\n        The cross-correlogram data (1D array).\n    W : int, optional\n        The width of the Gaussian window (default is 30).\n    HF : float, optional\n        The height factor to modify the Gaussian peak (default is 0.6).\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, np.ndarray]\n        - pvals : array of p-values.\n        - pred : predicted values after convolution.\n        - qvals : q-values (1 - pvals).\n    \"\"\"\n\n    SDG = W / 2\n    if round(SDG) == SDG:  # even W\n        win = local_gausskernel(SDG, 6 * SDG + 1)\n        cidx = int(SDG * 3 + 1)\n    else:\n        win = local_gausskernel(SDG, 6 * SDG + 2)\n        cidx = int(SDG * 3 + 1.5)\n    win[cidx - 1] = win[cidx - 1] * (1 - HF)\n    win = win / sum(win)\n    pred = local_firfilt(cch, win)\n    pvals = 1 - poisson.cdf(cch - 1, pred) - poisson.pmf(cch, pred) * 0.5\n    qvals = 1 - pvals\n    return pvals, pred, qvals\n</code></pre>"},{"location":"reference/neuro_py/process/correlations/#neuro_py.process.correlations.compute_AutoCorrs","title":"<code>compute_AutoCorrs(spks, binsize=0.001, nbins=100)</code>","text":"<p>Compute autocorrelations for spike trains.</p> <p>Parameters:</p> Name Type Description Default <code>spks</code> <code>ndarray</code> <p>Nested ndarrays where each array contains the spike times for one neuron.</p> required <code>binsize</code> <code>float</code> <p>The size of each bin in seconds, by default 0.001 (1 ms).</p> <code>0.001</code> <code>nbins</code> <code>int</code> <p>The number of bins for the autocorrelation, by default 100.</p> <code>100</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame where each column represents the autocorrelation of the corresponding neuron. The index is the time lag, and the values are the autocorrelations.</p> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def compute_AutoCorrs(\n    spks: np.ndarray, binsize: float = 0.001, nbins: int = 100\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute autocorrelations for spike trains.\n\n    Parameters\n    ----------\n    spks : np.ndarray\n        Nested ndarrays where each array contains the spike times for one neuron.\n    binsize : float, optional\n        The size of each bin in seconds, by default 0.001 (1 ms).\n    nbins : int, optional\n        The number of bins for the autocorrelation, by default 100.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame where each column represents the autocorrelation of the corresponding neuron.\n        The index is the time lag, and the values are the autocorrelations.\n    \"\"\"\n    # First let's prepare a pandas dataframe to receive the data\n    times = np.arange(0, binsize * (nbins + 1), binsize) - (nbins * binsize) / 2\n    autocorrs = pd.DataFrame(index=times, columns=np.arange(len(spks)))\n\n    # Now we can iterate over the dictionnary of spikes\n    for i, s in enumerate(spks):\n        if len(s) == 0:\n            continue\n        # Calling the crossCorr function\n        autocorrs[i] = crossCorr(s, s, binsize, nbins)\n\n    # And don't forget to replace the 0 ms for 0\n    autocorrs.loc[0] = 0.0\n    return autocorrs\n</code></pre>"},{"location":"reference/neuro_py/process/correlations/#neuro_py.process.correlations.compute_cross_correlogram","title":"<code>compute_cross_correlogram(X, dt=1.0, window=0.5)</code>","text":"<p>Compute pairwise cross-correlograms between signals in an array.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>N-dimensional array of shape (n_signals, n_timepoints) representing the signals.</p> required <code>dt</code> <code>float</code> <p>Time step between samples in seconds, default is 1.0.</p> <code>1.0</code> <code>window</code> <code>float</code> <p>Window size in seconds for the cross-correlogram. The output will include values within +/- window from the center. If None, returns the full correlogram.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pairwise cross-correlogram with time lags as the index and signal pairs as columns.</p> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def compute_cross_correlogram(\n    X: np.ndarray, dt: float = 1.0, window: float = 0.5\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute pairwise cross-correlograms between signals in an array.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        N-dimensional array of shape (n_signals, n_timepoints) representing the signals.\n    dt : float, optional\n        Time step between samples in seconds, default is 1.0.\n    window : float, optional\n        Window size in seconds for the cross-correlogram. The output will include values\n        within +/- window from the center. If None, returns the full correlogram.\n\n    Returns\n    -------\n    pd.DataFrame\n        Pairwise cross-correlogram with time lags as the index and signal pairs as columns.\n    \"\"\"\n\n    crosscorrs = {}\n    pairs = list(itertools.combinations(np.arange(X.shape[0]), 2))\n    for i, j in pairs:\n        auc = signal.correlate(X[i], X[j])\n        times = signal.correlation_lags(len(X[i]), len(X[j])) * dt\n        # normalize by coeff\n        normalizer = np.sqrt((X[i] ** 2).sum(axis=0) * (X[j] ** 2).sum(axis=0))\n        auc /= normalizer\n\n        crosscorrs[(i, j)] = pd.Series(index=times, data=auc, dtype=\"float32\")\n    crosscorrs = pd.DataFrame.from_dict(crosscorrs)\n\n    if window is None:\n        return crosscorrs\n    else:\n        return crosscorrs[(crosscorrs.index &gt;= -window) &amp; (crosscorrs.index &lt;= window)]\n</code></pre>"},{"location":"reference/neuro_py/process/correlations/#neuro_py.process.correlations.local_firfilt","title":"<code>local_firfilt(x, W)</code>","text":"<p>Apply a FIR filter to the input signal x using the provided filter coefficients W.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The input signal to be filtered.</p> required <code>W</code> <code>ndarray</code> <p>The FIR filter coefficients.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The filtered signal.</p> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def local_firfilt(x: np.ndarray, W: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Apply a FIR filter to the input signal x using the provided filter coefficients W.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        The input signal to be filtered.\n    W : np.ndarray\n        The FIR filter coefficients.\n\n    Returns\n    -------\n    np.ndarray\n        The filtered signal.\n    \"\"\"\n    C = int(len(W))\n    D = int(np.ceil(C / 2) - 1)\n    xx = [np.flipud(x[:C]), x, np.flipud(x[-C:])]\n    xx = list(itertools.chain(*xx))\n    Y = signal.lfilter(W, 1, xx)\n    Y = Y[C + D : len(Y) - C + D]\n    return Y\n</code></pre>"},{"location":"reference/neuro_py/process/correlations/#neuro_py.process.correlations.local_gausskernel","title":"<code>local_gausskernel(sigma, N)</code>","text":"<p>Generate a Gaussian kernel with the given standard deviation and size.</p> <p>Parameters:</p> Name Type Description Default <code>sigma</code> <code>float</code> <p>The standard deviation of the Gaussian.</p> required <code>N</code> <code>int</code> <p>The size of the kernel (number of points). Must be an odd number.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 1D Gaussian kernel.</p> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def local_gausskernel(sigma: float, N: int) -&gt; np.ndarray:\n    \"\"\"\n    Generate a Gaussian kernel with the given standard deviation and size.\n\n    Parameters\n    ----------\n    sigma : float\n        The standard deviation of the Gaussian.\n    N : int\n        The size of the kernel (number of points). Must be an odd number.\n\n    Returns\n    -------\n    np.ndarray\n        A 1D Gaussian kernel.\n    \"\"\"\n    x = np.arange(-(N - 1) / 2, ((N - 1) / 2) + 1)\n    k = 1 / (2 * np.pi * sigma) * np.exp(-(x**2 / 2 / sigma**2))\n    return k\n</code></pre>"},{"location":"reference/neuro_py/process/correlations/#neuro_py.process.correlations.pairwise_corr","title":"<code>pairwise_corr(X, method='pearson', pairs=None)</code>","text":"<p>Compute pairwise correlations between all rows of a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2D numpy array of shape (n, p), where n is the number of rows (variables) and p is the number of columns (features).</p> required <code>method</code> <code>str</code> <p>Correlation method to use ('pearson', 'spearman', or 'kendall'), by default \"pearson\".</p> <code>'pearson'</code> <code>pairs</code> <code>ndarray</code> <p>Array of shape (m, 2) specifying the pairs of rows to compute correlations between. If None, computes correlations for all unique row pairs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rho</code> <code>ndarray</code> <p>Array of correlation coefficients.</p> <code>pval</code> <code>ndarray</code> <p>Array of p-values for the correlation tests.</p> <code>pairs</code> <code>ndarray</code> <p>Array of pairs (indices) for which correlations were computed.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the method is not 'pearson', 'spearman', or 'kendall'.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.random.rand(10, 5)\n&gt;&gt;&gt; rho, pval, pairs = pairwise_corr(X, method=\"spearman\")\n</code></pre> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def pairwise_corr(\n    X: np.ndarray, method: str = \"pearson\", pairs: Optional[np.ndarray] = None\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute pairwise correlations between all rows of a matrix.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2D numpy array of shape (n, p), where n is the number of rows (variables) and p is the number of columns (features).\n    method : str, optional\n        Correlation method to use ('pearson', 'spearman', or 'kendall'), by default \"pearson\".\n    pairs : np.ndarray, optional\n        Array of shape (m, 2) specifying the pairs of rows to compute correlations between.\n        If None, computes correlations for all unique row pairs.\n\n    Returns\n    -------\n    rho : np.ndarray\n        Array of correlation coefficients.\n    pval : np.ndarray\n        Array of p-values for the correlation tests.\n    pairs : np.ndarray\n        Array of pairs (indices) for which correlations were computed.\n\n    Raises\n    ------\n    ValueError\n        If the method is not 'pearson', 'spearman', or 'kendall'.\n\n    Examples\n    -------\n    &gt;&gt;&gt; X = np.random.rand(10, 5)\n    &gt;&gt;&gt; rho, pval, pairs = pairwise_corr(X, method=\"spearman\")\n    \"\"\"\n    if pairs is None:\n        x = np.arange(0, X.shape[0])\n        pairs = np.array(list(itertools.combinations(x, 2)))\n\n    rho = []\n    pval = []\n    for i, s in enumerate(pairs):\n        if method == \"pearson\":\n            rho_, pval_ = stats.pearsonr(X[s[0], :], X[s[1], :])\n        elif method == \"spearman\":\n            rho_, pval_ = stats.spearmanr(X[s[0], :], X[s[1], :])\n        elif method == \"kendall\":\n            rho_, pval_ = stats.kendalltau(X[s[0], :], X[s[1], :])\n        else:\n            raise ValueError(\"method must be pearson, spearman or kendall\")\n        rho.append(rho_)\n        pval.append(pval_)\n    return rho, pval, pairs\n</code></pre>"},{"location":"reference/neuro_py/process/correlations/#neuro_py.process.correlations.pairwise_cross_corr","title":"<code>pairwise_cross_corr(spks, binsize=0.001, nbins=100, return_index=False, pairs=None, deconvolve=False)</code>","text":"<p>Compute pairwise time-lagged cross-correlations between spike trains of different cells.</p> <p>Parameters:</p> Name Type Description Default <code>spks</code> <code>ndarray</code> <p>Nested numpy arrays, where each array contains the spike times for a cell.</p> required <code>binsize</code> <code>float</code> <p>The size of time bins in seconds. Default is 0.001 (1 ms).</p> <code>0.001</code> <code>nbins</code> <code>int</code> <p>Number of bins to use for the correlation window. Default is 100.</p> <code>100</code> <code>return_index</code> <code>bool</code> <p>Whether to return the index (pairs) of cells used for the correlation. Default is False.</p> <code>False</code> <code>pairs</code> <code>ndarray</code> <p>Precomputed list of pairs of cells (indices) to compute the cross-correlation for. If None, all unique pairs will be computed. Default is None.</p> <code>None</code> <code>deconvolve</code> <code>bool</code> <p>Whether to apply deconvolution when computing the cross-correlation. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>crosscorrs</code> <code>DataFrame</code> <p>A pandas DataFrame of shape (t, n_pairs), where t is the time axis and n_pairs are the pairs of cells.</p> <code>pairs</code> <code>(ndarray, optional)</code> <p>The pairs of cells for which cross-correlations were computed. Returned only if <code>return_index</code> is True.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spks = np.array([np.random.rand(100), np.random.rand(100)])\n&gt;&gt;&gt; crosscorrs, pairs = pairwise_cross_corr(spks, binsize=0.01, nbins=50, return_index=True)\n</code></pre> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def pairwise_cross_corr(\n    spks: np.ndarray,\n    binsize: float = 0.001,\n    nbins: int = 100,\n    return_index: bool = False,\n    pairs: Optional[np.ndarray] = None,\n    deconvolve: bool = False,\n) -&gt; Tuple[pd.DataFrame, Optional[np.ndarray]]:\n    \"\"\"\n    Compute pairwise time-lagged cross-correlations between spike trains of different cells.\n\n    Parameters\n    ----------\n    spks : np.ndarray\n        Nested numpy arrays, where each array contains the spike times for a cell.\n    binsize : float, optional\n        The size of time bins in seconds. Default is 0.001 (1 ms).\n    nbins : int, optional\n        Number of bins to use for the correlation window. Default is 100.\n    return_index : bool, optional\n        Whether to return the index (pairs) of cells used for the correlation. Default is False.\n    pairs : np.ndarray, optional\n        Precomputed list of pairs of cells (indices) to compute the cross-correlation for.\n        If None, all unique pairs will be computed. Default is None.\n    deconvolve : bool, optional\n        Whether to apply deconvolution when computing the cross-correlation. Default is False.\n\n    Returns\n    -------\n    crosscorrs : pd.DataFrame\n        A pandas DataFrame of shape (t, n_pairs), where t is the time axis and n_pairs are the pairs of cells.\n    pairs : np.ndarray, optional\n        The pairs of cells for which cross-correlations were computed. Returned only if `return_index` is True.\n\n    Examples\n    -------\n    &gt;&gt;&gt; spks = np.array([np.random.rand(100), np.random.rand(100)])\n    &gt;&gt;&gt; crosscorrs, pairs = pairwise_cross_corr(spks, binsize=0.01, nbins=50, return_index=True)\n    \"\"\"\n    # Get unique combo without repeats\n    if pairs is None:\n        x = np.arange(0, spks.shape[0])\n        pairs = np.array(list(itertools.combinations(x, 2)))\n\n    # prepare a pandas dataframe to receive the data\n    times = np.linspace(-(nbins * binsize) / 2, (nbins * binsize) / 2, nbins + 1)\n\n    def compute_crosscorr(pair):\n        i, j = pair\n        crosscorr = crossCorr(spks[i], spks[j], binsize, nbins)\n        return crosscorr\n\n    def compute_crosscorr_deconvolve(pair):\n        i, j = pair\n        crosscorr, _ = deconvolve_peth(spks[i], spks[j], binsize, nbins)\n        return crosscorr\n\n    if deconvolve:\n        crosscorrs = [compute_crosscorr_deconvolve(pair) for pair in pairs]\n    else:\n        crosscorrs = [compute_crosscorr(pair) for pair in pairs]\n\n    crosscorrs = pd.DataFrame(\n        index=times,\n        data=np.array(crosscorrs).T,\n        columns=np.arange(len(pairs)),\n    )\n\n    if return_index:\n        return crosscorrs, pairs\n    else:\n        return crosscorrs\n</code></pre>"},{"location":"reference/neuro_py/process/correlations/#neuro_py.process.correlations.pairwise_spatial_corr","title":"<code>pairwise_spatial_corr(X, return_index=False, pairs=None)</code>","text":"<p>Compute pairwise spatial correlations between cells' spatial maps.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 3D numpy array of shape (n_cells, n_space, n_space) representing the spatial maps of cells.</p> required <code>return_index</code> <code>bool</code> <p>If True, returns the indices of the cell pairs used for the correlation.</p> <code>False</code> <code>pairs</code> <code>ndarray</code> <p>Array of cell pairs for which to compute the correlation. If not provided, all unique pairs are used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>spatial_corr</code> <code>ndarray</code> <p>Array containing the Pearson correlation coefficients for each pair of cells.</p> <code>pairs</code> <code>(ndarray, optional)</code> <p>Array of cell pairs used for the correlation (if return_index is True).</p> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def pairwise_spatial_corr(\n    X: np.ndarray, return_index: bool = False, pairs: np.ndarray = None\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Compute pairwise spatial correlations between cells' spatial maps.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        A 3D numpy array of shape (n_cells, n_space, n_space) representing the spatial maps of cells.\n    return_index : bool, optional\n        If True, returns the indices of the cell pairs used for the correlation.\n    pairs : np.ndarray, optional\n        Array of cell pairs for which to compute the correlation. If not provided, all unique pairs are used.\n\n    Returns\n    -------\n    spatial_corr : np.ndarray\n        Array containing the Pearson correlation coefficients for each pair of cells.\n    pairs : np.ndarray, optional\n        Array of cell pairs used for the correlation (if return_index is True).\n    \"\"\"\n    # Get unique combo without repeats\n    if pairs is None:\n        x = np.arange(0, X.shape[0])\n        pairs = np.array(list(itertools.combinations(x, 2)))\n\n    spatial_corr = []\n    # Now we can iterate over spikes\n    for i, s in enumerate(pairs):\n        # Calling the crossCorr function\n        x1 = X[s[0], :, :].flatten()\n        x2 = X[s[1], :, :].flatten()\n        bad_idx = np.isnan(x1) | np.isnan(x2)\n        spatial_corr.append(np.corrcoef(x1[~bad_idx], x2[~bad_idx])[0, 1])\n\n    if return_index:\n        return np.array(spatial_corr), pairs\n    else:\n        return np.array(spatial_corr)\n</code></pre>"},{"location":"reference/neuro_py/process/correlations/#neuro_py.process.correlations.sig_mod","title":"<code>sig_mod(cch, binsize=0.005, sig_window=0.2, alpha=0.001, W=30)</code>","text":"<p>Assess the significance of cross-correlogram values using Poisson statistics.</p> <p>Parameters:</p> Name Type Description Default <code>cch</code> <code>ndarray</code> <p>The cross-correlogram data (1D array).</p> required <code>binsize</code> <code>float</code> <p>The size of each bin in seconds (default is 0.005).</p> <code>0.005</code> <code>sig_window</code> <code>float</code> <p>The window size to consider for significance (default is 0.2).</p> <code>0.2</code> <code>alpha</code> <code>float</code> <p>The significance level (default is 0.001).</p> <code>0.001</code> <code>W</code> <code>int</code> <p>The width of the Gaussian window for convolution (default is 30).</p> <code>30</code> <p>Returns:</p> Type Description <code>tuple[bool, ndarray, ndarray, ndarray, ndarray]</code> <ul> <li>sig : boolean indicating whether the cross-correlogram is significant.</li> <li>hiBound : upper bound for significance.</li> <li>loBound : lower bound for significance.</li> <li>pvals : array of p-values.</li> <li>pred : predicted values after convolution.</li> </ul> Source code in <code>neuro_py/process/correlations.py</code> <pre><code>def sig_mod(\n    cch: np.ndarray,\n    binsize: float = 0.005,\n    sig_window: float = 0.2,\n    alpha: float = 0.001,\n    W: int = 30\n) -&gt; Tuple[bool, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Assess the significance of cross-correlogram values using Poisson statistics.\n\n    Parameters\n    ----------\n    cch : np.ndarray\n        The cross-correlogram data (1D array).\n    binsize : float, optional\n        The size of each bin in seconds (default is 0.005).\n    sig_window : float, optional\n        The window size to consider for significance (default is 0.2).\n    alpha : float, optional\n        The significance level (default is 0.001).\n    W : int, optional\n        The width of the Gaussian window for convolution (default is 30).\n\n    Returns\n    -------\n    tuple[bool, np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n        - sig : boolean indicating whether the cross-correlogram is significant.\n        - hiBound : upper bound for significance.\n        - loBound : lower bound for significance.\n        - pvals : array of p-values.\n        - pred : predicted values after convolution.\n    \"\"\"\n    # check and correct for negative values\n    if np.any(cch &lt; 0):\n        cch = cch + np.abs(min(cch))\n\n    pvals, pred, qvals = cch_conv(cch, W)\n\n    nBonf = int(sig_window / binsize) * 2\n    hiBound = poisson.ppf(1 - alpha / nBonf, pred)\n    loBound = poisson.ppf(alpha / nBonf, pred)\n\n    center_bins = np.arange(\n        int(len(cch) / 2 - 0.1 / binsize), int(len(cch) / 2 + 0.1 / binsize)\n    )\n    # at least 2 bins more extreme than bound to be sig\n    sig = (sum(cch[center_bins] &gt; max(hiBound)) &gt; 2) | (\n        sum(cch[center_bins] &lt; min(loBound)) &gt; 2\n    )\n    return sig, hiBound, loBound, pvals, pred\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/","title":"neuro_py.process.intervals","text":""},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals._find_intersecting_intervals","title":"<code>_find_intersecting_intervals(set1, set2)</code>","text":"<p>Find the amount of time two sets of intervals are intersecting each other for each interval in set1.</p> <p>Parameters:</p> Name Type Description Default <code>set1</code> <code>ndarray</code> <p>An array of intervals represented as pairs of start and end times.</p> required <code>set2</code> <code>ndarray</code> <p>An array of intervals represented as pairs of start and end times.</p> required <p>Returns:</p> Type Description <code>list of float</code> <p>A list of floats, where each float represents the amount of time the corresponding interval in set1 intersects with any interval in set2.</p> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>@jit(nopython=True)\ndef _find_intersecting_intervals(set1: np.ndarray, set2: np.ndarray) -&gt; List[float]:\n    \"\"\"\n    Find the amount of time two sets of intervals are intersecting each other for each interval in set1.\n\n    Parameters\n    ----------\n    set1 : ndarray\n        An array of intervals represented as pairs of start and end times.\n    set2 : ndarray\n        An array of intervals represented as pairs of start and end times.\n\n    Returns\n    -------\n    list of float\n        A list of floats, where each float represents the amount of time the\n        corresponding interval in set1 intersects with any interval in set2.\n    \"\"\"\n    intersecting_intervals = []\n    for i, (start1, end1) in enumerate(set1):\n        # Check if any of the intervals in set2 intersect with the current interval in set1\n        for start2, end2 in set2:\n            if start2 &lt;= end1 and end2 &gt;= start1:\n                # Calculate the amount of intersection between the two intervals\n                intersection = min(end1, end2) - max(start1, start2)\n                intersecting_intervals.append(intersection)\n                break\n        else:\n            intersecting_intervals.append(0)  # No intersection found\n\n    return intersecting_intervals\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals.find_intersecting_intervals","title":"<code>find_intersecting_intervals(set1, set2, return_indices=True)</code>","text":"<p>Find the amount of time two sets of intervals are intersecting each other for each intersection.</p> <p>Parameters:</p> Name Type Description Default <code>set1</code> <code>nelpy EpochArray</code> <p>The first set of intervals to check for intersections.</p> required <code>set2</code> <code>nelpy EpochArray</code> <p>The second set of intervals to check for intersections.</p> required <code>return_indices</code> <code>bool</code> <p>If True, return the indices of the intervals in set2 that intersect with each interval in set1. If False, return the amount of time each interval in set1 intersects with any interval in set2.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[ndarray, List[bool]]</code> <p>If return_indices is True, returns a boolean array indicating whether each interval in set1 intersects with any interval in set2. If return_indices is False, returns a NumPy array with the amount of time each interval in set1 intersects with any interval in set2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; set1 = nel.EpochArray([(1, 3), (5, 7), (9, 10)])\n&gt;&gt;&gt; set2 = nel.EpochArray([(2, 4), (6, 8)])\n&gt;&gt;&gt; find_intersecting_intervals(set1, set2)\n[True, True, False]\n&gt;&gt;&gt; find_intersecting_intervals(set1, set2, return_indices=False)\n[1, 2, 0]\n</code></pre> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def find_intersecting_intervals(\n    set1: nel.EpochArray, set2: nel.EpochArray, return_indices: bool = True\n) -&gt; Union[np.ndarray, List[bool]]:\n    \"\"\"\n    Find the amount of time two sets of intervals are intersecting each other for each intersection.\n\n    Parameters\n    ----------\n    set1 : nelpy EpochArray\n        The first set of intervals to check for intersections.\n    set2 : nelpy EpochArray\n        The second set of intervals to check for intersections.\n    return_indices : bool, optional\n        If True, return the indices of the intervals in set2 that intersect with each interval in set1.\n        If False, return the amount of time each interval in set1 intersects with any interval in set2.\n\n    Returns\n    -------\n    Union[np.ndarray, List[bool]]\n        If return_indices is True, returns a boolean array indicating whether each interval in set1 intersects with any interval in set2.\n        If return_indices is False, returns a NumPy array with the amount of time each interval in set1 intersects with any interval in set2.\n\n    Examples\n    --------\n    &gt;&gt;&gt; set1 = nel.EpochArray([(1, 3), (5, 7), (9, 10)])\n    &gt;&gt;&gt; set2 = nel.EpochArray([(2, 4), (6, 8)])\n    &gt;&gt;&gt; find_intersecting_intervals(set1, set2)\n    [True, True, False]\n    &gt;&gt;&gt; find_intersecting_intervals(set1, set2, return_indices=False)\n    [1, 2, 0]\n    \"\"\"\n    if not isinstance(set1, core.IntervalArray) &amp; isinstance(set2, core.IntervalArray):\n        raise ValueError(\"only EpochArrays are supported\")\n\n    intersection = np.array(_find_intersecting_intervals(set1.data, set2.data))\n    if return_indices:\n        return intersection &gt; 0\n    return intersection\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals.find_interval","title":"<code>find_interval(logical)</code>","text":"<p>Find consecutive intervals of True values in a list of boolean values.</p> <p>Parameters:</p> Name Type Description Default <code>logical</code> <code>List[bool]</code> <p>The list of boolean values.</p> required <p>Returns:</p> Type Description <code>List[Tuple[int, int]]</code> <p>A list of tuples representing the start and end indices of each consecutive interval of True values in the logical list.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; find_interval([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1])\n[(2, 4), (6, 7), (10, 11)]\n&gt;&gt;&gt; find_interval([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1])\n[(0, 2), (4, 5), (9, 10)]\n</code></pre> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def find_interval(logical: List[bool]) -&gt; List[Tuple[int, int]]:\n    \"\"\"\n    Find consecutive intervals of True values in a list of boolean values.\n\n    Parameters\n    ----------\n    logical : List[bool]\n        The list of boolean values.\n\n    Returns\n    -------\n    List[Tuple[int, int]]\n        A list of tuples representing the start and end indices of each consecutive interval of True values in the logical list.\n\n    Examples\n    --------\n    &gt;&gt;&gt; find_interval([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1])\n    [(2, 4), (6, 7), (10, 11)]\n    &gt;&gt;&gt; find_interval([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1])\n    [(0, 2), (4, 5), (9, 10)]\n    \"\"\"\n    intervals = []\n    start = None\n    for i, value in enumerate(logical):\n        if value and start is None:\n            start = i\n        elif not value and start is not None:\n            intervals.append((start, i - 1))\n            start = None\n    if start is not None:\n        intervals.append((start, len(logical) - 1))\n    return intervals\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals.get_overlapping_intervals","title":"<code>get_overlapping_intervals(start, stop, interval_width, slideby)</code>","text":"<p>Generate overlapping intervals within a specified time range.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>float</code> <p>The start time of the time range.</p> required <code>stop</code> <code>float</code> <p>The stop time of the time range.</p> required <code>interval_width</code> <code>float</code> <p>The width of each interval in seconds.</p> required <code>slideby</code> <code>float</code> <p>The amount to slide the interval by in seconds.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D array containing (start, stop) pairs for all overlapping intervals.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_overlapping_intervals(0, 10, 2, 1)\narray([[0, 2],\n    [1, 3],\n    [2, 4],\n    [3, 5],\n    [4, 6],\n    [5, 7],\n    [6, 8],\n    [7, 9]])\n</code></pre> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def get_overlapping_intervals(\n    start: float, stop: float, interval_width: float, slideby: float\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate overlapping intervals within a specified time range.\n\n    Parameters\n    ----------\n    start : float\n        The start time of the time range.\n    stop : float\n        The stop time of the time range.\n    interval_width : float\n        The width of each interval in seconds.\n    slideby : float\n        The amount to slide the interval by in seconds.\n\n    Returns\n    -------\n    np.ndarray\n        A 2D array containing (start, stop) pairs for all overlapping intervals.\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_overlapping_intervals(0, 10, 2, 1)\n    array([[0, 2],\n        [1, 3],\n        [2, 4],\n        [3, 5],\n        [4, 6],\n        [5, 7],\n        [6, 8],\n        [7, 9]])\n    \"\"\"\n    starts = np.arange(start, stop - interval_width, slideby)\n    stops = starts + interval_width\n    return np.column_stack((starts, stops))\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals.in_intervals","title":"<code>in_intervals(timestamps, intervals, return_interval=False, shift=False)</code>","text":"<p>Find which timestamps fall within the given intervals.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>ndarray</code> <p>An array of timestamp values. Assumes sorted.</p> required <code>intervals</code> <code>ndarray</code> <p>An array of time intervals, represented as pairs of start and end times.</p> required <code>return_interval</code> <code>(bool, optional(default=False))</code> <p>If True, return the index of the interval to which each timestamp belongs.</p> <code>False</code> <code>shift</code> <code>(bool, optional(default=False))</code> <p>If True, return the shifted timestamps</p> <code>False</code> <p>Returns:</p> Name Type Description <code>in_interval</code> <code>ndarray</code> <p>A logical index indicating which timestamps fall within the intervals.</p> <code>interval</code> <code>(ndarray, optional)</code> <p>A ndarray indicating for each timestamps which interval it was within.</p> <code>shifted_timestamps</code> <code>(ndarray, optional)</code> <p>The shifted timestamps</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; timestamps = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n&gt;&gt;&gt; intervals = np.array([[2, 4], [5, 7]])\n&gt;&gt;&gt; in_intervals(timestamps, intervals)\narray([False,  True,  True,  True,  True,  True,  True, False])\n</code></pre> <pre><code>&gt;&gt;&gt; in_intervals(timestamps, intervals, return_interval=True)\n(array([False,  True,  True,  True,  True,  True,  True, False]),\narray([nan,  0.,  0.,  0.,  1.,  1.,  1., nan]))\n</code></pre> <pre><code>&gt;&gt;&gt; in_intervals(timestamps, intervals, shift=True)\n(array([False,  True,  True,  True,  True,  True,  True, False]),\narray([0, 1, 2, 2, 3, 4]))\n</code></pre> <pre><code>&gt;&gt;&gt; in_intervals(timestamps, intervals, return_interval=True, shift=True)\n(array([False,  True,  True,  True,  True,  True,  True, False]),\narray([0, 0, 0, 1, 1, 1]),\narray([0, 1, 2, 2, 3, 4]))\n</code></pre> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def in_intervals(\n    timestamps: np.ndarray,\n    intervals: np.ndarray,\n    return_interval: bool = False,\n    shift: bool = False,\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]]:\n    \"\"\"\n    Find which timestamps fall within the given intervals.\n\n    Parameters\n    ----------\n    timestamps : ndarray\n        An array of timestamp values. Assumes sorted.\n    intervals : ndarray\n        An array of time intervals, represented as pairs of start and end times.\n    return_interval : bool, optional (default=False)\n        If True, return the index of the interval to which each timestamp belongs.\n    shift : bool, optional (default=False)\n        If True, return the shifted timestamps\n\n    Returns\n    -------\n    in_interval : ndarray\n        A logical index indicating which timestamps fall within the intervals.\n    interval : ndarray, optional\n        A ndarray indicating for each timestamps which interval it was within.\n    shifted_timestamps : ndarray, optional\n        The shifted timestamps\n\n    Examples\n    --------\n    &gt;&gt;&gt; timestamps = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n    &gt;&gt;&gt; intervals = np.array([[2, 4], [5, 7]])\n    &gt;&gt;&gt; in_intervals(timestamps, intervals)\n    array([False,  True,  True,  True,  True,  True,  True, False])\n\n    &gt;&gt;&gt; in_intervals(timestamps, intervals, return_interval=True)\n    (array([False,  True,  True,  True,  True,  True,  True, False]),\n    array([nan,  0.,  0.,  0.,  1.,  1.,  1., nan]))\n\n    &gt;&gt;&gt; in_intervals(timestamps, intervals, shift=True)\n    (array([False,  True,  True,  True,  True,  True,  True, False]),\n    array([0, 1, 2, 2, 3, 4]))\n\n    &gt;&gt;&gt; in_intervals(timestamps, intervals, return_interval=True, shift=True)\n    (array([False,  True,  True,  True,  True,  True,  True, False]),\n    array([0, 0, 0, 1, 1, 1]),\n    array([0, 1, 2, 2, 3, 4]))\n    \"\"\"\n    in_interval = np.zeros(timestamps.shape, dtype=np.bool_)\n    interval = np.full(timestamps.shape, np.nan)\n\n    for i, (start, end) in enumerate(intervals):\n        # Find the leftmost index of a timestamp that is &gt;= start\n        left = np.searchsorted(timestamps, start, side=\"left\")\n        if left == len(timestamps):\n            # If start is greater than all timestamps, skip this interval\n            continue\n        # Find the rightmost index of a timestamp that is &lt;= end\n        right = np.searchsorted(timestamps, end, side=\"right\")\n        if right == left:\n            # If there are no timestamps in the interval, skip it\n            continue\n        # Mark the timestamps in the interval\n        in_interval[left:right] = True\n        interval[left:right] = i\n\n    if shift:\n        # Restrict to the timestamps that fall within the intervals\n        interval = interval[in_interval].astype(int)\n\n        # Calculate shifts based on intervals\n        shifts = np.insert(np.cumsum(intervals[1:, 0] - intervals[:-1, 1]), 0, 0)[\n            interval\n        ]\n\n        # Apply shifts to timestamps\n        shifted_timestamps = timestamps[in_interval] - shifts - intervals[0, 0]\n\n    if return_interval and shift:\n        return in_interval, interval, shifted_timestamps\n\n    if return_interval:\n        return in_interval, interval\n\n    if shift:\n        return in_interval, shifted_timestamps\n\n    return in_interval\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals.in_intervals_interval","title":"<code>in_intervals_interval(timestamps, intervals)</code>","text":"<p>for each timestamps value, the index of the interval to which it belongs (nan = none)</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>ndarray</code> <p>An array of timestamp values. assumes sorted</p> required <code>intervals</code> <code>ndarray</code> <p>An array of time intervals, represented as pairs of start and end times.</p> required <p>Returns:</p> Name Type Description <code>ndarray</code> <p>A ndarray indicating for each timestamps which interval it was within.</p> <code>Note</code> <code>produces same result as in_intervals with return_interval=True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; timestamps = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n&gt;&gt;&gt; intervals = np.array([[2, 4], [5, 7]])\n&gt;&gt;&gt; in_intervals_interval(timestamps, intervals)\narray([nan,  0,  0,  0,  1,  1,  1, nan])\n</code></pre> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>@jit(nopython=True, parallel=True)\ndef in_intervals_interval(timestamps: np.ndarray, intervals: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    for each timestamps value, the index of the interval to which it belongs (nan = none)\n\n    Parameters\n    ----------\n    timestamps : ndarray\n        An array of timestamp values. assumes sorted\n    intervals : ndarray\n        An array of time intervals, represented as pairs of start and end times.\n\n    Returns\n    -------\n    ndarray\n        A ndarray indicating for each timestamps which interval it was within.\n\n    Note: produces same result as in_intervals with return_interval=True\n\n    Examples\n    --------\n    &gt;&gt;&gt; timestamps = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n    &gt;&gt;&gt; intervals = np.array([[2, 4], [5, 7]])\n    &gt;&gt;&gt; in_intervals_interval(timestamps, intervals)\n    array([nan,  0,  0,  0,  1,  1,  1, nan])\n    \"\"\"\n    in_interval = np.full(timestamps.shape, np.nan)\n    for i in numba.prange(intervals.shape[0]):\n        start, end = intervals[i]\n        mask = (timestamps &gt;= start) &amp; (timestamps &lt;= end)\n        in_interval[mask] = i\n\n    return in_interval\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals.overlap_intersect","title":"<code>overlap_intersect(epoch, interval, return_indices=True)</code>","text":"<p>Returns the epochs with overlap with the given interval.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The epochs to check.</p> required <code>interval</code> <code>IntervalArray</code> <p>The interval to check for overlap.</p> required <code>return_indices</code> <code>bool</code> <p>If True, returns the indices of the overlapping epochs. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>EpochArray</code> <p>The epochs with overlap with the interval.</p> <code>(Tuple[EpochArray, ndarray], optional)</code> <p>If <code>return_indices</code> is True, also returns the indices of the overlapping epochs.</p> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def overlap_intersect(\n    epoch: nel.EpochArray, interval: nel.IntervalArray, return_indices: bool = True\n) -&gt; Union[nel.EpochArray, Tuple[nel.EpochArray, np.ndarray]]:\n    \"\"\"\n    Returns the epochs with overlap with the given interval.\n\n    Parameters\n    ----------\n    epoch : nelpy.EpochArray\n        The epochs to check.\n    interval : nelpy.IntervalArray\n        The interval to check for overlap.\n    return_indices : bool, optional\n        If True, returns the indices of the overlapping epochs. Default is True.\n\n    Returns\n    -------\n    nelpy.EpochArray\n        The epochs with overlap with the interval.\n    Tuple[nelpy.EpochArray, np.ndarray], optional\n        If `return_indices` is True, also returns the indices of the overlapping epochs.\n    \"\"\"\n    new_intervals = []\n    indices = []\n    for epa in epoch:\n        if any((interval.starts &lt; epa.stop) &amp; (interval.stops &gt; epa.start)):\n            new_intervals.append([epa.start, epa.stop])\n            cand_ep_idx = np.where(\n                (interval.starts &lt; epa.stop) &amp; (interval.stops &gt; epa.start)\n            )\n            indices.append(cand_ep_idx[0][0])\n    out = type(epoch)(new_intervals)\n    out._domain = epoch.domain\n    if return_indices:\n        return out, indices\n    return out\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals.randomize_epochs","title":"<code>randomize_epochs(epoch, randomize_each=True, start_stop=None)</code>","text":"<p>Randomly shifts the epochs of a EpochArray object and wraps them around the original time boundaries.</p> <p>This method takes a EpochArray object as input, and can either randomly shift each epoch by a different amount (if <code>randomize_each</code> is True) or shift all the epochs by the same amount (if <code>randomize_each</code> is False). In either case, the method wraps the shifted epochs around the original time boundaries to make sure they remain within the original time range. It then returns the modified EpochArray object.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The EpochArray object whose epochs should be shifted and wrapped.</p> required <code>randomize_each</code> <code>bool</code> <p>If True, each epoch will be shifted by a different random amount. If False, all the epochs will be shifted by the same random amount. Defaults to True.</p> <code>True</code> <code>start_stop</code> <code>array</code> <p>If not None, time support will be taken from start_stop</p> <code>None</code> <p>Returns:</p> Name Type Description <code>new_epochs</code> <code>EpochArray</code> <p>The modified EpochArray object with the shifted and wrapped epochs.</p> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def randomize_epochs(\n    epoch: EpochArray,\n    randomize_each: bool = True,\n    start_stop: Optional[np.ndarray] = None,\n) -&gt; EpochArray:\n    \"\"\"\n    Randomly shifts the epochs of a EpochArray object and wraps them around the original time boundaries.\n\n    This method takes a EpochArray object as input, and can either randomly shift each epoch by a different amount\n    (if `randomize_each` is True) or shift all the epochs by the same amount (if `randomize_each` is False).\n    In either case, the method wraps the shifted epochs around the original time boundaries to make sure they remain\n    within the original time range. It then returns the modified EpochArray object.\n\n    Parameters\n    ----------\n    epoch : EpochArray\n        The EpochArray object whose epochs should be shifted and wrapped.\n    randomize_each : bool, optional\n        If True, each epoch will be shifted by a different random amount.\n        If False, all the epochs will be shifted by the same random amount. Defaults to True.\n    start_stop : array, optional\n        If not None, time support will be taken from start_stop\n\n    Returns\n    -------\n    new_epochs : EpochArray\n        The modified EpochArray object with the shifted and wrapped epochs.\n    \"\"\"\n\n    def wrap_intervals(intervals, start, stop):\n        idx = np.any(intervals &gt; stop, axis=1)\n        intervals[idx] = intervals[idx] - stop + start\n\n        idx = np.any(intervals &lt; start, axis=1)\n        intervals[idx] = intervals[idx] - start + stop\n        return intervals\n\n    new_epochs = epoch.copy()\n\n    if start_stop is None:\n        start = new_epochs.start\n        stop = new_epochs.stop\n    else:\n        start, stop = start_stop\n\n    ts_range = stop - start\n\n    if randomize_each:\n        # Randomly shift each epoch by a different amount\n        random_order = random.sample(\n            range(-int(ts_range), int(ts_range)), new_epochs.n_intervals\n        )\n\n        new_intervals = new_epochs.data + np.expand_dims(random_order, axis=1)\n        new_epochs._data = wrap_intervals(new_intervals, start, stop)\n    else:\n        # Shift all the epochs by the same amount\n        random_shift = random.randint(-int(ts_range), int(ts_range))\n        new_epochs._data = wrap_intervals((new_epochs.data + random_shift), start, stop)\n\n    if not new_epochs.isempty:\n        if np.any(new_epochs.data[:, 1] - new_epochs.data[:, 0] &lt; 0):\n            raise ValueError(\"start must be less than or equal to stop\")\n\n    new_epochs._sort()\n\n    return new_epochs\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals.shift_epoch_array","title":"<code>shift_epoch_array(epoch, epoch_shift)</code>","text":"<p>Shift an EpochArray by another EpochArray.</p> <p>Shifting means that intervals in 'epoch' will be relative to intervals in 'epoch_shift' as if 'epoch_shift' intervals were without gaps.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The intervals to shift.</p> required <code>epoch_shift</code> <code>EpochArray</code> <p>The intervals to shift by.</p> required <p>Returns:</p> Type Description <code>EpochArray</code> <p>The shifted EpochArray.</p> Notes <p>This function restricts 'epoch' to those within 'epoch_shift' as epochs between 'epoch_shift' intervals would result in a duration of 0.</p> <p>Visual representation: inputs:     epoch       =   [  ]   [  ][]  []     epoch_shift =   [    ][]   [    ] becomes:     epoch       =   [  ]  [  ]    []     epoch_shift =   [    ][][    ]</p> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def shift_epoch_array(\n    epoch: nel.EpochArray, epoch_shift: nel.EpochArray\n) -&gt; nel.EpochArray:\n    \"\"\"\n    Shift an EpochArray by another EpochArray.\n\n    Shifting means that intervals in 'epoch' will be relative to\n    intervals in 'epoch_shift' as if 'epoch_shift' intervals were without gaps.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray\n        The intervals to shift.\n    epoch_shift : nel.EpochArray\n        The intervals to shift by.\n\n    Returns\n    -------\n    nel.EpochArray\n        The shifted EpochArray.\n\n    Notes\n    -----\n    This function restricts 'epoch' to those within 'epoch_shift' as\n    epochs between 'epoch_shift' intervals would result in a duration of 0.\n\n    Visual representation:\n    inputs:\n        epoch       =   [  ]   [  ] [  ]  []\n        epoch_shift =   [    ] [    ]   [    ]\n    becomes:\n        epoch       =   [  ]  [  ]    []\n        epoch_shift =   [    ][    ][    ]\n    \"\"\"\n    # input validation\n    if not isinstance(epoch, nel.EpochArray):\n        raise TypeError(\"epoch must be a nelpy EpochArray\")\n    if not isinstance(epoch_shift, nel.EpochArray):\n        raise TypeError(\"epoch_shift must be a nelpy EpochArray\")\n\n    # restrict epoch to epoch_shift and extract starts and stops\n    epoch_starts, epoch_stops = epoch[epoch_shift].data.T\n\n    # shift starts and stops by epoch_shift\n    _, epoch_starts_shifted = in_intervals(epoch_starts, epoch_shift.data, shift=True)\n    _, epoch_stops_shifted = in_intervals(epoch_stops, epoch_shift.data, shift=True)\n\n    # shift time support as well, if one exists\n    support_starts_shifted, support_stops_shifted = -np.inf, np.inf\n    if epoch.domain.start != -np.inf:\n        _, support_starts_shifted = in_intervals(\n            epoch.domain.start, epoch_shift.data, shift=True\n        )\n    if epoch.domain.stop != np.inf:\n        _, support_stops_shifted = in_intervals(\n            epoch.domain.stop, epoch_shift.data, shift=True\n        )\n\n    session_domain = nel.EpochArray([support_starts_shifted, support_stops_shifted])\n\n    # package shifted intervals into epoch array with shifted time support\n    return nel.EpochArray(\n        np.array([epoch_starts_shifted, epoch_stops_shifted]).T, domain=session_domain\n    )\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals.split_epoch_by_width","title":"<code>split_epoch_by_width(intervals, bin_width=0.001)</code>","text":"<p>Generate combined intervals (start, stop) at a specified width within given intervals.</p> <p>Parameters:</p> Name Type Description Default <code>intervals</code> <code>List[Tuple[float, float]]</code> <p>A list of (start, end) tuples representing intervals.</p> required <code>bin_width</code> <code>float</code> <p>The width of each bin in seconds. Default is 0.001 (1 ms).</p> <code>0.001</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D array containing (start, stop) pairs for all bins across intervals.</p> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def split_epoch_by_width(\n    intervals: List[Tuple[float, float]], bin_width: float = 0.001\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate combined intervals (start, stop) at a specified width within given intervals.\n\n    Parameters\n    ----------\n    intervals : List[Tuple[float, float]]\n        A list of (start, end) tuples representing intervals.\n    bin_width : float\n        The width of each bin in seconds. Default is 0.001 (1 ms).\n\n    Returns\n    -------\n    np.ndarray\n        A 2D array containing (start, stop) pairs for all bins across intervals.\n    \"\"\"\n    bin_intervals = []\n    for start, end in intervals:\n        # Generate bin edges\n        edges = np.arange(start, end, bin_width)\n        edges = np.append(edges, end)  # Ensure the final end is included\n        # Generate intervals (start, stop) for each bin\n        intervals = np.stack((edges[:-1], edges[1:]), axis=1)\n        bin_intervals.append(intervals)\n    return np.vstack(bin_intervals)\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals.split_epoch_equal_parts","title":"<code>split_epoch_equal_parts(intervals, n_parts, return_epoch_array=True)</code>","text":"<p>Split multiple intervals into equal parts.</p> <p>Parameters:</p> Name Type Description Default <code>intervals</code> <code>(array - like, shape(n_intervals, 2))</code> <p>The intervals to split.</p> required <code>n_parts</code> <code>int</code> <p>The number of parts to split each interval into.</p> required <code>return_epoch_array</code> <code>bool</code> <p>If True, returns the intervals as a nelpy.EpochArray object. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>split_intervals</code> <code>(array - like, shape(n_intervals * n_parts, 2) or EpochArray)</code> <p>The split intervals.</p> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def split_epoch_equal_parts(\n    intervals: np.ndarray, n_parts: int, return_epoch_array: bool = True\n) -&gt; Union[np.ndarray, nel.EpochArray]:\n    \"\"\"\n    Split multiple intervals into equal parts.\n\n    Parameters\n    ----------\n    intervals : array-like, shape (n_intervals, 2)\n        The intervals to split.\n    n_parts : int\n        The number of parts to split each interval into.\n    return_epoch_array : bool, optional\n        If True, returns the intervals as a nelpy.EpochArray object. Defaults to True.\n\n    Returns\n    -------\n    split_intervals : array-like, shape (n_intervals * n_parts, 2) or nelpy.EpochArray\n        The split intervals.\n    \"\"\"\n    # Ensure intervals is a numpy array\n    intervals = np.asarray(intervals)\n\n    # Number of intervals\n    n_intervals = intervals.shape[0]\n\n    # Preallocate the output array\n    split_intervals = np.zeros((n_intervals * n_parts, 2))\n\n    for i, interval in enumerate(intervals):\n        start, end = interval\n        epoch_parts = np.linspace(start, end, n_parts + 1)\n        epoch_parts = np.vstack((epoch_parts[:-1], epoch_parts[1:])).T\n        split_intervals[i * n_parts : (i + 1) * n_parts] = epoch_parts\n\n    if return_epoch_array:\n        return nel.EpochArray(split_intervals)\n    return split_intervals\n</code></pre>"},{"location":"reference/neuro_py/process/intervals/#neuro_py.process.intervals.truncate_epoch","title":"<code>truncate_epoch(epoch, time=3600)</code>","text":"<p>Truncates an EpochArray to achieve a specified cumulative time duration.</p> <p>This function takes an input EpochArray 'epoch' and a 'time' value representing the desired cumulative time duration in seconds. It returns a new EpochArray containing intervals that cumulatively match the specified time.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>EpochArray</code> <p>The input EpochArray containing intervals to be truncated.</p> required <code>time</code> <code>Union[int, float]</code> <p>The desired cumulative time in seconds (default is 3600).</p> <code>3600</code> <p>Returns:</p> Type Description <code>EpochArray</code> <p>A new EpochArray containing intervals that cumulatively match the specified time.</p> Algorithm <ol> <li>Calculate the cumulative lengths of intervals in the 'epoch'.</li> <li>If the cumulative time of the 'epoch' is already less than or equal to 'time',     return the original 'epoch'.</li> <li>Find the last interval that fits within the specified 'time' and create a new EpochArray     'truncated_intervals' with intervals up to that point.</li> <li>To achieve the desired cumulative time, calculate the remaining time needed to reach 'time'.</li> <li>Add portions of the next interval to 'truncated_intervals' until the desired 'time' is reached     or all intervals are used.</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_data = [(0, 2), (3, 6), (8, 10)]\n&gt;&gt;&gt; epoch = nel.EpochArray(epoch_data)\n&gt;&gt;&gt; truncated_epoch = truncate_epoch(epoch, time=7)\n</code></pre> Source code in <code>neuro_py/process/intervals.py</code> <pre><code>def truncate_epoch(\n    epoch: nel.EpochArray, time: Union[int, float] = 3600\n) -&gt; nel.EpochArray:\n    \"\"\"\n    Truncates an EpochArray to achieve a specified cumulative time duration.\n\n    This function takes an input EpochArray 'epoch' and a 'time' value representing\n    the desired cumulative time duration in seconds. It returns a new EpochArray\n    containing intervals that cumulatively match the specified time.\n\n    Parameters\n    ----------\n    epoch : nel.EpochArray\n        The input EpochArray containing intervals to be truncated.\n    time : Union[int, float], optional\n        The desired cumulative time in seconds (default is 3600).\n\n    Returns\n    -------\n    nel.EpochArray\n        A new EpochArray containing intervals that cumulatively match\n        the specified time.\n\n    Algorithm\n    ---------\n    1. Calculate the cumulative lengths of intervals in the 'epoch'.\n    2. If the cumulative time of the 'epoch' is already less than or equal to 'time',\n        return the original 'epoch'.\n    3. Find the last interval that fits within the specified 'time' and create a new EpochArray\n        'truncated_intervals' with intervals up to that point.\n    4. To achieve the desired cumulative time, calculate the remaining time needed to reach 'time'.\n    5. Add portions of the next interval to 'truncated_intervals' until the desired 'time' is reached\n        or all intervals are used.\n\n    Examples\n    --------\n    &gt;&gt;&gt; epoch_data = [(0, 2), (3, 6), (8, 10)]\n    &gt;&gt;&gt; epoch = nel.EpochArray(epoch_data)\n    &gt;&gt;&gt; truncated_epoch = truncate_epoch(epoch, time=7)\n    \"\"\"\n\n    if epoch.isempty:\n        return epoch\n\n    # calcuate cumulative lengths\n    cumulative_lengths = epoch.lengths.cumsum()\n\n    # No truncation needed\n    if cumulative_lengths[-1] &lt;= time:\n        return epoch\n\n    # Find the last interval that fits within the time and make new epoch\n    idx = cumulative_lengths &lt;= time\n    truncated_intervals = nel.EpochArray(epoch.data[idx])\n\n    # It's unlikely that the last interval will fit perfectly, so add the remainder from the next interval\n    #   until the epoch is the desired length\n    interval_i = 0\n    while (time - truncated_intervals.duration) &gt; 1e-10 or interval_i &gt; len(epoch):\n        # Add the last interval\n        next_interval = int(np.where(cumulative_lengths &gt;= time)[0][interval_i])\n\n        remainder = (\n            nel.EpochArray(\n                [\n                    epoch[next_interval].start,\n                    epoch[next_interval].start + (time - truncated_intervals.duration),\n                ]\n            )\n            &amp; epoch[next_interval]\n        )\n        truncated_intervals = truncated_intervals | remainder\n        interval_i += 1\n\n    return truncated_intervals\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/","title":"neuro_py.process.peri_event","text":""},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.compute_psth","title":"<code>compute_psth(spikes, event, bin_width=0.002, n_bins=100, window=None)</code>","text":"<p>Compute the Peri-Stimulus Time Histogram (PSTH) from spike trains.</p> <p>This function calculates the PSTH for a given set of spike times aligned to specific events. The PSTH provides a histogram of spike counts in response to the events over a defined time window.</p> <p>Parameters:</p> Name Type Description Default <code>spikes</code> <code>ndarray</code> <p>An array of spike times for multiple trials, with each trial in a separate row.</p> required <code>event</code> <code>ndarray</code> <p>An array of event times to which the spikes are aligned.</p> required <code>bin_width</code> <code>float</code> <p>Width of each time bin in seconds (default is 0.002 seconds).</p> <code>0.002</code> <code>n_bins</code> <code>int</code> <p>Number of bins to create for the histogram (default is 100).</p> <code>100</code> <code>window</code> <code>list</code> <p>Time window around each event to consider for the PSTH. If None, a symmetric window is created based on <code>n_bins</code> and <code>bin_width</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame containing the PSTH, indexed by time bins and columns representing each trial's PSTH.</p> Notes <p>If the specified window is not symmetric around 0, it is adjusted to be symmetric.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spikes = np.array([[0.1, 0.15, 0.2], [0.1, 0.12, 0.13]])\n&gt;&gt;&gt; event = np.array([0.1, 0.3])\n&gt;&gt;&gt; psth = compute_psth(spikes, event)\n</code></pre> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def compute_psth(\n    spikes: np.ndarray,\n    event: np.ndarray,\n    bin_width: float = 0.002,\n    n_bins: int = 100,\n    window: list = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the Peri-Stimulus Time Histogram (PSTH) from spike trains.\n\n    This function calculates the PSTH for a given set of spike times aligned to specific events.\n    The PSTH provides a histogram of spike counts in response to the events over a defined time window.\n\n    Parameters\n    ----------\n    spikes : np.ndarray\n        An array of spike times for multiple trials, with each trial in a separate row.\n    event : np.ndarray\n        An array of event times to which the spikes are aligned.\n    bin_width : float, optional\n        Width of each time bin in seconds (default is 0.002 seconds).\n    n_bins : int, optional\n        Number of bins to create for the histogram (default is 100).\n    window : list, optional\n        Time window around each event to consider for the PSTH. If None, a symmetric window is created based on `n_bins` and `bin_width`.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing the PSTH, indexed by time bins and columns representing each trial's PSTH.\n\n    Notes\n    -----\n    If the specified window is not symmetric around 0, it is adjusted to be symmetric.\n\n    Examples\n    -------\n    &gt;&gt;&gt; spikes = np.array([[0.1, 0.15, 0.2], [0.1, 0.12, 0.13]])\n    &gt;&gt;&gt; event = np.array([0.1, 0.3])\n    &gt;&gt;&gt; psth = compute_psth(spikes, event)\n    \"\"\"\n    if window is not None:\n        window_original = None\n        # check if window is symmetric around 0, if not make it so\n        if ((window[1] - window[0]) / 2 != window[1]) | (\n            (window[1] - window[0]) / -2 != window[0]\n        ):\n            window_original = np.array(window)\n            window = [-np.max(np.abs(window)), np.max(np.abs(window))]\n\n        times = np.arange(window[0], window[1] + bin_width / 2, bin_width)\n        n_bins = len(times) - 1\n    else:\n        times = np.linspace(\n            -(n_bins * bin_width) / 2, (n_bins * bin_width) / 2, n_bins + 1\n        )\n\n    ccg = pd.DataFrame(index=times, columns=np.arange(len(spikes)))\n    # Now we can iterate over spikes\n    for i, s in enumerate(spikes):\n        ccg[i] = crossCorr(event, s, bin_width, n_bins)\n\n    # if window was not symmetric, remove the extra bins\n    if window is not None:\n        if window_original is not None:\n            ccg = ccg.loc[window_original[0] : window_original[1], :]\n    return ccg\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.count_events","title":"<code>count_events(events, time_ref, time_range)</code>","text":"<p>Count the number of events that occur within a given time range after each reference event.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>ndarray</code> <p>A 1D array of event times.</p> required <code>time_ref</code> <code>ndarray</code> <p>A 1D array of reference times.</p> required <code>time_range</code> <code>tuple of (float, float)</code> <p>A tuple containing the start and end times of the time range.</p> required <p>Returns:</p> Name Type Description <code>counts</code> <code>ndarray</code> <p>A 1D array of event counts, one for each reference time (same length as time_ref).</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def count_events(\n    events: np.ndarray, time_ref: np.ndarray, time_range: Tuple[float, float]\n) -&gt; np.ndarray:\n    \"\"\"\n    Count the number of events that occur within a given time range after each reference event.\n\n    Parameters\n    ----------\n    events : np.ndarray\n        A 1D array of event times.\n    time_ref : np.ndarray\n        A 1D array of reference times.\n    time_range : tuple of (float, float)\n        A tuple containing the start and end times of the time range.\n\n    Returns\n    -------\n    counts : np.ndarray\n        A 1D array of event counts, one for each reference time (same length as time_ref).\n    \"\"\"\n    # Initialize an array to store the event counts\n    counts = np.zeros_like(time_ref)\n\n    # Iterate over the reference times\n    for i, r in enumerate(time_ref):\n        # Check if any events occur within the time range\n        idx = (events &gt; r + time_range[0]) &amp; (events &lt; r + time_range[1])\n        # Increment the event count if any events are found\n        counts[i] = len(events[idx])\n\n    return counts\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.count_in_interval","title":"<code>count_in_interval(st, event_starts, event_stops, par_type='counts')</code>","text":"<p>Count timestamps in specified intervals and return a matrix where each column represents the counts for each spike train over given event epochs.</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>ndarray</code> <p>A 1D array where each element is a spike train for a unit.</p> required <code>event_starts</code> <code>ndarray</code> <p>A 1D array containing the start times of events.</p> required <code>event_stops</code> <code>ndarray</code> <p>A 1D array containing the stop times of events.</p> required <code>par_type</code> <code>str</code> <p>The type of count calculation to perform: - 'counts': returns raw counts of spikes in the intervals. - 'binary': returns a binary matrix indicating presence (1) or absence (0) of spikes. - 'firing_rate': returns the firing rate calculated as counts divided by the interval duration. Defaults to 'binary'.</p> <code>'counts'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D array (n units x n epochs) where each column shows the counts (or binary values or firing rates) per unit for each epoch.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def count_in_interval(\n    st: np.ndarray,\n    event_starts: np.ndarray,\n    event_stops: np.ndarray,\n    par_type: str = \"counts\",\n) -&gt; np.ndarray:\n    \"\"\"\n    Count timestamps in specified intervals and return a matrix where each\n    column represents the counts for each spike train over given event epochs.\n\n    Parameters\n    ----------\n    st : np.ndarray\n        A 1D array where each element is a spike train for a unit.\n\n    event_starts : np.ndarray\n        A 1D array containing the start times of events.\n\n    event_stops : np.ndarray\n        A 1D array containing the stop times of events.\n\n    par_type : str, optional\n        The type of count calculation to perform:\n        - 'counts': returns raw counts of spikes in the intervals.\n        - 'binary': returns a binary matrix indicating presence (1) or absence (0) of spikes.\n        - 'firing_rate': returns the firing rate calculated as counts divided by the interval duration.\n        Defaults to 'binary'.\n\n    Returns\n    -------\n    np.ndarray\n        A 2D array (n units x n epochs) where each column shows the counts (or binary values or firing rates)\n        per unit for each epoch.\n    \"\"\"\n    # convert to numpy array\n    event_starts, event_stops = np.array(event_starts), np.array(event_stops)\n\n    # initialize matrix\n    unit_mat = np.zeros((len(st), (len(event_starts))))\n\n    # loop over units and bin spikes into epochs\n    for i, s in enumerate(st):\n        idx1 = np.searchsorted(s, event_starts, \"right\")\n        idx2 = np.searchsorted(s, event_stops, \"left\")\n        unit_mat[i, :] = idx2 - idx1\n\n    par_type_funcs = {\n        \"counts\": lambda x: x,\n        \"binary\": lambda x: (x &gt; 0) * 1,\n        \"firing_rate\": lambda x: x / (event_stops - event_starts),\n    }\n    calc_func = par_type_funcs[par_type]\n    unit_mat = calc_func(unit_mat)\n\n    return unit_mat\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.crossCorr","title":"<code>crossCorr(t1, t2, binsize, nbins)</code>","text":"<p>Perform the discrete cross-correlogram of two time series.</p> <p>This function calculates the firing rate of the series 't2' relative to the timings of 't1'. The units should be in seconds for all arguments.</p> <p>Parameters:</p> Name Type Description Default <code>t1</code> <code>ndarray</code> <p>First time series.</p> required <code>t2</code> <code>ndarray</code> <p>Second time series.</p> required <code>binsize</code> <code>float</code> <p>Size of the bin in seconds.</p> required <code>nbins</code> <code>int</code> <p>Number of bins.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Cross-correlogram of the two time series.</p> Notes <p>This implementation is based on the work of Guillaume Viejo. References: - https://github.com/PeyracheLab/StarterPack/blob/master/python/main6_autocorr.py - https://github.com/pynapple-org/pynapple/blob/main/pynapple/process/correlograms.py</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>@jit(nopython=True)\ndef crossCorr(\n    t1: np.ndarray,\n    t2: np.ndarray,\n    binsize: float,\n    nbins: int,\n) -&gt; np.ndarray:\n    \"\"\"\n    Perform the discrete cross-correlogram of two time series.\n\n    This function calculates the firing rate of the series 't2' relative to the timings of 't1'.\n    The units should be in seconds for all arguments.\n\n    Parameters\n    ----------\n    t1 : np.ndarray\n        First time series.\n    t2 : np.ndarray\n        Second time series.\n    binsize : float\n        Size of the bin in seconds.\n    nbins : int\n        Number of bins.\n\n    Returns\n    -------\n    np.ndarray\n        Cross-correlogram of the two time series.\n\n    Notes\n    -----\n    This implementation is based on the work of Guillaume Viejo.\n    References:\n    - https://github.com/PeyracheLab/StarterPack/blob/master/python/main6_autocorr.py\n    - https://github.com/pynapple-org/pynapple/blob/main/pynapple/process/correlograms.py\n    \"\"\"\n    # Calculate the length of the input time series\n    nt1 = len(t1)\n    nt2 = len(t2)\n\n    # Ensure that 'nbins' is an odd number\n    if np.floor(nbins / 2) * 2 == nbins:\n        nbins = nbins + 1\n\n    # Calculate the half-width of the cross-correlogram window\n    w = (nbins / 2) * binsize\n    C = np.zeros(nbins)\n    i2 = 1\n\n    # Iterate through the first time series\n    for i1 in range(nt1):\n        lbound = t1[i1] - w\n\n        # Find the index of the first element in 't2' that is within 'lbound'\n        while i2 &lt; nt2 and t2[i2] &lt; lbound:\n            i2 = i2 + 1\n\n        # Find the index of the last element in 't2' that is within 'lbound'\n        while i2 &gt; 1 and t2[i2 - 1] &gt; lbound:\n            i2 = i2 - 1\n\n        rbound = lbound\n        last_index = i2\n\n        # Calculate the cross-correlogram values for each bin\n        for j in range(nbins):\n            k = 0\n            rbound = rbound + binsize\n\n            # Count the number of elements in 't2' that fall within the bin\n            while last_index &lt; nt2 and t2[last_index] &lt; rbound:\n                last_index = last_index + 1\n                k = k + 1\n\n            C[j] += k\n\n    # Normalize the cross-correlogram by dividing by the total observation time and bin size\n    C = C / (nt1 * binsize)\n\n    return C\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.deconvolve_peth","title":"<code>deconvolve_peth(signal, events, bin_width=0.002, n_bins=100)</code>","text":"<p>Perform deconvolution of a peri-event time histogram (PETH) signal.</p> <p>This function calculates the deconvolved signal based on the input signal and events.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>ndarray</code> <p>An array representing the discrete events.</p> required <code>events</code> <code>ndarray</code> <p>An array representing the discrete events.</p> required <code>bin_width</code> <code>float</code> <p>The width of a time bin in seconds (default is 0.002 seconds).</p> <code>0.002</code> <code>n_bins</code> <code>int</code> <p>The number of bins to use in the PETH (default is 100 bins).</p> <code>100</code> <p>Returns:</p> Name Type Description <code>deconvolved</code> <code>ndarray</code> <p>An array representing the deconvolved signal.</p> <code>times</code> <code>ndarray</code> <p>An array representing the time points corresponding to the bins.</p> Notes <p>Based on DeconvolvePETH.m from https://github.com/ayalab1/neurocode/blob/master/spikes/DeconvolvePETH.m</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def deconvolve_peth(\n    signal: np.ndarray, events: np.ndarray, bin_width: float = 0.002, n_bins: int = 100\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Perform deconvolution of a peri-event time histogram (PETH) signal.\n\n    This function calculates the deconvolved signal based on the input signal and events.\n\n    Parameters\n    ----------\n    signal : np.ndarray\n        An array representing the discrete events.\n    events : np.ndarray\n        An array representing the discrete events.\n    bin_width : float, optional\n        The width of a time bin in seconds (default is 0.002 seconds).\n    n_bins : int, optional\n        The number of bins to use in the PETH (default is 100 bins).\n\n    Returns\n    -------\n    deconvolved : np.ndarray\n        An array representing the deconvolved signal.\n    times : np.ndarray\n        An array representing the time points corresponding to the bins.\n\n    Notes\n    -----\n    Based on DeconvolvePETH.m from https://github.com/ayalab1/neurocode/blob/master/spikes/DeconvolvePETH.m\n    \"\"\"\n\n    # calculate time lags for peth\n    times = np.linspace(-(n_bins * bin_width) / 2, (n_bins * bin_width) / 2, n_bins + 1)\n\n    # Calculate the autocorrelogram of the signal and the PETH of the events and the signal\n    autocorrelogram = crossCorr(signal, signal, bin_width, n_bins * 2)\n    raw_peth = crossCorr(events, signal, bin_width, n_bins * 2)\n\n    # If raw_peth all zeros, return zeros\n    if not raw_peth.any():\n        return np.zeros(len(times)), times\n\n    # Subtract the mean value from the raw_peth\n    const = np.mean(raw_peth)\n    raw_peth = raw_peth - const\n\n    # Calculate the Toeplitz matrix using the autocorrelogram and\n    #   the cross-correlation of the autocorrelogram\n    T0 = toeplitz(\n        autocorrelogram,\n        np.hstack([autocorrelogram[0], np.zeros(len(autocorrelogram) - 1)]),\n    )\n    T = T0[n_bins:, : n_bins + 1]\n\n    # Calculate the deconvolved signal by solving a linear equation\n    deconvolved = np.linalg.solve(\n        T, raw_peth[int(n_bins / 2) : int(n_bins / 2 * 3 + 1)].T + const / len(events)\n    )\n\n    return deconvolved, times\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.event_spiking_threshold","title":"<code>event_spiking_threshold(spikes, events, window=[-0.5, 0.5], event_size=0.1, spiking_thres=0, binsize=0.01, sigma=0.02, min_units=6, show_fig=False)</code>","text":"<p>event_spiking_threshold: filter events based on spiking threshold</p> <p>Parameters:</p> Name Type Description Default <code>spikes</code> <code>SpikeTrainArray</code> <p>Spike train array of neurons.</p> required <code>events</code> <code>ndarray</code> <p>Event times in seconds.</p> required <code>window</code> <code>list of float</code> <p>Time window (in seconds) to compute event-triggered average, by default [-0.5, 0.5].</p> <code>[-0.5, 0.5]</code> <code>event_size</code> <code>float</code> <p>Time window (in seconds) around event to measure firing response, by default 0.1.</p> <code>0.1</code> <code>spiking_thres</code> <code>float</code> <p>Spiking threshold in z-score units, by default 0.</p> <code>0</code> <code>binsize</code> <code>float</code> <p>Bin size (in seconds) for time-binning the spike trains, by default 0.01.</p> <code>0.01</code> <code>sigma</code> <code>float</code> <p>Standard deviation (in seconds) for Gaussian smoothing of spike counts, by default 0.02.</p> <code>0.02</code> <code>min_units</code> <code>int</code> <p>Minimum number of units required to compute event-triggered average, by default 6.</p> <code>6</code> <code>show_fig</code> <code>bool</code> <p>If True, plots the figure of event-triggered spiking activity, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean array indicating valid events that meet the spiking threshold.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; basepath = r\"U:\\data\\hpc_ctx_project\\HP04\\day_32_20240430\"\n&gt;&gt;&gt; ripples = loading.load_ripples_events(basepath, return_epoch_array=False)\n&gt;&gt;&gt; st, cell_metrics = loading.load_spikes(\n        basepath,\n        brainRegion=\"CA1\",\n        support=nel.EpochArray([0, loading.load_epoch(basepath).iloc[-1].stopTime])\n    )\n&gt;&gt;&gt; idx = event_spiking_threshold(st, ripples.peaks.values, show_fig=True)\n&gt;&gt;&gt; print(f\"Number of valid ripples: {idx.sum()} out of {len(ripples)}\")\nNumber of valid ripples: 9244 out of 12655\n</code></pre> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def event_spiking_threshold(\n    spikes: SpikeTrainArray,\n    events: np.ndarray,\n    window: list = [-0.5, 0.5],\n    event_size: float = 0.1,\n    spiking_thres: float = 0,\n    binsize: float = 0.01,\n    sigma: float = 0.02,\n    min_units: int = 6,\n    show_fig: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    event_spiking_threshold: filter events based on spiking threshold\n\n    Parameters\n    ----------\n    spikes : nel.SpikeTrainArray\n        Spike train array of neurons.\n    events : np.ndarray\n        Event times in seconds.\n    window : list of float, optional\n        Time window (in seconds) to compute event-triggered average, by default [-0.5, 0.5].\n    event_size : float, optional\n        Time window (in seconds) around event to measure firing response, by default 0.1.\n    spiking_thres : float, optional\n        Spiking threshold in z-score units, by default 0.\n    binsize : float, optional\n        Bin size (in seconds) for time-binning the spike trains, by default 0.01.\n    sigma : float, optional\n        Standard deviation (in seconds) for Gaussian smoothing of spike counts, by default 0.02.\n    min_units : int, optional\n        Minimum number of units required to compute event-triggered average, by default 6.\n    show_fig : bool, optional\n        If True, plots the figure of event-triggered spiking activity, by default False.\n\n    Returns\n    -------\n    np.ndarray\n        Boolean array indicating valid events that meet the spiking threshold.\n\n    Examples\n    -------\n    &gt;&gt;&gt; basepath = r\"U:\\\\data\\\\hpc_ctx_project\\\\HP04\\\\day_32_20240430\"\n    &gt;&gt;&gt; ripples = loading.load_ripples_events(basepath, return_epoch_array=False)\n    &gt;&gt;&gt; st, cell_metrics = loading.load_spikes(\n            basepath,\n            brainRegion=\"CA1\",\n            support=nel.EpochArray([0, loading.load_epoch(basepath).iloc[-1].stopTime])\n        )\n    &gt;&gt;&gt; idx = event_spiking_threshold(st, ripples.peaks.values, show_fig=True)\n    &gt;&gt;&gt; print(f\"Number of valid ripples: {idx.sum()} out of {len(ripples)}\")\n    Number of valid ripples: 9244 out of 12655\n\n    \"\"\"\n\n    # check if there are enough units to compute a confident event triggered average\n    if spikes.n_active &lt; min_units:\n        return np.ones(len(events), dtype=bool)\n\n    # bin spikes\n    bst = spikes.bin(ds=binsize).smooth(sigma=sigma)\n    # sum over all neurons and zscore\n    bst = bst.data.sum(axis=0)\n    bst = (bst - bst.mean()) / bst.std()\n    # get event triggered average\n    avg_signal, time_lags = event_triggered_average_fast(\n        bst[np.newaxis, :],\n        events,\n        sampling_rate=int(1 / binsize),\n        window=window,\n        return_average=False,\n    )\n    # get the event response within the event size\n    idx = (time_lags &gt;= -event_size) &amp; (time_lags &lt;= event_size)\n    event_response = avg_signal[0, idx, :].mean(axis=0)\n\n    # get events that are above threshold\n    valid_events = event_response &gt; spiking_thres\n\n    if show_fig:\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        sorted_idx = np.argsort(event_response)\n\n        fig, ax = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n        ax[0].imshow(\n            avg_signal[0, :, sorted_idx],\n            aspect=\"auto\",\n            extent=[time_lags[0], time_lags[-1], 0, len(event_response)],\n            vmin=-2,\n            vmax=2,\n            origin=\"lower\",\n            interpolation=\"nearest\",\n        )\n        ax[0].axhline(\n            np.where(event_response[sorted_idx] &gt; spiking_thres)[0][0],\n            color=\"r\",\n            linestyle=\"--\",\n        )\n        ax[1].plot(event_response[sorted_idx], np.arange(len(event_response)))\n        ax[1].axvline(spiking_thres, color=\"r\", linestyle=\"--\")\n        ax[0].set_xlabel(\"Time from event (s)\")\n        ax[0].set_ylabel(\"Event index\")\n        ax[1].set_xlabel(\"Average response\")\n        ax[1].set_ylabel(\"Event index\")\n        sns.despine()\n\n    return valid_events\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.event_triggered_average","title":"<code>event_triggered_average(timestamps, signal, events, sampling_rate=None, window=[-0.5, 0.5], return_pandas=False)</code>","text":"<p>Calculates the spike-triggered averages of signals in a time window relative to the event times of corresponding events for multiple signals.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>ndarray</code> <p>A 1D array of timestamps corresponding to the signal samples.</p> required <code>signal</code> <code>ndarray</code> <p>A 2D array of shape (n_samples, n_signals) containing the signal values.</p> required <code>events</code> <code>Union[ndarray, List[ndarray]]</code> <p>One or more 1D arrays of event times. If a single array is provided, it will be multiplied n-fold to match the number of signals.</p> required <code>sampling_rate</code> <code>Union[float, None]</code> <p>The sampling rate of the signal. If not provided, it will be calculated based on the timestamps.</p> <code>None</code> <code>window</code> <code>List[float]</code> <p>A list containing two elements: the start and stop times relative to an event for the time interval of signal averaging. Default is [-0.5, 0.5].</p> <code>[-0.5, 0.5]</code> <code>return_pandas</code> <code>bool</code> <p>If True, return the result as a Pandas DataFrame. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple containing the event-triggered averages of the signals and the corresponding time lags.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m1 = assembly_reactivation.AssemblyReact(basepath=r\"Z:\\Data\\HMC2\\day5\")\n</code></pre> <pre><code>&gt;&gt;&gt; m1.load_data()\n&gt;&gt;&gt; m1.get_weights(epoch=m1.epochs[1])\n&gt;&gt;&gt; assembly_act = m1.get_assembly_act()\n</code></pre> <pre><code>&gt;&gt;&gt; peth_avg, time_lags = event_triggered_average(\n...    assembly_act.abscissa_vals, assembly_act.data.T, m1.ripples.starts, window=[-0.5, 0.5]\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; plt.plot(time_lags,peth_avg)\n&gt;&gt;&gt; plt.show()\n</code></pre> Notes <p>The function is adapted from elephant.sta.spike_triggered_average to be used with ndarray.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def event_triggered_average(\n    timestamps: np.ndarray,\n    signal: np.ndarray,\n    events: Union[np.ndarray, List[np.ndarray]],\n    sampling_rate: Union[float, None] = None,\n    window: List[float] = [-0.5, 0.5],\n    return_pandas: bool = False,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculates the spike-triggered averages of signals in a time window\n    relative to the event times of corresponding events for multiple signals.\n\n    Parameters\n    ----------\n    timestamps : np.ndarray\n        A 1D array of timestamps corresponding to the signal samples.\n\n    signal : np.ndarray\n        A 2D array of shape (n_samples, n_signals) containing the signal values.\n\n    events : Union[np.ndarray, List[np.ndarray]]\n        One or more 1D arrays of event times. If a single array is provided,\n        it will be multiplied n-fold to match the number of signals.\n\n    sampling_rate : Union[float, None], optional\n        The sampling rate of the signal. If not provided, it will be calculated\n        based on the timestamps.\n\n    window : List[float], optional\n        A list containing two elements: the start and stop times relative to an event\n        for the time interval of signal averaging. Default is [-0.5, 0.5].\n\n    return_pandas : bool, optional\n        If True, return the result as a Pandas DataFrame. Default is False.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing the event-triggered averages of the signals and the\n        corresponding time lags.\n\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; m1 = assembly_reactivation.AssemblyReact(basepath=r\"Z:\\\\Data\\\\HMC2\\\\day5\")\n\n    &gt;&gt;&gt; m1.load_data()\n    &gt;&gt;&gt; m1.get_weights(epoch=m1.epochs[1])\n    &gt;&gt;&gt; assembly_act = m1.get_assembly_act()\n\n    &gt;&gt;&gt; peth_avg, time_lags = event_triggered_average(\n    ...    assembly_act.abscissa_vals, assembly_act.data.T, m1.ripples.starts, window=[-0.5, 0.5]\n    ... )\n\n    &gt;&gt;&gt; plt.plot(time_lags,peth_avg)\n    &gt;&gt;&gt; plt.show()\n\n    Notes\n    -----\n    The function is adapted from elephant.sta.spike_triggered_average to be used with ndarray.\n    \"\"\"\n\n    # check inputs\n    if len(window) != 2:\n        raise ValueError(\n            \"'window' must be a tuple of 2 elements, not {}\".format(len(window))\n        )\n\n    if window[0] &gt; window[1]:\n        raise ValueError(\n            \"'window' first value must be less than second value, not {}\".format(\n                len(window)\n            )\n        )\n\n    if not isinstance(timestamps, np.ndarray):\n        raise ValueError(\n            \"'timestamps' must be a numpy ndarray, not {}\".format(type(timestamps))\n        )\n\n    if not isinstance(signal, np.ndarray):\n        raise ValueError(\n            \"'signal' must be a numpy ndarray, not {}\".format(type(signal))\n        )\n\n    if not isinstance(events, (list, np.ndarray)):\n        raise ValueError(\n            \"'events' must be a numpy ndarray or list, not {}\".format(type(events))\n        )\n\n    if signal.shape[0] != timestamps.shape[0]:\n        raise ValueError(\"'signal' and 'timestamps' must have the same number of rows\")\n\n    if len(timestamps.shape) &gt; 1:\n        raise ValueError(\n            \"'timestamps' must be a 1D array, not {}\".format(len(timestamps.shape))\n        )\n\n    window_starttime, window_stoptime = window\n\n    if len(signal.shape) == 1:\n        signal = np.expand_dims(signal, -1)\n\n    _, num_signals = signal.shape\n\n    if sampling_rate is None:\n        sampling_rate = 1 / stats.mode(np.diff(timestamps), keepdims=True)[0][0]\n\n    # window_bins: number of bins of the chosen averaging interval\n    window_bins = int(np.ceil(((window_stoptime - window_starttime) * sampling_rate)))\n    # result_sta: array containing finally the spike-triggered averaged signal\n    result_sta = np.zeros((window_bins, num_signals))\n    # setting of correct times of the spike-triggered average\n    # relative to the spike\n    time_lags = np.linspace(window_starttime, window_stoptime, window_bins)\n\n    used_events = np.zeros(num_signals, dtype=int)\n    total_used_events = 0\n\n    for i in range(num_signals):\n        # summing over all respective signal intervals around spiketimes\n        for event in events:\n            # locate signal in time range\n            idx = (timestamps &gt;= event + window_starttime) &amp; (\n                timestamps &lt;= event + window_stoptime\n            )\n\n            # for speed, instead of checking if we have enough time each iteration, just skip if we don't\n            try:\n                result_sta[:, i] += signal[idx, i]\n            except Exception:\n                continue\n            # counting of the used event\n            used_events[i] += 1\n\n        # normalization\n        result_sta[:, i] = result_sta[:, i] / used_events[i]\n\n        total_used_events += used_events[i]\n\n    if total_used_events == 0:\n        warnings.warn(\"No events at all was either found or used for averaging\")\n\n    if return_pandas:\n        return pd.DataFrame(\n            index=time_lags, columns=np.arange(result_sta.shape[1]), data=result_sta\n        )\n\n    return result_sta, time_lags\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.event_triggered_average_fast","title":"<code>event_triggered_average_fast(signal, events, sampling_rate, window=[-0.5, 0.5], return_average=True, return_pandas=False)</code>","text":"<p>Calculate the event-triggered average of a signal.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>ndarray</code> <p>A 2D array of signal data with shape (channels, timepoints).</p> required <code>events</code> <code>ndarray</code> <p>A 1D array of event times.</p> required <code>sampling_rate</code> <code>int</code> <p>The sampling rate of the signal in Hz.</p> required <code>window</code> <code>Union[list, Tuple[float, float]]</code> <p>A list or tuple specifying the time window (in seconds) to average the signal around each event. Defaults to [-0.5, 0.5].</p> <code>[-0.5, 0.5]</code> <code>return_average</code> <code>bool</code> <p>Whether to return the average of the event-triggered average. Defaults to True. If False, returns the full event-triggered average matrix (channels x timepoints x events).</p> <code>True</code> <code>return_pandas</code> <code>bool</code> <p>If True, returns the average as a Pandas DataFrame. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, DataFrame]</code> <p>If <code>return_average</code> is True, returns the event-triggered average of the signal (channels x timepoints) or a Pandas DataFrame if <code>return_pandas</code> is True. If <code>return_average</code> is False, returns the full event-triggered average matrix.</p> <code>ndarray</code> <p>An array of time lags corresponding to the event-triggered averages.</p> Notes <ul> <li>The function filters out events that do not fit within the valid range of the signal   considering the specified window size.</li> </ul> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def event_triggered_average_fast(\n    signal: np.ndarray,\n    events: np.ndarray,\n    sampling_rate: int,\n    window: Union[list, Tuple[float, float]] = [-0.5, 0.5],\n    return_average: bool = True,\n    return_pandas: bool = False,\n) -&gt; Tuple[Union[np.ndarray, pd.DataFrame], np.ndarray]:\n    \"\"\"\n    Calculate the event-triggered average of a signal.\n\n    Parameters\n    ----------\n    signal : np.ndarray\n        A 2D array of signal data with shape (channels, timepoints).\n\n    events : np.ndarray\n        A 1D array of event times.\n\n    sampling_rate : int\n        The sampling rate of the signal in Hz.\n\n    window : Union[list, Tuple[float, float]], optional\n        A list or tuple specifying the time window (in seconds) to average the signal\n        around each event. Defaults to [-0.5, 0.5].\n\n    return_average : bool, optional\n        Whether to return the average of the event-triggered average. Defaults to True.\n        If False, returns the full event-triggered average matrix (channels x timepoints x events).\n\n    return_pandas : bool, optional\n        If True, returns the average as a Pandas DataFrame. Defaults to False.\n\n    Returns\n    -------\n    Union[np.ndarray, pd.DataFrame]\n        If `return_average` is True, returns the event-triggered average of the signal\n        (channels x timepoints) or a Pandas DataFrame if `return_pandas` is True.\n        If `return_average` is False, returns the full event-triggered average matrix.\n\n    np.ndarray\n        An array of time lags corresponding to the event-triggered averages.\n\n    Notes\n    -----\n    - The function filters out events that do not fit within the valid range of the signal\n      considering the specified window size.\n    \"\"\"\n\n    window_starttime, window_stoptime = window\n    window_bins = int(np.ceil(((window_stoptime - window_starttime) * sampling_rate)))\n    time_lags = np.linspace(window_starttime, window_stoptime, window_bins)\n\n    events = events[\n        (events * sampling_rate &gt; len(time_lags) / 2 + 1)\n        &amp; (events * sampling_rate &lt; signal.shape[1] - len(time_lags) / 2 + 1)\n    ]\n\n    avg_signal = np.zeros(\n        [signal.shape[0], len(time_lags), len(events)], dtype=signal.dtype\n    )\n\n    for i, event in enumerate(events):\n        ts_idx = np.arange(\n            np.round(event * sampling_rate) - len(time_lags) / 2,\n            np.round(event * sampling_rate) + len(time_lags) / 2,\n        ).astype(int)\n        avg_signal[:, :, i] = signal[:, ts_idx]\n\n    if return_pandas and return_average:\n        return pd.DataFrame(\n            index=time_lags,\n            columns=np.arange(signal.shape[0]),\n            data=bn.nanmean(avg_signal, axis=2).T,\n        )\n\n    if return_average:\n        return bn.nanmean(avg_signal, axis=2), time_lags\n    else:\n        return avg_signal, time_lags\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.event_triggered_average_irregular_sample","title":"<code>event_triggered_average_irregular_sample(timestamps, data, time_ref, bin_width=0.002, n_bins=100, window=None)</code>","text":"<p>Compute the average and standard deviation of data values within a window around each reference time, specifically for irregularly sampled data.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>ndarray</code> <p>A 1D array of times associated with data.</p> required <code>data</code> <code>ndarray</code> <p>A 1D array of data values.</p> required <code>time_ref</code> <code>ndarray</code> <p>A 1D array of reference times.</p> required <code>bin_width</code> <code>float</code> <p>The width of each bin in the window, in seconds. Default is 0.002 seconds.</p> <code>0.002</code> <code>n_bins</code> <code>int</code> <p>The number of bins in the window. Default is 100.</p> <code>100</code> <code>window</code> <code>Union[tuple, None]</code> <p>A tuple containing the start and end times of the window to be plotted around each reference time. If not provided, the window will be centered around each reference time and have a width of <code>n_bins * bin_width</code> seconds.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Two DataFrames: the first containing the average values, the second the standard deviation of data values within the window around each reference time.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def event_triggered_average_irregular_sample(\n    timestamps: np.ndarray,\n    data: np.ndarray,\n    time_ref: np.ndarray,\n    bin_width: float = 0.002,\n    n_bins: int = 100,\n    window: Union[tuple, None] = None,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Compute the average and standard deviation of data values within a window around\n    each reference time, specifically for irregularly sampled data.\n\n    Parameters\n    ----------\n    timestamps : np.ndarray\n        A 1D array of times associated with data.\n    data : np.ndarray\n        A 1D array of data values.\n    time_ref : np.ndarray\n        A 1D array of reference times.\n    bin_width : float, optional\n        The width of each bin in the window, in seconds. Default is 0.002 seconds.\n    n_bins : int, optional\n        The number of bins in the window. Default is 100.\n    window : Union[tuple, None], optional\n        A tuple containing the start and end times of the window to be plotted around each reference time.\n        If not provided, the window will be centered around each reference time and have a\n        width of `n_bins * bin_width` seconds.\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        Two DataFrames: the first containing the average values, the second the\n        standard deviation of data values within the window around each reference time.\n    \"\"\"\n\n    if window is not None:\n        times = np.arange(window[0], window[1] + bin_width, bin_width)\n    else:\n        times = np.linspace(\n            -(n_bins * bin_width) / 2, (n_bins * bin_width) / 2, n_bins + 1\n        )\n    x = []\n    y = []\n    for i, r in enumerate(time_ref):\n        idx = (timestamps &gt; r + times.min()) &amp; (timestamps &lt; r + times.max())\n        x.append((timestamps - r)[idx])\n        y.append(data[idx])\n\n    temp_df = pd.DataFrame()\n    if len(x) == 0:\n        return temp_df, temp_df\n    temp_df[\"time\"] = np.hstack(x)\n    temp_df[\"data\"] = np.hstack(y)\n    temp_df = temp_df.sort_values(by=\"time\", ascending=True)\n\n    average_val = np.zeros(len(times) - 1)\n    std_val = np.zeros(len(times) - 1)\n    for i in range(len(times) - 1):\n        average_val[i] = temp_df[\n            temp_df.time.between(times[i], times[i + 1])\n        ].data.mean()\n        std_val[i] = temp_df[temp_df.time.between(times[i], times[i + 1])].data.std()\n\n    avg = pd.DataFrame(index=times[:-1] + bin_width / 2)\n    avg[0] = average_val\n\n    std = pd.DataFrame(index=times[:-1] + bin_width / 2)\n    std[0] = std_val\n\n    return avg, std\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.get_rank_order","title":"<code>get_rank_order(st, epochs, method='peak_fr', ref='cells', padding=0.05, dt=0.001, sigma=0.01, min_units=5)</code>","text":"<p>Calculate the rank order of spike trains within specified epochs.</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>ndarray or array</code> <p>Spike train data. Can be a nelpy array containing spike times.</p> required <code>epochs</code> <code>EpochArray</code> <p>An object containing the epochs (windows) in which to calculate the rank order.</p> required <code>method</code> <code>str</code> <p>Method to calculate rank order. Choices are 'first_spike' or 'peak_fr'. Defaults to 'peak_fr'.</p> <code>'peak_fr'</code> <code>ref</code> <code>str</code> <p>Reference frame for rank order. Choices are 'cells' or 'epoch'. Defaults to 'cells'.</p> <code>'cells'</code> <code>padding</code> <code>float</code> <p>Padding (in seconds) to apply to the epochs. Defaults to 0.05 seconds.</p> <code>0.05</code> <code>dt</code> <code>float</code> <p>Bin width (in seconds) for finding relative time in the epoch reference. Defaults to 0.001 seconds.</p> <code>0.001</code> <code>sigma</code> <code>float</code> <p>Smoothing sigma (in seconds) for the 'peak_fr' method. Defaults to 0.01 seconds.</p> <code>0.01</code> <code>min_units</code> <code>int</code> <p>Minimum number of active units required to compute the rank order. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>median_rank</code> <code>ndarray</code> <p>The median rank order across all epochs, normalized between 0 and 1.</p> <code>rank_order</code> <code>ndarray</code> <p>A 2D array of rank orders, where each column corresponds to an epoch, and each row corresponds to a cell, normalized between 0 and 1.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; st, _ = loading.load_spikes(basepath, putativeCellType='Pyr')\n&gt;&gt;&gt; forward_replay = nel.EpochArray(np.array([starts, stops]).T)\n&gt;&gt;&gt; median_rank, rank_order = get_rank_order(st, forward_replay)\n</code></pre> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def get_rank_order(\n    st: SpikeTrainArray,  # Assuming 'nelpy.array' is a custom type\n    epochs: EpochArray,\n    method: str = \"peak_fr\",  # 'first_spike' or 'peak_fr'\n    ref: str = \"cells\",  # 'cells' or 'epoch'\n    padding: float = 0.05,\n    dt: float = 0.001,\n    sigma: float = 0.01,\n    min_units: int = 5,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate the rank order of spike trains within specified epochs.\n\n    Parameters\n    ----------\n    st : np.ndarray or nelpy.array\n        Spike train data. Can be a nelpy array containing spike times.\n\n    epochs : nelpy.EpochArray\n        An object containing the epochs (windows) in which to calculate the rank order.\n\n    method : str, optional\n        Method to calculate rank order. Choices are 'first_spike' or 'peak_fr'.\n        Defaults to 'peak_fr'.\n\n    ref : str, optional\n        Reference frame for rank order. Choices are 'cells' or 'epoch'.\n        Defaults to 'cells'.\n\n    padding : float, optional\n        Padding (in seconds) to apply to the epochs. Defaults to 0.05 seconds.\n\n    dt : float, optional\n        Bin width (in seconds) for finding relative time in the epoch reference.\n        Defaults to 0.001 seconds.\n\n    sigma : float, optional\n        Smoothing sigma (in seconds) for the 'peak_fr' method. Defaults to 0.01 seconds.\n\n    min_units : int, optional\n        Minimum number of active units required to compute the rank order. Defaults to 5.\n\n    Returns\n    -------\n    median_rank : np.ndarray\n        The median rank order across all epochs, normalized between 0 and 1.\n\n    rank_order : np.ndarray\n        A 2D array of rank orders, where each column corresponds to an epoch,\n        and each row corresponds to a cell, normalized between 0 and 1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; st, _ = loading.load_spikes(basepath, putativeCellType='Pyr')\n    &gt;&gt;&gt; forward_replay = nel.EpochArray(np.array([starts, stops]).T)\n    &gt;&gt;&gt; median_rank, rank_order = get_rank_order(st, forward_replay)\n    \"\"\"\n    # filter out specific warnings\n    warnings.filterwarnings(\n        \"ignore\", message=\"ignoring events outside of eventarray support\"\n    )\n    warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\")\n\n    if method not in [\"first_spike\", \"peak_fr\"]:\n        raise Exception(\"method \" + method + \" not implemented\")\n    if ref not in [\"cells\", \"epoch\"]:\n        raise Exception(\"ref \" + ref + \" not implemented\")\n\n    def get_min_ts(st_temp):\n        min_ts = []\n        for ts in st_temp.data:\n            # nan if no spikes\n            if len(ts) == 0:\n                min_ts.append(np.nan)\n            else:\n                min_ts.append(np.nanmin(ts))\n        return min_ts\n\n    def rank_order_first_spike(st_epoch, epochs, dt, min_units, ref):\n        # set up empty matrix for rank order\n        rank_order = np.ones([st_epoch.data.shape[0], epochs.n_intervals]) * np.nan\n\n        unit_id = np.arange(st_epoch.data.shape[0])\n        st_epoch._abscissa.support = epochs\n\n        # iter over every event\n        for event_i, st_temp in enumerate(st_epoch):\n            if ref == \"cells\":\n                # get firing order\n                idx = np.array(st_temp.get_event_firing_order()) - 1\n                # reorder unit ids by order and remove non-active\n                units = unit_id[idx][st_temp.n_events[idx] &gt; 0]\n                # how many are left?\n                nUnits = len(units)\n\n                if nUnits &lt; min_units:\n                    rank_order[:, event_i] = np.nan\n                else:\n                    # arange 1 to n units in order of units\n                    rank_order[units, event_i] = np.arange(nUnits)\n                    # normalize by n units\n                    rank_order[units, event_i] = rank_order[units, event_i] / nUnits\n            elif ref == \"epoch\":\n                # find first spike time for each cell\n                min_ts = get_min_ts(st_temp)\n                # make time stamps for interpolation\n                epoch_ts = np.arange(epochs[event_i].start, epochs[event_i].stop, dt)\n                # make normalized range 0-1\n                norm_range = np.linspace(0, 1, len(epoch_ts))\n                # get spike order relative to normalized range\n                if len(min_ts) &lt; min_units:\n                    rank_order[:, event_i] = np.nan\n                else:\n                    rank_order[:, event_i] = np.interp(min_ts, epoch_ts, norm_range)\n        return rank_order\n\n    def rank_order_fr(st, epochs, dt, sigma, min_units, ref):\n        # set up empty matrix for rank order\n        rank_order = np.zeros([st.data.shape[0], epochs.n_intervals]) * np.nan\n\n        unit_id = np.arange(st.data.shape[0])\n\n        edges = split_epoch_by_width(epochs.data, dt)\n\n        z_t = count_in_interval(st.data, edges[:, 0], edges[:, 1], par_type=\"counts\")\n        _, interval_id = in_intervals(edges[:, 0], epochs.data, return_interval=True)\n\n        # iter over epochs\n        for event_i, epochs_temp in enumerate(epochs):\n            # smooth spike train in order to estimate peak\n            # z_t_temp.smooth(sigma=sigma, inplace=True)\n            z_t_temp = z_t[:, interval_id == event_i]\n            # smooth spike train in order to estimate peak\n            z_t_temp = gaussian_filter1d(z_t_temp, sigma / dt, axis=1)\n            if ref == \"cells\":\n                # find loc of each peak and get sorted idx of active units\n                idx = np.argsort(np.argmax(z_t_temp, axis=1))\n                # reorder unit ids by order and remove non-active\n                units = unit_id[idx][np.sum(z_t_temp[idx, :] &gt; 0, axis=1) &gt; 0]\n\n                nUnits = len(units)\n\n                if nUnits &lt; min_units:\n                    rank_order[:, event_i] = np.nan\n                else:\n                    # arange 1 to n units in order of units\n                    rank_order[units, event_i] = np.arange(nUnits)\n                    # normalize by n units\n                    rank_order[units, event_i] = rank_order[units, event_i] / nUnits\n            elif ref == \"epoch\":\n                # iterate over each cell\n                for cell_i, unit in enumerate(z_t_temp):\n                    # if the cell is not active apply nan\n                    if not np.any(unit &gt; 0):\n                        rank_order[cell_i, event_i] = np.nan\n                    else:\n                        # calculate normalized rank order (0-1)\n                        rank_order[cell_i, event_i] = np.argmax(unit) / len(unit)\n        return rank_order\n\n    # expand epochs by padding amount\n    epochs = epochs.expand(padding)\n\n    # check if no active cells\n    if st.n_active == 0:\n        return np.tile(np.nan, st.data.shape), np.tile(\n            np.nan, (st.data.shape[0], epochs.n_intervals)\n        )\n\n    # check if there are any spikes in the epoch\n    st_epoch = count_in_interval(\n        st.data, epochs.starts, epochs.stops, par_type=\"counts\"\n    )\n\n    # if no spikes in epoch, break out\n    if (st_epoch == 0).all():\n        return np.tile(np.nan, st.data.shape), np.tile(\n            np.nan, (st.data.shape[0], epochs.n_intervals)\n        )\n\n    # set up empty matrix for rank order\n    if method == \"peak_fr\":\n        rank_order = rank_order_fr(st, epochs, dt, sigma, min_units, ref)\n    elif method == \"first_spike\":\n        rank_order = rank_order_first_spike(st[epochs], epochs, dt, min_units, ref)\n    else:\n        raise Exception(\"method \" + method + \" not implemented\")\n\n    return np.nanmedian(rank_order, axis=1), rank_order\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.get_raster_points","title":"<code>get_raster_points(data, time_ref, bin_width=0.002, n_bins=100, window=None)</code>","text":"<p>Generate points for a raster plot centered around each reference time in the <code>time_ref</code> array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A 1D array of time values.</p> required <code>time_ref</code> <code>ndarray</code> <p>A 1D array of reference times.</p> required <code>bin_width</code> <code>float</code> <p>The width of each bin in the raster plot, in seconds. Default is 0.002 seconds.</p> <code>0.002</code> <code>n_bins</code> <code>int</code> <p>The number of bins in the raster plot. Default is 100.</p> <code>100</code> <code>window</code> <code>tuple</code> <p>A tuple containing the start and end times of the window to be plotted around each reference time. If not provided, the window will be centered around each reference time and have a width of <code>n_bins * bin_width</code> seconds.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>A 1D array of x values representing the time offsets of each data point relative to the corresponding reference time.</p> <code>y</code> <code>ndarray</code> <p>A 1D array of y values representing the reference times.</p> <code>times</code> <code>ndarray</code> <p>A 1D array of time values corresponding to the bins in the raster plot.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>@jit(nopython=True)\ndef get_raster_points(\n    data: np.ndarray,\n    time_ref: np.ndarray,\n    bin_width: float = 0.002,\n    n_bins: int = 100,\n    window: Optional[Tuple[float, float]] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate points for a raster plot centered around each reference time in the `time_ref` array.\n\n    Parameters\n    ----------\n    data : ndarray\n        A 1D array of time values.\n    time_ref : ndarray\n        A 1D array of reference times.\n    bin_width : float, optional\n        The width of each bin in the raster plot, in seconds. Default is 0.002 seconds.\n    n_bins : int, optional\n        The number of bins in the raster plot. Default is 100.\n    window : tuple, optional\n        A tuple containing the start and end times of the window to be plotted around each reference time.\n        If not provided, the window will be centered around each reference time and have a width of `n_bins * bin_width` seconds.\n\n    Returns\n    -------\n    x : ndarray\n        A 1D array of x values representing the time offsets of each data point relative to the corresponding reference time.\n    y : ndarray\n        A 1D array of y values representing the reference times.\n    times : ndarray\n        A 1D array of time values corresponding to the bins in the raster plot.\n    \"\"\"\n    if window is not None:\n        times = np.arange(window[0], window[1] + bin_width / 2, bin_width)\n    else:\n        times = np.linspace(\n            -(n_bins * bin_width) / 2, (n_bins * bin_width) / 2, n_bins + 1\n        )\n\n    x = np.empty(0)\n    y = np.empty(0)\n    for i, r in enumerate(time_ref):\n        idx = (data &gt; r + times.min()) &amp; (data &lt; r + times.max())\n        cur_data = data[idx]\n        x = np.concatenate((x, cur_data - r))\n        y = np.concatenate((y, np.ones_like(cur_data) * i))\n\n    return x, y, times\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.joint_peth","title":"<code>joint_peth(peth_1, peth_2, smooth_std=2)</code>","text":"<p>joint_peth - produce a joint histogram for the co-occurrence of two sets of signals around events.</p> <p>This analysis tests for interactions. For example, the interaction of ripples and spindles around the occurrence of delta waves. It is a good way to control whether the relationships between two variables is entirely explained by a third variable (the events serving as basis for the PETHs).</p> <p>Parameters:</p> Name Type Description Default <code>peth_1</code> <code>ndarray</code> <p>The first peri-event time histogram (PETH) signal, shape (n_events, n_time).</p> required <code>peth_2</code> <code>ndarray</code> <p>The second peri-event time histogram (PETH) signal, shape (n_events, n_time).</p> required <code>smooth_std</code> <code>float</code> <p>The standard deviation of the Gaussian smoothing kernel (default is 2).</p> <code>2</code> <p>Returns:</p> Name Type Description <code>joint</code> <code>ndarray</code> <p>The joint histogram of the two PETH signals (n_time, n_time).</p> <code>expected</code> <code>ndarray</code> <p>The expected histogram of the two PETH signals (n_time, n_time).</p> <code>difference</code> <code>ndarray</code> <p>The difference between the joint and expected histograms of the two PETH signals (n_time, n_time).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.process.peri_event import joint_peth, peth_matrix, joint_peth\n&gt;&gt;&gt; from neuro_py.spikes.spike_tools import get_spindices\n&gt;&gt;&gt; from neuro_py.io import loading\n</code></pre> <pre><code>&gt;&gt;&gt; # load ripples, delta waves, and PFC pyramidal cell spikes from basepath\n</code></pre> <pre><code>&gt;&gt;&gt; basepath = r\"Z:\\Data\\HMC1\\day8\"\n</code></pre> <pre><code>&gt;&gt;&gt; ripples = loading.load_ripples_events(basepath, return_epoch_array=True)\n&gt;&gt;&gt; delta_waves = loading.load_events(basepath, epoch_name=\"deltaWaves\")\n&gt;&gt;&gt; st,cm = loading.load_spikes(basepath,brainRegion=\"PFC\",putativeCellType=\"Pyr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # flatten spikes (nelpy has .flatten(), but get_spindices is much faster)\n&gt;&gt;&gt; spikes = get_spindices(st.data)\n</code></pre> <pre><code>&gt;&gt;&gt; # create peri-event time histograms (PETHs) for the three signals\n&gt;&gt;&gt; window=[-1,1]\n&gt;&gt;&gt; labels = [\"spikes\", \"ripple\", \"delta\"]\n&gt;&gt;&gt; peth_1,ts = peth_matrix(spikes.spike_times.values, delta_waves.starts, bin_width=0.02, n_bins=101)\n&gt;&gt;&gt; peth_2,ts = peth_matrix(ripples.starts, delta_waves.starts, bin_width=0.02, n_bins=101)\n</code></pre> <pre><code>&gt;&gt;&gt; # calculate the joint, expected, and difference histograms\n&gt;&gt;&gt; joint, expected, difference = joint_peth(peth_1.T, peth_2.T, smooth_std=2)\n</code></pre> Notes <p>Note: sometimes the difference between \"joint\" and \"expected\" may be dominated due to brain state effects (e.g. if both ripples are spindles are more common around delta waves taking place in early SWS and have decreased rates around delta waves in late SWS, then all the values of \"joint\" would be larger than the value of \"expected\". In such a case, to investigate the timing effects in particular and ignore such global changes (correlations across the rows of \"PETH1\" and \"PETH2\"), consider normalizing the rows of the PETHs before calling joint_peth.</p> <p>See Sirota et al. (2003)</p> <p>Adapted from JointPETH.m, Copyright (C) 2018-2022 by Ralitsa Todorova</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def joint_peth(\n    peth_1: np.ndarray, peth_2: np.ndarray, smooth_std: float = 2\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    joint_peth - produce a joint histogram for the co-occurrence of two sets of signals around events.\n\n    This analysis tests for interactions. For example, the interaction of\n    ripples and spindles around the occurrence of delta waves. It is a good way\n    to control whether the relationships between two variables is entirely explained\n    by a third variable (the events serving as basis for the PETHs).\n\n    Parameters\n    ----------\n    peth_1 : np.ndarray\n        The first peri-event time histogram (PETH) signal, shape (n_events, n_time).\n    peth_2 : np.ndarray\n        The second peri-event time histogram (PETH) signal, shape (n_events, n_time).\n    smooth_std : float, optional\n        The standard deviation of the Gaussian smoothing kernel (default is 2).\n\n    Returns\n    -------\n    joint : np.ndarray\n        The joint histogram of the two PETH signals (n_time, n_time).\n    expected : np.ndarray\n        The expected histogram of the two PETH signals (n_time, n_time).\n    difference : np.ndarray\n        The difference between the joint and expected histograms of the two PETH signals (n_time, n_time).\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.process.peri_event import joint_peth, peth_matrix, joint_peth\n    &gt;&gt;&gt; from neuro_py.spikes.spike_tools import get_spindices\n    &gt;&gt;&gt; from neuro_py.io import loading\n\n    &gt;&gt;&gt; # load ripples, delta waves, and PFC pyramidal cell spikes from basepath\n\n    &gt;&gt;&gt; basepath = r\"Z:\\\\Data\\\\HMC1\\\\day8\"\n\n    &gt;&gt;&gt; ripples = loading.load_ripples_events(basepath, return_epoch_array=True)\n    &gt;&gt;&gt; delta_waves = loading.load_events(basepath, epoch_name=\"deltaWaves\")\n    &gt;&gt;&gt; st,cm = loading.load_spikes(basepath,brainRegion=\"PFC\",putativeCellType=\"Pyr\")\n\n    &gt;&gt;&gt; # flatten spikes (nelpy has .flatten(), but get_spindices is much faster)\n    &gt;&gt;&gt; spikes = get_spindices(st.data)\n\n    &gt;&gt;&gt; # create peri-event time histograms (PETHs) for the three signals\n    &gt;&gt;&gt; window=[-1,1]\n    &gt;&gt;&gt; labels = [\"spikes\", \"ripple\", \"delta\"]\n    &gt;&gt;&gt; peth_1,ts = peth_matrix(spikes.spike_times.values, delta_waves.starts, bin_width=0.02, n_bins=101)\n    &gt;&gt;&gt; peth_2,ts = peth_matrix(ripples.starts, delta_waves.starts, bin_width=0.02, n_bins=101)\n\n    &gt;&gt;&gt; # calculate the joint, expected, and difference histograms\n    &gt;&gt;&gt; joint, expected, difference = joint_peth(peth_1.T, peth_2.T, smooth_std=2)\n\n    Notes\n    -----\n    Note: sometimes the difference between \"joint\" and \"expected\" may be dominated due to\n    brain state effects (e.g. if both ripples are spindles are more common around delta\n    waves taking place in early SWS and have decreased rates around delta waves in late\n    SWS, then all the values of \"joint\" would be larger than the value of \"expected\".\n    In such a case, to investigate the timing effects in particular and ignore such\n    global changes (correlations across the rows of \"PETH1\" and \"PETH2\"), consider\n    normalizing the rows of the PETHs before calling joint_peth.\n\n    See Sirota et al. (2003)\n\n    Adapted from JointPETH.m, Copyright (C) 2018-2022 by Ralitsa Todorova\n    \"\"\"\n    from scipy.ndimage import gaussian_filter\n\n    # make inputs np.ndarrays\n    peth_1 = np.array(peth_1)\n    peth_2 = np.array(peth_2)\n\n    # calculate the joint histogram\n    joint = peth_1.T @ peth_2\n\n    # smooth the 2d joint histogram\n    joint = gaussian_filter(joint, smooth_std)\n\n    # calculate the expected histogram\n    expected = np.tile(np.nanmean(peth_1, axis=0), [peth_1.shape[0], 1]).T @ np.tile(\n        np.nanmean(peth_2, axis=0), [peth_2.shape[0], 1]\n    )\n\n    # smooth the 2d expected histogram\n    expected = gaussian_filter(expected, smooth_std)\n\n    # normalize the joint and expected histograms\n    joint = joint / peth_1.shape[0]\n    expected = expected / peth_1.shape[0]\n\n    # square root the joint and expected histograms so result is Hz\n    joint = np.sqrt(joint)\n    expected = np.sqrt(expected)\n\n    # calculate the difference between the joint and expected histograms\n    difference = joint - expected\n\n    return joint, expected, difference\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.nearest_event_delay","title":"<code>nearest_event_delay(ts_1, ts_2)</code>","text":"<p>Return for each timestamp in ts_1 the nearest timestamp in ts_2 and the delay between the two.</p> <p>Parameters:</p> Name Type Description Default <code>ts_1</code> <code>ndarray</code> <p>1D array of timestamps.</p> required <code>ts_2</code> <code>ndarray</code> <p>1D array of timestamps (must be monotonically increasing).</p> required <p>Returns:</p> Name Type Description <code>nearest_ts</code> <code>ndarray</code> <p>Nearest timestamps in ts_2 for each timestamp in ts_1.</p> <code>delays</code> <code>ndarray</code> <p>Delays between ts_1 and nearest_ts.</p> <code>nearest_index</code> <code>ndarray</code> <p>Index of nearest_ts in ts_2.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If ts_1 or ts_2 are empty or not monotonically increasing.</p> Notes <p>Both ts_1 and ts_2 must be monotonically increasing arrays of timestamps.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>def nearest_event_delay(\n    ts_1: np.ndarray, ts_2: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Return for each timestamp in ts_1 the nearest timestamp in ts_2 and the delay between the two.\n\n    Parameters\n    ----------\n    ts_1 : np.ndarray\n        1D array of timestamps.\n    ts_2 : np.ndarray\n        1D array of timestamps (must be monotonically increasing).\n\n    Returns\n    -------\n    nearest_ts : np.ndarray\n        Nearest timestamps in ts_2 for each timestamp in ts_1.\n    delays : np.ndarray\n        Delays between ts_1 and nearest_ts.\n    nearest_index : np.ndarray\n        Index of nearest_ts in ts_2.\n\n    Raises\n    ------\n    ValueError\n        If ts_1 or ts_2 are empty or not monotonically increasing.\n\n    Notes\n    -----\n    Both ts_1 and ts_2 must be monotonically increasing arrays of timestamps.\n    \"\"\"\n    ts_1, ts_2 = np.array(ts_1), np.array(ts_2)\n\n    if not np.all(np.diff(ts_2) &gt; 0):\n        raise ValueError(\"ts_2 must be monotonically increasing\")\n\n    if not np.all(np.diff(ts_1) &gt; 0):\n        raise ValueError(\"ts_1 must be monotonically increasing\")\n    # check if empty\n    if len(ts_1) == 0:\n        raise ValueError(\"ts_1 is empty\")\n    if len(ts_2) == 0:\n        raise ValueError(\"ts_2 is empty\")\n\n    # Use searchsorted to find the indices where elements of ts_1 should be inserted\n    nearest_indices = np.searchsorted(ts_2, ts_1, side=\"left\")\n\n    # Calculate indices for the elements before and after the insertion points\n    before = np.maximum(nearest_indices - 1, 0)\n    after = np.minimum(nearest_indices, len(ts_2) - 1)\n\n    # Determine the nearest timestamp for each element in ts_1\n    nearest_ts = np.where(\n        np.abs(ts_1 - ts_2[before]) &lt; np.abs(ts_1 - ts_2[after]),\n        ts_2[before],\n        ts_2[after],\n    )\n\n    # Calculate delays between ts_1 and nearest_ts\n    delays = ts_1 - nearest_ts\n\n    # Find the nearest_index using the absolute difference\n    absolute_diff_before = np.abs(ts_1 - ts_2[before])\n    absolute_diff_after = np.abs(ts_1 - ts_2[after])\n    nearest_index = np.where(absolute_diff_before &lt; absolute_diff_after, before, after)\n\n    return nearest_ts, delays, nearest_index\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.peth_matrix","title":"<code>peth_matrix(data, time_ref, bin_width=0.002, n_bins=100, window=None)</code>","text":"<p>Generate a peri-event time histogram (PETH) matrix.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A 1D array of time values.</p> required <code>time_ref</code> <code>ndarray</code> <p>A 1D array of reference times.</p> required <code>bin_width</code> <code>float</code> <p>The width of each bin in the PETH matrix, in seconds. Default is 0.002 seconds.</p> <code>0.002</code> <code>n_bins</code> <code>int</code> <p>The number of bins in the PETH matrix. Default is 100.</p> <code>100</code> <code>window</code> <code>tuple</code> <p>A tuple containing the start and end times of the window to be plotted around each reference time. If not provided, the window will be centered around each reference time and have a width of <code>n_bins * bin_width</code> seconds.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>H</code> <code>ndarray</code> <p>A 2D array representing the PETH matrix.</p> <code>t</code> <code>ndarray</code> <p>A 1D array of time values corresponding to the bins in the PETH matrix.</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>@jit(nopython=True, parallel=True)\ndef peth_matrix(\n    data: np.ndarray,\n    time_ref: np.ndarray,\n    bin_width: float = 0.002,\n    n_bins: int = 100,\n    window: Union[list, None] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate a peri-event time histogram (PETH) matrix.\n\n    Parameters\n    ----------\n    data : ndarray\n        A 1D array of time values.\n    time_ref : ndarray\n        A 1D array of reference times.\n    bin_width : float, optional\n        The width of each bin in the PETH matrix, in seconds. Default is 0.002 seconds.\n    n_bins : int, optional\n        The number of bins in the PETH matrix. Default is 100.\n    window : tuple, optional\n        A tuple containing the start and end times of the window to be plotted around each reference time.\n        If not provided, the window will be centered around each reference time and have a width of `n_bins * bin_width` seconds.\n\n    Returns\n    -------\n    H : ndarray\n        A 2D array representing the PETH matrix.\n    t : ndarray\n        A 1D array of time values corresponding to the bins in the PETH matrix.\n\n    \"\"\"\n    if window is not None:\n        times = np.arange(window[0], window[1] + bin_width / 2, bin_width)\n        n_bins = len(times) - 1\n    else:\n        times = (\n            np.arange(0, bin_width * n_bins, bin_width)\n            - (bin_width * n_bins) / 2\n            + bin_width / 2\n        )\n\n    H = np.zeros((len(times), len(time_ref)))\n\n    for event_i in prange(len(time_ref)):\n        H[:, event_i] = crossCorr([time_ref[event_i]], data, bin_width, n_bins)\n\n    return H * bin_width, times\n</code></pre>"},{"location":"reference/neuro_py/process/peri_event/#neuro_py.process.peri_event.relative_times","title":"<code>relative_times(t, intervals, values=np.array([0, 1]))</code>","text":"<p>Calculate relative times and interval IDs for a set of time points.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray</code> <p>An array of time points.</p> required <code>intervals</code> <code>ndarray</code> <p>An array of time intervals, represented as pairs of start and end times.</p> required <code>values</code> <code>ndarray</code> <p>An array of values to assign to interval bounds. The default is [0,1].</p> <code>array([0, 1])</code> <p>Returns:</p> Name Type Description <code>rt</code> <code>ndarray</code> <p>An array of relative times, one for each time point (same len as t).</p> <code>intervalID</code> <code>ndarray</code> <p>An array of interval IDs, one for each time point (same len as t).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; t = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n&gt;&gt;&gt; intervals = np.array([[1, 3], [4, 6], [7, 9]])\n&gt;&gt;&gt; relative_times(t, intervals)\n    (array([nan, 0. , 0.5, 1. , 0. , 0.5, 1. , 0. , 0.5, 1. ]),\n    array([nan,  0.,  0.,  0.,  1.,  1.,  1.,  2.,  2.,  2.]))\n</code></pre> <pre><code>&gt;&gt;&gt; t = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n&gt;&gt;&gt; intervals = np.array([[1, 3], [4, 6], [7, 9]])\n&gt;&gt;&gt; values = np.array([0, 2*np.pi])\n&gt;&gt;&gt; relative_times(t, intervals, values)\n    (array([       nan, 0.        , 3.14159265, 6.28318531, 0.        ,\n            3.14159265, 6.28318531, 0.        , 3.14159265, 6.28318531]),\n    array([nan,  0.,  0.,  0.,  1.,  1.,  1.,  2.,  2.,  2.]))\n</code></pre> Notes <p>Intervals are defined as pairs of start and end times. The relative time is the time within the interval, normalized to the interval duration. The interval ID is the index of the interval in the intervals array. The values array can be used to assign a value to each interval.</p> <p>By Ryan H, based on RelativeTimes.m by Ralitsa Todorova</p> Source code in <code>neuro_py/process/peri_event.py</code> <pre><code>@jit(nopython=True)\ndef relative_times(\n    t: np.ndarray, intervals: np.ndarray, values: np.ndarray = np.array([0, 1])\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate relative times and interval IDs for a set of time points.\n\n    Parameters\n    ----------\n    t : np.ndarray\n        An array of time points.\n    intervals : np.ndarray\n        An array of time intervals, represented as pairs of start and end times.\n    values : np.ndarray, optional\n        An array of values to assign to interval bounds. The default is [0,1].\n\n    Returns\n    -------\n    rt : np.ndarray\n        An array of relative times, one for each time point (same len as t).\n    intervalID : np.ndarray\n        An array of interval IDs, one for each time point (same len as t).\n\n    Examples\n    --------\n    &gt;&gt;&gt; t = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    &gt;&gt;&gt; intervals = np.array([[1, 3], [4, 6], [7, 9]])\n    &gt;&gt;&gt; relative_times(t, intervals)\n        (array([nan, 0. , 0.5, 1. , 0. , 0.5, 1. , 0. , 0.5, 1. ]),\n        array([nan,  0.,  0.,  0.,  1.,  1.,  1.,  2.,  2.,  2.]))\n\n    &gt;&gt;&gt; t = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    &gt;&gt;&gt; intervals = np.array([[1, 3], [4, 6], [7, 9]])\n    &gt;&gt;&gt; values = np.array([0, 2*np.pi])\n    &gt;&gt;&gt; relative_times(t, intervals, values)\n        (array([       nan, 0.        , 3.14159265, 6.28318531, 0.        ,\n                3.14159265, 6.28318531, 0.        , 3.14159265, 6.28318531]),\n        array([nan,  0.,  0.,  0.,  1.,  1.,  1.,  2.,  2.,  2.]))\n\n    Notes\n    -----\n    Intervals are defined as pairs of start and end times. The relative time is the time\n    within the interval, normalized to the interval duration. The interval ID is the index\n    of the interval in the intervals array. The values array can be used to assign a value\n    to each interval.\n\n    By Ryan H, based on RelativeTimes.m by Ralitsa Todorova\n\n    \"\"\"\n\n    rt = np.zeros(len(t), dtype=np.float64) * np.nan\n    intervalID = np.zeros(len(t), dtype=np.float64) * np.nan\n\n    start_times = intervals[:, 0]\n    end_times = intervals[:, 1]\n    values_diff = values[1] - values[0]\n    intervals_diff = end_times - start_times\n    intervals_scale = values_diff / intervals_diff\n\n    for i in range(len(t)):\n        idx = np.searchsorted(start_times, t[i])\n        if idx &gt; 0 and t[i] &lt;= end_times[idx - 1]:\n            interval_i = idx - 1\n        elif idx &lt; len(start_times) and t[i] == start_times[idx]:\n            interval_i = idx\n        else:\n            continue\n\n        scale = intervals_scale[interval_i]\n        rt[i] = ((t[i] - start_times[interval_i]) * scale) + values[0]\n        intervalID[i] = interval_i\n\n    return rt, intervalID\n</code></pre>"},{"location":"reference/neuro_py/process/precession_utils/","title":"neuro_py.process.precession_utils","text":""},{"location":"reference/neuro_py/process/precession_utils/#neuro_py.process.precession_utils.acf_power","title":"<code>acf_power(acf, norm=True)</code>","text":"<p>Compute the power spectrum of the signal by calculating the FFT of the autocorrelation function (ACF).</p> <p>Parameters:</p> Name Type Description Default <code>acf</code> <code>ndarray</code> <p>1D array of counts for the ACF.</p> required <code>norm</code> <code>bool</code> <p>If True, normalize the power spectrum. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>psd</code> <code>ndarray</code> <p>1D array representing the power spectrum of the signal.</p> Notes <p>The power spectrum is computed by taking the Fourier Transform of the ACF, then squaring the absolute values of the FFT result. The Nyquist frequency is accounted for by returning only the first half of the spectrum.</p> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def acf_power(acf: np.ndarray, norm: Optional[bool] = True) -&gt; np.ndarray:\n    \"\"\"\n    Compute the power spectrum of the signal by calculating the FFT of the autocorrelation function (ACF).\n\n    Parameters\n    ----------\n    acf : np.ndarray\n        1D array of counts for the ACF.\n    norm : bool, optional\n        If True, normalize the power spectrum. Default is True.\n\n    Returns\n    -------\n    psd : np.ndarray\n        1D array representing the power spectrum of the signal.\n\n    Notes\n    -----\n    The power spectrum is computed by taking the Fourier Transform of the ACF,\n    then squaring the absolute values of the FFT result.\n    The Nyquist frequency is accounted for by returning only the first half of the spectrum.\n    \"\"\"\n\n    # Take the FFT\n    fft = np.fft.fft(acf)\n\n    # Compute the power spectrum\n    pow = np.abs(fft) ** 2\n\n    # Account for Nyquist frequency\n    psd = pow[: pow.shape[0] // 2]\n\n    # Normalize if required\n    if norm:\n        psd = psd / np.trapz(psd)\n\n    return psd\n</code></pre>"},{"location":"reference/neuro_py/process/precession_utils/#neuro_py.process.precession_utils.corrcc","title":"<code>corrcc(alpha1, alpha2, axis=None)</code>","text":"<p>Circular correlation coefficient for two circular random variables.</p> <p>Parameters:</p> Name Type Description Default <code>alpha1</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>alpha2</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>axis</code> <code>Optional[int]</code> <p>The axis along which to compute the correlation coefficient. If None, compute over the entire array (default is None).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rho</code> <code>float</code> <p>Circular-circular correlation coefficient.</p> <code>pval</code> <code>float</code> <p>p-value for testing the significance of the correlation coefficient.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; alpha1 = np.array([0.1, 0.2, 0.4, 0.5])\n&gt;&gt;&gt; alpha2 = np.array([0.3, 0.6, 0.2, 0.8])\n&gt;&gt;&gt; rho, pval = corrcc(alpha1, alpha2)\n&gt;&gt;&gt; print(f\"Circular correlation: {rho}, p-value: {pval}\")\n</code></pre> Notes <p>The function computes the correlation between two sets of angles using a method that adjusts for circular data. The significance of the correlation coefficient is tested using the fact that the test statistic is approximately normally distributed.</p> References <p>Jammalamadaka et al (2001)</p> <p>Original code: https://github.com/circstat/pycircstat Modified by: Salman Qasim, 11/12/2018</p> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def corrcc(\n    alpha1: np.ndarray, alpha2: np.ndarray, axis: Optional[int] = None\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Circular correlation coefficient for two circular random variables.\n\n    Parameters\n    ----------\n    alpha1 : np.ndarray\n        Sample of angles in radians.\n    alpha2 : np.ndarray\n        Sample of angles in radians.\n    axis : Optional[int], optional\n        The axis along which to compute the correlation coefficient.\n        If None, compute over the entire array (default is None).\n\n    Returns\n    -------\n    rho : float\n        Circular-circular correlation coefficient.\n    pval : float\n        p-value for testing the significance of the correlation coefficient.\n\n    Examples\n    --------\n    &gt;&gt;&gt; alpha1 = np.array([0.1, 0.2, 0.4, 0.5])\n    &gt;&gt;&gt; alpha2 = np.array([0.3, 0.6, 0.2, 0.8])\n    &gt;&gt;&gt; rho, pval = corrcc(alpha1, alpha2)\n    &gt;&gt;&gt; print(f\"Circular correlation: {rho}, p-value: {pval}\")\n\n    Notes\n    -----\n    The function computes the correlation between two sets of angles using a\n    method that adjusts for circular data. The significance of the correlation\n    coefficient is tested using the fact that the test statistic is approximately\n    normally distributed.\n\n    References\n    ----------\n    Jammalamadaka et al (2001)\n\n    Original code: https://github.com/circstat/pycircstat\n    Modified by: Salman Qasim, 11/12/2018\n    \"\"\"\n    assert alpha1.shape == alpha2.shape, \"Input dimensions do not match.\"\n\n    n = len(alpha1)\n\n    # center data on circular mean\n    alpha1_centered, alpha2_centered = pcs.center(alpha1, alpha2, axis=axis)\n\n    num = np.sum(np.sin(alpha1_centered) * np.sin(alpha2_centered), axis=axis)\n    den = np.sqrt(\n        np.sum(np.sin(alpha1_centered) ** 2, axis=axis)\n        * np.sum(np.sin(alpha2_centered) ** 2, axis=axis)\n    )\n    # compute correlation coefficient from p. 176\n    rho = num / den\n\n    # Modification:\n    # significance of this correlation coefficient can be tested using the fact that Z is approx. normal\n\n    l20 = np.mean(np.sin(alpha1_centered) ** 2)\n    l02 = np.mean(np.sin(alpha2_centered) ** 2)\n    l22 = np.mean((np.sin(alpha1_centered) ** 2) * (np.sin(alpha2_centered) ** 2))\n    z = np.sqrt((n * l20 * l02) / l22) * rho\n    pval = 2 * (1 - sp.stats.norm.cdf(np.abs(z)))  # two-sided test\n\n    return rho, pval\n</code></pre>"},{"location":"reference/neuro_py/process/precession_utils/#neuro_py.process.precession_utils.corrcc_uniform","title":"<code>corrcc_uniform(alpha1, alpha2, axis=None)</code>","text":"<p>Circular correlation coefficient for two circular random variables. Use this function if at least one of the variables may follow a uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>alpha1</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>alpha2</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>axis</code> <code>Optional[int]</code> <p>The axis along which to compute the correlation coefficient. If None, compute over the entire array (default is None).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rho</code> <code>float</code> <p>Circular-circular correlation coefficient.</p> <code>pval</code> <code>float</code> <p>p-value for testing the significance of the correlation coefficient.</p> Notes <p>This method accounts for cases where one or both of the circular variables may follow a uniform distribution. The significance of the correlation coefficient is tested using a normal approximation of the Z statistic.</p> References <p>Jammalamadaka, et al (2001).</p> <p>Original code: https://github.com/circstat/pycircstat Modified by: Salman Qasim, 11/12/2018 https://github.com/HoniSanders/measure_phaseprec/blob/master/cl_corr.m</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; alpha1 = np.array([0.1, 0.2, 0.4, 0.5])\n&gt;&gt;&gt; alpha2 = np.array([0.3, 0.6, 0.2, 0.8])\n&gt;&gt;&gt; rho, pval = corrcc_uniform(alpha1, alpha2)\n&gt;&gt;&gt; print(f\"Circular correlation: {rho}, p-value: {pval}\")\n</code></pre> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def corrcc_uniform(\n    alpha1: np.ndarray, alpha2: np.ndarray, axis: Optional[int] = None\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Circular correlation coefficient for two circular random variables.\n    Use this function if at least one of the variables may follow a uniform distribution.\n\n    Parameters\n    ----------\n    alpha1 : np.ndarray\n        Sample of angles in radians.\n    alpha2 : np.ndarray\n        Sample of angles in radians.\n    axis : Optional[int], optional\n        The axis along which to compute the correlation coefficient.\n        If None, compute over the entire array (default is None).\n\n    Returns\n    -------\n    rho : float\n        Circular-circular correlation coefficient.\n    pval : float\n        p-value for testing the significance of the correlation coefficient.\n\n    Notes\n    -----\n    This method accounts for cases where one or both of the circular variables\n    may follow a uniform distribution. The significance of the correlation coefficient\n    is tested using a normal approximation of the Z statistic.\n\n    References\n    ----------\n    Jammalamadaka, et al (2001).\n\n    Original code: https://github.com/circstat/pycircstat\n    Modified by: Salman Qasim, 11/12/2018\n    https://github.com/HoniSanders/measure_phaseprec/blob/master/cl_corr.m\n\n    Examples\n    --------\n    &gt;&gt;&gt; alpha1 = np.array([0.1, 0.2, 0.4, 0.5])\n    &gt;&gt;&gt; alpha2 = np.array([0.3, 0.6, 0.2, 0.8])\n    &gt;&gt;&gt; rho, pval = corrcc_uniform(alpha1, alpha2)\n    &gt;&gt;&gt; print(f\"Circular correlation: {rho}, p-value: {pval}\")\n    \"\"\"\n\n    assert alpha1.shape == alpha2.shape, \"Input dimensions do not match.\"\n\n    n = len(alpha1)\n\n    # center data on circular mean\n    alpha1_centered, alpha2_centered = pcs.center(alpha1, alpha2, axis=axis)\n\n    # One of the sample means is not well defined due to uniform distribution of data\n    # so take the difference of the resultant vector length for the sum and difference\n    # of the alphas\n    num = pcs.resultant_vector_length(alpha1 - alpha2) - pcs.resultant_vector_length(\n        alpha1 + alpha2\n    )\n    den = 2 * np.sqrt(\n        np.sum(np.sin(alpha1_centered) ** 2, axis=axis)\n        * np.sum(np.sin(alpha2_centered) ** 2, axis=axis)\n    )\n    rho = n * num / den\n    # significance of this correlation coefficient can be tested using the fact that Z\n    # is approx. normal\n\n    l20 = np.mean(np.sin(alpha1_centered) ** 2)\n    l02 = np.mean(np.sin(alpha2_centered) ** 2)\n    l22 = np.mean((np.sin(alpha1_centered) ** 2) * (np.sin(alpha2_centered) ** 2))\n    z = np.sqrt((n * l20 * l02) / l22) * rho\n    pval = 2 * (1 - sp.stats.norm.cdf(np.abs(z)))  # two-sided test\n\n    return rho, pval\n</code></pre>"},{"location":"reference/neuro_py/process/precession_utils/#neuro_py.process.precession_utils.fast_acf","title":"<code>fast_acf(counts, width, bin_width, cut_peak=True)</code>","text":"<p>Compute the Auto-Correlation Function (ACF) in a fast manner using Numba.</p> <p>This function calculates the ACF of a given variable of interest, such as spike times or spike phases, leveraging the <code>pcorrelate</code> function for efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>ndarray</code> <p>1D array of the variable of interest (e.g., spike times or spike phases).</p> required <code>width</code> <code>float</code> <p>Time window for the ACF computation.</p> required <code>bin_width</code> <code>float</code> <p>Width of the bins for the ACF.</p> required <code>cut_peak</code> <code>bool</code> <p>If True, the largest central peak will be replaced for subsequent fitting. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>acf</code> <code>ndarray</code> <p>1D array of counts for the ACF.</p> <code>bins</code> <code>ndarray</code> <p>1D array of lag bins for the ACF.</p> Notes <ul> <li>The ACF is calculated over a specified time window and returns the   counts of the ACF along with the corresponding bins.</li> <li>The <code>cut_peak</code> parameter allows for the adjustment of the ACF peak, which   can be useful for fitting processes.</li> </ul> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def fast_acf(\n    counts: np.ndarray, width: float, bin_width: float, cut_peak: bool = True\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the Auto-Correlation Function (ACF) in a fast manner using Numba.\n\n    This function calculates the ACF of a given variable of interest, such as\n    spike times or spike phases, leveraging the `pcorrelate` function for efficiency.\n\n    Parameters\n    ----------\n    counts : np.ndarray\n        1D array of the variable of interest (e.g., spike times or spike phases).\n    width : float\n        Time window for the ACF computation.\n    bin_width : float\n        Width of the bins for the ACF.\n    cut_peak : bool, optional\n        If True, the largest central peak will be replaced for subsequent fitting. Default is True.\n\n    Returns\n    -------\n    acf : np.ndarray\n        1D array of counts for the ACF.\n    bins : np.ndarray\n        1D array of lag bins for the ACF.\n\n    Notes\n    -----\n    - The ACF is calculated over a specified time window and returns the\n      counts of the ACF along with the corresponding bins.\n    - The `cut_peak` parameter allows for the adjustment of the ACF peak, which\n      can be useful for fitting processes.\n    \"\"\"\n\n    n_b = int(np.ceil(width / bin_width))  # Num. edges per side\n    # Define the edges of the bins (including rightmost bin)\n    bins = np.linspace(-width, width, 2 * n_b, endpoint=True)\n    temp = pcorrelate(counts, counts, np.split(bins, 2)[1])\n    acf = np.ones(bins.shape[0] - 1)\n    acf[0 : temp.shape[0]] = np.flip(temp)\n    acf[temp.shape[0]] = temp[0]\n    acf[temp.shape[0] + 1 :] = temp\n\n    if cut_peak:\n        acf[np.nanargmax(acf)] = np.sort(acf)[-2]\n\n    return acf, bins\n</code></pre>"},{"location":"reference/neuro_py/process/precession_utils/#neuro_py.process.precession_utils.nonspatial_phase_precession","title":"<code>nonspatial_phase_precession(unwrapped_spike_phases, width=4 * 2 * np.pi, bin_width=np.pi / 3, cut_peak=True, norm=True, psd_lims=[0.65, 1.55], upsample=4, smooth_sigma=1)</code>","text":"<p>Compute the nonspatial spike-LFP relationship modulation index.</p> <p>Parameters:</p> Name Type Description Default <code>unwrapped_spike_phases</code> <code>ndarray</code> <p>1D array of spike phases that have been linearly unwrapped.</p> required <code>width</code> <code>float</code> <p>Time window for ACF in cycles (default = 4 cycles).</p> <code>4 * 2 * pi</code> <code>bin_width</code> <code>float</code> <p>Width of bins in radians (default = pi/3 radians).</p> <code>pi / 3</code> <code>cut_peak</code> <code>bool</code> <p>Whether or not the largest central peak should be replaced for subsequent fitting.</p> <code>True</code> <code>norm</code> <code>bool</code> <p>To normalize the ACF or not.</p> <code>True</code> <code>psd_lims</code> <code>List[float]</code> <p>Limits of the PSD to consider for peak finding (default = [0.65, 1.55]).</p> <code>[0.65, 1.55]</code> <code>upsample</code> <code>int</code> <p>Upsampling factor (default = 4).</p> <code>4</code> <code>smooth_sigma</code> <code>float</code> <p>Standard deviation for Gaussian smoothing of the PSD (default = 1).</p> <code>1</code> <p>Returns:</p> Name Type Description <code>max_freq</code> <code>float</code> <p>Relative spike-LFP frequency of the PSD peak.</p> <code>MI</code> <code>float</code> <p>Modulation index of non-spatial phase relationship.</p> <code>psd</code> <code>ndarray</code> <p>Power spectral density of interest.</p> <code>frequencies</code> <code>ndarray</code> <p>Frequencies corresponding to the PSD.</p> <code>acf</code> <code>ndarray</code> <p>Autocorrelation function.</p> Notes <p>The modulation index (MI) is computed based on the maximum peak of the power spectral density (PSD) within specified frequency limits.</p> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def nonspatial_phase_precession(\n    unwrapped_spike_phases: np.ndarray,\n    width: float = 4 * 2 * np.pi,\n    bin_width: float = np.pi / 3,\n    cut_peak: bool = True,\n    norm: bool = True,\n    psd_lims: List[float] = [0.65, 1.55],\n    upsample: int = 4,\n    smooth_sigma: float = 1,\n) -&gt; Tuple[float, float, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the nonspatial spike-LFP relationship modulation index.\n\n    Parameters\n    ----------\n    unwrapped_spike_phases : np.ndarray\n        1D array of spike phases that have been linearly unwrapped.\n    width : float\n        Time window for ACF in cycles (default = 4 cycles).\n    bin_width : float\n        Width of bins in radians (default = pi/3 radians).\n    cut_peak : bool\n        Whether or not the largest central peak should be replaced for subsequent fitting.\n    norm : bool\n        To normalize the ACF or not.\n    psd_lims : List[float]\n        Limits of the PSD to consider for peak finding (default = [0.65, 1.55]).\n    upsample : int\n        Upsampling factor (default = 4).\n    smooth_sigma : float\n        Standard deviation for Gaussian smoothing of the PSD (default = 1).\n\n    Returns\n    -------\n    max_freq : float\n        Relative spike-LFP frequency of the PSD peak.\n    MI : float\n        Modulation index of non-spatial phase relationship.\n    psd : np.ndarray\n        Power spectral density of interest.\n    frequencies : np.ndarray\n        Frequencies corresponding to the PSD.\n    acf : np.ndarray\n        Autocorrelation function.\n\n    Notes\n    -----\n    The modulation index (MI) is computed based on the maximum peak of the power\n    spectral density (PSD) within specified frequency limits.\n    \"\"\"\n\n    frequencies = (\n        (np.arange(2 * (width // bin_width) - 1))\n        * (2 * np.pi)\n        / (2 * width - bin_width)\n    )\n\n    frequencies = np.interp(\n        np.arange(0, len(frequencies), 1 / upsample),\n        np.arange(0, len(frequencies)),\n        frequencies,\n    )\n\n    freqs_of_interest = np.intersect1d(\n        np.where(frequencies &gt; psd_lims[0]), np.where(frequencies &lt; psd_lims[1])\n    )\n\n    acf, _ = fast_acf(unwrapped_spike_phases, width, bin_width, cut_peak=cut_peak)\n    psd = acf_power(acf, norm=norm)\n\n    # upsample 2x psd\n    psd = np.interp(np.arange(0, len(psd), 1 / upsample), np.arange(0, len(psd)), psd)\n    # smooth psd with gaussian filter\n    psd = gaussian_filter1d(psd, smooth_sigma)\n\n    # FIND ALL LOCAL MAXIMA IN WINDOW OF INTEREST\n    all_peaks = find_peaks(psd[freqs_of_interest], None)[0]\n\n    # make sure there is a peak\n    if ~np.any(all_peaks):\n        return (\n            np.nan,\n            np.nan,\n            psd[freqs_of_interest],\n            frequencies[freqs_of_interest],\n            acf,\n        )\n\n    max_peak = np.max(psd[freqs_of_interest][all_peaks])\n    max_idx = [all_peaks[np.argmax(psd[freqs_of_interest][all_peaks])]]\n    max_freq = frequencies[freqs_of_interest][max_idx]\n    MI = max_peak / np.trapz(psd[freqs_of_interest])\n\n    return max_freq, MI, psd[freqs_of_interest], frequencies[freqs_of_interest], acf\n</code></pre>"},{"location":"reference/neuro_py/process/precession_utils/#neuro_py.process.precession_utils.pcorrelate","title":"<code>pcorrelate(t, u, bins)</code>","text":"<p>Compute the correlation of two arrays of discrete events (point-process).</p> <p>This function computes the correlation of two time series of events using an arbitrary array of lag-bins. It implements the algorithm described in Laurence (2006) (https://doi.org/10.1364/OL.31.000829).</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray</code> <p>First array of \"points\" to correlate. The array needs to be monotonically increasing.</p> required <code>u</code> <code>ndarray</code> <p>Second array of \"points\" to correlate. The array needs to be monotonically increasing.</p> required <code>bins</code> <code>ndarray</code> <p>Array of bin edges where correlation is computed.</p> required <p>Returns:</p> Name Type Description <code>G</code> <code>ndarray</code> <p>Array containing the correlation of <code>t</code> and <code>u</code>. The size is <code>len(bins) - 1</code>.</p> Notes <ul> <li>This method is designed for efficiently computing the correlation between   two point processes, such as photon arrival times or event positions.</li> <li>The algorithm is implemented with a focus on performance, leveraging   Numba for JIT compilation.</li> </ul> References <p>Laurence, T., et al. (2006).</p> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>@numba.jit(nopython=True)\ndef pcorrelate(t: np.ndarray, u: np.ndarray, bins: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the correlation of two arrays of discrete events (point-process).\n\n    This function computes the correlation of two time series of events\n    using an arbitrary array of lag-bins. It implements the algorithm described\n    in Laurence (2006) (https://doi.org/10.1364/OL.31.000829).\n\n    Parameters\n    ----------\n    t : np.ndarray\n        First array of \"points\" to correlate. The array needs to be monotonically increasing.\n    u : np.ndarray\n        Second array of \"points\" to correlate. The array needs to be monotonically increasing.\n    bins : np.ndarray\n        Array of bin edges where correlation is computed.\n\n    Returns\n    -------\n    G : np.ndarray\n        Array containing the correlation of `t` and `u`. The size is `len(bins) - 1`.\n\n    Notes\n    -----\n    - This method is designed for efficiently computing the correlation between\n      two point processes, such as photon arrival times or event positions.\n    - The algorithm is implemented with a focus on performance, leveraging\n      Numba for JIT compilation.\n\n    References\n    ----------\n    Laurence, T., et al. (2006).\n    \"\"\"\n    nbins = len(bins) - 1\n\n    # Array of counts (histogram)\n    counts = np.zeros(nbins, dtype=np.int64)\n\n    # For each bins, imin is the index of first `u` &gt;= of each left bin edge\n    imin = np.zeros(nbins, dtype=np.int64)\n    # For each bins, imax is the index of first `u` &gt;= of each right bin edge\n    imax = np.zeros(nbins, dtype=np.int64)\n\n    # For each ti, perform binning of (u - ti) and accumulate counts in Y\n    for ti in t:\n        for k, (tau_min, tau_max) in enumerate(zip(bins[:-1], bins[1:])):\n            if k == 0:\n                j = imin[k]\n                # We start by finding the index of the first `u` element\n                # which is &gt;= of the first bin edge `tau_min`\n                while j &lt; len(u):\n                    if u[j] - ti &gt;= tau_min:\n                        break\n                    j += 1\n\n            imin[k] = j\n            if imax[k] &gt; j:\n                j = imax[k]\n            while j &lt; len(u):\n                if u[j] - ti &gt;= tau_max:\n                    break\n                j += 1\n            imax[k] = j\n            # Now j is the index of the first `u` element &gt;= of\n            # the next bin left edge\n        counts += imax - imin\n    G = counts / np.diff(bins)\n    return G\n</code></pre>"},{"location":"reference/neuro_py/process/precession_utils/#neuro_py.process.precession_utils.spatial_phase_precession","title":"<code>spatial_phase_precession(circ, lin, slope_bounds=[-3 * np.pi, 3 * np.pi])</code>","text":"<p>Compute the circular-linear correlation as described in https://pubmed.ncbi.nlm.nih.gov/22487609/.</p> <p>Parameters:</p> Name Type Description Default <code>circ</code> <code>ndarray</code> <p>Circular data in radians (e.g., spike phases).</p> required <code>lin</code> <code>ndarray</code> <p>Linear data (e.g., spike positions).</p> required <code>slope_bounds</code> <code>Union[List[float], Tuple[float, float]]</code> <p>The slope range for optimization (default is [-3 * np.pi, 3 * np.pi]).</p> <code>[-3 * pi, 3 * pi]</code> <p>Returns:</p> Name Type Description <code>rho</code> <code>float</code> <p>Circular-linear correlation coefficient.</p> <code>pval</code> <code>float</code> <p>p-value for testing the significance of the correlation coefficient.</p> <code>sl</code> <code>float</code> <p>Slope of the circular-linear correlation.</p> <code>offs</code> <code>float</code> <p>Offset of the circular-linear correlation.</p> Notes <p>This method computes a circular-linear correlation and can handle cases where one or both variables may follow a uniform distribution. It differs from the linear-circular correlation used in other studies (e.g., https://science.sciencemag.org/content/340/6138/1342).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; circ = np.random.uniform(0, 2 * np.pi, 100)\n&gt;&gt;&gt; lin = np.random.uniform(0, 1, 100)\n&gt;&gt;&gt; rho, pval, sl, offs = spatial_phase_precession(circ, lin)\n&gt;&gt;&gt; print(f\"Correlation: {rho}, p-value: {pval}, slope: {sl}, offset: {offs}\")\n</code></pre> Source code in <code>neuro_py/process/precession_utils.py</code> <pre><code>def spatial_phase_precession(\n    circ: np.ndarray,\n    lin: np.ndarray,\n    slope_bounds: Union[List[float], Tuple[float, float]] = [-3 * np.pi, 3 * np.pi],\n) -&gt; Tuple[float, float, float, float]:\n    \"\"\"\n    Compute the circular-linear correlation as described in https://pubmed.ncbi.nlm.nih.gov/22487609/.\n\n    Parameters\n    ----------\n    circ : np.ndarray\n        Circular data in radians (e.g., spike phases).\n    lin : np.ndarray\n        Linear data (e.g., spike positions).\n    slope_bounds : Union[List[float], Tuple[float, float]], optional\n        The slope range for optimization (default is [-3 * np.pi, 3 * np.pi]).\n\n    Returns\n    -------\n    rho : float\n        Circular-linear correlation coefficient.\n    pval : float\n        p-value for testing the significance of the correlation coefficient.\n    sl : float\n        Slope of the circular-linear correlation.\n    offs : float\n        Offset of the circular-linear correlation.\n\n    Notes\n    -----\n    This method computes a circular-linear correlation and can handle cases\n    where one or both variables may follow a uniform distribution. It differs from\n    the linear-circular correlation used in other studies (e.g., https://science.sciencemag.org/content/340/6138/1342).\n\n    Examples\n    -------\n    &gt;&gt;&gt; circ = np.random.uniform(0, 2 * np.pi, 100)\n    &gt;&gt;&gt; lin = np.random.uniform(0, 1, 100)\n    &gt;&gt;&gt; rho, pval, sl, offs = spatial_phase_precession(circ, lin)\n    &gt;&gt;&gt; print(f\"Correlation: {rho}, p-value: {pval}, slope: {sl}, offset: {offs}\")\n    \"\"\"\n\n    # Get rid of all the nans in this data\n    nan_index = np.logical_or(np.isnan(circ), np.isnan(lin))\n    circ = circ[~nan_index]\n    lin = lin[~nan_index]\n\n    # Make sure there are still valid data\n    if np.size(lin) == 0:\n        return np.nan, np.nan, np.nan, np.nan\n\n    def myfun1(p):\n        return -np.sqrt(\n            (np.sum(np.cos(circ - (p * lin))) / len(circ)) ** 2\n            + (np.sum(np.sin(circ - (p * lin))) / len(circ)) ** 2\n        )\n\n    # finding the optimal slope, note that we have to restrict the range of slopes\n\n    sl = sp.optimize.fminbound(\n        myfun1,\n        slope_bounds[0] / (np.max(lin) - np.min(lin)),\n        slope_bounds[1] / (np.max(lin) - np.min(lin)),\n    )\n\n    # calculate offset\n    offs = np.arctan2(\n        np.sum(np.sin(circ - (sl * lin))), np.sum(np.cos(circ - (sl * lin)))\n    )\n    # offs = (offs + np.pi) % (2 * np.pi) - np.pi\n    offs = np.arctan2(np.sin(offs), np.cos(offs))\n\n    # circular variable derived from the linearization\n    linear_circ = np.mod(abs(sl) * lin, 2 * np.pi)\n\n    # # marginal distributions:\n    p1, z1 = pcs.rayleigh(circ)\n    p2, z2 = pcs.rayleigh(linear_circ)\n\n    # circular-linear correlation:\n    if (p1 &gt; 0.5) | (p2 &gt; 0.5):\n        # This means at least one of our variables may be a uniform distribution\n        rho, pval = corrcc_uniform(circ, linear_circ)\n    else:\n        rho, pval = corrcc(circ, linear_circ)\n\n    # Assign the correct sign to rho\n    if sl &lt; 0:\n        rho = -np.abs(rho)\n    else:\n        rho = np.abs(rho)\n\n    # if offs &lt; 0:\n    #     offs = offs + 2 * np.pi\n    # if offs &gt; np.pi:\n    #     offs = offs - 2 * np.pi\n\n    return rho, pval, sl, offs\n</code></pre>"},{"location":"reference/neuro_py/process/pychronux/","title":"neuro_py.process.pychronux","text":""},{"location":"reference/neuro_py/process/pychronux/#neuro_py.process.pychronux.dpsschk","title":"<code>dpsschk(tapers, N, Fs)</code>","text":"<p>Check and generate DPSS tapers.</p> <p>Parameters:</p> Name Type Description Default <code>tapers</code> <code>Union[ndarray, Tuple[float, int]]</code> <p>Input can be either an array representing [NW, K] or a tuple with the number of tapers and the maximum number of tapers.</p> required <code>N</code> <code>int</code> <p>Number of points for FFT.</p> required <code>Fs</code> <code>float</code> <p>Sampling frequency.</p> required <p>Returns:</p> Name Type Description <code>tapers</code> <code>ndarray</code> <p>Tapers matrix, shape [tapers, eigenvalues].</p> Notes <p>The function computes DPSS (Discrete Prolate Spheroidal Sequences) tapers and scales them by the square root of the sampling frequency.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def dpsschk(\n    tapers: Union[np.ndarray, Tuple[float, int]], N: int, Fs: float\n) -&gt; np.ndarray:\n    \"\"\"\n    Check and generate DPSS tapers.\n\n    Parameters\n    ----------\n    tapers : Union[np.ndarray, Tuple[float, int]]\n        Input can be either an array representing [NW, K] or a tuple with\n        the number of tapers and the maximum number of tapers.\n    N : int\n        Number of points for FFT.\n    Fs : float\n        Sampling frequency.\n\n    Returns\n    -------\n    tapers : np.ndarray\n        Tapers matrix, shape [tapers, eigenvalues].\n\n    Notes\n    -----\n    The function computes DPSS (Discrete Prolate Spheroidal Sequences) tapers\n    and scales them by the square root of the sampling frequency.\n    \"\"\"\n    tapers, eigs = dpss(N, NW=tapers[0], Kmax=tapers[1], sym=False, return_ratios=True)\n    tapers = tapers * np.sqrt(Fs)\n    tapers = tapers.T\n    return tapers\n</code></pre>"},{"location":"reference/neuro_py/process/pychronux/#neuro_py.process.pychronux.get_tapers","title":"<code>get_tapers(N, bandwidth, *, fs=1.0, min_lambda=0.95, n_tapers=None)</code>","text":"<p>Compute tapers and associated energy concentrations for the Thomson multitaper method.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>int</code> <p>Length of taper.</p> required <code>bandwidth</code> <code>float</code> <p>Bandwidth of taper, in Hz.</p> required <code>fs</code> <code>float</code> <p>Sampling rate, in Hz. Default is 1 Hz.</p> <code>1.0</code> <code>min_lambda</code> <code>float</code> <p>Minimum energy concentration that each taper must satisfy. Default is 0.95.</p> <code>0.95</code> <code>n_tapers</code> <code>Optional[int]</code> <p>Number of tapers to compute. Default is to use all tapers that satisfy 'min_lambda'.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tapers</code> <code>ndarray</code> <p>Array of tapers with shape (n_tapers, N).</p> <code>lambdas</code> <code>ndarray</code> <p>Energy concentrations for each taper with shape (n_tapers,).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If not enough tapers are available or if none of the tapers satisfy the minimum energy concentration criteria.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def get_tapers(\n    N: int,\n    bandwidth: float,\n    *,\n    fs: float = 1.0,\n    min_lambda: float = 0.95,\n    n_tapers: Optional[int] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute tapers and associated energy concentrations for the Thomson\n    multitaper method.\n\n    Parameters\n    ----------\n    N : int\n        Length of taper.\n    bandwidth : float\n        Bandwidth of taper, in Hz.\n    fs : float, optional\n        Sampling rate, in Hz. Default is 1 Hz.\n    min_lambda : float, optional\n        Minimum energy concentration that each taper must satisfy. Default is 0.95.\n    n_tapers : Optional[int], optional\n        Number of tapers to compute. Default is to use all tapers that satisfy 'min_lambda'.\n\n    Returns\n    -------\n    tapers : np.ndarray\n        Array of tapers with shape (n_tapers, N).\n    lambdas : np.ndarray\n        Energy concentrations for each taper with shape (n_tapers,).\n\n    Raises\n    ------\n    ValueError\n        If not enough tapers are available or if none of the tapers satisfy the\n        minimum energy concentration criteria.\n    \"\"\"\n\n    NW = bandwidth * N / fs\n    K = int(np.ceil(2 * NW)) - 1\n    if n_tapers is not None:\n        K = min(K, n_tapers)\n    if K &lt; 1:\n        raise ValueError(\n            f\"Not enough tapers, with 'NW' of {NW}. Increase the bandwidth or \"\n            \"use more data points\"\n        )\n\n    tapers, lambdas = dpss(N, NW=NW, Kmax=K, sym=False, norm=2, return_ratios=True)\n    mask = lambdas &gt; min_lambda\n    if not np.sum(mask) &gt; 0:\n        raise ValueError(\n            \"None of the tapers satisfied the minimum energy concentration\"\n            f\" criteria of {min_lambda}\"\n        )\n    tapers = tapers[mask]\n    lambdas = lambdas[mask]\n\n    if n_tapers is not None:\n        if n_tapers &gt; tapers.shape[0]:\n            raise ValueError(\n                f\"'n_tapers' of {n_tapers} is greater than the {tapers.shape[0]}\"\n                f\" that satisfied the minimum energy concentration criteria of {min_lambda}\"\n            )\n        tapers = tapers[:n_tapers]\n        lambdas = lambdas[:n_tapers]\n\n    return tapers, lambdas\n</code></pre>"},{"location":"reference/neuro_py/process/pychronux/#neuro_py.process.pychronux.getfgrid","title":"<code>getfgrid(Fs, nfft, fpass)</code>","text":"<p>Get frequency grid for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>Fs</code> <code>int</code> <p>Sampling frequency.</p> required <code>nfft</code> <code>int</code> <p>Number of points for FFT.</p> required <code>fpass</code> <code>List[float]</code> <p>Frequency range to evaluate (as [fmin, fmax]).</p> required <p>Returns:</p> Name Type Description <code>f</code> <code>ndarray</code> <p>Frequency vector within the specified range.</p> <code>findx</code> <code>ndarray</code> <p>Boolean array indicating the indices of the frequency vector that fall within the specified range.</p> Notes <p>The frequency vector is computed based on the sampling frequency and the number of FFT points. Only frequencies within the range defined by <code>fpass</code> are returned.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def getfgrid(Fs: int, nfft: int, fpass: List[float]) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Get frequency grid for evaluation.\n\n    Parameters\n    ----------\n    Fs : int\n        Sampling frequency.\n    nfft : int\n        Number of points for FFT.\n    fpass : List[float]\n        Frequency range to evaluate (as [fmin, fmax]).\n\n    Returns\n    -------\n    f : np.ndarray\n        Frequency vector within the specified range.\n    findx : np.ndarray\n        Boolean array indicating the indices of the frequency vector that fall within the specified range.\n\n    Notes\n    -----\n    The frequency vector is computed based on the sampling frequency and the number of FFT points.\n    Only frequencies within the range defined by `fpass` are returned.\n    \"\"\"\n    df = Fs / nfft\n    f = np.arange(0, Fs + df, df)\n    f = f[0:nfft]\n    findx = (f &gt;= fpass[0]) &amp; (f &lt;= fpass[-1])\n    f = f[findx]\n    return f, findx\n</code></pre>"},{"location":"reference/neuro_py/process/pychronux/#neuro_py.process.pychronux.mtcoherencept","title":"<code>mtcoherencept(data1, data2, Fs, fpass, NW=2.5, n_tapers=4, time_support=None, tapers=None, tapers_ts=None, nfft=None)</code>","text":"<p>Multitaper coherence for point processes.</p> <p>Parameters:</p> Name Type Description Default <code>data1</code> <code>ndarray</code> <p>Array of spike times for the first signal (in seconds).</p> required <code>data2</code> <code>ndarray</code> <p>Array of spike times for the second signal (in seconds).</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency.</p> required <code>fpass</code> <code>list</code> <p>Frequency range to evaluate as [min_freq, max_freq].</p> required <code>NW</code> <code>Union[int, float]</code> <p>Time-bandwidth product, by default 2.5.</p> <code>2.5</code> <code>n_tapers</code> <code>int</code> <p>Number of tapers, by default 4.</p> <code>4</code> <code>time_support</code> <code>Union[list, None]</code> <p>Time range to evaluate, by default None.</p> <code>None</code> <code>tapers</code> <code>Union[ndarray, None]</code> <p>Precomputed tapers, given as [NW, K] or [tapers, eigenvalues], by default None.</p> <code>None</code> <code>tapers_ts</code> <code>Union[ndarray, None]</code> <p>Taper time series, by default None.</p> <code>None</code> <code>nfft</code> <code>Optional[int]</code> <p>Number of points for FFT, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Coherence between the two point processes.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtcoherencept(\n    data1: np.ndarray,\n    data2: np.ndarray,\n    Fs: int,\n    fpass: list,\n    NW: Union[int, float] = 2.5,\n    n_tapers: int = 4,\n    time_support: Union[list, None] = None,\n    tapers: Union[np.ndarray, None] = None,\n    tapers_ts: Union[np.ndarray, None] = None,\n    nfft: Optional[int] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Multitaper coherence for point processes.\n\n    Parameters\n    ----------\n    data1 : np.ndarray\n        Array of spike times for the first signal (in seconds).\n    data2 : np.ndarray\n        Array of spike times for the second signal (in seconds).\n    Fs : int\n        Sampling frequency.\n    fpass : list\n        Frequency range to evaluate as [min_freq, max_freq].\n    NW : Union[int, float], optional\n        Time-bandwidth product, by default 2.5.\n    n_tapers : int, optional\n        Number of tapers, by default 4.\n    time_support : Union[list, None], optional\n        Time range to evaluate, by default None.\n    tapers : Union[np.ndarray, None], optional\n        Precomputed tapers, given as [NW, K] or [tapers, eigenvalues], by default None.\n    tapers_ts : Union[np.ndarray, None], optional\n        Taper time series, by default None.\n    nfft : Optional[int], optional\n        Number of points for FFT, by default None.\n\n    Returns\n    -------\n    pd.DataFrame\n        Coherence between the two point processes.\n    \"\"\"\n    # Check if data is a single unit and put in array\n    if isinstance(data1, np.ndarray):\n        data1 = np.array([data1])\n    if isinstance(data2, np.ndarray):\n        data2 = np.array([data2])\n\n    # Compute power spectral densities (PSD) for both spike trains\n    psd1 = mtspectrumpt(\n        data1, Fs, fpass, NW, n_tapers, time_support, tapers, tapers_ts, nfft\n    )\n    psd2 = mtspectrumpt(\n        data2, Fs, fpass, NW, n_tapers, time_support, tapers, tapers_ts, nfft\n    )\n\n    # Compute cross-spectral density (CSD) between the two spike trains\n    csd = mtcsdpt(\n        data1, data2, Fs, fpass, NW, n_tapers, time_support, tapers, tapers_ts, nfft\n    )\n\n    # Calculate coherence: |Sxy(f)|^2 / (Sxx(f) * Syy(f))\n    coherence = np.abs(csd[\"CSD\"].values) ** 2 / (psd1.values * psd2.values).flatten()\n\n    # Return coherence as a pandas DataFrame\n    coherence_df = pd.DataFrame(index=csd.index, data=coherence, columns=[\"Coherence\"])\n    return coherence_df\n</code></pre>"},{"location":"reference/neuro_py/process/pychronux/#neuro_py.process.pychronux.mtcsdpt","title":"<code>mtcsdpt(data1, data2, Fs, fpass, NW=2.5, n_tapers=4, time_support=None, tapers=None, tapers_ts=None, nfft=None)</code>","text":"<p>Multitaper cross-spectral density (CSD) for point processes.</p> <p>Parameters:</p> Name Type Description Default <code>data1</code> <code>ndarray</code> <p>Array of spike times for the first signal (in seconds).</p> required <code>data2</code> <code>ndarray</code> <p>Array of spike times for the second signal (in seconds).</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency.</p> required <code>fpass</code> <code>list</code> <p>Frequency range to evaluate as [min_freq, max_freq].</p> required <code>NW</code> <code>Union[int, float]</code> <p>Time-bandwidth product, by default 2.5.</p> <code>2.5</code> <code>n_tapers</code> <code>int</code> <p>Number of tapers, by default 4.</p> <code>4</code> <code>time_support</code> <code>Union[list, None]</code> <p>Time range to evaluate, by default None.</p> <code>None</code> <code>tapers</code> <code>Union[ndarray, None]</code> <p>Precomputed tapers, given as [NW, K] or [tapers, eigenvalues], by default None.</p> <code>None</code> <code>tapers_ts</code> <code>Union[ndarray, None]</code> <p>Taper time series, by default None.</p> <code>None</code> <code>nfft</code> <code>Optional[int]</code> <p>Number of points for FFT, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Cross-spectral density between the two point processes.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtcsdpt(\n    data1: np.ndarray,\n    data2: np.ndarray,\n    Fs: int,\n    fpass: list,\n    NW: Union[int, float] = 2.5,\n    n_tapers: int = 4,\n    time_support: Union[list, None] = None,\n    tapers: Union[np.ndarray, None] = None,\n    tapers_ts: Union[np.ndarray, None] = None,\n    nfft: Optional[int] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Multitaper cross-spectral density (CSD) for point processes.\n\n    Parameters\n    ----------\n    data1 : np.ndarray\n        Array of spike times for the first signal (in seconds).\n    data2 : np.ndarray\n        Array of spike times for the second signal (in seconds).\n    Fs : int\n        Sampling frequency.\n    fpass : list\n        Frequency range to evaluate as [min_freq, max_freq].\n    NW : Union[int, float], optional\n        Time-bandwidth product, by default 2.5.\n    n_tapers : int, optional\n        Number of tapers, by default 4.\n    time_support : Union[list, None], optional\n        Time range to evaluate, by default None.\n    tapers : Union[np.ndarray, None], optional\n        Precomputed tapers, given as [NW, K] or [tapers, eigenvalues], by default None.\n    tapers_ts : Union[np.ndarray, None], optional\n        Taper time series, by default None.\n    nfft : Optional[int], optional\n        Number of points for FFT, by default None.\n\n    Returns\n    -------\n    pd.DataFrame\n        Cross-spectral density between the two point processes.\n    \"\"\"\n    if time_support is not None:\n        mintime, maxtime = time_support\n    else:\n        mintime = min(np.min(data1), np.min(data2))\n        maxtime = max(np.max(data1), np.max(data2))\n    dt = 1 / Fs\n\n    # Create tapers if not provided\n    if tapers is None:\n        tapers_ts = np.arange(mintime - dt, maxtime + dt, dt)\n        N = len(tapers_ts)\n        tapers, eigens = dpss(N, NW, n_tapers, return_ratios=True)\n\n    tapers = tapers.T\n    N = len(tapers_ts)\n\n    # Number of points in FFT\n    if nfft is None:\n        nfft = np.max([int(2 ** np.ceil(np.log2(N))), N])\n    f, findx = getfgrid(Fs, nfft, fpass)\n\n    # Compute the multitaper Fourier transforms of both spike trains\n    J1, Msp1, Nsp1 = mtfftpt(data1, tapers, nfft, tapers_ts, f, findx)\n    J2, Msp2, Nsp2 = mtfftpt(data2, tapers, nfft, tapers_ts, f, findx)\n\n    # Cross-spectral density: Sxy = mean(conjugate(J1) * J2)\n    csd = np.real(np.mean(np.conj(J1) * J2, axis=1))\n\n    csd_df = pd.DataFrame(index=f, data=csd, columns=[\"CSD\"])\n    return csd_df\n</code></pre>"},{"location":"reference/neuro_py/process/pychronux/#neuro_py.process.pychronux.mtfftc","title":"<code>mtfftc(data, tapers, nfft, Fs)</code>","text":"<p>Multi-taper Fourier Transform - Continuous Data (Single Signal)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>1D array of data (samples).</p> required <code>tapers</code> <code>ndarray</code> <p>Precomputed DPSS tapers with shape (samples, tapers).</p> required <code>nfft</code> <code>int</code> <p>Length of padded data for FFT.</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency.</p> required <p>Returns:</p> Name Type Description <code>J</code> <code>ndarray</code> <p>FFT in the form (nfft, K), where K is the number of tapers.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtfftc(data: np.ndarray, tapers: np.ndarray, nfft: int, Fs: int) -&gt; np.ndarray:\n    \"\"\"\n    Multi-taper Fourier Transform - Continuous Data (Single Signal)\n\n    Parameters\n    ----------\n    data : np.ndarray\n        1D array of data (samples).\n    tapers : np.ndarray\n        Precomputed DPSS tapers with shape (samples, tapers).\n    nfft : int\n        Length of padded data for FFT.\n    Fs : int\n        Sampling frequency.\n\n    Returns\n    -------\n    J : np.ndarray\n        FFT in the form (nfft, K), where K is the number of tapers.\n    \"\"\"\n    # Ensure data is 1D\n    if data.ndim != 1:\n        raise ValueError(\"Input data must be a 1D array.\")\n\n    NC = data.shape[0]  # Number of samples in data\n    NK, K = tapers.shape  # Number of samples and tapers\n\n    if NK != NC:\n        raise ValueError(\"Length of tapers is incompatible with length of data.\")\n\n    # Project data onto tapers\n    data_proj = data[:, np.newaxis] * tapers  # Shape: (samples, tapers)\n\n    # Compute FFT for each taper\n    J = np.fft.fft(data_proj, n=nfft, axis=0) / Fs  # Shape: (nfft, K)\n\n    return J\n</code></pre>"},{"location":"reference/neuro_py/process/pychronux/#neuro_py.process.pychronux.mtfftpt","title":"<code>mtfftpt(data, tapers, nfft, t, f, findx)</code>","text":"<p>Multitaper FFT for point process times.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>1D array of spike times (in seconds).</p> required <code>tapers</code> <code>ndarray</code> <p>Tapers from the DPSS method.</p> required <code>nfft</code> <code>int</code> <p>Number of points for FFT.</p> required <code>t</code> <code>ndarray</code> <p>Time vector.</p> required <code>f</code> <code>ndarray</code> <p>Frequency vector.</p> required <code>findx</code> <code>list of bool</code> <p>Frequency index.</p> required <p>Returns:</p> Name Type Description <code>J</code> <code>ndarray</code> <p>FFT of the data.</p> <code>Msp</code> <code>float</code> <p>Mean spikes per time.</p> <code>Nsp</code> <code>float</code> <p>Total number of spikes in data.</p> Notes <p>The function computes the multitaper FFT of spike times using the specified tapers and returns the FFT result, mean spikes, and total spike count.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtfftpt(\n    data: np.ndarray,\n    tapers: np.ndarray,\n    nfft: int,\n    t: np.ndarray,\n    f: np.ndarray,\n    findx: List[bool],\n) -&gt; Tuple[np.ndarray, float, float]:\n    \"\"\"\n    Multitaper FFT for point process times.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        1D array of spike times (in seconds).\n    tapers : np.ndarray\n        Tapers from the DPSS method.\n    nfft : int\n        Number of points for FFT.\n    t : np.ndarray\n        Time vector.\n    f : np.ndarray\n        Frequency vector.\n    findx : list of bool\n        Frequency index.\n\n    Returns\n    -------\n    J : np.ndarray\n        FFT of the data.\n    Msp : float\n        Mean spikes per time.\n    Nsp : float\n        Total number of spikes in data.\n\n    Notes\n    -----\n    The function computes the multitaper FFT of spike times using\n    the specified tapers and returns the FFT result, mean spikes,\n    and total spike count.\n    \"\"\"\n    K = tapers.shape[1]\n    nfreq = len(f)\n\n    # get the FFT of the tapers\n    H = np.zeros((nfft, K), dtype=np.complex128)\n    for i in np.arange(K):\n        H[:, i] = np.fft.fft(tapers[:, i], nfft, axis=0)\n\n    H = H[findx, :]\n    w = 2 * np.pi * f\n    dtmp = data\n    indx = np.logical_and(dtmp &gt;= np.min(t), dtmp &lt;= np.max(t))\n    if len(indx):\n        dtmp = dtmp[indx]\n    Nsp = len(dtmp)\n\n    # get the mean spike rate\n    Msp = Nsp / len(t)\n\n    if Msp != 0:\n        # Interpolate spike times for each taper\n        data_proj = np.empty((len(dtmp), K))\n        for i in range(K):\n            data_proj[:, i] = np.interp(dtmp, t, tapers[:, i])\n\n        def compute_J(k):\n            J_k = np.zeros(nfreq, dtype=np.complex128)\n            for i, freq in enumerate(w):\n                phase = -1j * freq * (dtmp - t[0])\n                J_k[i] = np.sum(np.exp(phase) * data_proj[:, k])\n            return J_k\n\n        J = np.array(Parallel(n_jobs=-1)(delayed(compute_J)(k) for k in range(K))).T\n\n        J -= H * Msp\n    else:\n        # No spikes: return zeros\n        J = np.zeros((nfreq, K), dtype=np.complex128)\n\n    return J, Msp, Nsp\n</code></pre>"},{"location":"reference/neuro_py/process/pychronux/#neuro_py.process.pychronux.mtspectrumc","title":"<code>mtspectrumc(data, Fs, fpass, tapers)</code>","text":"<p>Compute the multitaper power spectrum for continuous data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>1D array of continuous data (e.g., LFP).</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency in Hz.</p> required <code>fpass</code> <code>list</code> <p>Frequency range to evaluate as [min_freq, max_freq].</p> required <code>tapers</code> <code>ndarray</code> <p>Tapers array with shape [NW, K] or [tapers, eigenvalues].</p> required <p>Returns:</p> Name Type Description <code>S</code> <code>Series</code> <p>Power spectrum with frequencies as the index.</p> Notes <p>This function utilizes the multitaper method for spectral estimation and returns the power spectrum as a pandas Series.</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtspectrumc(\n    data: np.ndarray, Fs: int, fpass: list, tapers: np.ndarray\n) -&gt; pd.Series:\n    \"\"\"\n    Compute the multitaper power spectrum for continuous data.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        1D array of continuous data (e.g., LFP).\n    Fs : int\n        Sampling frequency in Hz.\n    fpass : list\n        Frequency range to evaluate as [min_freq, max_freq].\n    tapers : np.ndarray\n        Tapers array with shape [NW, K] or [tapers, eigenvalues].\n\n    Returns\n    -------\n    S : pd.Series\n        Power spectrum with frequencies as the index.\n\n    Notes\n    -----\n    This function utilizes the multitaper method for spectral estimation\n    and returns the power spectrum as a pandas Series.\n    \"\"\"\n    N = len(data)\n    nfft = np.max(\n        [int(2 ** np.ceil(np.log2(N))), N]\n    )  # number of points in fft of prolates\n    # get the frequency grid\n    f, findx = getfgrid(Fs, nfft, fpass)\n    # get the fft of the tapers\n    tapers = dpsschk(tapers, N, Fs)\n    # get the fft of the data\n    J = mtfftc(data, tapers, nfft, Fs)\n    # restrict fft of tapers to required frequencies\n    J = J[findx, :]\n    # get the power spectrum\n    S = np.real(np.mean(np.conj(J) * J, 1))\n    # return the power spectrum\n    return pd.Series(index=f, data=S)\n</code></pre>"},{"location":"reference/neuro_py/process/pychronux/#neuro_py.process.pychronux.mtspectrumpt","title":"<code>mtspectrumpt(data, Fs, fpass, NW=2.5, n_tapers=4, time_support=None, tapers=None, tapers_ts=None, nfft=None)</code>","text":"<p>Multitaper power spectrum estimation for point process data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Array of spike times (in seconds).</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency.</p> required <code>fpass</code> <code>list of float</code> <p>Frequency range to evaluate.</p> required <code>NW</code> <code>Union[int, float]</code> <p>Time-bandwidth product (default is 2.5).</p> <code>2.5</code> <code>n_tapers</code> <code>int</code> <p>Number of tapers (default is 4).</p> <code>4</code> <code>time_support</code> <code>Union[list, None]</code> <p>Time range to evaluate (default is None).</p> <code>None</code> <code>tapers</code> <code>Union[ndarray, None]</code> <p>Precomputed tapers, given as [NW, K] or [tapers, eigenvalues] (default is None).</p> <code>None</code> <code>tapers_ts</code> <code>Union[ndarray, None]</code> <p>Taper time series (default is None).</p> <code>None</code> <code>nfft</code> <code>Optional[int]</code> <p>Number of points for FFT (default is None).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the power spectrum.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spec = pychronux.mtspectrumpt(\n&gt;&gt;&gt;    st.data,\n&gt;&gt;&gt;    100,\n&gt;&gt;&gt;    [1, 20],\n&gt;&gt;&gt;    NW=3,\n&gt;&gt;&gt;    n_tapers=5,\n&gt;&gt;&gt;    time_support=[st.support.start, st.support.stop],\n&gt;&gt;&gt;    nfft=500,\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def mtspectrumpt(\n    data: np.ndarray,\n    Fs: int,\n    fpass: list,\n    NW: Union[int, float] = 2.5,\n    n_tapers: int = 4,\n    time_support: Union[list, None] = None,\n    tapers: Union[np.ndarray, None] = None,\n    tapers_ts: Union[np.ndarray, None] = None,\n    nfft: Optional[int] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Multitaper power spectrum estimation for point process data.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Array of spike times (in seconds).\n    Fs : int\n        Sampling frequency.\n    fpass : list of float\n        Frequency range to evaluate.\n    NW : Union[int, float], optional\n        Time-bandwidth product (default is 2.5).\n    n_tapers : int, optional\n        Number of tapers (default is 4).\n    time_support : Union[list, None], optional\n        Time range to evaluate (default is None).\n    tapers : Union[np.ndarray, None], optional\n        Precomputed tapers, given as [NW, K] or [tapers, eigenvalues] (default is None).\n    tapers_ts : Union[np.ndarray, None], optional\n        Taper time series (default is None).\n    nfft : Optional[int], optional\n        Number of points for FFT (default is None).\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing the power spectrum.\n\n\n    Examples\n    -------\n    &gt;&gt;&gt; spec = pychronux.mtspectrumpt(\n    &gt;&gt;&gt;    st.data,\n    &gt;&gt;&gt;    100,\n    &gt;&gt;&gt;    [1, 20],\n    &gt;&gt;&gt;    NW=3,\n    &gt;&gt;&gt;    n_tapers=5,\n    &gt;&gt;&gt;    time_support=[st.support.start, st.support.stop],\n    &gt;&gt;&gt;    nfft=500,\n    &gt;&gt;&gt; )\n    \"\"\"\n\n    # check data\n    if len(data) == 0:\n        return pd.DataFrame()\n\n    # check frequency range\n    if fpass[0] &gt; fpass[1]:\n        raise ValueError(\n            \"Invalid frequency range: fpass[0] should be less than fpass[1].\"\n        )\n\n    if time_support is not None:\n        mintime, maxtime = time_support\n    else:\n        if data.dtype == np.object_:\n            mintime = np.min(np.concatenate(data))\n            maxtime = np.max(np.concatenate(data))\n        else:\n            mintime = np.min(data)\n            maxtime = np.max(data)\n\n    dt = 1 / Fs\n\n    if tapers is None:\n        tapers_ts = np.arange(mintime - dt, maxtime + dt, dt)\n        N = len(tapers_ts)\n        tapers, eigens = dpss(N, NW, n_tapers, return_ratios=True)\n        tapers = tapers.T\n\n    if tapers_ts is None:\n        tapers_ts = np.arange(mintime - dt, maxtime + dt, dt)\n\n    N = len(tapers_ts)\n    # number of points in fft of prolates\n    if nfft is None:\n        nfft = np.max([int(2 ** np.ceil(np.log2(N))), N])\n    f, findx = getfgrid(Fs, nfft, fpass)\n\n    spec = np.zeros((len(f), len(data)))\n    for i, d in enumerate(data):\n        J, _, _ = mtfftpt(d, tapers, nfft, tapers_ts, f, findx)\n        spec[:, i] = np.real(np.mean(np.conj(J) * J, 1))\n\n    spectrum_df = pd.DataFrame(index=f, columns=np.arange(len(data)), dtype=np.float64)\n    spectrum_df[:] = spec\n    return spectrum_df\n</code></pre>"},{"location":"reference/neuro_py/process/pychronux/#neuro_py.process.pychronux.point_spectra","title":"<code>point_spectra(times, Fs=1250, freq_range=[1, 20], tapers0=[3, 5], pad=0, nfft=None)</code>","text":"<p>Compute multitaper power spectrum for point processes.</p> <p>Parameters:</p> Name Type Description Default <code>times</code> <code>ndarray</code> <p>Array of spike times (in seconds).</p> required <code>Fs</code> <code>int</code> <p>Sampling frequency (default is 1250 Hz).</p> <code>1250</code> <code>freq_range</code> <code>List[float]</code> <p>Frequency range to evaluate (default is [1, 20] Hz).</p> <code>[1, 20]</code> <code>tapers0</code> <code>List[int]</code> <p>Time-bandwidth product and number of tapers (default is [3, 5]). The time-bandwidth product is used to compute the tapers.</p> <code>[3, 5]</code> <code>pad</code> <code>int</code> <p>Padding for the FFT (default is 0).</p> <code>0</code> <code>nfft</code> <code>Optional[int]</code> <p>Number of points for FFT (default is None).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>spectra</code> <code>ndarray</code> <p>Power spectrum.</p> <code>f</code> <code>ndarray</code> <p>Frequency vector.</p> Notes <p>Alternative function to <code>mtspectrumpt</code> for computing the power spectrum</p> Source code in <code>neuro_py/process/pychronux.py</code> <pre><code>def point_spectra(\n    times: np.ndarray,\n    Fs: int = 1250,\n    freq_range: List[float] = [1, 20],\n    tapers0: List[int] = [3, 5],\n    pad: int = 0,\n    nfft: Optional[int] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute multitaper power spectrum for point processes.\n\n    Parameters\n    ----------\n    times : np.ndarray\n        Array of spike times (in seconds).\n    Fs : int, optional\n        Sampling frequency (default is 1250 Hz).\n    freq_range : List[float], optional\n        Frequency range to evaluate (default is [1, 20] Hz).\n    tapers0 : List[int], optional\n        Time-bandwidth product and number of tapers (default is [3, 5]).\n        The time-bandwidth product is used to compute the tapers.\n    pad : int, optional\n        Padding for the FFT (default is 0).\n    nfft : Optional[int], optional\n        Number of points for FFT (default is None).\n\n    Returns\n    -------\n    spectra : np.ndarray\n        Power spectrum.\n    f : np.ndarray\n        Frequency vector.\n\n    Notes\n    -----\n    Alternative function to `mtspectrumpt` for computing the power spectrum\n    \"\"\"\n\n    # generate frequency grid\n    timesRange = [min(times), max(times)]\n    window = np.floor(np.diff(timesRange))\n    nSamplesPerWindow = int(np.round(Fs * window[0]))\n    if nfft is None:\n        nfft = np.max(\n            [(int(2 ** np.ceil(np.log2(nSamplesPerWindow))) + pad), nSamplesPerWindow]\n        )\n    fAll = np.linspace(0, Fs, int(nfft))\n    frequency_ind = (fAll &gt;= freq_range[0]) &amp; (fAll &lt;= freq_range[1])\n\n    # Generate tapers\n    tapers, _ = dpss(nSamplesPerWindow, tapers0[0], tapers0[1], return_ratios=True)\n    tapers = tapers * np.sqrt(Fs)\n\n    # Compute FFT of tapers and restrict to required frequencies\n    H = np.fft.fft(tapers, n=nfft, axis=1)  # Shape: (K, nfft)\n    H = H[:, frequency_ind]  # Shape: (K, Nf)\n\n    # Angular frequencies\n    f = fAll[frequency_ind]\n    w = 2 * np.pi * f\n\n    # Time grid\n    timegrid = np.linspace(timesRange[0], timesRange[1], nSamplesPerWindow)\n\n    # Ensure times are within range\n    data = times[(times &gt;= timegrid[0]) &amp; (times &lt;= timegrid[-1])]\n\n    # Project spike times onto tapers\n    data_proj = [np.interp(data, timegrid, taper) for taper in tapers]\n    data_proj = np.vstack(data_proj)  # Shape: (K, len(data))\n\n    # Compute multitaper spectrum\n    exponential = np.exp(\n        np.outer(-1j * w, (data - timegrid[0]))\n    )  # Shape: (Nf, len(data))\n    J = exponential @ data_proj.T - H.T * len(data) / len(timegrid)  # Shape: (Nf, K)\n    spectra = np.squeeze(np.mean(np.real(np.conj(J) * J), axis=1))  # Mean across tapers\n\n    return spectra, f\n</code></pre>"},{"location":"reference/neuro_py/process/utils/","title":"neuro_py.process.utils","text":""},{"location":"reference/neuro_py/process/utils/#neuro_py.process.utils.average_diagonal","title":"<code>average_diagonal(mat)</code>","text":"<p>Average values over all offset diagonals of a 2D array.</p> <p>Parameters:</p> Name Type Description Default <code>mat</code> <code>ndarray</code> <p>2D array from which to compute the average values over diagonals.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>ndarray</code> <p>1D array containing the average values over all offset diagonals.</p> Notes <p>The method used for computing averages is based on the concept of accumulating values along each diagonal offset and then dividing by the number of elements in each diagonal.</p> Reference <p>https://stackoverflow.com/questions/71362928/average-values-over-all-offset-diagonals</p> Source code in <code>neuro_py/process/utils.py</code> <pre><code>def average_diagonal(mat: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Average values over all offset diagonals of a 2D array.\n\n    Parameters\n    ----------\n    mat : np.ndarray\n        2D array from which to compute the average values over diagonals.\n\n    Returns\n    -------\n    output : np.ndarray\n        1D array containing the average values over all offset diagonals.\n\n    Notes\n    -----\n    The method used for computing averages is based on the concept of\n    accumulating values along each diagonal offset and then dividing by\n    the number of elements in each diagonal.\n\n    Reference\n    ---------\n    https://stackoverflow.com/questions/71362928/average-values-over-all-offset-diagonals\n    \"\"\"\n    n = mat.shape[0]\n    output = np.zeros(n * 2 - 1, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        output[i : i + n] += mat[n - 1 - i]\n    output[0:n] /= np.arange(1, n + 1, 1, dtype=np.float64)\n    output[n:] /= np.arange(n - 1, 0, -1, dtype=np.float64)\n    return output\n</code></pre>"},{"location":"reference/neuro_py/process/utils/#neuro_py.process.utils.circular_shift","title":"<code>circular_shift(m, s)</code>","text":"<p>Circularly shift matrix rows or columns by specified amounts.</p> <p>Each matrix row (or column) is circularly shifted by a different amount.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>ndarray</code> <p>Matrix to rotate. Should be a 2D array.</p> required <code>s</code> <code>ndarray</code> <p>Shift amounts for each row (horizontal vector) or column (vertical vector). Should be a 1D array.</p> required <p>Returns:</p> Name Type Description <code>shifted</code> <code>ndarray</code> <p>Matrix <code>m</code> with rows (or columns) circularly shifted by the amounts in <code>s</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>s</code> is not a vector of integers or if <code>m</code> is not a 2D matrix. If the sizes of <code>m</code> and <code>s</code> are incompatible.</p> Notes <p>This function is adapted from CircularShift.m, Copyright (C) 2012 by Micha\u00ebl Zugaro.</p> Source code in <code>neuro_py/process/utils.py</code> <pre><code>def circular_shift(m: np.ndarray, s: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Circularly shift matrix rows or columns by specified amounts.\n\n    Each matrix row (or column) is circularly shifted by a different amount.\n\n    Parameters\n    ----------\n    m : np.ndarray\n        Matrix to rotate. Should be a 2D array.\n    s : np.ndarray\n        Shift amounts for each row (horizontal vector) or column (vertical vector).\n        Should be a 1D array.\n\n    Returns\n    -------\n    shifted : np.ndarray\n        Matrix `m` with rows (or columns) circularly shifted by the amounts in `s`.\n\n    Raises\n    ------\n    ValueError\n        If `s` is not a vector of integers or if `m` is not a 2D matrix.\n        If the sizes of `m` and `s` are incompatible.\n\n    Notes\n    -----\n    This function is adapted from CircularShift.m, Copyright (C) 2012 by Micha\u00ebl Zugaro.\n    \"\"\"\n    # Check number of parameters\n    if len(s.shape) != 1:\n        raise ValueError(\"Second parameter is not a vector of integers.\")\n    if len(m.shape) != 2:\n        raise ValueError(\"First parameter is not a 2D matrix.\")\n\n    mm, nm = m.shape\n    # if s is 1d array, add dimension\n    if len(s.shape) == 1:\n        s = s[np.newaxis, :]\n    ms, ns = s.shape\n\n    # Check parameter sizes\n    if mm != ms and nm != ns:\n        raise ValueError(\"Incompatible parameter sizes.\")\n\n    # The algorithm below works along columns; transpose if necessary\n    s = -np.ravel(s)\n    if ns == 1:\n        m = m.T\n        mm, nm = m.shape\n\n    # Shift matrix S, where Sij is the vertical shift for element ij\n    shift = np.tile(s, (mm, 1))\n\n    # Before we start, each element Mij has a linear index Aij.\n    # After circularly shifting the rows, it will have a linear index Bij.\n    # We now construct Bij.\n\n    # First, create matrix C where each item Cij = i (row number)\n    lines = np.tile(np.arange(mm)[:, np.newaxis], (1, nm))\n    # Next, update C so that Cij becomes the target row number (after circular shift)\n    lines = np.mod(lines + shift, mm)\n    # lines[lines == 0] = mm\n    # Finally, transform Cij into a linear index, yielding Bij\n    indices = lines + np.tile(np.arange(nm) * mm, (mm, 1))\n\n    # Circular shift (reshape so that it is not transformed into a vector)\n    shifted = m.ravel()[(indices.flatten() - 1).astype(int)].reshape(mm, nm)\n\n    # flip matrix right to left\n    # shifted = np.fliplr(shifted)\n\n    shifted = np.flipud(shifted)\n\n    # Transpose back if necessary\n    if ns == 1:\n        shifted = shifted.T\n\n    return shifted\n</code></pre>"},{"location":"reference/neuro_py/process/utils/#neuro_py.process.utils.compute_image_spread","title":"<code>compute_image_spread(X, exponent=2, normalize=True)</code>","text":"<p>Compute the spread of an image using the square root of a weighted moment.</p> <p>The spread is calculated as the square root of a weighted moment of the image, where the weights are derived from the deviations of each pixel from the center of mass (COM) of the image.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2D numpy array of shape (numBinsY, numBinsX). If <code>normalize</code> is True, the input is assumed to represent a probability distribution.</p> required <code>exponent</code> <code>float</code> <p>The exponent used in the moment calculation. Default is 2.</p> <code>2</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the input array so that its sum is 1. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>spread</code> <code>float</code> <p>The computed spread, defined as the square root of the weighted moment.</p> <code>image_moment</code> <code>float</code> <p>The raw weighted moment of the image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; X = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n&gt;&gt;&gt; spread, image_moment = compute_image_spread(X, exponent=2)\n&gt;&gt;&gt; print(spread)\n0.5704157028642128\n&gt;&gt;&gt; print(image_moment)\n0.325374074074074\n</code></pre> References <p>Widloski &amp; Foster, 2022</p> Source code in <code>neuro_py/process/utils.py</code> <pre><code>def compute_image_spread(\n    X: np.ndarray, exponent: float = 2, normalize: bool = True\n) -&gt; Tuple[float, float]:\n    \"\"\"\n    Compute the spread of an image using the square root of a weighted moment.\n\n    The spread is calculated as the square root of a weighted moment of the image,\n    where the weights are derived from the deviations of each pixel from the\n    center of mass (COM) of the image.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        A 2D numpy array of shape (numBinsY, numBinsX). If `normalize` is True,\n        the input is assumed to represent a probability distribution.\n    exponent : float, optional\n        The exponent used in the moment calculation. Default is 2.\n    normalize : bool, optional\n        If True, normalize the input array so that its sum is 1. Default is True.\n\n    Returns\n    -------\n    spread : float\n        The computed spread, defined as the square root of the weighted moment.\n    image_moment : float\n        The raw weighted moment of the image.\n\n    Examples\n    --------\n    &gt;&gt;&gt; X = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n    &gt;&gt;&gt; spread, image_moment = compute_image_spread(X, exponent=2)\n    &gt;&gt;&gt; print(spread)\n    0.5704157028642128\n    &gt;&gt;&gt; print(image_moment)\n    0.325374074074074\n\n    References\n    ----------\n    Widloski &amp; Foster, 2022\n    \"\"\"\n    if np.allclose(X, 0):\n        return np.nan, np.nan  # Return NaN if the input is all zero\n\n    if normalize:\n        X = X / np.nansum(X)  # Normalize the input\n\n    numBinsY, numBinsX = X.shape\n\n    # Compute center of mass (COM) for the X (columns) direction.\n    cols = np.arange(1, numBinsX + 1)\n    sumX = np.nansum(X, axis=0)  # sum over rows, shape: (numBinsX,)\n    totalX = np.nansum(sumX)\n    # Add a small correction term\n    comX = np.nansum(sumX * cols) / totalX + 0.5 / numBinsX\n\n    # Compute center of mass for the Y (rows) direction.\n    rows = np.arange(1, numBinsY + 1)\n    sumY = np.nansum(X, axis=1)  # sum over columns, shape: (numBinsY,)\n    totalY = np.nansum(sumY)\n    comY = np.nansum(sumY * rows) / totalY + 0.5 / numBinsY\n\n    # Create a meshgrid for the bin indices (using 1-indexing like MATLAB)\n    XX, YY = np.meshgrid(np.arange(1, numBinsX + 1), np.arange(1, numBinsY + 1))\n\n    # Compute the weighted moment using the product of the deviations raised to the given exponent.\n    # For each bin, we compute:\n    #     |XX - comX|^exponent * |YY - comY|^exponent * X(i,j)\n    moment = np.nansum(\n        (np.abs(XX - comX) ** exponent) * (np.abs(YY - comY) ** exponent) * X\n    )\n\n    # Normalize by the total probability.\n    image_moment = moment / np.nansum(X)\n\n    # The spread is the square root of the image moment.\n    spread = np.sqrt(image_moment)\n\n    return spread, image_moment\n</code></pre>"},{"location":"reference/neuro_py/process/utils/#neuro_py.process.utils.remove_inactive_cells","title":"<code>remove_inactive_cells(st, cell_metrics=None, epochs=None, min_spikes=100)</code>","text":"<p>remove_inactive_cells: Remove cells with fewer than min_spikes spikes per sub-epoch</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>SpikeTrainArray</code> <p>SpikeTrainArray object containing spike times for multiple cells.</p> required <code>cell_metrics</code> <code>DataFrame</code> <p>DataFrame containing metrics for each cell (e.g., quality metrics).</p> <code>None</code> <code>epochs</code> <code>EpochArray or list of EpochArray</code> <p>If a list of EpochArray objects is provided, each EpochArray object is treated as a sub-epoch. If a single EpochArray object is provided, each interval in the EpochArray object is treated as a sub-epoch.</p> <code>None</code> <code>min_spikes</code> <code>int</code> <p>Minimum number of spikes required per sub-epoch to retain a cell. Default is 100.</p> <code>100</code> <p>Returns:</p> Type Description <code>Tuple[SpikeTrainArray, Union[DataFrame, None]]</code> <p>A tuple containing: - SpikeTrainArray object with inactive cells removed. - DataFrame containing cell metrics with inactive cells removed (if provided).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.process.intervals import truncate_epoch\n&gt;&gt;&gt; from neuro_py.session.locate_epochs import (\n&gt;&gt;&gt;     find_multitask_pre_post,\n&gt;&gt;&gt;     compress_repeated_epochs,\n&gt;&gt;&gt; )\n&gt;&gt;&gt; from neuro_py.io import loading\n&gt;&gt;&gt; import nelpy as nel\n&gt;&gt;&gt; from neuro_py.process.utils import remove_inactive_cells\n</code></pre> <pre><code>&gt;&gt;&gt; # load data from session\n&gt;&gt;&gt; basepath = r\"Z:\\Data\\hpc_ctx_project\\HP04\\day_1_20240320\"\n</code></pre> <pre><code>&gt;&gt;&gt; # load spikes and cell metrics (cm)\n&gt;&gt;&gt; st, cm = loading.load_spikes(basepath, brainRegion=\"CA1\", putativeCellType=\"Pyr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # load epochs and apply multitask epoch restrictions\n&gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n&gt;&gt;&gt; epoch_df = compress_repeated_epochs(epoch_df)\n&gt;&gt;&gt; pre_task_post = find_multitask_pre_post(\n&gt;&gt;&gt;     epoch_df.environment, post_sleep_flank=True, pre_sleep_common=True\n&gt;&gt;&gt; )\n</code></pre> <pre><code>&gt;&gt;&gt; beh_epochs = nel.EpochArray(\n&gt;&gt;&gt;     epoch_df.iloc[pre_task_post[0]][[\"startTime\", \"stopTime\"]].values\n&gt;&gt;&gt; )\n&gt;&gt;&gt; # load sleep states to restrict to NREM and theta\n&gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n&gt;&gt;&gt; nrem_epochs = nel.EpochArray(\n&gt;&gt;&gt;     state_dict[\"NREMstate\"],\n&gt;&gt;&gt; )\n&gt;&gt;&gt; theta_epochs = nel.EpochArray(\n&gt;&gt;&gt;     state_dict[\"THETA\"],\n&gt;&gt;&gt; )\n&gt;&gt;&gt; # create list of restricted epochs\n&gt;&gt;&gt; restict_epochs = []\n&gt;&gt;&gt; for epoch, epoch_label in zip(beh_epochs, [\"pre\", \"task\", \"post\"]):\n&gt;&gt;&gt;     if epoch_label in \"pre\":\n&gt;&gt;&gt;         # get cumulative hours of sleep\n&gt;&gt;&gt;         epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=3600)\n&gt;&gt;&gt;     elif epoch_label in \"post\":\n&gt;&gt;&gt;         # get cumulative hours of sleep\n&gt;&gt;&gt;         epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=3600)\n&gt;&gt;&gt;     else:\n&gt;&gt;&gt;         # get theta during task\n&gt;&gt;&gt;         epoch_restrict = epoch &amp; theta_epochs\n&gt;&gt;&gt;     restict_epochs.append(epoch_restrict)\n</code></pre> <pre><code>&gt;&gt;&gt; # remove inactive cells\n&gt;&gt;&gt; st, cm = remove_inactive_cells(st, cm, restict_epochs)\n</code></pre> Source code in <code>neuro_py/process/utils.py</code> <pre><code>def remove_inactive_cells(\n    st: nel.core._eventarray.SpikeTrainArray,\n    cell_metrics: Union[pd.DataFrame, None] = None,\n    epochs: Union[\n        List[nel.core._intervalarray.EpochArray],\n        nel.core._intervalarray.EpochArray,\n        None,\n    ] = None,\n    min_spikes: int = 100,\n) -&gt; Tuple[nel.core._eventarray.SpikeTrainArray, Union[pd.DataFrame, None]]:\n    \"\"\"\n    remove_inactive_cells: Remove cells with fewer than min_spikes spikes per sub-epoch\n\n    Parameters\n    ----------\n    st : SpikeTrainArray\n        SpikeTrainArray object containing spike times for multiple cells.\n\n    cell_metrics : pd.DataFrame, optional\n        DataFrame containing metrics for each cell (e.g., quality metrics).\n\n    epochs : EpochArray or list of EpochArray, optional\n        If a list of EpochArray objects is provided, each EpochArray object\n        is treated as a sub-epoch. If a single EpochArray object is provided,\n        each interval in the EpochArray object is treated as a sub-epoch.\n\n    min_spikes : int, optional\n        Minimum number of spikes required per sub-epoch to retain a cell.\n        Default is 100.\n\n    Returns\n    -------\n    Tuple[SpikeTrainArray, Union[pd.DataFrame, None]]\n        A tuple containing:\n        - SpikeTrainArray object with inactive cells removed.\n        - DataFrame containing cell metrics with inactive cells removed (if provided).\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.process.intervals import truncate_epoch\n    &gt;&gt;&gt; from neuro_py.session.locate_epochs import (\n    &gt;&gt;&gt;     find_multitask_pre_post,\n    &gt;&gt;&gt;     compress_repeated_epochs,\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; from neuro_py.io import loading\n    &gt;&gt;&gt; import nelpy as nel\n    &gt;&gt;&gt; from neuro_py.process.utils import remove_inactive_cells\n\n    &gt;&gt;&gt; # load data from session\n    &gt;&gt;&gt; basepath = r\"Z:\\Data\\hpc_ctx_project\\HP04\\day_1_20240320\"\n\n    &gt;&gt;&gt; # load spikes and cell metrics (cm)\n    &gt;&gt;&gt; st, cm = loading.load_spikes(basepath, brainRegion=\"CA1\", putativeCellType=\"Pyr\")\n\n    &gt;&gt;&gt; # load epochs and apply multitask epoch restrictions\n    &gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n    &gt;&gt;&gt; epoch_df = compress_repeated_epochs(epoch_df)\n    &gt;&gt;&gt; pre_task_post = find_multitask_pre_post(\n    &gt;&gt;&gt;     epoch_df.environment, post_sleep_flank=True, pre_sleep_common=True\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; beh_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     epoch_df.iloc[pre_task_post[0]][[\"startTime\", \"stopTime\"]].values\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; # load sleep states to restrict to NREM and theta\n    &gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n    &gt;&gt;&gt; nrem_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     state_dict[\"NREMstate\"],\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; theta_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     state_dict[\"THETA\"],\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; # create list of restricted epochs\n    &gt;&gt;&gt; restict_epochs = []\n    &gt;&gt;&gt; for epoch, epoch_label in zip(beh_epochs, [\"pre\", \"task\", \"post\"]):\n    &gt;&gt;&gt;     if epoch_label in \"pre\":\n    &gt;&gt;&gt;         # get cumulative hours of sleep\n    &gt;&gt;&gt;         epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=3600)\n    &gt;&gt;&gt;     elif epoch_label in \"post\":\n    &gt;&gt;&gt;         # get cumulative hours of sleep\n    &gt;&gt;&gt;         epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=3600)\n    &gt;&gt;&gt;     else:\n    &gt;&gt;&gt;         # get theta during task\n    &gt;&gt;&gt;         epoch_restrict = epoch &amp; theta_epochs\n    &gt;&gt;&gt;     restict_epochs.append(epoch_restrict)\n\n    &gt;&gt;&gt; # remove inactive cells\n    &gt;&gt;&gt; st, cm = remove_inactive_cells(st, cm, restict_epochs)\n    \"\"\"\n\n    def return_results(st, cell_metrics):\n        if cell_metrics is None:\n            return st\n        else:\n            return st, cell_metrics\n\n    # check data types\n    if not isinstance(st, nel.core._eventarray.SpikeTrainArray):\n        raise ValueError(\"st must be a SpikeTrainArray object\")\n\n    if not isinstance(cell_metrics, (pd.core.frame.DataFrame, type(None))):\n        raise ValueError(\"cell_metrics must be a DataFrame object\")\n\n    if not isinstance(epochs, (nel.core._intervalarray.EpochArray, list)):\n        raise ValueError(\n            \"epochs must be an EpochArray object or a list of EpochArray objects\"\n        )\n\n    if isinstance(epochs, list):\n        for epoch in epochs:\n            if not isinstance(epoch, nel.core._intervalarray.EpochArray):\n                raise ValueError(\"list of epochs must contain EpochArray objects\")\n\n    # check if st is empty\n    if st.isempty:\n        return return_results(st, cell_metrics)\n\n    # check if epochs is empty\n    if isinstance(epochs, nel.core._intervalarray.EpochArray):\n        if epochs.isempty:\n            return return_results(st, cell_metrics)\n\n    # check if cell_metrics is empty\n    if cell_metrics is not None and cell_metrics.empty:\n        return return_results(st, cell_metrics)\n\n    # check if min_spikes is less than 1\n    if min_spikes &lt; 1:\n        return return_results(st, cell_metrics)\n\n    # check if st and cell_metrics have the same number of units\n    if cell_metrics is not None and st.n_units != cell_metrics.shape[0]:\n        # assert error message\n        raise ValueError(\"st and cell_metrics must have the same number of units\")\n\n    spk_thres_met = []\n    # check if each cell has at least min_spikes spikes in each epoch\n    for epoch_restrict in epochs:\n        if st[epoch_restrict].isempty:\n            spk_thres_met.append([False] * st.n_units)\n            continue\n        spk_thres_met.append(st[epoch_restrict].n_events &gt;= min_spikes)\n\n    good_idx = np.vstack(spk_thres_met).all(axis=0)\n\n    # remove inactive cells\n    st = st.iloc[:, good_idx]\n    if cell_metrics is not None:\n        cell_metrics = cell_metrics[good_idx]\n\n    return return_results(st, cell_metrics)\n</code></pre>"},{"location":"reference/neuro_py/process/utils/#neuro_py.process.utils.remove_inactive_cells_pre_task_post","title":"<code>remove_inactive_cells_pre_task_post(st, cell_metrics=None, beh_epochs=None, nrem_epochs=None, theta_epochs=None, min_spikes=100, nrem_time=3600)</code>","text":"<p>remove_inactive_cells_pre_task_post: Remove cells with fewer than min_spikes spikes per pre/task/post</p> <p>Parameters:</p> Name Type Description Default <code>st</code> <code>SpikeTrainArray</code> <p>SpikeTrainArray object containing spike times for multiple cells.</p> required <code>cell_metrics</code> <code>DataFrame</code> <p>DataFrame containing metrics for each cell (e.g., quality metrics).</p> <code>None</code> <code>beh_epochs</code> <code>EpochArray</code> <p>EpochArray object containing pre/task/post epochs.</p> <code>None</code> <code>nrem_epochs</code> <code>EpochArray</code> <p>EpochArray object containing NREM epochs.</p> <code>None</code> <code>theta_epochs</code> <code>EpochArray</code> <p>EpochArray object containing theta epochs.</p> <code>None</code> <code>min_spikes</code> <code>int</code> <p>Minimum number of spikes required per pre/task/post. Default is 100.</p> <code>100</code> <code>nrem_time</code> <code>int or float</code> <p>Time in seconds to truncate NREM epochs. Default is 3600 seconds.</p> <code>3600</code> <p>Returns:</p> Type Description <code>Tuple[SpikeTrainArray, Union[DataFrame, None]]</code> <p>A tuple containing: - SpikeTrainArray object with inactive cells removed. - DataFrame containing cell metrics with inactive cells removed (if provided).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from neuro_py.process.utils import remove_inactive_cells_pre_task_post\n&gt;&gt;&gt; from neuro_py.io import loading\n&gt;&gt;&gt; from neuro_py.session.locate_epochs import (\n&gt;&gt;&gt;     find_multitask_pre_post,\n&gt;&gt;&gt;     compress_repeated_epochs,\n&gt;&gt;&gt; )\n&gt;&gt;&gt; mport nelpy as nel\n</code></pre> <pre><code>&gt;&gt;&gt; # load data from session\n&gt;&gt;&gt; basepath = r\"Z:\\Data\\hpc_ctx_project\\HP04\\day_1_20240320\"\n</code></pre> <pre><code>&gt;&gt;&gt; # load spikes and cell metrics (cm)\n&gt;&gt;&gt; st, cm = loading.load_spikes(basepath, brainRegion=\"CA1\", putativeCellType=\"Pyr\")\n</code></pre> <pre><code>&gt;&gt;&gt; # load epochs and apply multitask epoch restrictions\n&gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n&gt;&gt;&gt; epoch_df = compress_repeated_epochs(epoch_df)\n&gt;&gt;&gt; pre_task_post = find_multitask_pre_post(\n&gt;&gt;&gt;     epoch_df.environment, post_sleep_flank=True, pre_sleep_common=True\n&gt;&gt;&gt; )\n</code></pre> <pre><code>&gt;&gt;&gt; beh_epochs = nel.EpochArray(\n&gt;&gt;&gt;     epoch_df.iloc[pre_task_post[0]][[\"startTime\", \"stopTime\"]].values\n&gt;&gt;&gt; )\n</code></pre> <pre><code>&gt;&gt;&gt; # load sleep states to restrict to NREM and theta\n&gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n&gt;&gt;&gt; nrem_epochs = nel.EpochArray(\n&gt;&gt;&gt;     state_dict[\"NREMstate\"],\n&gt;&gt;&gt; )\n&gt;&gt;&gt; theta_epochs = nel.EpochArray(\n&gt;&gt;&gt;     state_dict[\"THETA\"],\n&gt;&gt;&gt; )\n</code></pre> <pre><code>&gt;&gt;&gt; st,cm = remove_inactive_cells_pre_task_post(st,cm,beh_epochs,nrem_epochs,theta_epochs)\n</code></pre> Source code in <code>neuro_py/process/utils.py</code> <pre><code>def remove_inactive_cells_pre_task_post(\n    st: nel.core._eventarray.SpikeTrainArray,\n    cell_metrics: Union[pd.core.frame.DataFrame, None] = None,\n    beh_epochs: nel.core._intervalarray.EpochArray = None,\n    nrem_epochs: nel.core._intervalarray.EpochArray = None,\n    theta_epochs: nel.core._intervalarray.EpochArray = None,\n    min_spikes: int = 100,\n    nrem_time: Union[int, float] = 3600,\n) -&gt; tuple:\n    \"\"\"\n    remove_inactive_cells_pre_task_post: Remove cells with fewer than min_spikes spikes per pre/task/post\n\n    Parameters\n    ----------\n    st : SpikeTrainArray\n        SpikeTrainArray object containing spike times for multiple cells.\n\n    cell_metrics : pd.DataFrame, optional\n        DataFrame containing metrics for each cell (e.g., quality metrics).\n\n    beh_epochs : EpochArray\n        EpochArray object containing pre/task/post epochs.\n\n    nrem_epochs : EpochArray\n        EpochArray object containing NREM epochs.\n\n    theta_epochs : EpochArray\n        EpochArray object containing theta epochs.\n\n    min_spikes : int, optional\n        Minimum number of spikes required per pre/task/post. Default is 100.\n\n    nrem_time : int or float, optional\n        Time in seconds to truncate NREM epochs. Default is 3600 seconds.\n\n    Returns\n    -------\n    Tuple[nel.core._eventarray.SpikeTrainArray, Union[pd.DataFrame, None]]\n        A tuple containing:\n        - SpikeTrainArray object with inactive cells removed.\n        - DataFrame containing cell metrics with inactive cells removed (if provided).\n\n    Examples\n    -------\n    &gt;&gt;&gt; from neuro_py.process.utils import remove_inactive_cells_pre_task_post\n    &gt;&gt;&gt; from neuro_py.io import loading\n    &gt;&gt;&gt; from neuro_py.session.locate_epochs import (\n    &gt;&gt;&gt;     find_multitask_pre_post,\n    &gt;&gt;&gt;     compress_repeated_epochs,\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; mport nelpy as nel\n\n    &gt;&gt;&gt; # load data from session\n    &gt;&gt;&gt; basepath = r\"Z:\\Data\\hpc_ctx_project\\HP04\\day_1_20240320\"\n\n    &gt;&gt;&gt; # load spikes and cell metrics (cm)\n    &gt;&gt;&gt; st, cm = loading.load_spikes(basepath, brainRegion=\"CA1\", putativeCellType=\"Pyr\")\n\n    &gt;&gt;&gt; # load epochs and apply multitask epoch restrictions\n    &gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n    &gt;&gt;&gt; epoch_df = compress_repeated_epochs(epoch_df)\n    &gt;&gt;&gt; pre_task_post = find_multitask_pre_post(\n    &gt;&gt;&gt;     epoch_df.environment, post_sleep_flank=True, pre_sleep_common=True\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; beh_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     epoch_df.iloc[pre_task_post[0]][[\"startTime\", \"stopTime\"]].values\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; # load sleep states to restrict to NREM and theta\n    &gt;&gt;&gt; state_dict = loading.load_SleepState_states(basepath)\n    &gt;&gt;&gt; nrem_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     state_dict[\"NREMstate\"],\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt; theta_epochs = nel.EpochArray(\n    &gt;&gt;&gt;     state_dict[\"THETA\"],\n    &gt;&gt;&gt; )\n\n    &gt;&gt;&gt; st,cm = remove_inactive_cells_pre_task_post(st,cm,beh_epochs,nrem_epochs,theta_epochs)\n    \"\"\"\n\n    # check data types (further checks are done in remove_inactive_cells)\n    if not isinstance(beh_epochs, nel.core._intervalarray.EpochArray):\n        raise ValueError(\"beh_epochs must be an EpochArray object\")\n\n    if not isinstance(nrem_epochs, nel.core._intervalarray.EpochArray):\n        raise ValueError(\"nrem_epochs must be an EpochArray object\")\n\n    if not isinstance(theta_epochs, nel.core._intervalarray.EpochArray):\n        raise ValueError(\"theta_epochs must be an EpochArray object\")\n\n    # create list of restricted epochs\n    restict_epochs = []\n    for epoch, epoch_label in zip(beh_epochs, [\"pre\", \"task\", \"post\"]):\n        if epoch_label in \"pre\":\n            # get cumulative hours of sleep\n            epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=nrem_time)\n        elif epoch_label in \"post\":\n            # get cumulative hours of sleep\n            epoch_restrict = truncate_epoch(epoch &amp; nrem_epochs, time=nrem_time)\n        else:\n            # get theta during task\n            epoch_restrict = epoch &amp; theta_epochs\n        restict_epochs.append(epoch_restrict)\n\n    return remove_inactive_cells(\n        st, cell_metrics, restict_epochs, min_spikes=min_spikes\n    )\n</code></pre>"},{"location":"reference/neuro_py/raw/","title":"neuro_py.raw","text":""},{"location":"reference/neuro_py/raw/#neuro_py.raw.cut_artifacts","title":"<code>cut_artifacts(filepath, n_channels, cut_intervals, precision='int16', output_filepath=None)</code>","text":"<p>Remove user-defined periods from recordings in a binary file, resulting in a shorter file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the original binary file.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels in the file.</p> required <code>cut_intervals</code> <code>List[Tuple[int, int]]</code> <p>List of intervals (start, end) in sample indices to remove. Assumes sorted and non-overlapping.</p> required <code>precision</code> <code>str</code> <p>Data precision, by default \"int16\".</p> <code>'int16'</code> <code>output_filepath</code> <code>str</code> <p>Path to save the modified binary file. If None, appends \"_cut\" to the original filename.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/raw/preprocessing.py</code> <pre><code>def cut_artifacts(\n    filepath: str,\n    n_channels: int,\n    cut_intervals: List[Tuple[int, int]],\n    precision: str = \"int16\",\n    output_filepath: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Remove user-defined periods from recordings in a binary file, resulting in a shorter file.\n\n    Parameters\n    ----------\n    filepath : str\n        Path to the original binary file.\n    n_channels : int\n        Number of channels in the file.\n    cut_intervals : List[Tuple[int, int]]\n        List of intervals (start, end) in sample indices to remove. Assumes sorted and non-overlapping.\n    precision : str, optional\n        Data precision, by default \"int16\".\n    output_filepath : str, optional\n        Path to save the modified binary file. If None, appends \"_cut\" to the original filename.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"File '{filepath}' does not exist.\")\n\n    # Set default output filepath\n    if output_filepath is None:\n        output_filepath = os.path.splitext(filepath)[0] + \"_cut.dat\"\n\n    # Check for valid intervals\n    for start, end in cut_intervals:\n        if start &gt;= end:\n            raise ValueError(\n                f\"Invalid interval: ({start}, {end}). Start must be less than end.\"\n            )\n\n    # Map the original file and calculate parameters\n    bytes_size = np.dtype(precision).itemsize\n    with open(filepath, \"rb\") as f:\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n\n    data = np.memmap(filepath, dtype=precision, mode=\"r\", shape=(n_samples, n_channels))\n\n    # Identify the indices to keep\n    keep_mask = np.ones(n_samples, dtype=bool)\n    for start, end in cut_intervals:\n        if 0 &lt;= start &lt; n_samples and 0 &lt; end &lt;= n_samples:\n            keep_mask[start:end] = False\n        else:\n            warnings.warn(\n                f\"Interval ({start}, {end}) is out of bounds and was skipped.\"\n            )\n\n    keep_indices = np.flatnonzero(keep_mask)\n\n    # Create a new binary file with only the retained data\n    with open(output_filepath, \"wb\") as output_file:\n        for start_idx in range(0, len(keep_indices), 10_000):  # Process in chunks\n            chunk_indices = keep_indices[start_idx : start_idx + 10_000]\n            output_file.write(data[chunk_indices].tobytes())\n\n    del data  # Release memory-mapped file\n</code></pre>"},{"location":"reference/neuro_py/raw/#neuro_py.raw.cut_artifacts_intan","title":"<code>cut_artifacts_intan(folder_name, n_channels_amplifier, cut_intervals, verbose=True)</code>","text":"<p>Cut specified artifact intervals from Intan data files.</p> <p>This function iterates through a set of Intan data files (amplifier, auxiliary, digitalin, digitalout, analogin, time, and supply), and for each file, it removes artifacts within the specified intervals by invoking the <code>cut_artifacts</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>folder_name</code> <code>str</code> <p>The folder where the Intan data files are located.</p> required <code>n_channels_amplifier</code> <code>int</code> <p>The number of amplifier channels used in the amplifier data file.</p> required <code>cut_intervals</code> <code>List[Tuple[int, int]]</code> <p>A list of intervals (start, end) in sample indices to remove artifacts. Each tuple represents the start and end sample index for an artifact to be cut. Assumes sorted and non-overlapping intervals.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This function modifies the files in place, so there is no return value.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the amplifier data file does not exist in the provided folder.</p> <code>ValueError</code> <p>If video files are found in the folder, as this function does not support video files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fs = 20_000\n&gt;&gt;&gt; cut_artifacts_intan(\n...     folder_name = r\"path/to/data\",\n...     n_channels_amplifier = 128,\n...     cut_intervals = (np.array([[394.4, 394.836], [400, 401], [404, 405]]) * fs).astype(int)\n... )\n</code></pre> Source code in <code>neuro_py/raw/preprocessing.py</code> <pre><code>def cut_artifacts_intan(\n    folder_name: str,\n    n_channels_amplifier: int,\n    cut_intervals: List[Tuple[int, int]],\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Cut specified artifact intervals from Intan data files.\n\n    This function iterates through a set of Intan data files (amplifier, auxiliary,\n    digitalin, digitalout, analogin, time, and supply), and for each file, it removes\n    artifacts within the specified intervals by invoking the `cut_artifacts` function.\n\n    Parameters\n    ----------\n    folder_name : str\n        The folder where the Intan data files are located.\n    n_channels_amplifier : int\n        The number of amplifier channels used in the amplifier data file.\n    cut_intervals : List[Tuple[int, int]]\n        A list of intervals (start, end) in sample indices to remove artifacts.\n        Each tuple represents the start and end sample index for an artifact to be cut.\n        Assumes sorted and non-overlapping intervals.\n\n    Returns\n    -------\n    None\n        This function modifies the files in place, so there is no return value.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the amplifier data file does not exist in the provided folder.\n    ValueError\n        If video files are found in the folder, as this function does not support video files.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fs = 20_000\n    &gt;&gt;&gt; cut_artifacts_intan(\n    ...     folder_name = r\"path/to/data\",\n    ...     n_channels_amplifier = 128,\n    ...     cut_intervals = (np.array([[394.4, 394.836], [400, 401], [404, 405]]) * fs).astype(int)\n    ... )\n    \"\"\"\n\n    # refuse to cut artifacts if any video file exist in folder\n    video_files = [f for f in os.listdir(folder_name) if f.endswith(\".avi\")]\n    if video_files:\n        raise ValueError(f\"Video files found in folder, refusing to cut: {video_files}\")\n\n    # Define data types for each file (from Intan documentation)\n    files_table = {\n        \"amplifier\": \"int16\",\n        \"auxiliary\": \"uint16\",\n        \"digitalin\": \"uint16\",\n        \"digitalout\": \"uint16\",\n        \"analogin\": \"uint16\",\n        \"time\": \"int32\",\n        \"supply\": \"uint16\",\n    }\n\n    # determine number of samples from amplifier file\n    amplifier_file_path = os.path.join(folder_name, \"amplifier.dat\")\n    if not os.path.exists(amplifier_file_path):\n        raise FileNotFoundError(f\"File '{amplifier_file_path}' does not exist.\")\n\n    # get number of bytes per sample\n    bytes_size = np.dtype(files_table[\"amplifier\"]).itemsize\n\n    # each file should have the same number of samples\n    n_samples = os.path.getsize(amplifier_file_path) // (\n        n_channels_amplifier * bytes_size\n    )\n\n    for file_name, precision in files_table.items():\n        file_path = os.path.join(folder_name, f\"{file_name}.dat\")\n\n        if os.path.exists(file_path):\n            if verbose:\n                print(f\"Processing {file_name}.dat file...\")\n\n            # get number of bytes per sample\n            bytes_size = np.dtype(precision).itemsize\n\n            # determine number of channels from n_samples\n            n_channels = int(os.path.getsize(file_path) / n_samples / bytes_size)\n\n            # for time file, cut and offset timestamps\n            if file_name == \"time\":\n                output_filepath = os.path.splitext(file_path)[0] + \"_cut.dat\"\n\n                with open(output_filepath, \"wb\") as output_file:\n                    # time indices as continuous array\n                    filtered_time = np.arange(\n                        n_samples - sum(end - start for start, end in cut_intervals),\n                        dtype=np.int32,\n                    )\n\n                    # write to file\n                    output_file.write(filtered_time.tobytes())\n            else:\n                # cut artifacts\n                cut_artifacts(file_path, n_channels, cut_intervals, precision)\n\n    # Calculate the expected number of samples after cutting\n    total_samples_cut = sum(end - start for start, end in cut_intervals)\n    expected_n_samples = n_samples - total_samples_cut\n\n    # === Validation Section ===\n    # Verify all `_cut.dat` files have the correct number of samples\n    for file_name, precision in files_table.items():\n        output_file_path = os.path.join(folder_name, f\"{file_name}_cut.dat\")\n        original_file_path = os.path.join(folder_name, f\"{file_name}.dat\")\n\n        if os.path.exists(output_file_path) and os.path.exists(original_file_path):\n            # Dynamically calculate the number of channels\n            bytes_size = np.dtype(precision).itemsize\n            n_channels = os.path.getsize(original_file_path) // (n_samples * bytes_size)\n\n            # Calculate the expected file size\n            expected_size = expected_n_samples * n_channels * bytes_size\n            actual_size = os.path.getsize(output_file_path)\n\n            if actual_size != expected_size:\n                raise RuntimeError(\n                    f\"{file_name}_cut.dat has an incorrect size. \"\n                    f\"Expected {expected_size} bytes but found {actual_size} bytes.\"\n                )\n</code></pre>"},{"location":"reference/neuro_py/raw/#neuro_py.raw.fill_missing_channels","title":"<code>fill_missing_channels(basepath, n_channels, filename, missing_channels, precision='int16', chunk_size=10000)</code>","text":"<p>Fill missing channels in a large binary file with zeros, processing in chunks. This function is useful when some channels were accidently deactivated during recording.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder containing the binary file.</p> required <code>n_channels</code> <code>int</code> <p>Total number of channels in the binary file (including the missing ones).</p> required <code>filename</code> <code>str</code> <p>Name of the binary file to modify.</p> required <code>missing_channels</code> <code>List[int]</code> <p>List of missing channel indices to be filled with zeros.</p> required <code>precision</code> <code>str</code> <p>Data precision, by default \"int16\".</p> <code>'int16'</code> <code>chunk_size</code> <code>int</code> <p>Number of samples per chunk, by default 10,000.</p> <code>10000</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the modified binary file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fill_missing_channels(\n...    r\"U:\\data\\hpc_ctx_project\\HP13\\HP13_day1_20241030\\HP13_cheeseboard_241030_153710\",\n...    128,\n...    'amplifier.dat',\n...    missing_channels = [0]\n... )\n</code></pre> Source code in <code>neuro_py/raw/preprocessing.py</code> <pre><code>def fill_missing_channels(\n    basepath: str,\n    n_channels: int,\n    filename: str,\n    missing_channels: List[int],\n    precision: str = \"int16\",\n    chunk_size: int = 10_000,\n) -&gt; str:\n    \"\"\"\n    Fill missing channels in a large binary file with zeros, processing in chunks.\n    This function is useful when some channels were accidently deactivated during recording.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder containing the binary file.\n    n_channels : int\n        Total number of channels in the binary file (including the missing ones).\n    filename : str\n        Name of the binary file to modify.\n    missing_channels : List[int]\n        List of missing channel indices to be filled with zeros.\n    precision : str, optional\n        Data precision, by default \"int16\".\n    chunk_size : int, optional\n        Number of samples per chunk, by default 10,000.\n\n    Returns\n    -------\n    str\n        Path to the modified binary file.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fill_missing_channels(\n    ...    r\"U:\\\\data\\\\hpc_ctx_project\\\\HP13\\\\HP13_day1_20241030\\\\HP13_cheeseboard_241030_153710\",\n    ...    128,\n    ...    'amplifier.dat',\n    ...    missing_channels = [0]\n    ... )\n    \"\"\"\n    file_path = os.path.join(basepath, filename)\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"Binary file '{file_path}' does not exist.\")\n\n    dtype = np.dtype(precision)\n    bytes_per_sample = dtype.itemsize\n    present_channels = [ch for ch in range(n_channels) if ch not in missing_channels]\n\n    # Calculate total number of samples\n    file_size = os.path.getsize(file_path)\n    n_samples = file_size // (bytes_per_sample * (n_channels - len(missing_channels)))\n    if file_size % (bytes_per_sample * (n_channels - len(missing_channels))) != 0:\n        raise ValueError(\"Data size is not consistent with expected shape.\")\n\n    # Prepare output file path\n    new_file_path = os.path.join(basepath, f\"corrected_{filename}\")\n\n    # Process file in chunks\n    with open(file_path, \"rb\") as f_in, open(new_file_path, \"wb\") as f_out:\n        for start in range(0, n_samples, chunk_size):\n            # Read a chunk of data\n            chunk = np.fromfile(\n                f_in,\n                dtype=dtype,\n                count=chunk_size * (n_channels - len(missing_channels)),\n            )\n            chunk = chunk.reshape(-1, n_channels - len(missing_channels))\n\n            # Create a new array with missing channels filled with zeros\n            chunk_full = np.zeros((chunk.shape[0], n_channels), dtype=dtype)\n            chunk_full[:, present_channels] = chunk\n\n            # Write the chunk with missing channels added to the new file\n            chunk_full.tofile(f_out)\n\n    return new_file_path\n</code></pre>"},{"location":"reference/neuro_py/raw/#neuro_py.raw.phy_log_to_epocharray","title":"<code>phy_log_to_epocharray(filename, merge_gap=30)</code>","text":"<p>Extract timestamps from a Phy log file and convert them to a nel.EpochArray. Will estimate the amount of time it took to spikesort a session.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path to the Phy log file.</p> required <code>merge_gap</code> <code>float</code> <p>The number of seconds to merge timestamps, by default 30</p> <code>30</code> <p>Returns:</p> Type Description <code>EpochArray</code> <p>A nel.EpochArray containing the timestamps.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import neuro_py as npy\n&gt;&gt;&gt; filename = r\"D:\\KiloSort\\HP18\\hp18_day11_20250415\\Kilosort_2025-04-16_224949\\phy.log\"\n&gt;&gt;&gt; timestamps = npy.raw.phy_log_to_epocharray(filename)\n&gt;&gt;&gt; timestamps\n&lt;EpochArray at 0x1f6c7da5710: 80 epochs&gt; of length 4:02:01:591 hours\n</code></pre> Source code in <code>neuro_py/raw/spike_sorting.py</code> <pre><code>def phy_log_to_epocharray(filename: str, merge_gap: float = 30):\n    \"\"\"\n    Extract timestamps from a Phy log file and convert them to a nel.EpochArray.\n    Will estimate the amount of time it took to spikesort a session.\n\n    Parameters\n    ----------\n    filename : str\n        The path to the Phy log file.\n    merge_gap : float, optional\n        The number of seconds to merge timestamps, by default 30\n\n    Returns\n    -------\n    nel.EpochArray\n        A nel.EpochArray containing the timestamps.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import neuro_py as npy\n    &gt;&gt;&gt; filename = r\"D:\\KiloSort\\HP18\\hp18_day11_20250415\\Kilosort_2025-04-16_224949\\phy.log\"\n    &gt;&gt;&gt; timestamps = npy.raw.phy_log_to_epocharray(filename)\n    &gt;&gt;&gt; timestamps\n    &lt;EpochArray at 0x1f6c7da5710: 80 epochs&gt; of length 4:02:01:591 hours\n\n    \"\"\"\n\n    # Read the log file\n    try:\n        with open(filename, \"r\") as file:\n            log_lines = file.readlines()\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Log file not found: {filename}\")\n\n    # Define the regex pattern to extract timestamps\n    timestamp_pattern = re.compile(r\"\\x1b\\[\\d+m(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\")\n\n    # Extract timestamps using the regex pattern\n    timestamps = []\n    for line in log_lines:\n        match = timestamp_pattern.search(line)\n        if match:\n            timestamps.append(match.group(1))\n\n    # Create a Pandas DataFrame\n    df = pd.DataFrame(timestamps, columns=[\"Timestamp\"])\n\n    # Convert the 'Timestamp' column to datetime format\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], format=\"%H:%M:%S.%f\")\n\n    # Convert timestamps to total seconds (including milliseconds)\n    df[\"Seconds\"] = (\n        df[\"Timestamp\"].dt.hour * 3600\n        + df[\"Timestamp\"].dt.minute * 60\n        + df[\"Timestamp\"].dt.second\n        + df[\"Timestamp\"].dt.microsecond / 1e6\n    )\n    df[\"continous\"] = df.Seconds.diff().abs().cumsum()\n\n    intervals = np.array([df.continous[1:], df.continous[1:]]).T\n\n    return nel.EpochArray(intervals).merge(gap=merge_gap)\n</code></pre>"},{"location":"reference/neuro_py/raw/#neuro_py.raw.remove_artifacts","title":"<code>remove_artifacts(filepath, n_channels, zero_intervals, precision='int16', mode='linear', channels_to_remove=None)</code>","text":"<p>Silence user-defined periods from recordings in a binary file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the binary file.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels in the file.</p> required <code>zero_intervals</code> <code>List[Tuple[int, int]]</code> <p>List of intervals (start, end) in sample indices to zero out.</p> required <code>precision</code> <code>str</code> <p>Data precision, by default \"int16\".</p> <code>'int16'</code> <code>mode</code> <code>str</code> <p>Mode of interpolation. Options are:</p> <ul> <li>\"zeros\": Zero out the interval.</li> <li>\"linear\": Interpolate linearly between the start and end of the interval (default).</li> <li>\"gaussian\": (Not implemented, TBD) Interpolate using a Gaussian function with the same variance as in the recordings, on a per-channel basis.</li> </ul> <code>'linear'</code> <code>channels_to_remove</code> <code>List[int]</code> <p>List of channels (0-based indices) to remove artifacts from. If None, remove artifacts from all channels.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fs = 20_000\n&gt;&gt;&gt; remove_artifacts(\n...     r\"U:\\data\\hpc_ctx_project\\HP13\\HP13_day12_20241112\\HP13_day12_20241112.dat\",\n...     n_channels=128,\n...     zero_intervals=(bad_intervals.data * fs).astype(int),\n...     channels_to_remove=[0, 1, 2]  # Only remove artifacts from channels 0, 1, and 2\n... )\n</code></pre> Source code in <code>neuro_py/raw/preprocessing.py</code> <pre><code>def remove_artifacts(\n    filepath: str,\n    n_channels: int,\n    zero_intervals: List[Tuple[int, int]],\n    precision: str = \"int16\",\n    mode: str = \"linear\",\n    channels_to_remove: Optional[List[int]] = None,\n) -&gt; None:\n    \"\"\"\n    Silence user-defined periods from recordings in a binary file.\n\n    Parameters\n    ----------\n    filepath : str\n        Path to the binary file.\n    n_channels : int\n        Number of channels in the file.\n    zero_intervals : List[Tuple[int, int]]\n        List of intervals (start, end) in sample indices to zero out.\n    precision : str, optional\n        Data precision, by default \"int16\".\n    mode : str, optional\n        Mode of interpolation. Options are:\n\n        - **\"zeros\"**: Zero out the interval.\n        - **\"linear\"**: Interpolate linearly between the start and end of the interval (default).\n        - **\"gaussian\"**: *(Not implemented, TBD)* Interpolate using a Gaussian function with the same variance as in the recordings, on a per-channel basis.\n\n    channels_to_remove : List[int], optional\n        List of channels (0-based indices) to remove artifacts from. If None, remove artifacts from all channels.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; fs = 20_000\n    &gt;&gt;&gt; remove_artifacts(\n    ...     r\"U:\\\\data\\\\hpc_ctx_project\\\\HP13\\\\HP13_day12_20241112\\\\HP13_day12_20241112.dat\",\n    ...     n_channels=128,\n    ...     zero_intervals=(bad_intervals.data * fs).astype(int),\n    ...     channels_to_remove=[0, 1, 2]  # Only remove artifacts from channels 0, 1, and 2\n    ... )\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(filepath):\n        warnings.warn(\"File does not exist.\")\n        return\n\n    # Open the file in memory-mapped mode for read/write\n    bytes_size = np.dtype(precision).itemsize\n    with open(filepath, \"rb\") as f:\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n\n    # Map the file to memory in read-write mode\n    data = np.memmap(\n        filepath, dtype=precision, mode=\"r+\", shape=(n_samples, n_channels)\n    )\n    try:\n        # if shape is (2,) then it is a single interval, then add dimension\n        if np.shape(zero_intervals) == (2,):\n            zero_intervals = np.expand_dims(zero_intervals, axis=0)\n\n        # If no specific channels are provided, process all channels\n        if channels_to_remove is None:\n            channels_to_remove = list(range(n_channels))\n\n        # Zero out the specified intervals\n        if mode == \"zeros\":\n            zero_value = np.zeros((1, n_channels), dtype=precision)\n            for start, end in zero_intervals:\n                if 0 &lt;= start &lt; n_samples and 0 &lt; end &lt;= n_samples:\n                    data[start:end, channels_to_remove] = zero_value[\n                        0, channels_to_remove\n                    ]\n                else:\n                    warnings.warn(\n                        f\"Interval ({start}, {end}) is out of bounds and was skipped.\"\n                    )\n        elif mode == \"linear\":\n            for start, end in zero_intervals:\n                if 0 &lt;= start &lt; n_samples and 0 &lt; end &lt;= n_samples:\n                    for ch in channels_to_remove:\n                        # Compute float interpolation and round before casting\n                        interpolated = np.linspace(\n                            data[start, ch],\n                            data[end, ch],\n                            end - start,\n                        ).astype(data.dtype)  # Ensure consistent dtype\n                        data[start:end, ch] = interpolated\n                else:\n                    warnings.warn(\n                        f\"Interval ({start}, {end}) is out of bounds and was skipped.\"\n                    )\n        elif mode == \"gaussian\":\n            # not implemented error message\n            raise NotImplementedError(\"Gaussian mode not implemented.\")\n\n            # max_samples = 10_000\n            # rng = np.random.default_rng()\n\n            # # Compute valid regions and sample\n            # valid_mask = np.ones(n_samples, dtype=bool)\n            # for start, end in zero_intervals:\n            #     valid_mask[start:end] = False\n\n            # valid_indices = np.flatnonzero(valid_mask)\n            # sampled_indices = rng.choice(\n            #     valid_indices, size=min(max_samples, len(valid_indices)), replace=False\n            # )\n            # sampled_data = data[sampled_indices, :]\n\n            # # Compute mean and std for each channel\n            # means = np.mean(sampled_data, axis=0)\n            # stds = np.std(sampled_data, axis=0)\n\n            # from scipy.signal import butter, filtfilt\n\n            # def bandpass_filter(signal, lowcut, highcut, fs, order=4):\n            #     nyquist = 0.5 * fs\n            #     low = lowcut / nyquist\n            #     high = highcut / nyquist\n            #     b, a = butter(order, [low, high], btype=\"band\")\n            #     return filtfilt(b, a, signal, axis=0)\n\n            # # Parameters for bandpass filter\n            # lowcut = 0.5\n            # highcut = 100\n\n            # for start, end in zero_intervals:\n            #     if 0 &lt;= start &lt; n_samples and 0 &lt; end &lt;= n_samples:\n            #         interval_length = end - start\n            #         raw_noise = rng.normal(\n            #             loc=means, scale=stds, size=(interval_length, n_channels)\n            #         ).astype(precision)\n\n            #         # Apply bandpass filter with handling for potential issues\n            #         try:\n            #             filtered_noise = bandpass_filter(raw_noise, lowcut, highcut, fs)\n            #             filtered_noise = np.nan_to_num(filtered_noise, nan=0.0)\n            #         except ValueError:\n            #             warnings.warn(f\"Filtering failed for interval ({start}, {end}), skipping.\")\n            #             continue\n\n            #         # Prevent overwriting with unexpected data types\n            #         data[start:end, :] = filtered_noise.astype(data.dtype)\n            #     else:\n            #         warnings.warn(f\"Interval ({start}, {end}) is out of bounds and was skipped.\")\n\n    finally:\n        # Explicitly flush and release the memory-mapped object\n        data.flush()\n        del data\n        gc.collect()\n\n    # Save a log file with intervals zeroed out\n    log_file = os.path.splitext(filepath)[0] + \"_zeroed_intervals.log\"\n    try:\n        with open(log_file, \"w\") as f:\n            f.write(f\"Zeroed intervals: {zero_intervals.tolist()}\\n\")\n    except Exception as e:\n        warnings.warn(f\"Failed to create log file: {e}\")\n</code></pre>"},{"location":"reference/neuro_py/raw/#neuro_py.raw.reorder_channels","title":"<code>reorder_channels(file_path, n_channels, channel_order, precision='int16', num_processes=8)</code>","text":"<p>Reorder channels in a large binary file, processing in chunks. This function is useful when you want to reorder the channels in a binary file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file of the binary file to modify.</p> required <code>n_channels</code> <code>int</code> <p>Total number of channels in the binary file.</p> required <code>channel_order</code> <code>List[int]</code> <p>List of channel indices specifying the new order of channels.</p> required <code>precision</code> <code>str</code> <p>Data precision, by default \"int16\".</p> <code>'int16'</code> <code>chunk_size</code> <code>int</code> <p>Number of samples per chunk, by default 10,000.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; reorder_channels(\n...    r\"U:\\data\\hpc_ctx_project\\HP13\\HP13_day1_20241030\\HP13_cheeseboard_241030_153710\\amplifier.dat\",\n...    128,\n...    channel_order = [1, 0, 3, 2, ...]\n... )\n</code></pre> Source code in <code>neuro_py/raw/preprocessing.py</code> <pre><code>def reorder_channels(\n    file_path: str,\n    n_channels: int,\n    channel_order: List[int],\n    precision: str = \"int16\",\n    num_processes: int = 8,  # Adjust based on your CPU cores\n) -&gt; str:\n    \"\"\"\n    Reorder channels in a large binary file, processing in chunks.\n    This function is useful when you want to reorder the channels in a binary file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the file of the binary file to modify.\n    n_channels : int\n        Total number of channels in the binary file.\n    channel_order : List[int]\n        List of channel indices specifying the new order of channels.\n    precision : str, optional\n        Data precision, by default \"int16\".\n    chunk_size : int, optional\n        Number of samples per chunk, by default 10,000.\n\n    Examples\n    --------\n    &gt;&gt;&gt; reorder_channels(\n    ...    r\"U:\\\\data\\\\hpc_ctx_project\\\\HP13\\\\HP13_day1_20241030\\\\HP13_cheeseboard_241030_153710\\\\amplifier.dat\",\n    ...    128,\n    ...    channel_order = [1, 0, 3, 2, ...]\n    ... )\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"Binary file '{file_path}' does not exist.\")\n\n    dtype = np.dtype(precision)\n    bytes_per_sample = dtype.itemsize\n\n    # Calculate total number of samples\n    file_size = os.path.getsize(file_path)\n    n_samples = file_size // (bytes_per_sample * n_channels)\n    if file_size % (bytes_per_sample * n_channels) != 0:\n        raise ValueError(\"Data size is not consistent with expected shape.\")\n\n    # Prepare output file path\n    filename = os.path.basename(file_path)\n    basepath = os.path.dirname(file_path)\n    new_file_path = os.path.join(basepath, f\"reordered_{filename}\")\n\n    # Create an empty output file of the correct size\n    with open(new_file_path, \"wb\") as f:\n        f.write(np.zeros(n_samples * n_channels, dtype=dtype).tobytes())\n\n    # Split the work into chunks for parallel processing\n    chunk_size = n_samples // num_processes\n    chunks = [\n        (\n            i * chunk_size,\n            (i + 1) * chunk_size,\n            file_path,\n            n_channels,\n            channel_order,\n            dtype,\n            new_file_path,\n        )\n        for i in range(num_processes)\n    ]\n\n    # Handle the last chunk if n_samples is not divisible by num_processes\n    if n_samples % num_processes != 0:\n        chunks.append(\n            (\n                num_processes * chunk_size,\n                n_samples,\n                file_path,\n                n_channels,\n                channel_order,\n                dtype,\n                new_file_path,\n            )\n        )\n\n    # Process chunks in parallel\n    with Pool(num_processes) as pool:\n        pool.map(__process_chunk, chunks)\n</code></pre>"},{"location":"reference/neuro_py/raw/#neuro_py.raw.spike_sorting_progress","title":"<code>spike_sorting_progress(file, wait_time=300, hue='amp')</code>","text":"<p>Monitor the progress of a spike sorting process by checking the number of unsorted clusters in a tsv file. Will plot the progress in real-time and estimate the remaining time.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path to the cluster_info.tsv file containing the spike sorting results.</p> required <code>wait_time</code> <code>float</code> <p>The time to wait between checks, in seconds. Default is 300 seconds (5 minutes).</p> <code>300</code> <code>hue</code> <code>str</code> <p>The column to use for the hue in the plot. Default is \"amp\". (\"id\",\"amp\",\"ch\",\"depth\",\"fr\",\"group\",\"n_spikes\",\"sh\")</p> <code>'amp'</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import neuro_py as npy\n&gt;&gt;&gt; npy.raw.spike_sorting_progress(r\"D:\\KiloSort\\hp18_day12_20250416\\Kilosort_2025-04-17_161532\\cluster_info.tsv\")\n</code></pre> Notes <p>This function assumes a specific way of spike sorting in phy: - Once cleaning/merging is done for a unit, the unit is marked good - Needs at least 1 unit marked good to start the process</p> Source code in <code>neuro_py/raw/spike_sorting.py</code> <pre><code>def spike_sorting_progress(file: str, wait_time: float = 300, hue: str = \"amp\"):\n    \"\"\"\n    Monitor the progress of a spike sorting process by checking the number of unsorted clusters in a tsv file.\n    Will plot the progress in real-time and estimate the remaining time.\n\n    Parameters\n    ----------\n    file : str\n        The path to the cluster_info.tsv file containing the spike sorting results.\n    wait_time : float, optional\n        The time to wait between checks, in seconds. Default is 300 seconds (5 minutes).\n    hue : str, optional\n        The column to use for the hue in the plot. Default is \"amp\". (\"id\",\"amp\",\"ch\",\"depth\",\"fr\",\"group\",\"n_spikes\",\"sh\")\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; import neuro_py as npy\n    &gt;&gt;&gt; npy.raw.spike_sorting_progress(r\"D:\\KiloSort\\hp18_day12_20250416\\Kilosort_2025-04-17_161532\\cluster_info.tsv\")\n\n    Notes\n    ------\n    This function assumes a specific way of spike sorting in phy:\n    - Once cleaning/merging is done for a unit, the unit is marked good\n    - Needs at least 1 unit marked good to start the process\n\n\n    \"\"\"\n\n    # dark mode plotting\n    plt.style.use(\"dark_background\")\n\n    def safe_read_csv(file, retries=3):\n        for _ in range(retries):\n            try:\n                return pd.read_csv(file, sep=\"\\t\")\n            except (pd.errors.EmptyDataError, PermissionError):\n                time.sleep(1)\n        raise IOError(f\"Failed to read {file} after {retries} attempts.\")\n\n    # Function to count unsorted clusters\n    def count_unsorted_clusters(file):\n        df = safe_read_csv(file)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=FutureWarning)\n            df[\"group\"].replace(np.nan, \"unsorted\", inplace=True)\n\n        first_good = df.query('group==\"good\"').index[0]\n        df_before_good = df.loc[: first_good - 1].copy()\n\n        n_unsorted = df_before_good.query('group==\"unsorted\"').shape[0]\n        n_sorted = df.query('group==\"good\"').shape[0]\n        return n_unsorted, n_sorted\n\n    # Initial count of unsorted clusters\n    initial_unsorted, _ = count_unsorted_clusters(file)\n    print(f\"Initial unsorted clusters: {initial_unsorted}\")\n\n    # Set a flag to indicate whether the process is complete\n    completed = False\n    # List to store the time and unsorted count for rate calculation\n    time_unsorted_data = [(0, initial_unsorted)]\n\n    # Create a figure and axes for the plot\n    plt.ion()  # Enable interactive mode\n    fig, ax = plt.subplots(\n        2, 1, figsize=(10, 7), sharex=True, gridspec_kw={\"height_ratios\": [1, 4]}\n    )\n\n    while not completed:\n        time.sleep(wait_time)  # Wait for x minutes\n\n        current_unsorted, n_sorted = count_unsorted_clusters(file)\n        current_time = (\n            time_unsorted_data[-1][0] + wait_time / 60\n        )  # Increment time by x minutes\n        time_unsorted_data.append((current_time, current_unsorted))\n\n        sorted_clusters = (\n            time_unsorted_data[0][1] - current_unsorted\n        )  # Number of clusters sorted so far\n\n        if sorted_clusters &gt; 0:\n            # Use all previous data points to calculate an average rate of sorting\n            total_time_elapsed = sum(\n                [\n                    time_unsorted_data[i + 1][0] - time_unsorted_data[i][0]\n                    for i in range(len(time_unsorted_data) - 1)\n                ]\n            )\n            total_clusters_sorted = sum(\n                [\n                    time_unsorted_data[i][1] - time_unsorted_data[i + 1][1]\n                    for i in range(len(time_unsorted_data) - 1)\n                ]\n            )\n\n            average_rate_of_sorting = (\n                total_clusters_sorted / total_time_elapsed\n            )  # Average clusters sorted per minute\n            estimated_time_remaining = (\n                current_unsorted / average_rate_of_sorting\n            )  # Estimate the time remaining\n\n            progress_text = (\n                f\"Time elapsed: {current_time} minutes\\n\"\n                f\"Current unsorted clusters: {current_unsorted}\\n\"\n                f\"Sorted clusters: {n_sorted}\\n\"\n                f\"Average rate of sorting: {average_rate_of_sorting:.2f} clusters/minute\\n\"\n                f\"Estimated time to completion: {estimated_time_remaining:.2f} minutes\"\n            )\n\n            # Check if sorting is complete\n            if current_unsorted == 0:\n                completed = True\n                progress_text += \"\\nSorting complete!\"\n        else:\n            progress_text = f\"No progress made in the last {current_time} minutes. Check if the process is working correctly.\"\n\n        # Update the plot\n        df = safe_read_csv(file)\n        df[\"group\"] = df[\"group\"].replace(np.nan, \"unsorted\")\n\n        # Clear only the plot axes (not the printed output)\n        for a in ax:\n            a.clear()\n\n        # Set plot limits\n        ax[0].set_xlim(df.sh.min(), df.sh.max())\n        ax[1].set_xlim(df.sh.min(), df.sh.max())\n\n        # Create the stripplot\n        sns.stripplot(\n            data=df[[\"depth\", \"sh\"]]\n            .value_counts()\n            .reset_index()\n            .sort_values(by=[\"sh\"]),\n            x=\"sh\",\n            y=\"depth\",\n            ax=ax[1],\n            jitter=False,\n            legend=False,\n            color=\"white\",\n            alpha=0.5,\n        )\n\n        sns.stripplot(\n            data=df.query('group==\"good\"'),\n            x=\"sh\",\n            y=\"depth\",\n            hue=hue,\n            ax=ax[1],\n            jitter=True,\n            legend=False,\n            palette=\"rainbow\",\n        )\n\n        count_df = (\n            df.query('group==\"good\"')\n            .groupby(\"sh\", observed=True)\n            .size()\n            .reset_index(name=\"counts\")\n        )\n\n        sns.barplot(data=count_df, x=\"sh\", y=\"counts\", ax=ax[0], palette=\"winter\")\n        for p in ax[0].patches:\n            ax[0].annotate(\n                f\"{int(p.get_height())}\",\n                (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                ha=\"center\",\n                va=\"bottom\",\n                fontsize=8,\n                color=\"white\",\n            )\n        ax[0].set_ylabel(\"Number of \\n clusters\")\n        ax[1].set_xlabel(\"Shank\")\n        ax[1].set_ylabel(\"Depth (um)\")\n\n        sns.despine()\n        # Draw the plot\n        plt.draw()\n        plt.pause(0.01)  # Pause to allow the plot to updateg\n\n        # Display progress text below the plot\n        clear_output(wait=True)  # Clear only the previous plot and progress text\n        display(fig)  # Display the updated plot\n        print(progress_text)  # Display the progress text below the plot\n\n    # Close the interactive plot after completion\n    plt.ioff()\n    plt.show()\n</code></pre>"},{"location":"reference/neuro_py/raw/preprocessing/","title":"neuro_py.raw.preprocessing","text":""},{"location":"reference/neuro_py/raw/preprocessing/#neuro_py.raw.preprocessing.cut_artifacts","title":"<code>cut_artifacts(filepath, n_channels, cut_intervals, precision='int16', output_filepath=None)</code>","text":"<p>Remove user-defined periods from recordings in a binary file, resulting in a shorter file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the original binary file.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels in the file.</p> required <code>cut_intervals</code> <code>List[Tuple[int, int]]</code> <p>List of intervals (start, end) in sample indices to remove. Assumes sorted and non-overlapping.</p> required <code>precision</code> <code>str</code> <p>Data precision, by default \"int16\".</p> <code>'int16'</code> <code>output_filepath</code> <code>str</code> <p>Path to save the modified binary file. If None, appends \"_cut\" to the original filename.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/raw/preprocessing.py</code> <pre><code>def cut_artifacts(\n    filepath: str,\n    n_channels: int,\n    cut_intervals: List[Tuple[int, int]],\n    precision: str = \"int16\",\n    output_filepath: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Remove user-defined periods from recordings in a binary file, resulting in a shorter file.\n\n    Parameters\n    ----------\n    filepath : str\n        Path to the original binary file.\n    n_channels : int\n        Number of channels in the file.\n    cut_intervals : List[Tuple[int, int]]\n        List of intervals (start, end) in sample indices to remove. Assumes sorted and non-overlapping.\n    precision : str, optional\n        Data precision, by default \"int16\".\n    output_filepath : str, optional\n        Path to save the modified binary file. If None, appends \"_cut\" to the original filename.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"File '{filepath}' does not exist.\")\n\n    # Set default output filepath\n    if output_filepath is None:\n        output_filepath = os.path.splitext(filepath)[0] + \"_cut.dat\"\n\n    # Check for valid intervals\n    for start, end in cut_intervals:\n        if start &gt;= end:\n            raise ValueError(\n                f\"Invalid interval: ({start}, {end}). Start must be less than end.\"\n            )\n\n    # Map the original file and calculate parameters\n    bytes_size = np.dtype(precision).itemsize\n    with open(filepath, \"rb\") as f:\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n\n    data = np.memmap(filepath, dtype=precision, mode=\"r\", shape=(n_samples, n_channels))\n\n    # Identify the indices to keep\n    keep_mask = np.ones(n_samples, dtype=bool)\n    for start, end in cut_intervals:\n        if 0 &lt;= start &lt; n_samples and 0 &lt; end &lt;= n_samples:\n            keep_mask[start:end] = False\n        else:\n            warnings.warn(\n                f\"Interval ({start}, {end}) is out of bounds and was skipped.\"\n            )\n\n    keep_indices = np.flatnonzero(keep_mask)\n\n    # Create a new binary file with only the retained data\n    with open(output_filepath, \"wb\") as output_file:\n        for start_idx in range(0, len(keep_indices), 10_000):  # Process in chunks\n            chunk_indices = keep_indices[start_idx : start_idx + 10_000]\n            output_file.write(data[chunk_indices].tobytes())\n\n    del data  # Release memory-mapped file\n</code></pre>"},{"location":"reference/neuro_py/raw/preprocessing/#neuro_py.raw.preprocessing.cut_artifacts_intan","title":"<code>cut_artifacts_intan(folder_name, n_channels_amplifier, cut_intervals, verbose=True)</code>","text":"<p>Cut specified artifact intervals from Intan data files.</p> <p>This function iterates through a set of Intan data files (amplifier, auxiliary, digitalin, digitalout, analogin, time, and supply), and for each file, it removes artifacts within the specified intervals by invoking the <code>cut_artifacts</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>folder_name</code> <code>str</code> <p>The folder where the Intan data files are located.</p> required <code>n_channels_amplifier</code> <code>int</code> <p>The number of amplifier channels used in the amplifier data file.</p> required <code>cut_intervals</code> <code>List[Tuple[int, int]]</code> <p>A list of intervals (start, end) in sample indices to remove artifacts. Each tuple represents the start and end sample index for an artifact to be cut. Assumes sorted and non-overlapping intervals.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This function modifies the files in place, so there is no return value.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the amplifier data file does not exist in the provided folder.</p> <code>ValueError</code> <p>If video files are found in the folder, as this function does not support video files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fs = 20_000\n&gt;&gt;&gt; cut_artifacts_intan(\n...     folder_name = r\"path/to/data\",\n...     n_channels_amplifier = 128,\n...     cut_intervals = (np.array([[394.4, 394.836], [400, 401], [404, 405]]) * fs).astype(int)\n... )\n</code></pre> Source code in <code>neuro_py/raw/preprocessing.py</code> <pre><code>def cut_artifacts_intan(\n    folder_name: str,\n    n_channels_amplifier: int,\n    cut_intervals: List[Tuple[int, int]],\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Cut specified artifact intervals from Intan data files.\n\n    This function iterates through a set of Intan data files (amplifier, auxiliary,\n    digitalin, digitalout, analogin, time, and supply), and for each file, it removes\n    artifacts within the specified intervals by invoking the `cut_artifacts` function.\n\n    Parameters\n    ----------\n    folder_name : str\n        The folder where the Intan data files are located.\n    n_channels_amplifier : int\n        The number of amplifier channels used in the amplifier data file.\n    cut_intervals : List[Tuple[int, int]]\n        A list of intervals (start, end) in sample indices to remove artifacts.\n        Each tuple represents the start and end sample index for an artifact to be cut.\n        Assumes sorted and non-overlapping intervals.\n\n    Returns\n    -------\n    None\n        This function modifies the files in place, so there is no return value.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the amplifier data file does not exist in the provided folder.\n    ValueError\n        If video files are found in the folder, as this function does not support video files.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fs = 20_000\n    &gt;&gt;&gt; cut_artifacts_intan(\n    ...     folder_name = r\"path/to/data\",\n    ...     n_channels_amplifier = 128,\n    ...     cut_intervals = (np.array([[394.4, 394.836], [400, 401], [404, 405]]) * fs).astype(int)\n    ... )\n    \"\"\"\n\n    # refuse to cut artifacts if any video file exist in folder\n    video_files = [f for f in os.listdir(folder_name) if f.endswith(\".avi\")]\n    if video_files:\n        raise ValueError(f\"Video files found in folder, refusing to cut: {video_files}\")\n\n    # Define data types for each file (from Intan documentation)\n    files_table = {\n        \"amplifier\": \"int16\",\n        \"auxiliary\": \"uint16\",\n        \"digitalin\": \"uint16\",\n        \"digitalout\": \"uint16\",\n        \"analogin\": \"uint16\",\n        \"time\": \"int32\",\n        \"supply\": \"uint16\",\n    }\n\n    # determine number of samples from amplifier file\n    amplifier_file_path = os.path.join(folder_name, \"amplifier.dat\")\n    if not os.path.exists(amplifier_file_path):\n        raise FileNotFoundError(f\"File '{amplifier_file_path}' does not exist.\")\n\n    # get number of bytes per sample\n    bytes_size = np.dtype(files_table[\"amplifier\"]).itemsize\n\n    # each file should have the same number of samples\n    n_samples = os.path.getsize(amplifier_file_path) // (\n        n_channels_amplifier * bytes_size\n    )\n\n    for file_name, precision in files_table.items():\n        file_path = os.path.join(folder_name, f\"{file_name}.dat\")\n\n        if os.path.exists(file_path):\n            if verbose:\n                print(f\"Processing {file_name}.dat file...\")\n\n            # get number of bytes per sample\n            bytes_size = np.dtype(precision).itemsize\n\n            # determine number of channels from n_samples\n            n_channels = int(os.path.getsize(file_path) / n_samples / bytes_size)\n\n            # for time file, cut and offset timestamps\n            if file_name == \"time\":\n                output_filepath = os.path.splitext(file_path)[0] + \"_cut.dat\"\n\n                with open(output_filepath, \"wb\") as output_file:\n                    # time indices as continuous array\n                    filtered_time = np.arange(\n                        n_samples - sum(end - start for start, end in cut_intervals),\n                        dtype=np.int32,\n                    )\n\n                    # write to file\n                    output_file.write(filtered_time.tobytes())\n            else:\n                # cut artifacts\n                cut_artifacts(file_path, n_channels, cut_intervals, precision)\n\n    # Calculate the expected number of samples after cutting\n    total_samples_cut = sum(end - start for start, end in cut_intervals)\n    expected_n_samples = n_samples - total_samples_cut\n\n    # === Validation Section ===\n    # Verify all `_cut.dat` files have the correct number of samples\n    for file_name, precision in files_table.items():\n        output_file_path = os.path.join(folder_name, f\"{file_name}_cut.dat\")\n        original_file_path = os.path.join(folder_name, f\"{file_name}.dat\")\n\n        if os.path.exists(output_file_path) and os.path.exists(original_file_path):\n            # Dynamically calculate the number of channels\n            bytes_size = np.dtype(precision).itemsize\n            n_channels = os.path.getsize(original_file_path) // (n_samples * bytes_size)\n\n            # Calculate the expected file size\n            expected_size = expected_n_samples * n_channels * bytes_size\n            actual_size = os.path.getsize(output_file_path)\n\n            if actual_size != expected_size:\n                raise RuntimeError(\n                    f\"{file_name}_cut.dat has an incorrect size. \"\n                    f\"Expected {expected_size} bytes but found {actual_size} bytes.\"\n                )\n</code></pre>"},{"location":"reference/neuro_py/raw/preprocessing/#neuro_py.raw.preprocessing.fill_missing_channels","title":"<code>fill_missing_channels(basepath, n_channels, filename, missing_channels, precision='int16', chunk_size=10000)</code>","text":"<p>Fill missing channels in a large binary file with zeros, processing in chunks. This function is useful when some channels were accidently deactivated during recording.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>Path to the folder containing the binary file.</p> required <code>n_channels</code> <code>int</code> <p>Total number of channels in the binary file (including the missing ones).</p> required <code>filename</code> <code>str</code> <p>Name of the binary file to modify.</p> required <code>missing_channels</code> <code>List[int]</code> <p>List of missing channel indices to be filled with zeros.</p> required <code>precision</code> <code>str</code> <p>Data precision, by default \"int16\".</p> <code>'int16'</code> <code>chunk_size</code> <code>int</code> <p>Number of samples per chunk, by default 10,000.</p> <code>10000</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the modified binary file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fill_missing_channels(\n...    r\"U:\\data\\hpc_ctx_project\\HP13\\HP13_day1_20241030\\HP13_cheeseboard_241030_153710\",\n...    128,\n...    'amplifier.dat',\n...    missing_channels = [0]\n... )\n</code></pre> Source code in <code>neuro_py/raw/preprocessing.py</code> <pre><code>def fill_missing_channels(\n    basepath: str,\n    n_channels: int,\n    filename: str,\n    missing_channels: List[int],\n    precision: str = \"int16\",\n    chunk_size: int = 10_000,\n) -&gt; str:\n    \"\"\"\n    Fill missing channels in a large binary file with zeros, processing in chunks.\n    This function is useful when some channels were accidently deactivated during recording.\n\n    Parameters\n    ----------\n    basepath : str\n        Path to the folder containing the binary file.\n    n_channels : int\n        Total number of channels in the binary file (including the missing ones).\n    filename : str\n        Name of the binary file to modify.\n    missing_channels : List[int]\n        List of missing channel indices to be filled with zeros.\n    precision : str, optional\n        Data precision, by default \"int16\".\n    chunk_size : int, optional\n        Number of samples per chunk, by default 10,000.\n\n    Returns\n    -------\n    str\n        Path to the modified binary file.\n\n    Examples\n    --------\n    &gt;&gt;&gt; fill_missing_channels(\n    ...    r\"U:\\\\data\\\\hpc_ctx_project\\\\HP13\\\\HP13_day1_20241030\\\\HP13_cheeseboard_241030_153710\",\n    ...    128,\n    ...    'amplifier.dat',\n    ...    missing_channels = [0]\n    ... )\n    \"\"\"\n    file_path = os.path.join(basepath, filename)\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"Binary file '{file_path}' does not exist.\")\n\n    dtype = np.dtype(precision)\n    bytes_per_sample = dtype.itemsize\n    present_channels = [ch for ch in range(n_channels) if ch not in missing_channels]\n\n    # Calculate total number of samples\n    file_size = os.path.getsize(file_path)\n    n_samples = file_size // (bytes_per_sample * (n_channels - len(missing_channels)))\n    if file_size % (bytes_per_sample * (n_channels - len(missing_channels))) != 0:\n        raise ValueError(\"Data size is not consistent with expected shape.\")\n\n    # Prepare output file path\n    new_file_path = os.path.join(basepath, f\"corrected_{filename}\")\n\n    # Process file in chunks\n    with open(file_path, \"rb\") as f_in, open(new_file_path, \"wb\") as f_out:\n        for start in range(0, n_samples, chunk_size):\n            # Read a chunk of data\n            chunk = np.fromfile(\n                f_in,\n                dtype=dtype,\n                count=chunk_size * (n_channels - len(missing_channels)),\n            )\n            chunk = chunk.reshape(-1, n_channels - len(missing_channels))\n\n            # Create a new array with missing channels filled with zeros\n            chunk_full = np.zeros((chunk.shape[0], n_channels), dtype=dtype)\n            chunk_full[:, present_channels] = chunk\n\n            # Write the chunk with missing channels added to the new file\n            chunk_full.tofile(f_out)\n\n    return new_file_path\n</code></pre>"},{"location":"reference/neuro_py/raw/preprocessing/#neuro_py.raw.preprocessing.remove_artifacts","title":"<code>remove_artifacts(filepath, n_channels, zero_intervals, precision='int16', mode='linear', channels_to_remove=None)</code>","text":"<p>Silence user-defined periods from recordings in a binary file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the binary file.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels in the file.</p> required <code>zero_intervals</code> <code>List[Tuple[int, int]]</code> <p>List of intervals (start, end) in sample indices to zero out.</p> required <code>precision</code> <code>str</code> <p>Data precision, by default \"int16\".</p> <code>'int16'</code> <code>mode</code> <code>str</code> <p>Mode of interpolation. Options are:</p> <ul> <li>\"zeros\": Zero out the interval.</li> <li>\"linear\": Interpolate linearly between the start and end of the interval (default).</li> <li>\"gaussian\": (Not implemented, TBD) Interpolate using a Gaussian function with the same variance as in the recordings, on a per-channel basis.</li> </ul> <code>'linear'</code> <code>channels_to_remove</code> <code>List[int]</code> <p>List of channels (0-based indices) to remove artifacts from. If None, remove artifacts from all channels.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fs = 20_000\n&gt;&gt;&gt; remove_artifacts(\n...     r\"U:\\data\\hpc_ctx_project\\HP13\\HP13_day12_20241112\\HP13_day12_20241112.dat\",\n...     n_channels=128,\n...     zero_intervals=(bad_intervals.data * fs).astype(int),\n...     channels_to_remove=[0, 1, 2]  # Only remove artifacts from channels 0, 1, and 2\n... )\n</code></pre> Source code in <code>neuro_py/raw/preprocessing.py</code> <pre><code>def remove_artifacts(\n    filepath: str,\n    n_channels: int,\n    zero_intervals: List[Tuple[int, int]],\n    precision: str = \"int16\",\n    mode: str = \"linear\",\n    channels_to_remove: Optional[List[int]] = None,\n) -&gt; None:\n    \"\"\"\n    Silence user-defined periods from recordings in a binary file.\n\n    Parameters\n    ----------\n    filepath : str\n        Path to the binary file.\n    n_channels : int\n        Number of channels in the file.\n    zero_intervals : List[Tuple[int, int]]\n        List of intervals (start, end) in sample indices to zero out.\n    precision : str, optional\n        Data precision, by default \"int16\".\n    mode : str, optional\n        Mode of interpolation. Options are:\n\n        - **\"zeros\"**: Zero out the interval.\n        - **\"linear\"**: Interpolate linearly between the start and end of the interval (default).\n        - **\"gaussian\"**: *(Not implemented, TBD)* Interpolate using a Gaussian function with the same variance as in the recordings, on a per-channel basis.\n\n    channels_to_remove : List[int], optional\n        List of channels (0-based indices) to remove artifacts from. If None, remove artifacts from all channels.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; fs = 20_000\n    &gt;&gt;&gt; remove_artifacts(\n    ...     r\"U:\\\\data\\\\hpc_ctx_project\\\\HP13\\\\HP13_day12_20241112\\\\HP13_day12_20241112.dat\",\n    ...     n_channels=128,\n    ...     zero_intervals=(bad_intervals.data * fs).astype(int),\n    ...     channels_to_remove=[0, 1, 2]  # Only remove artifacts from channels 0, 1, and 2\n    ... )\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(filepath):\n        warnings.warn(\"File does not exist.\")\n        return\n\n    # Open the file in memory-mapped mode for read/write\n    bytes_size = np.dtype(precision).itemsize\n    with open(filepath, \"rb\") as f:\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n\n    # Map the file to memory in read-write mode\n    data = np.memmap(\n        filepath, dtype=precision, mode=\"r+\", shape=(n_samples, n_channels)\n    )\n    try:\n        # if shape is (2,) then it is a single interval, then add dimension\n        if np.shape(zero_intervals) == (2,):\n            zero_intervals = np.expand_dims(zero_intervals, axis=0)\n\n        # If no specific channels are provided, process all channels\n        if channels_to_remove is None:\n            channels_to_remove = list(range(n_channels))\n\n        # Zero out the specified intervals\n        if mode == \"zeros\":\n            zero_value = np.zeros((1, n_channels), dtype=precision)\n            for start, end in zero_intervals:\n                if 0 &lt;= start &lt; n_samples and 0 &lt; end &lt;= n_samples:\n                    data[start:end, channels_to_remove] = zero_value[\n                        0, channels_to_remove\n                    ]\n                else:\n                    warnings.warn(\n                        f\"Interval ({start}, {end}) is out of bounds and was skipped.\"\n                    )\n        elif mode == \"linear\":\n            for start, end in zero_intervals:\n                if 0 &lt;= start &lt; n_samples and 0 &lt; end &lt;= n_samples:\n                    for ch in channels_to_remove:\n                        # Compute float interpolation and round before casting\n                        interpolated = np.linspace(\n                            data[start, ch],\n                            data[end, ch],\n                            end - start,\n                        ).astype(data.dtype)  # Ensure consistent dtype\n                        data[start:end, ch] = interpolated\n                else:\n                    warnings.warn(\n                        f\"Interval ({start}, {end}) is out of bounds and was skipped.\"\n                    )\n        elif mode == \"gaussian\":\n            # not implemented error message\n            raise NotImplementedError(\"Gaussian mode not implemented.\")\n\n            # max_samples = 10_000\n            # rng = np.random.default_rng()\n\n            # # Compute valid regions and sample\n            # valid_mask = np.ones(n_samples, dtype=bool)\n            # for start, end in zero_intervals:\n            #     valid_mask[start:end] = False\n\n            # valid_indices = np.flatnonzero(valid_mask)\n            # sampled_indices = rng.choice(\n            #     valid_indices, size=min(max_samples, len(valid_indices)), replace=False\n            # )\n            # sampled_data = data[sampled_indices, :]\n\n            # # Compute mean and std for each channel\n            # means = np.mean(sampled_data, axis=0)\n            # stds = np.std(sampled_data, axis=0)\n\n            # from scipy.signal import butter, filtfilt\n\n            # def bandpass_filter(signal, lowcut, highcut, fs, order=4):\n            #     nyquist = 0.5 * fs\n            #     low = lowcut / nyquist\n            #     high = highcut / nyquist\n            #     b, a = butter(order, [low, high], btype=\"band\")\n            #     return filtfilt(b, a, signal, axis=0)\n\n            # # Parameters for bandpass filter\n            # lowcut = 0.5\n            # highcut = 100\n\n            # for start, end in zero_intervals:\n            #     if 0 &lt;= start &lt; n_samples and 0 &lt; end &lt;= n_samples:\n            #         interval_length = end - start\n            #         raw_noise = rng.normal(\n            #             loc=means, scale=stds, size=(interval_length, n_channels)\n            #         ).astype(precision)\n\n            #         # Apply bandpass filter with handling for potential issues\n            #         try:\n            #             filtered_noise = bandpass_filter(raw_noise, lowcut, highcut, fs)\n            #             filtered_noise = np.nan_to_num(filtered_noise, nan=0.0)\n            #         except ValueError:\n            #             warnings.warn(f\"Filtering failed for interval ({start}, {end}), skipping.\")\n            #             continue\n\n            #         # Prevent overwriting with unexpected data types\n            #         data[start:end, :] = filtered_noise.astype(data.dtype)\n            #     else:\n            #         warnings.warn(f\"Interval ({start}, {end}) is out of bounds and was skipped.\")\n\n    finally:\n        # Explicitly flush and release the memory-mapped object\n        data.flush()\n        del data\n        gc.collect()\n\n    # Save a log file with intervals zeroed out\n    log_file = os.path.splitext(filepath)[0] + \"_zeroed_intervals.log\"\n    try:\n        with open(log_file, \"w\") as f:\n            f.write(f\"Zeroed intervals: {zero_intervals.tolist()}\\n\")\n    except Exception as e:\n        warnings.warn(f\"Failed to create log file: {e}\")\n</code></pre>"},{"location":"reference/neuro_py/raw/preprocessing/#neuro_py.raw.preprocessing.reorder_channels","title":"<code>reorder_channels(file_path, n_channels, channel_order, precision='int16', num_processes=8)</code>","text":"<p>Reorder channels in a large binary file, processing in chunks. This function is useful when you want to reorder the channels in a binary file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file of the binary file to modify.</p> required <code>n_channels</code> <code>int</code> <p>Total number of channels in the binary file.</p> required <code>channel_order</code> <code>List[int]</code> <p>List of channel indices specifying the new order of channels.</p> required <code>precision</code> <code>str</code> <p>Data precision, by default \"int16\".</p> <code>'int16'</code> <code>chunk_size</code> <code>int</code> <p>Number of samples per chunk, by default 10,000.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; reorder_channels(\n...    r\"U:\\data\\hpc_ctx_project\\HP13\\HP13_day1_20241030\\HP13_cheeseboard_241030_153710\\amplifier.dat\",\n...    128,\n...    channel_order = [1, 0, 3, 2, ...]\n... )\n</code></pre> Source code in <code>neuro_py/raw/preprocessing.py</code> <pre><code>def reorder_channels(\n    file_path: str,\n    n_channels: int,\n    channel_order: List[int],\n    precision: str = \"int16\",\n    num_processes: int = 8,  # Adjust based on your CPU cores\n) -&gt; str:\n    \"\"\"\n    Reorder channels in a large binary file, processing in chunks.\n    This function is useful when you want to reorder the channels in a binary file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the file of the binary file to modify.\n    n_channels : int\n        Total number of channels in the binary file.\n    channel_order : List[int]\n        List of channel indices specifying the new order of channels.\n    precision : str, optional\n        Data precision, by default \"int16\".\n    chunk_size : int, optional\n        Number of samples per chunk, by default 10,000.\n\n    Examples\n    --------\n    &gt;&gt;&gt; reorder_channels(\n    ...    r\"U:\\\\data\\\\hpc_ctx_project\\\\HP13\\\\HP13_day1_20241030\\\\HP13_cheeseboard_241030_153710\\\\amplifier.dat\",\n    ...    128,\n    ...    channel_order = [1, 0, 3, 2, ...]\n    ... )\n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"Binary file '{file_path}' does not exist.\")\n\n    dtype = np.dtype(precision)\n    bytes_per_sample = dtype.itemsize\n\n    # Calculate total number of samples\n    file_size = os.path.getsize(file_path)\n    n_samples = file_size // (bytes_per_sample * n_channels)\n    if file_size % (bytes_per_sample * n_channels) != 0:\n        raise ValueError(\"Data size is not consistent with expected shape.\")\n\n    # Prepare output file path\n    filename = os.path.basename(file_path)\n    basepath = os.path.dirname(file_path)\n    new_file_path = os.path.join(basepath, f\"reordered_{filename}\")\n\n    # Create an empty output file of the correct size\n    with open(new_file_path, \"wb\") as f:\n        f.write(np.zeros(n_samples * n_channels, dtype=dtype).tobytes())\n\n    # Split the work into chunks for parallel processing\n    chunk_size = n_samples // num_processes\n    chunks = [\n        (\n            i * chunk_size,\n            (i + 1) * chunk_size,\n            file_path,\n            n_channels,\n            channel_order,\n            dtype,\n            new_file_path,\n        )\n        for i in range(num_processes)\n    ]\n\n    # Handle the last chunk if n_samples is not divisible by num_processes\n    if n_samples % num_processes != 0:\n        chunks.append(\n            (\n                num_processes * chunk_size,\n                n_samples,\n                file_path,\n                n_channels,\n                channel_order,\n                dtype,\n                new_file_path,\n            )\n        )\n\n    # Process chunks in parallel\n    with Pool(num_processes) as pool:\n        pool.map(__process_chunk, chunks)\n</code></pre>"},{"location":"reference/neuro_py/raw/spike_sorting/","title":"neuro_py.raw.spike_sorting","text":""},{"location":"reference/neuro_py/raw/spike_sorting/#neuro_py.raw.spike_sorting.phy_log_to_epocharray","title":"<code>phy_log_to_epocharray(filename, merge_gap=30)</code>","text":"<p>Extract timestamps from a Phy log file and convert them to a nel.EpochArray. Will estimate the amount of time it took to spikesort a session.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path to the Phy log file.</p> required <code>merge_gap</code> <code>float</code> <p>The number of seconds to merge timestamps, by default 30</p> <code>30</code> <p>Returns:</p> Type Description <code>EpochArray</code> <p>A nel.EpochArray containing the timestamps.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import neuro_py as npy\n&gt;&gt;&gt; filename = r\"D:\\KiloSort\\HP18\\hp18_day11_20250415\\Kilosort_2025-04-16_224949\\phy.log\"\n&gt;&gt;&gt; timestamps = npy.raw.phy_log_to_epocharray(filename)\n&gt;&gt;&gt; timestamps\n&lt;EpochArray at 0x1f6c7da5710: 80 epochs&gt; of length 4:02:01:591 hours\n</code></pre> Source code in <code>neuro_py/raw/spike_sorting.py</code> <pre><code>def phy_log_to_epocharray(filename: str, merge_gap: float = 30):\n    \"\"\"\n    Extract timestamps from a Phy log file and convert them to a nel.EpochArray.\n    Will estimate the amount of time it took to spikesort a session.\n\n    Parameters\n    ----------\n    filename : str\n        The path to the Phy log file.\n    merge_gap : float, optional\n        The number of seconds to merge timestamps, by default 30\n\n    Returns\n    -------\n    nel.EpochArray\n        A nel.EpochArray containing the timestamps.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import neuro_py as npy\n    &gt;&gt;&gt; filename = r\"D:\\KiloSort\\HP18\\hp18_day11_20250415\\Kilosort_2025-04-16_224949\\phy.log\"\n    &gt;&gt;&gt; timestamps = npy.raw.phy_log_to_epocharray(filename)\n    &gt;&gt;&gt; timestamps\n    &lt;EpochArray at 0x1f6c7da5710: 80 epochs&gt; of length 4:02:01:591 hours\n\n    \"\"\"\n\n    # Read the log file\n    try:\n        with open(filename, \"r\") as file:\n            log_lines = file.readlines()\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"Log file not found: {filename}\")\n\n    # Define the regex pattern to extract timestamps\n    timestamp_pattern = re.compile(r\"\\x1b\\[\\d+m(\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\")\n\n    # Extract timestamps using the regex pattern\n    timestamps = []\n    for line in log_lines:\n        match = timestamp_pattern.search(line)\n        if match:\n            timestamps.append(match.group(1))\n\n    # Create a Pandas DataFrame\n    df = pd.DataFrame(timestamps, columns=[\"Timestamp\"])\n\n    # Convert the 'Timestamp' column to datetime format\n    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], format=\"%H:%M:%S.%f\")\n\n    # Convert timestamps to total seconds (including milliseconds)\n    df[\"Seconds\"] = (\n        df[\"Timestamp\"].dt.hour * 3600\n        + df[\"Timestamp\"].dt.minute * 60\n        + df[\"Timestamp\"].dt.second\n        + df[\"Timestamp\"].dt.microsecond / 1e6\n    )\n    df[\"continous\"] = df.Seconds.diff().abs().cumsum()\n\n    intervals = np.array([df.continous[1:], df.continous[1:]]).T\n\n    return nel.EpochArray(intervals).merge(gap=merge_gap)\n</code></pre>"},{"location":"reference/neuro_py/raw/spike_sorting/#neuro_py.raw.spike_sorting.spike_sorting_progress","title":"<code>spike_sorting_progress(file, wait_time=300, hue='amp')</code>","text":"<p>Monitor the progress of a spike sorting process by checking the number of unsorted clusters in a tsv file. Will plot the progress in real-time and estimate the remaining time.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path to the cluster_info.tsv file containing the spike sorting results.</p> required <code>wait_time</code> <code>float</code> <p>The time to wait between checks, in seconds. Default is 300 seconds (5 minutes).</p> <code>300</code> <code>hue</code> <code>str</code> <p>The column to use for the hue in the plot. Default is \"amp\". (\"id\",\"amp\",\"ch\",\"depth\",\"fr\",\"group\",\"n_spikes\",\"sh\")</p> <code>'amp'</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import neuro_py as npy\n&gt;&gt;&gt; npy.raw.spike_sorting_progress(r\"D:\\KiloSort\\hp18_day12_20250416\\Kilosort_2025-04-17_161532\\cluster_info.tsv\")\n</code></pre> Notes <p>This function assumes a specific way of spike sorting in phy: - Once cleaning/merging is done for a unit, the unit is marked good - Needs at least 1 unit marked good to start the process</p> Source code in <code>neuro_py/raw/spike_sorting.py</code> <pre><code>def spike_sorting_progress(file: str, wait_time: float = 300, hue: str = \"amp\"):\n    \"\"\"\n    Monitor the progress of a spike sorting process by checking the number of unsorted clusters in a tsv file.\n    Will plot the progress in real-time and estimate the remaining time.\n\n    Parameters\n    ----------\n    file : str\n        The path to the cluster_info.tsv file containing the spike sorting results.\n    wait_time : float, optional\n        The time to wait between checks, in seconds. Default is 300 seconds (5 minutes).\n    hue : str, optional\n        The column to use for the hue in the plot. Default is \"amp\". (\"id\",\"amp\",\"ch\",\"depth\",\"fr\",\"group\",\"n_spikes\",\"sh\")\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    &gt;&gt;&gt; import neuro_py as npy\n    &gt;&gt;&gt; npy.raw.spike_sorting_progress(r\"D:\\KiloSort\\hp18_day12_20250416\\Kilosort_2025-04-17_161532\\cluster_info.tsv\")\n\n    Notes\n    ------\n    This function assumes a specific way of spike sorting in phy:\n    - Once cleaning/merging is done for a unit, the unit is marked good\n    - Needs at least 1 unit marked good to start the process\n\n\n    \"\"\"\n\n    # dark mode plotting\n    plt.style.use(\"dark_background\")\n\n    def safe_read_csv(file, retries=3):\n        for _ in range(retries):\n            try:\n                return pd.read_csv(file, sep=\"\\t\")\n            except (pd.errors.EmptyDataError, PermissionError):\n                time.sleep(1)\n        raise IOError(f\"Failed to read {file} after {retries} attempts.\")\n\n    # Function to count unsorted clusters\n    def count_unsorted_clusters(file):\n        df = safe_read_csv(file)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=FutureWarning)\n            df[\"group\"].replace(np.nan, \"unsorted\", inplace=True)\n\n        first_good = df.query('group==\"good\"').index[0]\n        df_before_good = df.loc[: first_good - 1].copy()\n\n        n_unsorted = df_before_good.query('group==\"unsorted\"').shape[0]\n        n_sorted = df.query('group==\"good\"').shape[0]\n        return n_unsorted, n_sorted\n\n    # Initial count of unsorted clusters\n    initial_unsorted, _ = count_unsorted_clusters(file)\n    print(f\"Initial unsorted clusters: {initial_unsorted}\")\n\n    # Set a flag to indicate whether the process is complete\n    completed = False\n    # List to store the time and unsorted count for rate calculation\n    time_unsorted_data = [(0, initial_unsorted)]\n\n    # Create a figure and axes for the plot\n    plt.ion()  # Enable interactive mode\n    fig, ax = plt.subplots(\n        2, 1, figsize=(10, 7), sharex=True, gridspec_kw={\"height_ratios\": [1, 4]}\n    )\n\n    while not completed:\n        time.sleep(wait_time)  # Wait for x minutes\n\n        current_unsorted, n_sorted = count_unsorted_clusters(file)\n        current_time = (\n            time_unsorted_data[-1][0] + wait_time / 60\n        )  # Increment time by x minutes\n        time_unsorted_data.append((current_time, current_unsorted))\n\n        sorted_clusters = (\n            time_unsorted_data[0][1] - current_unsorted\n        )  # Number of clusters sorted so far\n\n        if sorted_clusters &gt; 0:\n            # Use all previous data points to calculate an average rate of sorting\n            total_time_elapsed = sum(\n                [\n                    time_unsorted_data[i + 1][0] - time_unsorted_data[i][0]\n                    for i in range(len(time_unsorted_data) - 1)\n                ]\n            )\n            total_clusters_sorted = sum(\n                [\n                    time_unsorted_data[i][1] - time_unsorted_data[i + 1][1]\n                    for i in range(len(time_unsorted_data) - 1)\n                ]\n            )\n\n            average_rate_of_sorting = (\n                total_clusters_sorted / total_time_elapsed\n            )  # Average clusters sorted per minute\n            estimated_time_remaining = (\n                current_unsorted / average_rate_of_sorting\n            )  # Estimate the time remaining\n\n            progress_text = (\n                f\"Time elapsed: {current_time} minutes\\n\"\n                f\"Current unsorted clusters: {current_unsorted}\\n\"\n                f\"Sorted clusters: {n_sorted}\\n\"\n                f\"Average rate of sorting: {average_rate_of_sorting:.2f} clusters/minute\\n\"\n                f\"Estimated time to completion: {estimated_time_remaining:.2f} minutes\"\n            )\n\n            # Check if sorting is complete\n            if current_unsorted == 0:\n                completed = True\n                progress_text += \"\\nSorting complete!\"\n        else:\n            progress_text = f\"No progress made in the last {current_time} minutes. Check if the process is working correctly.\"\n\n        # Update the plot\n        df = safe_read_csv(file)\n        df[\"group\"] = df[\"group\"].replace(np.nan, \"unsorted\")\n\n        # Clear only the plot axes (not the printed output)\n        for a in ax:\n            a.clear()\n\n        # Set plot limits\n        ax[0].set_xlim(df.sh.min(), df.sh.max())\n        ax[1].set_xlim(df.sh.min(), df.sh.max())\n\n        # Create the stripplot\n        sns.stripplot(\n            data=df[[\"depth\", \"sh\"]]\n            .value_counts()\n            .reset_index()\n            .sort_values(by=[\"sh\"]),\n            x=\"sh\",\n            y=\"depth\",\n            ax=ax[1],\n            jitter=False,\n            legend=False,\n            color=\"white\",\n            alpha=0.5,\n        )\n\n        sns.stripplot(\n            data=df.query('group==\"good\"'),\n            x=\"sh\",\n            y=\"depth\",\n            hue=hue,\n            ax=ax[1],\n            jitter=True,\n            legend=False,\n            palette=\"rainbow\",\n        )\n\n        count_df = (\n            df.query('group==\"good\"')\n            .groupby(\"sh\", observed=True)\n            .size()\n            .reset_index(name=\"counts\")\n        )\n\n        sns.barplot(data=count_df, x=\"sh\", y=\"counts\", ax=ax[0], palette=\"winter\")\n        for p in ax[0].patches:\n            ax[0].annotate(\n                f\"{int(p.get_height())}\",\n                (p.get_x() + p.get_width() / 2.0, p.get_height()),\n                ha=\"center\",\n                va=\"bottom\",\n                fontsize=8,\n                color=\"white\",\n            )\n        ax[0].set_ylabel(\"Number of \\n clusters\")\n        ax[1].set_xlabel(\"Shank\")\n        ax[1].set_ylabel(\"Depth (um)\")\n\n        sns.despine()\n        # Draw the plot\n        plt.draw()\n        plt.pause(0.01)  # Pause to allow the plot to updateg\n\n        # Display progress text below the plot\n        clear_output(wait=True)  # Clear only the previous plot and progress text\n        display(fig)  # Display the updated plot\n        print(progress_text)  # Display the progress text below the plot\n\n    # Close the interactive plot after completion\n    plt.ioff()\n    plt.show()\n</code></pre>"},{"location":"reference/neuro_py/session/","title":"neuro_py.session","text":""},{"location":"reference/neuro_py/session/#neuro_py.session.compress_repeated_epochs","title":"<code>compress_repeated_epochs(epoch_df, epoch_name=None)</code>","text":"<p>Compress repeated epochs in an epoch DataFrame. If consecutive epochs have the same name, they will be combined into a single epoch with the earliest startTime and the latest stopTime.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_df</code> <code>DataFrame</code> <p>A DataFrame containing epoch information. Must have columns <code>environment</code>, <code>startTime</code>, and <code>stopTime</code>.</p> required <code>epoch_name</code> <code>str</code> <p>If provided, only compress epochs with this specific name. If None, compress all consecutive epochs with the same name.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame where consecutive epochs with the same name are compressed into a single epoch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_df = pd.DataFrame({\n...     'environment': ['sleep', 'sleep', 'wmaze', 'wmaze', 'sleep'],\n...     'startTime': [0, 100, 200, 300, 400],\n...     'stopTime': [99, 199, 299, 399, 499]\n... })\n&gt;&gt;&gt; compress_repeated_epochs(epoch_df)\n  environment  startTime  stopTime\n0       sleep          0       199\n1       wmaze        200       399\n2       sleep        400       499\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def compress_repeated_epochs(epoch_df, epoch_name=None):\n    \"\"\"\n    Compress repeated epochs in an epoch DataFrame. If consecutive epochs have the same name,\n    they will be combined into a single epoch with the earliest startTime and the latest stopTime.\n\n    Parameters\n    ----------\n    epoch_df : pd.DataFrame\n        A DataFrame containing epoch information. Must have columns `environment`, `startTime`, and `stopTime`.\n    epoch_name : str, optional\n        If provided, only compress epochs with this specific name. If None, compress all consecutive epochs with the same name.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame where consecutive epochs with the same name are compressed into a single epoch.\n\n    Examples\n    -------\n    &gt;&gt;&gt; epoch_df = pd.DataFrame({\n    ...     'environment': ['sleep', 'sleep', 'wmaze', 'wmaze', 'sleep'],\n    ...     'startTime': [0, 100, 200, 300, 400],\n    ...     'stopTime': [99, 199, 299, 399, 499]\n    ... })\n    &gt;&gt;&gt; compress_repeated_epochs(epoch_df)\n      environment  startTime  stopTime\n    0       sleep          0       199\n    1       wmaze        200       399\n    2       sleep        400       499\n    \"\"\"\n    if epoch_name is None:\n        match = np.zeros([epoch_df.environment.shape[0]])\n        match[match == 0] = np.nan\n        for i, ep in enumerate(epoch_df.environment[:-1]):\n            if np.isnan(match[i]):\n                # find match in current and next epoch\n                if ep == epoch_df.environment.iloc[i + 1]:\n                    match[i : i + 2] = i\n                    # given match, see if there are more matches\n                    for match_i in np.arange(1, epoch_df.environment[:-1].shape[0]):\n                        if i + 1 + match_i == epoch_df.environment.shape[0]:\n                            break\n                        if ep == epoch_df.environment.iloc[i + 1 + match_i]:\n                            match[i : i + 1 + match_i + 1] = i\n                        else:\n                            break\n    else:\n        match = np.zeros([epoch_df.environment.shape[0]])\n        match[match == 0] = np.nan\n        for i, ep in enumerate(epoch_df.environment[:-1]):\n            if np.isnan(match[i]):\n                # find match in current and next epoch\n                if (ep == epoch_df.environment.iloc[i + 1]) &amp; (ep == epoch_name):\n                    match[i : i + 2] = i\n                    # given match, see if there are more matches\n                    for match_i in np.arange(1, epoch_df.environment[:-1].shape[0]):\n                        if i + 1 + match_i == epoch_df.environment.shape[0]:\n                            break\n                        if ep == epoch_df.environment.iloc[i + 1 + match_i]:\n                            match[i : i + 1 + match_i + 1] = i\n                        else:\n                            break\n\n    for i in range(len(match)):\n        if np.isnan(match[i]):\n            # make nans large numbers that are unlikely to be real epoch\n            match[i] = (i + 1) * 2000\n\n    # iter through each epoch indicator to get start and stop\n    results = pd.DataFrame()\n    no_nan_match = match[~np.isnan(match)]\n    for m in pd.unique(no_nan_match):\n        temp_dict = {}\n        for item in epoch_df.keys():\n            temp_dict[item] = epoch_df[match == m][item].iloc[0]\n\n        temp_dict[\"startTime\"] = epoch_df[match == m].startTime.min()\n        temp_dict[\"stopTime\"] = epoch_df[match == m].stopTime.max()\n\n        temp_df = pd.DataFrame.from_dict(temp_dict, orient=\"index\").T\n\n        results = pd.concat([results, temp_df], ignore_index=True)\n    return results\n</code></pre>"},{"location":"reference/neuro_py/session/#neuro_py.session.find_env_paradigm_pre_task_post","title":"<code>find_env_paradigm_pre_task_post(epoch_df, env='sleep', paradigm='memory')</code>","text":"<p>Find indices of epochs that match a sequence of environment and paradigm patterns, specifically looking for a pre-task-post structure.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_df</code> <code>DataFrame</code> <p>DataFrame containing epoch information with columns such as 'environment' and 'behavioralParadigm'.</p> required <code>env</code> <code>str</code> <p>The environment pattern to search for (default is \"sleep\").</p> <code>'sleep'</code> <code>paradigm</code> <code>str</code> <p>The behavioral paradigm pattern to search for (default is \"memory\").</p> <code>'memory'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A boolean array where <code>True</code> indicates that the epoch is part of a pre-task-post sequence (i.e., sleep-task-sleep) based on the provided environment and paradigm.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_df = pd.DataFrame({\n...     'name': ['EE.042', 'EE.045', 'EE.046', 'EE.049', 'EE.050'],\n...     'startTime': [0.0, 995.9384, 3336.3928, 5722.444, 7511.244],\n...     'stopTime': [995.9384, 3336.3928, 5722.444, 7511.244, 9387.644],\n...     'environment': ['sleep', 'tmaze', 'sleep', 'tmaze', 'sleep'],\n...     'behavioralParadigm': [np.nan, 'Spontaneous alternation task', np.nan, 'Working memory task', np.nan]\n... })\n&gt;&gt;&gt; idx = find_env_paradigm_pre_task_post(epoch_df)\n&gt;&gt;&gt; epoch_df[idx]\n      name  startTime   stopTime environment        behavioralParadigm\n2  EE.046   3336.3928  5722.444       sleep                        NaN\n3  EE.049   5722.444   7511.244      tmaze         Working memory task\n4  EE.050   7511.244   9387.644       sleep                        NaN\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def find_env_paradigm_pre_task_post(\n    epoch_df: pd.DataFrame, env: str = \"sleep\", paradigm: str = \"memory\"\n) -&gt; np.ndarray:\n    \"\"\"\n    Find indices of epochs that match a sequence of environment and paradigm\n    patterns, specifically looking for a pre-task-post structure.\n\n    Parameters\n    ----------\n    epoch_df : pd.DataFrame\n        DataFrame containing epoch information with columns such as 'environment' and 'behavioralParadigm'.\n    env : str, optional\n        The environment pattern to search for (default is \"sleep\").\n    paradigm : str, optional\n        The behavioral paradigm pattern to search for (default is \"memory\").\n\n    Returns\n    -------\n    np.ndarray\n        A boolean array where `True` indicates that the epoch is part of a pre-task-post sequence\n        (i.e., sleep-task-sleep) based on the provided environment and paradigm.\n\n    Examples\n    -------\n    &gt;&gt;&gt; epoch_df = pd.DataFrame({\n    ...     'name': ['EE.042', 'EE.045', 'EE.046', 'EE.049', 'EE.050'],\n    ...     'startTime': [0.0, 995.9384, 3336.3928, 5722.444, 7511.244],\n    ...     'stopTime': [995.9384, 3336.3928, 5722.444, 7511.244, 9387.644],\n    ...     'environment': ['sleep', 'tmaze', 'sleep', 'tmaze', 'sleep'],\n    ...     'behavioralParadigm': [np.nan, 'Spontaneous alternation task', np.nan, 'Working memory task', np.nan]\n    ... })\n    &gt;&gt;&gt; idx = find_env_paradigm_pre_task_post(epoch_df)\n    &gt;&gt;&gt; epoch_df[idx]\n          name  startTime   stopTime environment        behavioralParadigm\n    2  EE.046   3336.3928  5722.444       sleep                        NaN\n    3  EE.049   5722.444   7511.244      tmaze         Working memory task\n    4  EE.050   7511.244   9387.644       sleep                        NaN\n    \"\"\"\n    # compress back to back sleep epochs\n    epoch_df_ = compress_repeated_epochs(epoch_df, epoch_name=\"sleep\")\n    # make col with env and paradigm\n    epoch_df_[\"sleep_ind\"] = (\n        epoch_df_.environment + \"_\" + epoch_df_.behavioralParadigm.astype(str)\n    )\n    # locate env and paradigm of choice with this col\n    epoch_df_[\"sleep_ind\"] = epoch_df_[\"sleep_ind\"].str.contains(env + \"|\" + paradigm)\n    # the pattern we are looking for is all True\n\n    # https://stackoverflow.com/questions/48710783/pandas-find-and-index-rows-that-match-row-sequence-pattern\n    pat = np.asarray([True, True, True])\n    N = len(pat)\n    idx = (\n        epoch_df_[\"sleep_ind\"]\n        .rolling(window=N, min_periods=N)\n        .apply(lambda x: (x == pat).all())\n        .mask(lambda x: x == 0)\n        .bfill(limit=N - 1)\n        .fillna(0)\n        .astype(bool)\n    ).values\n    return idx\n</code></pre>"},{"location":"reference/neuro_py/session/#neuro_py.session.find_epoch_pattern","title":"<code>find_epoch_pattern(env, pattern)</code>","text":"<p>Finds the first occurrence of a contiguous pattern of epochs in the environment list.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>list or Series</code> <p>The environment list or pandas Series representing the epochs.</p> required <code>pattern</code> <code>list of str</code> <p>The pattern to search for in the environment list.</p> required <p>Returns:</p> Type Description <code>tuple of (np.ndarray, np.ndarray) or (None, None)</code> <p>Returns a tuple where the first element is a boolean mask indicating the positions of the found pattern, and the second element is an array of indices where the pattern occurs. If the pattern is not found, returns (None, None).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n&gt;&gt;&gt; pattern_idx,_ = find_epoch_pattern(epoch_df.environment,['sleep','linear','sleep'])\n&gt;&gt;&gt; epoch_df.loc[pattern_idx]\n    name                    startTime       stopTime        environment     behavioralParadigm      notes\n0   preSleep_210411_064951  0.0000      9544.56315  sleep       NaN                 NaN\n1   maze_210411_095201          9544.5632   11752.80635     linear      novel                   novel\n2   postSleep_210411_103522 11752.8064      23817.68955     sleep       novel                   novel\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def find_epoch_pattern(\n    env: Union[List[str], pd.Series], pattern: List[str]\n) -&gt; Union[Tuple[np.ndarray, np.ndarray], Tuple[None, None]]:\n    \"\"\"\n    Finds the first occurrence of a contiguous pattern of epochs in the environment list.\n\n    Parameters\n    ----------\n    env : list or pd.Series\n        The environment list or pandas Series representing the epochs.\n    pattern : list of str\n        The pattern to search for in the environment list.\n\n    Returns\n    -------\n    tuple of (np.ndarray, np.ndarray) or (None, None)\n        Returns a tuple where the first element is a boolean mask indicating the positions of the found pattern,\n        and the second element is an array of indices where the pattern occurs.\n        If the pattern is not found, returns (None, None).\n\n    Examples\n    -------\n    &gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n    &gt;&gt;&gt; pattern_idx,_ = find_epoch_pattern(epoch_df.environment,['sleep','linear','sleep'])\n    &gt;&gt;&gt; epoch_df.loc[pattern_idx]\n        name\t                startTime\tstopTime\tenvironment\tbehavioralParadigm\tnotes\n    0\tpreSleep_210411_064951\t0.0000\t    9544.56315\tsleep\t    NaN\t                NaN\n    1\tmaze_210411_095201\t    9544.5632\t11752.80635\tlinear\t    novel\t            novel\n    2\tpostSleep_210411_103522\t11752.8064\t23817.68955\tsleep\t    novel\t            novel\n    \"\"\"\n\n    env = list(env)\n    pattern = list(pattern)\n\n    if len(env) &lt; len(pattern):\n        return None, None\n\n    dummy = np.zeros(len(env))\n\n    for i in range(len(env) - len(pattern) + 1):\n        if pattern == env[i : i + len(pattern)]:\n            dummy[i : i + len(pattern)] = 1\n            dummy = dummy == 1\n            return dummy, np.arange(i, i + len(pattern))\n    return None, None\n</code></pre>"},{"location":"reference/neuro_py/session/#neuro_py.session.find_multitask_pre_post","title":"<code>find_multitask_pre_post(env, task_tag=None, post_sleep_flank=False, pre_sleep_common=False)</code>","text":"<p>Find the row indices for pre-task/post-task sleep epochs in the given environment from a DataFrame column.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Series</code> <p>Column from the DataFrame representing the session epochs data.</p> required <code>task_tag</code> <code>str</code> <p>A string indicating the task(s) (e.g., \"linear\", \"linear|box\") to filter for. If None, all non-sleep epochs are considered as task epochs.</p> <code>None</code> <code>post_sleep_flank</code> <code>bool</code> <p>If True, ensure that the post-task sleep epoch directly follows the task.</p> <code>False</code> <code>pre_sleep_common</code> <code>bool</code> <p>If True, use the first pre-task sleep epoch as the pre-task sleep for all tasks.</p> <code>False</code> <p>Returns:</p> Type Description <code>list of list of int, or None</code> <p>A list of indices for pre-task, task, and post-task epochs in the format [pre_task, task, post_task]. If no such sequence is found, returns None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_df = pd.DataFrame({\n...     'environment': ['sleep', 'linear', 'sleep', 'box', 'sleep']\n... })\n&gt;&gt;&gt; find_multitask_pre_post(epoch_df['environment'], task_tag='linear')\n[[0, 1, 2]]\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def find_multitask_pre_post(\n    env: pd.Series,\n    task_tag: Union[None, str] = None,\n    post_sleep_flank: bool = False,\n    pre_sleep_common: bool = False,\n) -&gt; Union[List[List[int]], None]:\n    \"\"\"\n    Find the row indices for pre-task/post-task sleep epochs in the given environment from a DataFrame column.\n\n    Parameters\n    ----------\n    env : pd.Series\n        Column from the DataFrame representing the session epochs data.\n    task_tag : str, optional\n        A string indicating the task(s) (e.g., \"linear\", \"linear|box\") to filter for.\n        If None, all non-sleep epochs are considered as task epochs.\n    post_sleep_flank : bool, optional\n        If True, ensure that the post-task sleep epoch directly follows the task.\n    pre_sleep_common : bool, optional\n        If True, use the first pre-task sleep epoch as the pre-task sleep for all tasks.\n\n    Returns\n    -------\n    list of list of int, or None\n        A list of indices for pre-task, task, and post-task epochs in the format [pre_task, task, post_task].\n        If no such sequence is found, returns None.\n\n    Examples\n    -------\n    &gt;&gt;&gt; epoch_df = pd.DataFrame({\n    ...     'environment': ['sleep', 'linear', 'sleep', 'box', 'sleep']\n    ... })\n    &gt;&gt;&gt; find_multitask_pre_post(epoch_df['environment'], task_tag='linear')\n    [[0, 1, 2]]\n    \"\"\"\n    # Find the row indices that contain the search string in the specified column\n    if task_tag is None:\n        task_bool = ~env.str.contains(\"sleep\", case=False)\n    else:\n        task_bool = env.str.contains(task_tag, case=False)\n    sleep_bool = env.str.contains(\"sleep\", case=False)\n\n    # find the task indices\n    task_idx = np.where(task_bool)[0]\n    # remove 0 index, task can never be first\n    task_idx = task_idx[task_idx != 0]\n    # find the sleep indices\n    sleep_idx = np.where(sleep_bool)[0]\n\n    pre_task_post = []\n    for task in task_idx:\n        temp = sleep_idx - task\n        pre_task = sleep_idx[temp &lt; 0]\n        post_task = sleep_idx[temp &gt; 0]\n\n        if len(post_task) == 0:\n            logging.warning(\"no post_task sleep for task epoch \" + str(task))\n        elif len(pre_task) == 0:\n            logging.warning(\"no pre_task sleep for task epoch \" + str(task))\n        else:\n            pre_task_post.append([pre_task[-1], task, post_task[0]])\n\n    if len(pre_task_post) == 0:\n        pre_task_post = None\n\n    # search for epochs where the last epoch is 1 more than the first epoch\n    if post_sleep_flank and pre_task_post is not None:\n        pre_task_post_ = []\n        for seq in pre_task_post:\n            if seq[-1] - seq[1] == 1:\n                pre_task_post_.append(seq)\n        pre_task_post = pre_task_post_\n\n    # make the first pre task sleep the same pre task in subsequent tasks\n    if pre_sleep_common and pre_task_post is not None:\n        pre_task_post_ = []\n        for seq in pre_task_post:\n            pre_task_post_.append([pre_task_post[0][0], seq[1], seq[2]])\n        pre_task_post = pre_task_post_\n\n    return pre_task_post\n</code></pre>"},{"location":"reference/neuro_py/session/#neuro_py.session.find_pre_task_post","title":"<code>find_pre_task_post(env, pre_post_label='sleep')</code>","text":"<p>Finds the first contiguous epochs that meet the pre/task/post pattern in the environment list.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>list or ndarray</code> <p>List or array of environment labels (e.g., 'sleep', 'wmaze', etc.).</p> required <code>pre_post_label</code> <code>str</code> <p>Label used to identify pre and post sleep epochs (default is 'sleep').</p> <code>'sleep'</code> <p>Returns:</p> Name Type Description <code>dummy</code> <code>ndarray or None</code> <p>A boolean array where the identified pre/task/post epochs are marked as True. If no pattern is found, returns None.</p> <code>indices</code> <code>list or None</code> <p>A list of indices where the pre/task/post epochs are found. If no pattern is found, returns None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; env = ['sleep', 'wmaze', 'sleep']\n&gt;&gt;&gt; find_pre_task_post(env)\n(array([ True,  True,  True]), [0, 1, 2])\n</code></pre> Notes <p>This function identifies a pattern where the pre-task-post epochs are of the form: - pre-sleep (pre_post_label) - task (any label other than pre_post_label) - post-sleep (pre_post_label)</p> <p>The function returns the indices of the first occurrence of such a pattern.</p> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def find_pre_task_post(\n    env: Union[List[str], np.ndarray], pre_post_label: str = \"sleep\"\n) -&gt; Tuple[Union[np.ndarray, None], Union[List[int], None]]:\n    \"\"\"\n    Finds the first contiguous epochs that meet the pre/task/post pattern in the environment list.\n\n    Parameters\n    ----------\n    env : list or np.ndarray\n        List or array of environment labels (e.g., 'sleep', 'wmaze', etc.).\n    pre_post_label : str, optional\n        Label used to identify pre and post sleep epochs (default is 'sleep').\n\n    Returns\n    -------\n    dummy : np.ndarray or None\n        A boolean array where the identified pre/task/post epochs are marked as True.\n        If no pattern is found, returns None.\n    indices : list or None\n        A list of indices where the pre/task/post epochs are found. If no pattern is found, returns None.\n\n    Examples\n    -------\n    &gt;&gt;&gt; env = ['sleep', 'wmaze', 'sleep']\n    &gt;&gt;&gt; find_pre_task_post(env)\n    (array([ True,  True,  True]), [0, 1, 2])\n\n    Notes\n    -----\n    This function identifies a pattern where the pre-task-post epochs are of the form:\n    - pre-sleep (pre_post_label)\n    - task (any label other than pre_post_label)\n    - post-sleep (pre_post_label)\n\n    The function returns the indices of the first occurrence of such a pattern.\n    \"\"\"\n    if len(env) &lt; 3:\n        return None, None\n    numeric_idx = (pre_post_label == env) * 1\n    dummy = np.zeros_like(numeric_idx) == 1\n    if all(numeric_idx[:3] == [1, 0, 1]):\n        dummy[:3] = True\n        return dummy, [0, 1, 2]\n    else:\n        for i in np.arange(len(numeric_idx) + 3):\n            if 3 + i &gt; len(numeric_idx):\n                return None, None\n            if all(numeric_idx[0 + i : 3 + i] == [1, 0, 1]):\n                dummy[0 + i : 3 + i] = True\n                return dummy, [0, 1, 2] + i\n</code></pre>"},{"location":"reference/neuro_py/session/#neuro_py.session.find_pre_task_post_optimize_novel","title":"<code>find_pre_task_post_optimize_novel(epoch_df, novel_indicators=[1, 'novel', '1'])</code>","text":"<p>Find pre-task-post epochs in the DataFrame, optimizing for novel epochs.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_df</code> <code>DataFrame</code> <p>DataFrame containing epochs information with 'environment' and 'behavioralParadigm' columns.</p> required <code>novel_indicators</code> <code>list of [int, str]</code> <p>List of indicators used to identify novel epochs in the 'behavioralParadigm' column (default is [1, \"novel\", \"1\"]).</p> <code>[1, 'novel', '1']</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>A DataFrame with pre-task-post epochs, or None if no such pattern is found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n&gt;&gt;&gt; epoch_df = find_pre_task_post_optimize_novel(epoch_df)\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def find_pre_task_post_optimize_novel(\n    epoch_df: pd.DataFrame, novel_indicators: List[Union[int, str]] = [1, \"novel\", \"1\"]\n) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Find pre-task-post epochs in the DataFrame, optimizing for novel epochs.\n\n    Parameters\n    ----------\n    epoch_df : pd.DataFrame\n        DataFrame containing epochs information with 'environment' and 'behavioralParadigm' columns.\n    novel_indicators : list of [int, str], optional\n        List of indicators used to identify novel epochs in the 'behavioralParadigm' column (default is [1, \"novel\", \"1\"]).\n\n    Returns\n    -------\n    pd.DataFrame or None\n        A DataFrame with pre-task-post epochs, or None if no such pattern is found.\n\n    Examples\n    -------\n    &gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n    &gt;&gt;&gt; epoch_df = find_pre_task_post_optimize_novel(epoch_df)\n    \"\"\"\n    # set sleep to nan\n    epoch_df.loc[epoch_df.environment == \"sleep\", \"behavioralParadigm\"] = np.nan\n    # Search for novel epochs\n    novel_mask = epoch_df.behavioralParadigm.isin(novel_indicators)\n    if novel_mask.any():\n        # Find the first novel epoch\n        idx = np.where(novel_mask)[0][0]\n        # Select the first novel epoch and the epochs before and after it\n        mask = np.hstack([idx - 1, idx, idx + 1])\n        # If any of the epochs are negative, skip (this means the novel epoch was the first epoch)\n        if any(mask &lt; 0):\n            pass\n        else:\n            epoch_df_temp = epoch_df.loc[mask]\n            # Find pre task post epochs in this subset\n            idx = find_pre_task_post(epoch_df_temp.environment)\n            # If no pre task post epochs are found, skip\n            if idx is None or idx[0] is None:\n                pass\n            else:\n                epoch_df = epoch_df_temp.reset_index(drop=True)\n    # Find the first pre task post epoch in epoch_df, if the df was modified that will be used\n    idx, _ = find_pre_task_post(epoch_df.environment)\n    if idx is None:\n        return None\n    epoch_df = epoch_df.loc[idx].reset_index(drop=True)\n    return epoch_df\n</code></pre>"},{"location":"reference/neuro_py/session/#neuro_py.session.get_experience_level","title":"<code>get_experience_level(behavioralParadigm)</code>","text":"<p>Extract the experience level from the behavioralParadigm column.</p> <p>The experience level is the number of times the animal has run the task, inferred from the behavioralParadigm column.</p> <p>Parameters:</p> Name Type Description Default <code>behavioralParadigm</code> <code>Series</code> <p>A single entry or value from the behavioralParadigm column of an epoch.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The experience level as an integer. Returns NaN if experience cannot be determined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; experience = get_experience_level(current_epoch_df.iloc[1].behavioralParadigm)\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def get_experience_level(behavioralParadigm: pd.Series) -&gt; int:\n    \"\"\"\n    Extract the experience level from the behavioralParadigm column.\n\n    The experience level is the number of times the animal has run the task,\n    inferred from the behavioralParadigm column.\n\n    Parameters\n    ----------\n    behavioralParadigm : pd.Series\n        A single entry or value from the behavioralParadigm column of an epoch.\n\n    Returns\n    -------\n    int\n        The experience level as an integer. Returns NaN if experience cannot be determined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; experience = get_experience_level(current_epoch_df.iloc[1].behavioralParadigm)\n    \"\"\"\n    if behavioralParadigm == \"novel\":\n        experience = 1\n    else:\n        try:\n            # extract first number from string\n            experience = int(re.findall(r\"\\d+\", behavioralParadigm)[0])\n        except Exception:\n            try:\n                # extract experience level from behavioralParadigm column if it is a number\n                experience = int(behavioralParadigm)\n            except Exception:\n                experience = np.nan\n    return experience\n</code></pre>"},{"location":"reference/neuro_py/session/locate_epochs/","title":"neuro_py.session.locate_epochs","text":""},{"location":"reference/neuro_py/session/locate_epochs/#neuro_py.session.locate_epochs.compress_repeated_epochs","title":"<code>compress_repeated_epochs(epoch_df, epoch_name=None)</code>","text":"<p>Compress repeated epochs in an epoch DataFrame. If consecutive epochs have the same name, they will be combined into a single epoch with the earliest startTime and the latest stopTime.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_df</code> <code>DataFrame</code> <p>A DataFrame containing epoch information. Must have columns <code>environment</code>, <code>startTime</code>, and <code>stopTime</code>.</p> required <code>epoch_name</code> <code>str</code> <p>If provided, only compress epochs with this specific name. If None, compress all consecutive epochs with the same name.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame where consecutive epochs with the same name are compressed into a single epoch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_df = pd.DataFrame({\n...     'environment': ['sleep', 'sleep', 'wmaze', 'wmaze', 'sleep'],\n...     'startTime': [0, 100, 200, 300, 400],\n...     'stopTime': [99, 199, 299, 399, 499]\n... })\n&gt;&gt;&gt; compress_repeated_epochs(epoch_df)\n  environment  startTime  stopTime\n0       sleep          0       199\n1       wmaze        200       399\n2       sleep        400       499\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def compress_repeated_epochs(epoch_df, epoch_name=None):\n    \"\"\"\n    Compress repeated epochs in an epoch DataFrame. If consecutive epochs have the same name,\n    they will be combined into a single epoch with the earliest startTime and the latest stopTime.\n\n    Parameters\n    ----------\n    epoch_df : pd.DataFrame\n        A DataFrame containing epoch information. Must have columns `environment`, `startTime`, and `stopTime`.\n    epoch_name : str, optional\n        If provided, only compress epochs with this specific name. If None, compress all consecutive epochs with the same name.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame where consecutive epochs with the same name are compressed into a single epoch.\n\n    Examples\n    -------\n    &gt;&gt;&gt; epoch_df = pd.DataFrame({\n    ...     'environment': ['sleep', 'sleep', 'wmaze', 'wmaze', 'sleep'],\n    ...     'startTime': [0, 100, 200, 300, 400],\n    ...     'stopTime': [99, 199, 299, 399, 499]\n    ... })\n    &gt;&gt;&gt; compress_repeated_epochs(epoch_df)\n      environment  startTime  stopTime\n    0       sleep          0       199\n    1       wmaze        200       399\n    2       sleep        400       499\n    \"\"\"\n    if epoch_name is None:\n        match = np.zeros([epoch_df.environment.shape[0]])\n        match[match == 0] = np.nan\n        for i, ep in enumerate(epoch_df.environment[:-1]):\n            if np.isnan(match[i]):\n                # find match in current and next epoch\n                if ep == epoch_df.environment.iloc[i + 1]:\n                    match[i : i + 2] = i\n                    # given match, see if there are more matches\n                    for match_i in np.arange(1, epoch_df.environment[:-1].shape[0]):\n                        if i + 1 + match_i == epoch_df.environment.shape[0]:\n                            break\n                        if ep == epoch_df.environment.iloc[i + 1 + match_i]:\n                            match[i : i + 1 + match_i + 1] = i\n                        else:\n                            break\n    else:\n        match = np.zeros([epoch_df.environment.shape[0]])\n        match[match == 0] = np.nan\n        for i, ep in enumerate(epoch_df.environment[:-1]):\n            if np.isnan(match[i]):\n                # find match in current and next epoch\n                if (ep == epoch_df.environment.iloc[i + 1]) &amp; (ep == epoch_name):\n                    match[i : i + 2] = i\n                    # given match, see if there are more matches\n                    for match_i in np.arange(1, epoch_df.environment[:-1].shape[0]):\n                        if i + 1 + match_i == epoch_df.environment.shape[0]:\n                            break\n                        if ep == epoch_df.environment.iloc[i + 1 + match_i]:\n                            match[i : i + 1 + match_i + 1] = i\n                        else:\n                            break\n\n    for i in range(len(match)):\n        if np.isnan(match[i]):\n            # make nans large numbers that are unlikely to be real epoch\n            match[i] = (i + 1) * 2000\n\n    # iter through each epoch indicator to get start and stop\n    results = pd.DataFrame()\n    no_nan_match = match[~np.isnan(match)]\n    for m in pd.unique(no_nan_match):\n        temp_dict = {}\n        for item in epoch_df.keys():\n            temp_dict[item] = epoch_df[match == m][item].iloc[0]\n\n        temp_dict[\"startTime\"] = epoch_df[match == m].startTime.min()\n        temp_dict[\"stopTime\"] = epoch_df[match == m].stopTime.max()\n\n        temp_df = pd.DataFrame.from_dict(temp_dict, orient=\"index\").T\n\n        results = pd.concat([results, temp_df], ignore_index=True)\n    return results\n</code></pre>"},{"location":"reference/neuro_py/session/locate_epochs/#neuro_py.session.locate_epochs.find_env_paradigm_pre_task_post","title":"<code>find_env_paradigm_pre_task_post(epoch_df, env='sleep', paradigm='memory')</code>","text":"<p>Find indices of epochs that match a sequence of environment and paradigm patterns, specifically looking for a pre-task-post structure.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_df</code> <code>DataFrame</code> <p>DataFrame containing epoch information with columns such as 'environment' and 'behavioralParadigm'.</p> required <code>env</code> <code>str</code> <p>The environment pattern to search for (default is \"sleep\").</p> <code>'sleep'</code> <code>paradigm</code> <code>str</code> <p>The behavioral paradigm pattern to search for (default is \"memory\").</p> <code>'memory'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A boolean array where <code>True</code> indicates that the epoch is part of a pre-task-post sequence (i.e., sleep-task-sleep) based on the provided environment and paradigm.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_df = pd.DataFrame({\n...     'name': ['EE.042', 'EE.045', 'EE.046', 'EE.049', 'EE.050'],\n...     'startTime': [0.0, 995.9384, 3336.3928, 5722.444, 7511.244],\n...     'stopTime': [995.9384, 3336.3928, 5722.444, 7511.244, 9387.644],\n...     'environment': ['sleep', 'tmaze', 'sleep', 'tmaze', 'sleep'],\n...     'behavioralParadigm': [np.nan, 'Spontaneous alternation task', np.nan, 'Working memory task', np.nan]\n... })\n&gt;&gt;&gt; idx = find_env_paradigm_pre_task_post(epoch_df)\n&gt;&gt;&gt; epoch_df[idx]\n      name  startTime   stopTime environment        behavioralParadigm\n2  EE.046   3336.3928  5722.444       sleep                        NaN\n3  EE.049   5722.444   7511.244      tmaze         Working memory task\n4  EE.050   7511.244   9387.644       sleep                        NaN\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def find_env_paradigm_pre_task_post(\n    epoch_df: pd.DataFrame, env: str = \"sleep\", paradigm: str = \"memory\"\n) -&gt; np.ndarray:\n    \"\"\"\n    Find indices of epochs that match a sequence of environment and paradigm\n    patterns, specifically looking for a pre-task-post structure.\n\n    Parameters\n    ----------\n    epoch_df : pd.DataFrame\n        DataFrame containing epoch information with columns such as 'environment' and 'behavioralParadigm'.\n    env : str, optional\n        The environment pattern to search for (default is \"sleep\").\n    paradigm : str, optional\n        The behavioral paradigm pattern to search for (default is \"memory\").\n\n    Returns\n    -------\n    np.ndarray\n        A boolean array where `True` indicates that the epoch is part of a pre-task-post sequence\n        (i.e., sleep-task-sleep) based on the provided environment and paradigm.\n\n    Examples\n    -------\n    &gt;&gt;&gt; epoch_df = pd.DataFrame({\n    ...     'name': ['EE.042', 'EE.045', 'EE.046', 'EE.049', 'EE.050'],\n    ...     'startTime': [0.0, 995.9384, 3336.3928, 5722.444, 7511.244],\n    ...     'stopTime': [995.9384, 3336.3928, 5722.444, 7511.244, 9387.644],\n    ...     'environment': ['sleep', 'tmaze', 'sleep', 'tmaze', 'sleep'],\n    ...     'behavioralParadigm': [np.nan, 'Spontaneous alternation task', np.nan, 'Working memory task', np.nan]\n    ... })\n    &gt;&gt;&gt; idx = find_env_paradigm_pre_task_post(epoch_df)\n    &gt;&gt;&gt; epoch_df[idx]\n          name  startTime   stopTime environment        behavioralParadigm\n    2  EE.046   3336.3928  5722.444       sleep                        NaN\n    3  EE.049   5722.444   7511.244      tmaze         Working memory task\n    4  EE.050   7511.244   9387.644       sleep                        NaN\n    \"\"\"\n    # compress back to back sleep epochs\n    epoch_df_ = compress_repeated_epochs(epoch_df, epoch_name=\"sleep\")\n    # make col with env and paradigm\n    epoch_df_[\"sleep_ind\"] = (\n        epoch_df_.environment + \"_\" + epoch_df_.behavioralParadigm.astype(str)\n    )\n    # locate env and paradigm of choice with this col\n    epoch_df_[\"sleep_ind\"] = epoch_df_[\"sleep_ind\"].str.contains(env + \"|\" + paradigm)\n    # the pattern we are looking for is all True\n\n    # https://stackoverflow.com/questions/48710783/pandas-find-and-index-rows-that-match-row-sequence-pattern\n    pat = np.asarray([True, True, True])\n    N = len(pat)\n    idx = (\n        epoch_df_[\"sleep_ind\"]\n        .rolling(window=N, min_periods=N)\n        .apply(lambda x: (x == pat).all())\n        .mask(lambda x: x == 0)\n        .bfill(limit=N - 1)\n        .fillna(0)\n        .astype(bool)\n    ).values\n    return idx\n</code></pre>"},{"location":"reference/neuro_py/session/locate_epochs/#neuro_py.session.locate_epochs.find_epoch_pattern","title":"<code>find_epoch_pattern(env, pattern)</code>","text":"<p>Finds the first occurrence of a contiguous pattern of epochs in the environment list.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>list or Series</code> <p>The environment list or pandas Series representing the epochs.</p> required <code>pattern</code> <code>list of str</code> <p>The pattern to search for in the environment list.</p> required <p>Returns:</p> Type Description <code>tuple of (np.ndarray, np.ndarray) or (None, None)</code> <p>Returns a tuple where the first element is a boolean mask indicating the positions of the found pattern, and the second element is an array of indices where the pattern occurs. If the pattern is not found, returns (None, None).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n&gt;&gt;&gt; pattern_idx,_ = find_epoch_pattern(epoch_df.environment,['sleep','linear','sleep'])\n&gt;&gt;&gt; epoch_df.loc[pattern_idx]\n    name                    startTime       stopTime        environment     behavioralParadigm      notes\n0   preSleep_210411_064951  0.0000      9544.56315  sleep       NaN                 NaN\n1   maze_210411_095201          9544.5632   11752.80635     linear      novel                   novel\n2   postSleep_210411_103522 11752.8064      23817.68955     sleep       novel                   novel\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def find_epoch_pattern(\n    env: Union[List[str], pd.Series], pattern: List[str]\n) -&gt; Union[Tuple[np.ndarray, np.ndarray], Tuple[None, None]]:\n    \"\"\"\n    Finds the first occurrence of a contiguous pattern of epochs in the environment list.\n\n    Parameters\n    ----------\n    env : list or pd.Series\n        The environment list or pandas Series representing the epochs.\n    pattern : list of str\n        The pattern to search for in the environment list.\n\n    Returns\n    -------\n    tuple of (np.ndarray, np.ndarray) or (None, None)\n        Returns a tuple where the first element is a boolean mask indicating the positions of the found pattern,\n        and the second element is an array of indices where the pattern occurs.\n        If the pattern is not found, returns (None, None).\n\n    Examples\n    -------\n    &gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n    &gt;&gt;&gt; pattern_idx,_ = find_epoch_pattern(epoch_df.environment,['sleep','linear','sleep'])\n    &gt;&gt;&gt; epoch_df.loc[pattern_idx]\n        name\t                startTime\tstopTime\tenvironment\tbehavioralParadigm\tnotes\n    0\tpreSleep_210411_064951\t0.0000\t    9544.56315\tsleep\t    NaN\t                NaN\n    1\tmaze_210411_095201\t    9544.5632\t11752.80635\tlinear\t    novel\t            novel\n    2\tpostSleep_210411_103522\t11752.8064\t23817.68955\tsleep\t    novel\t            novel\n    \"\"\"\n\n    env = list(env)\n    pattern = list(pattern)\n\n    if len(env) &lt; len(pattern):\n        return None, None\n\n    dummy = np.zeros(len(env))\n\n    for i in range(len(env) - len(pattern) + 1):\n        if pattern == env[i : i + len(pattern)]:\n            dummy[i : i + len(pattern)] = 1\n            dummy = dummy == 1\n            return dummy, np.arange(i, i + len(pattern))\n    return None, None\n</code></pre>"},{"location":"reference/neuro_py/session/locate_epochs/#neuro_py.session.locate_epochs.find_multitask_pre_post","title":"<code>find_multitask_pre_post(env, task_tag=None, post_sleep_flank=False, pre_sleep_common=False)</code>","text":"<p>Find the row indices for pre-task/post-task sleep epochs in the given environment from a DataFrame column.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Series</code> <p>Column from the DataFrame representing the session epochs data.</p> required <code>task_tag</code> <code>str</code> <p>A string indicating the task(s) (e.g., \"linear\", \"linear|box\") to filter for. If None, all non-sleep epochs are considered as task epochs.</p> <code>None</code> <code>post_sleep_flank</code> <code>bool</code> <p>If True, ensure that the post-task sleep epoch directly follows the task.</p> <code>False</code> <code>pre_sleep_common</code> <code>bool</code> <p>If True, use the first pre-task sleep epoch as the pre-task sleep for all tasks.</p> <code>False</code> <p>Returns:</p> Type Description <code>list of list of int, or None</code> <p>A list of indices for pre-task, task, and post-task epochs in the format [pre_task, task, post_task]. If no such sequence is found, returns None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_df = pd.DataFrame({\n...     'environment': ['sleep', 'linear', 'sleep', 'box', 'sleep']\n... })\n&gt;&gt;&gt; find_multitask_pre_post(epoch_df['environment'], task_tag='linear')\n[[0, 1, 2]]\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def find_multitask_pre_post(\n    env: pd.Series,\n    task_tag: Union[None, str] = None,\n    post_sleep_flank: bool = False,\n    pre_sleep_common: bool = False,\n) -&gt; Union[List[List[int]], None]:\n    \"\"\"\n    Find the row indices for pre-task/post-task sleep epochs in the given environment from a DataFrame column.\n\n    Parameters\n    ----------\n    env : pd.Series\n        Column from the DataFrame representing the session epochs data.\n    task_tag : str, optional\n        A string indicating the task(s) (e.g., \"linear\", \"linear|box\") to filter for.\n        If None, all non-sleep epochs are considered as task epochs.\n    post_sleep_flank : bool, optional\n        If True, ensure that the post-task sleep epoch directly follows the task.\n    pre_sleep_common : bool, optional\n        If True, use the first pre-task sleep epoch as the pre-task sleep for all tasks.\n\n    Returns\n    -------\n    list of list of int, or None\n        A list of indices for pre-task, task, and post-task epochs in the format [pre_task, task, post_task].\n        If no such sequence is found, returns None.\n\n    Examples\n    -------\n    &gt;&gt;&gt; epoch_df = pd.DataFrame({\n    ...     'environment': ['sleep', 'linear', 'sleep', 'box', 'sleep']\n    ... })\n    &gt;&gt;&gt; find_multitask_pre_post(epoch_df['environment'], task_tag='linear')\n    [[0, 1, 2]]\n    \"\"\"\n    # Find the row indices that contain the search string in the specified column\n    if task_tag is None:\n        task_bool = ~env.str.contains(\"sleep\", case=False)\n    else:\n        task_bool = env.str.contains(task_tag, case=False)\n    sleep_bool = env.str.contains(\"sleep\", case=False)\n\n    # find the task indices\n    task_idx = np.where(task_bool)[0]\n    # remove 0 index, task can never be first\n    task_idx = task_idx[task_idx != 0]\n    # find the sleep indices\n    sleep_idx = np.where(sleep_bool)[0]\n\n    pre_task_post = []\n    for task in task_idx:\n        temp = sleep_idx - task\n        pre_task = sleep_idx[temp &lt; 0]\n        post_task = sleep_idx[temp &gt; 0]\n\n        if len(post_task) == 0:\n            logging.warning(\"no post_task sleep for task epoch \" + str(task))\n        elif len(pre_task) == 0:\n            logging.warning(\"no pre_task sleep for task epoch \" + str(task))\n        else:\n            pre_task_post.append([pre_task[-1], task, post_task[0]])\n\n    if len(pre_task_post) == 0:\n        pre_task_post = None\n\n    # search for epochs where the last epoch is 1 more than the first epoch\n    if post_sleep_flank and pre_task_post is not None:\n        pre_task_post_ = []\n        for seq in pre_task_post:\n            if seq[-1] - seq[1] == 1:\n                pre_task_post_.append(seq)\n        pre_task_post = pre_task_post_\n\n    # make the first pre task sleep the same pre task in subsequent tasks\n    if pre_sleep_common and pre_task_post is not None:\n        pre_task_post_ = []\n        for seq in pre_task_post:\n            pre_task_post_.append([pre_task_post[0][0], seq[1], seq[2]])\n        pre_task_post = pre_task_post_\n\n    return pre_task_post\n</code></pre>"},{"location":"reference/neuro_py/session/locate_epochs/#neuro_py.session.locate_epochs.find_pre_task_post","title":"<code>find_pre_task_post(env, pre_post_label='sleep')</code>","text":"<p>Finds the first contiguous epochs that meet the pre/task/post pattern in the environment list.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>list or ndarray</code> <p>List or array of environment labels (e.g., 'sleep', 'wmaze', etc.).</p> required <code>pre_post_label</code> <code>str</code> <p>Label used to identify pre and post sleep epochs (default is 'sleep').</p> <code>'sleep'</code> <p>Returns:</p> Name Type Description <code>dummy</code> <code>ndarray or None</code> <p>A boolean array where the identified pre/task/post epochs are marked as True. If no pattern is found, returns None.</p> <code>indices</code> <code>list or None</code> <p>A list of indices where the pre/task/post epochs are found. If no pattern is found, returns None.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; env = ['sleep', 'wmaze', 'sleep']\n&gt;&gt;&gt; find_pre_task_post(env)\n(array([ True,  True,  True]), [0, 1, 2])\n</code></pre> Notes <p>This function identifies a pattern where the pre-task-post epochs are of the form: - pre-sleep (pre_post_label) - task (any label other than pre_post_label) - post-sleep (pre_post_label)</p> <p>The function returns the indices of the first occurrence of such a pattern.</p> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def find_pre_task_post(\n    env: Union[List[str], np.ndarray], pre_post_label: str = \"sleep\"\n) -&gt; Tuple[Union[np.ndarray, None], Union[List[int], None]]:\n    \"\"\"\n    Finds the first contiguous epochs that meet the pre/task/post pattern in the environment list.\n\n    Parameters\n    ----------\n    env : list or np.ndarray\n        List or array of environment labels (e.g., 'sleep', 'wmaze', etc.).\n    pre_post_label : str, optional\n        Label used to identify pre and post sleep epochs (default is 'sleep').\n\n    Returns\n    -------\n    dummy : np.ndarray or None\n        A boolean array where the identified pre/task/post epochs are marked as True.\n        If no pattern is found, returns None.\n    indices : list or None\n        A list of indices where the pre/task/post epochs are found. If no pattern is found, returns None.\n\n    Examples\n    -------\n    &gt;&gt;&gt; env = ['sleep', 'wmaze', 'sleep']\n    &gt;&gt;&gt; find_pre_task_post(env)\n    (array([ True,  True,  True]), [0, 1, 2])\n\n    Notes\n    -----\n    This function identifies a pattern where the pre-task-post epochs are of the form:\n    - pre-sleep (pre_post_label)\n    - task (any label other than pre_post_label)\n    - post-sleep (pre_post_label)\n\n    The function returns the indices of the first occurrence of such a pattern.\n    \"\"\"\n    if len(env) &lt; 3:\n        return None, None\n    numeric_idx = (pre_post_label == env) * 1\n    dummy = np.zeros_like(numeric_idx) == 1\n    if all(numeric_idx[:3] == [1, 0, 1]):\n        dummy[:3] = True\n        return dummy, [0, 1, 2]\n    else:\n        for i in np.arange(len(numeric_idx) + 3):\n            if 3 + i &gt; len(numeric_idx):\n                return None, None\n            if all(numeric_idx[0 + i : 3 + i] == [1, 0, 1]):\n                dummy[0 + i : 3 + i] = True\n                return dummy, [0, 1, 2] + i\n</code></pre>"},{"location":"reference/neuro_py/session/locate_epochs/#neuro_py.session.locate_epochs.find_pre_task_post_optimize_novel","title":"<code>find_pre_task_post_optimize_novel(epoch_df, novel_indicators=[1, 'novel', '1'])</code>","text":"<p>Find pre-task-post epochs in the DataFrame, optimizing for novel epochs.</p> <p>Parameters:</p> Name Type Description Default <code>epoch_df</code> <code>DataFrame</code> <p>DataFrame containing epochs information with 'environment' and 'behavioralParadigm' columns.</p> required <code>novel_indicators</code> <code>list of [int, str]</code> <p>List of indicators used to identify novel epochs in the 'behavioralParadigm' column (default is [1, \"novel\", \"1\"]).</p> <code>[1, 'novel', '1']</code> <p>Returns:</p> Type Description <code>DataFrame or None</code> <p>A DataFrame with pre-task-post epochs, or None if no such pattern is found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n&gt;&gt;&gt; epoch_df = find_pre_task_post_optimize_novel(epoch_df)\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def find_pre_task_post_optimize_novel(\n    epoch_df: pd.DataFrame, novel_indicators: List[Union[int, str]] = [1, \"novel\", \"1\"]\n) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Find pre-task-post epochs in the DataFrame, optimizing for novel epochs.\n\n    Parameters\n    ----------\n    epoch_df : pd.DataFrame\n        DataFrame containing epochs information with 'environment' and 'behavioralParadigm' columns.\n    novel_indicators : list of [int, str], optional\n        List of indicators used to identify novel epochs in the 'behavioralParadigm' column (default is [1, \"novel\", \"1\"]).\n\n    Returns\n    -------\n    pd.DataFrame or None\n        A DataFrame with pre-task-post epochs, or None if no such pattern is found.\n\n    Examples\n    -------\n    &gt;&gt;&gt; epoch_df = loading.load_epoch(basepath)\n    &gt;&gt;&gt; epoch_df = find_pre_task_post_optimize_novel(epoch_df)\n    \"\"\"\n    # set sleep to nan\n    epoch_df.loc[epoch_df.environment == \"sleep\", \"behavioralParadigm\"] = np.nan\n    # Search for novel epochs\n    novel_mask = epoch_df.behavioralParadigm.isin(novel_indicators)\n    if novel_mask.any():\n        # Find the first novel epoch\n        idx = np.where(novel_mask)[0][0]\n        # Select the first novel epoch and the epochs before and after it\n        mask = np.hstack([idx - 1, idx, idx + 1])\n        # If any of the epochs are negative, skip (this means the novel epoch was the first epoch)\n        if any(mask &lt; 0):\n            pass\n        else:\n            epoch_df_temp = epoch_df.loc[mask]\n            # Find pre task post epochs in this subset\n            idx = find_pre_task_post(epoch_df_temp.environment)\n            # If no pre task post epochs are found, skip\n            if idx is None or idx[0] is None:\n                pass\n            else:\n                epoch_df = epoch_df_temp.reset_index(drop=True)\n    # Find the first pre task post epoch in epoch_df, if the df was modified that will be used\n    idx, _ = find_pre_task_post(epoch_df.environment)\n    if idx is None:\n        return None\n    epoch_df = epoch_df.loc[idx].reset_index(drop=True)\n    return epoch_df\n</code></pre>"},{"location":"reference/neuro_py/session/locate_epochs/#neuro_py.session.locate_epochs.get_experience_level","title":"<code>get_experience_level(behavioralParadigm)</code>","text":"<p>Extract the experience level from the behavioralParadigm column.</p> <p>The experience level is the number of times the animal has run the task, inferred from the behavioralParadigm column.</p> <p>Parameters:</p> Name Type Description Default <code>behavioralParadigm</code> <code>Series</code> <p>A single entry or value from the behavioralParadigm column of an epoch.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The experience level as an integer. Returns NaN if experience cannot be determined.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; experience = get_experience_level(current_epoch_df.iloc[1].behavioralParadigm)\n</code></pre> Source code in <code>neuro_py/session/locate_epochs.py</code> <pre><code>def get_experience_level(behavioralParadigm: pd.Series) -&gt; int:\n    \"\"\"\n    Extract the experience level from the behavioralParadigm column.\n\n    The experience level is the number of times the animal has run the task,\n    inferred from the behavioralParadigm column.\n\n    Parameters\n    ----------\n    behavioralParadigm : pd.Series\n        A single entry or value from the behavioralParadigm column of an epoch.\n\n    Returns\n    -------\n    int\n        The experience level as an integer. Returns NaN if experience cannot be determined.\n\n    Examples\n    --------\n    &gt;&gt;&gt; experience = get_experience_level(current_epoch_df.iloc[1].behavioralParadigm)\n    \"\"\"\n    if behavioralParadigm == \"novel\":\n        experience = 1\n    else:\n        try:\n            # extract first number from string\n            experience = int(re.findall(r\"\\d+\", behavioralParadigm)[0])\n        except Exception:\n            try:\n                # extract experience level from behavioralParadigm column if it is a number\n                experience = int(behavioralParadigm)\n            except Exception:\n                experience = np.nan\n    return experience\n</code></pre>"},{"location":"reference/neuro_py/spikes/","title":"neuro_py.spikes","text":""},{"location":"reference/neuro_py/spikes/#neuro_py.spikes.BurstIndex_Royer_2012","title":"<code>BurstIndex_Royer_2012(autocorrs)</code>","text":"<p>Calculate the burst index from Royer et al. (2012). The burst index ranges from -1 to 1, where: -1 indicates non-bursty behavior, and 1 indicates bursty behavior.</p> <p>Parameters:</p> Name Type Description Default <code>autocorrs</code> <code>DataFrame</code> <p>Autocorrelograms of spike trains, with time (in seconds) as index and correlation values as columns.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of burst indices for each autocorrelogram column.</p> Notes <p>The burst index is calculated as:     burst_idx = (peak - baseline) / max(peak, baseline)</p> <ul> <li>Peak is calculated as the maximum of the autocorrelogram between 2-9 ms.</li> <li>Baseline is calculated as the mean of the autocorrelogram between 40-50 ms.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; burst_idx = BurstIndex_Royer_2012(autocorr_df)\n</code></pre> Source code in <code>neuro_py/spikes/spike_tools.py</code> <pre><code>def BurstIndex_Royer_2012(autocorrs: pd.DataFrame) -&gt; list:\n    \"\"\"\n    Calculate the burst index from Royer et al. (2012).\n    The burst index ranges from -1 to 1, where:\n    -1 indicates non-bursty behavior, and 1 indicates bursty behavior.\n\n    Parameters\n    ----------\n    autocorrs : pd.DataFrame\n        Autocorrelograms of spike trains, with time (in seconds) as index and\n        correlation values as columns.\n\n    Returns\n    -------\n    list\n        List of burst indices for each autocorrelogram column.\n\n    Notes\n    -----\n    The burst index is calculated as:\n        burst_idx = (peak - baseline) / max(peak, baseline)\n\n    - Peak is calculated as the maximum of the autocorrelogram between 2-9 ms.\n    - Baseline is calculated as the mean of the autocorrelogram between 40-50 ms.\n\n    Examples\n    -------\n    &gt;&gt;&gt; burst_idx = BurstIndex_Royer_2012(autocorr_df)\n    \"\"\"\n    # peak range 2 - 9 ms\n    peak = autocorrs.loc[0.002:0.009].max()\n    # baseline idx 40 - 50 ms\n    baseline = autocorrs.loc[0.04:0.05].mean()\n\n    burst_idx = []\n    for p, b in zip(peak, baseline):\n\n        if (p is None) | (b is None):\n            burst_idx.append(np.nan)\n            continue\n        if p &gt; b:\n            burst_idx.append((p - b) / p)\n        elif p &lt; b:\n            burst_idx.append((p - b) / b)\n        else:\n            burst_idx.append(np.nan)\n    return burst_idx\n</code></pre>"},{"location":"reference/neuro_py/spikes/#neuro_py.spikes.get_spindices","title":"<code>get_spindices(data)</code>","text":"<p>Spike timestamps and IDs from each spike train in a time-sorted DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Spike times for each spike train, where each element is an array of spike times for a neuron.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Sorted spike times and the corresponding spikes' neuron IDs</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spike_trains = [np.array([0.1, 0.2, 0.4]), np.array([0.15, 0.35])]\n&gt;&gt;&gt; spikes = get_spindices(spike_trains)\n</code></pre> Source code in <code>neuro_py/spikes/spike_tools.py</code> <pre><code>def get_spindices(data: np.ndarray) -&gt; pd.DataFrame:\n    \"\"\"\n    Spike timestamps and IDs from each spike train in a time-sorted DataFrame.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Spike times for each spike train, where each element is an array of\n        spike times for a neuron.\n\n    Returns\n    -------\n    pd.DataFrame\n        Sorted spike times and the corresponding spikes' neuron IDs\n\n    Examples\n    -------\n    &gt;&gt;&gt; spike_trains = [np.array([0.1, 0.2, 0.4]), np.array([0.15, 0.35])]\n    &gt;&gt;&gt; spikes = get_spindices(spike_trains)\n    \"\"\"\n    spikes_id = np.repeat(np.arange(len(data)), [len(spk) for spk in data])\n\n    spikes = pd.DataFrame({\n        'spike_times': np.concatenate(data),\n        'spike_id': spikes_id\n    })\n    spikes.sort_values(\"spike_times\", inplace=True)\n    return spikes\n</code></pre>"},{"location":"reference/neuro_py/spikes/#neuro_py.spikes.select_burst_spikes","title":"<code>select_burst_spikes(spikes, mode='bursts', isiBursts=0.006, isiSpikes=0.02)</code>","text":"<p>Discriminate bursts versus single spikes based on inter-spike intervals.</p> <p>Parameters:</p> Name Type Description Default <code>spikes</code> <code>ndarray</code> <p>Array of spike times.</p> required <code>mode</code> <code>str</code> <p>Either 'bursts' (default) or 'single'.</p> <code>'bursts'</code> <code>isiBursts</code> <code>float</code> <p>Maximum inter-spike interval for bursts (default = 0.006 seconds).</p> <code>0.006</code> <code>isiSpikes</code> <code>float</code> <p>Minimum inter-spike interval for single spikes (default = 0.020 seconds).</p> <code>0.02</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A boolean array indicating for each spike whether it matches the criterion.</p> Notes <p>Adapted from: http://fmatoolbox.sourceforge.net/Contents/FMAToolbox/Analyses/SelectSpikes.html</p> Source code in <code>neuro_py/spikes/spike_tools.py</code> <pre><code>def select_burst_spikes(\n    spikes: np.ndarray,\n    mode: str = \"bursts\",\n    isiBursts: float = 0.006,\n    isiSpikes: float = 0.020,\n) -&gt; np.ndarray:\n    \"\"\"\n    Discriminate bursts versus single spikes based on inter-spike intervals.\n\n    Parameters\n    ----------\n    spikes : np.ndarray\n        Array of spike times.\n    mode : str, optional\n        Either 'bursts' (default) or 'single'.\n    isiBursts : float, optional\n        Maximum inter-spike interval for bursts (default = 0.006 seconds).\n    isiSpikes : float, optional\n        Minimum inter-spike interval for single spikes (default = 0.020 seconds).\n\n    Returns\n    -------\n    np.ndarray\n        A boolean array indicating for each spike whether it matches the criterion.\n\n    Notes\n    -----\n    Adapted from: http://fmatoolbox.sourceforge.net/Contents/FMAToolbox/Analyses/SelectSpikes.html\n    \"\"\"\n\n    dt = np.diff(spikes)\n\n    if mode == \"bursts\":\n        b = dt &lt; isiBursts\n        # either next or previous isi &lt; threshold\n        selected = np.insert(b, 0, False, axis=0) | np.append(b, False)\n    else:\n        s = dt &gt; isiSpikes\n        # either next or previous isi &gt; threshold\n        selected = np.insert(s, 0, False, axis=0) &amp; np.append(s, False)\n\n    return selected\n</code></pre>"},{"location":"reference/neuro_py/spikes/#neuro_py.spikes.spindices_to_ndarray","title":"<code>spindices_to_ndarray(spikes, spike_id=None)</code>","text":"<p>Convert spike times and spike IDs from a DataFrame into a list of arrays, where each array contains the spike times for a given spike train.</p> <p>Parameters:</p> Name Type Description Default <code>spikes</code> <code>DataFrame</code> <p>DataFrame containing 'spike_times' and 'spike_id' columns, sorted by 'spike_times'.</p> required <code>spike_id</code> <code>list or ndarray</code> <p>List or array of spike IDs to search for in the DataFrame. If None, all spike IDs are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>A list of arrays, each containing the spike times for a corresponding spike train.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spike_trains = spindices_to_ndarray(spikes_df, spike_id=[0, 1, 2])\n</code></pre> Source code in <code>neuro_py/spikes/spike_tools.py</code> <pre><code>def spindices_to_ndarray(\n    spikes: pd.DataFrame, spike_id: Union[List[int], np.ndarray, None] = None\n) -&gt; List[np.ndarray]:\n    \"\"\"\n    Convert spike times and spike IDs from a DataFrame into a list of arrays,\n    where each array contains the spike times for a given spike train.\n\n    Parameters\n    ----------\n    spikes : pd.DataFrame\n        DataFrame containing 'spike_times' and 'spike_id' columns, sorted by\n        'spike_times'.\n    spike_id : list or np.ndarray, optional\n        List or array of spike IDs to search for in the DataFrame. If None, all\n        spike IDs are used.\n\n    Returns\n    -------\n    List[np.ndarray]\n        A list of arrays, each containing the spike times for a corresponding\n        spike train.\n\n    Examples\n    -------\n    &gt;&gt;&gt; spike_trains = spindices_to_ndarray(spikes_df, spike_id=[0, 1, 2])\n    \"\"\"\n    if spike_id is None:\n        spike_id = spikes.spike_id.unique()\n    data = [\n        spikes[spikes.spike_id == spk_i].spike_times.values\n        for spk_i in spike_id\n    ]\n    return data\n</code></pre>"},{"location":"reference/neuro_py/spikes/spike_tools/","title":"neuro_py.spikes.spike_tools","text":""},{"location":"reference/neuro_py/spikes/spike_tools/#neuro_py.spikes.spike_tools.BurstIndex_Royer_2012","title":"<code>BurstIndex_Royer_2012(autocorrs)</code>","text":"<p>Calculate the burst index from Royer et al. (2012). The burst index ranges from -1 to 1, where: -1 indicates non-bursty behavior, and 1 indicates bursty behavior.</p> <p>Parameters:</p> Name Type Description Default <code>autocorrs</code> <code>DataFrame</code> <p>Autocorrelograms of spike trains, with time (in seconds) as index and correlation values as columns.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of burst indices for each autocorrelogram column.</p> Notes <p>The burst index is calculated as:     burst_idx = (peak - baseline) / max(peak, baseline)</p> <ul> <li>Peak is calculated as the maximum of the autocorrelogram between 2-9 ms.</li> <li>Baseline is calculated as the mean of the autocorrelogram between 40-50 ms.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; burst_idx = BurstIndex_Royer_2012(autocorr_df)\n</code></pre> Source code in <code>neuro_py/spikes/spike_tools.py</code> <pre><code>def BurstIndex_Royer_2012(autocorrs: pd.DataFrame) -&gt; list:\n    \"\"\"\n    Calculate the burst index from Royer et al. (2012).\n    The burst index ranges from -1 to 1, where:\n    -1 indicates non-bursty behavior, and 1 indicates bursty behavior.\n\n    Parameters\n    ----------\n    autocorrs : pd.DataFrame\n        Autocorrelograms of spike trains, with time (in seconds) as index and\n        correlation values as columns.\n\n    Returns\n    -------\n    list\n        List of burst indices for each autocorrelogram column.\n\n    Notes\n    -----\n    The burst index is calculated as:\n        burst_idx = (peak - baseline) / max(peak, baseline)\n\n    - Peak is calculated as the maximum of the autocorrelogram between 2-9 ms.\n    - Baseline is calculated as the mean of the autocorrelogram between 40-50 ms.\n\n    Examples\n    -------\n    &gt;&gt;&gt; burst_idx = BurstIndex_Royer_2012(autocorr_df)\n    \"\"\"\n    # peak range 2 - 9 ms\n    peak = autocorrs.loc[0.002:0.009].max()\n    # baseline idx 40 - 50 ms\n    baseline = autocorrs.loc[0.04:0.05].mean()\n\n    burst_idx = []\n    for p, b in zip(peak, baseline):\n\n        if (p is None) | (b is None):\n            burst_idx.append(np.nan)\n            continue\n        if p &gt; b:\n            burst_idx.append((p - b) / p)\n        elif p &lt; b:\n            burst_idx.append((p - b) / b)\n        else:\n            burst_idx.append(np.nan)\n    return burst_idx\n</code></pre>"},{"location":"reference/neuro_py/spikes/spike_tools/#neuro_py.spikes.spike_tools.get_spindices","title":"<code>get_spindices(data)</code>","text":"<p>Spike timestamps and IDs from each spike train in a time-sorted DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Spike times for each spike train, where each element is an array of spike times for a neuron.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Sorted spike times and the corresponding spikes' neuron IDs</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spike_trains = [np.array([0.1, 0.2, 0.4]), np.array([0.15, 0.35])]\n&gt;&gt;&gt; spikes = get_spindices(spike_trains)\n</code></pre> Source code in <code>neuro_py/spikes/spike_tools.py</code> <pre><code>def get_spindices(data: np.ndarray) -&gt; pd.DataFrame:\n    \"\"\"\n    Spike timestamps and IDs from each spike train in a time-sorted DataFrame.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Spike times for each spike train, where each element is an array of\n        spike times for a neuron.\n\n    Returns\n    -------\n    pd.DataFrame\n        Sorted spike times and the corresponding spikes' neuron IDs\n\n    Examples\n    -------\n    &gt;&gt;&gt; spike_trains = [np.array([0.1, 0.2, 0.4]), np.array([0.15, 0.35])]\n    &gt;&gt;&gt; spikes = get_spindices(spike_trains)\n    \"\"\"\n    spikes_id = np.repeat(np.arange(len(data)), [len(spk) for spk in data])\n\n    spikes = pd.DataFrame({\n        'spike_times': np.concatenate(data),\n        'spike_id': spikes_id\n    })\n    spikes.sort_values(\"spike_times\", inplace=True)\n    return spikes\n</code></pre>"},{"location":"reference/neuro_py/spikes/spike_tools/#neuro_py.spikes.spike_tools.select_burst_spikes","title":"<code>select_burst_spikes(spikes, mode='bursts', isiBursts=0.006, isiSpikes=0.02)</code>","text":"<p>Discriminate bursts versus single spikes based on inter-spike intervals.</p> <p>Parameters:</p> Name Type Description Default <code>spikes</code> <code>ndarray</code> <p>Array of spike times.</p> required <code>mode</code> <code>str</code> <p>Either 'bursts' (default) or 'single'.</p> <code>'bursts'</code> <code>isiBursts</code> <code>float</code> <p>Maximum inter-spike interval for bursts (default = 0.006 seconds).</p> <code>0.006</code> <code>isiSpikes</code> <code>float</code> <p>Minimum inter-spike interval for single spikes (default = 0.020 seconds).</p> <code>0.02</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A boolean array indicating for each spike whether it matches the criterion.</p> Notes <p>Adapted from: http://fmatoolbox.sourceforge.net/Contents/FMAToolbox/Analyses/SelectSpikes.html</p> Source code in <code>neuro_py/spikes/spike_tools.py</code> <pre><code>def select_burst_spikes(\n    spikes: np.ndarray,\n    mode: str = \"bursts\",\n    isiBursts: float = 0.006,\n    isiSpikes: float = 0.020,\n) -&gt; np.ndarray:\n    \"\"\"\n    Discriminate bursts versus single spikes based on inter-spike intervals.\n\n    Parameters\n    ----------\n    spikes : np.ndarray\n        Array of spike times.\n    mode : str, optional\n        Either 'bursts' (default) or 'single'.\n    isiBursts : float, optional\n        Maximum inter-spike interval for bursts (default = 0.006 seconds).\n    isiSpikes : float, optional\n        Minimum inter-spike interval for single spikes (default = 0.020 seconds).\n\n    Returns\n    -------\n    np.ndarray\n        A boolean array indicating for each spike whether it matches the criterion.\n\n    Notes\n    -----\n    Adapted from: http://fmatoolbox.sourceforge.net/Contents/FMAToolbox/Analyses/SelectSpikes.html\n    \"\"\"\n\n    dt = np.diff(spikes)\n\n    if mode == \"bursts\":\n        b = dt &lt; isiBursts\n        # either next or previous isi &lt; threshold\n        selected = np.insert(b, 0, False, axis=0) | np.append(b, False)\n    else:\n        s = dt &gt; isiSpikes\n        # either next or previous isi &gt; threshold\n        selected = np.insert(s, 0, False, axis=0) &amp; np.append(s, False)\n\n    return selected\n</code></pre>"},{"location":"reference/neuro_py/spikes/spike_tools/#neuro_py.spikes.spike_tools.spindices_to_ndarray","title":"<code>spindices_to_ndarray(spikes, spike_id=None)</code>","text":"<p>Convert spike times and spike IDs from a DataFrame into a list of arrays, where each array contains the spike times for a given spike train.</p> <p>Parameters:</p> Name Type Description Default <code>spikes</code> <code>DataFrame</code> <p>DataFrame containing 'spike_times' and 'spike_id' columns, sorted by 'spike_times'.</p> required <code>spike_id</code> <code>list or ndarray</code> <p>List or array of spike IDs to search for in the DataFrame. If None, all spike IDs are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>A list of arrays, each containing the spike times for a corresponding spike train.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spike_trains = spindices_to_ndarray(spikes_df, spike_id=[0, 1, 2])\n</code></pre> Source code in <code>neuro_py/spikes/spike_tools.py</code> <pre><code>def spindices_to_ndarray(\n    spikes: pd.DataFrame, spike_id: Union[List[int], np.ndarray, None] = None\n) -&gt; List[np.ndarray]:\n    \"\"\"\n    Convert spike times and spike IDs from a DataFrame into a list of arrays,\n    where each array contains the spike times for a given spike train.\n\n    Parameters\n    ----------\n    spikes : pd.DataFrame\n        DataFrame containing 'spike_times' and 'spike_id' columns, sorted by\n        'spike_times'.\n    spike_id : list or np.ndarray, optional\n        List or array of spike IDs to search for in the DataFrame. If None, all\n        spike IDs are used.\n\n    Returns\n    -------\n    List[np.ndarray]\n        A list of arrays, each containing the spike times for a corresponding\n        spike train.\n\n    Examples\n    -------\n    &gt;&gt;&gt; spike_trains = spindices_to_ndarray(spikes_df, spike_id=[0, 1, 2])\n    \"\"\"\n    if spike_id is None:\n        spike_id = spikes.spike_id.unique()\n    data = [\n        spikes[spikes.spike_id == spk_i].spike_times.values\n        for spk_i in spike_id\n    ]\n    return data\n</code></pre>"},{"location":"reference/neuro_py/stats/","title":"neuro_py.stats","text":""},{"location":"reference/neuro_py/stats/#neuro_py.stats.MultivariateRegressor","title":"<code>MultivariateRegressor</code>","text":"<p>               Bases: <code>object</code></p> <p>Multivariate Linear Regressor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>An n-by-d matrix of features.</p> required <code>Y</code> <code>ndarray</code> <p>An n-by-D matrix of targets.</p> required <code>reg</code> <code>Optional[float]</code> <p>A regularization parameter (default is None).</p> <code>None</code> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>class MultivariateRegressor(object):\n    \"\"\"\n    Multivariate Linear Regressor.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        An n-by-d matrix of features.\n    Y : np.ndarray\n        An n-by-D matrix of targets.\n    reg : Optional[float], optional\n        A regularization parameter (default is None).\n    \"\"\"\n\n    def __init__(self, X: np.ndarray, Y: np.ndarray, reg: Optional[float] = None):\n        if np.size(np.shape(X)) == 1:\n            X = np.reshape(X, (-1, 1))\n        if np.size(np.shape(Y)) == 1:\n            Y = np.reshape(Y, (-1, 1))\n        if reg is None:\n            reg = 0\n\n        W1 = np.linalg.pinv(np.dot(X.T, X) + reg * sparse.eye(np.size(X, 1)))\n        W2 = np.dot(X, W1)\n        self.W = np.dot(Y.T, W2)\n\n    def __str__(self) -&gt; str:\n        return \"Multivariate Linear Regression\"\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Return the predicted Y for input X.\"\"\"\n        if np.size(np.shape(X)) == 1:\n            X = np.reshape(X, (-1, 1))\n        return np.array(np.dot(X, self.W.T))\n\n    def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n        \"\"\"Return the coefficient of determination R^2 of the prediction.\"\"\"\n        y_pred = self.predict(X)\n        return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.MultivariateRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Return the predicted Y for input X.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return the predicted Y for input X.\"\"\"\n    if np.size(np.shape(X)) == 1:\n        X = np.reshape(X, (-1, 1))\n    return np.array(np.dot(X, self.W.T))\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.MultivariateRegressor.score","title":"<code>score(X, Y)</code>","text":"<p>Return the coefficient of determination R^2 of the prediction.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n    \"\"\"Return the coefficient of determination R^2 of the prediction.\"\"\"\n    y_pred = self.predict(X)\n    return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.ReducedRankRegressor","title":"<code>ReducedRankRegressor</code>","text":"<p>               Bases: <code>object</code></p> <p>Reduced Rank Regressor (linear 'bottlenecking' or 'multitask learning').</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>An n-by-d matrix of features.</p> required <code>Y</code> <code>ndarray</code> <p>An n-by-D matrix of targets.</p> required <code>rank</code> <code>int</code> <p>A rank constraint.</p> required <code>reg</code> <code>Optional[float]</code> <p>A regularization parameter (default is None).</p> <code>None</code> References <p>Implemented by Chris Rayner (2015). dchrisrayner AT gmail DOT com</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>class ReducedRankRegressor(object):\n    \"\"\"\n    Reduced Rank Regressor (linear 'bottlenecking' or 'multitask learning').\n\n    Parameters\n    ----------\n    X : np.ndarray\n        An n-by-d matrix of features.\n    Y : np.ndarray\n        An n-by-D matrix of targets.\n    rank : int\n        A rank constraint.\n    reg : Optional[float], optional\n        A regularization parameter (default is None).\n\n    References\n    ----\n    Implemented by Chris Rayner (2015).\n    dchrisrayner AT gmail DOT com\n    \"\"\"\n\n    def __init__(\n        self, X: np.ndarray, Y: np.ndarray, rank: int, reg: Optional[float] = None\n    ):\n        if np.size(np.shape(X)) == 1:\n            X = np.reshape(X, (-1, 1))\n        if np.size(np.shape(Y)) == 1:\n            Y = np.reshape(Y, (-1, 1))\n        if reg is None:\n            reg = 0\n        self.rank = rank\n\n        CXX = X.T @ X + reg * sparse.eye(np.size(X, 1))\n        CXY = X.T @ Y\n        _U, _S, V = np.linalg.svd(CXY.T @ (np.linalg.pinv(CXX) @ CXY))\n        self.W = V[0:rank, :].T\n        self.A = (np.linalg.pinv(CXX) @ (CXY @ self.W)).T\n\n    def __str__(self) -&gt; str:\n        return \"Reduced Rank Regressor (rank = {})\".format(self.rank)\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict Y from X.\"\"\"\n        if np.size(np.shape(X)) == 1:\n            X = np.reshape(X, (-1, 1))\n        return X @ (self.A.T @ self.W.T)\n\n    def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n        \"\"\"Score the model.\"\"\"\n        if np.size(np.shape(X)) == 1:\n            X = np.reshape(X, (-1, 1))\n        if np.size(np.shape(Y)) == 1:\n            Y = np.reshape(Y, (-1, 1))\n\n        y_pred = self.predict(X)\n        return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.ReducedRankRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predict Y from X.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Predict Y from X.\"\"\"\n    if np.size(np.shape(X)) == 1:\n        X = np.reshape(X, (-1, 1))\n    return X @ (self.A.T @ self.W.T)\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.ReducedRankRegressor.score","title":"<code>score(X, Y)</code>","text":"<p>Score the model.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n    \"\"\"Score the model.\"\"\"\n    if np.size(np.shape(X)) == 1:\n        X = np.reshape(X, (-1, 1))\n    if np.size(np.shape(Y)) == 1:\n        Y = np.reshape(Y, (-1, 1))\n\n    y_pred = self.predict(X)\n    return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.SystemIdentifier","title":"<code>SystemIdentifier</code>","text":"<p>               Bases: <code>object</code></p> <p>Simple Subspace System Identifier.</p> <p>This class identifies a linear dynamical system based on given input and output data using subspace methods.</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>An n-by-d matrix of control inputs.</p> required <code>Y</code> <code>ndarray</code> <p>An n-by-D matrix of output observations.</p> required <code>statedim</code> <code>int</code> <p>The dimension of the internal state variable.</p> required <code>reg</code> <code>float</code> <p>Regularization parameter (default is None, which is set to 0).</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>A</code> <code>ndarray</code> <p>State transition matrix.</p> <code>B</code> <code>ndarray</code> <p>Control input matrix.</p> <code>C</code> <code>ndarray</code> <p>Output matrix.</p> <code>D</code> <code>ndarray</code> <p>Feedforward matrix.</p> Source code in <code>neuro_py/stats/system_identifier.py</code> <pre><code>class SystemIdentifier(object):\n    \"\"\"\n    Simple Subspace System Identifier.\n\n    This class identifies a linear dynamical system based on given input and output data using subspace methods.\n\n    Parameters\n    ----------\n    U : np.ndarray\n        An n-by-d matrix of control inputs.\n    Y : np.ndarray\n        An n-by-D matrix of output observations.\n    statedim : int\n        The dimension of the internal state variable.\n    reg : float, optional\n        Regularization parameter (default is None, which is set to 0).\n\n    Attributes\n    ----------\n    A : np.ndarray\n        State transition matrix.\n    B : np.ndarray\n        Control input matrix.\n    C : np.ndarray\n        Output matrix.\n    D : np.ndarray\n        Feedforward matrix.\n    \"\"\"\n\n    def __init__(self, U: np.ndarray, Y: np.ndarray, statedim: int, reg: Union[float, None] = None):\n        if np.size(np.shape(U)) == 1:\n            U = np.reshape(U, (-1, 1))\n        if np.size(np.shape(Y)) == 1:\n            Y = np.reshape(Y, (-1, 1))\n        if reg is None:\n            reg = 0\n\n        yDim = np.size(Y, 1)\n        uDim = np.size(U, 1)\n\n        self.output_size = np.size(Y, 1)  # placeholder\n\n        # number of samples of past/future we'll mash together into a 'state'\n        width = 1\n        # total number of past/future pairings we get as a result\n        K = np.size(U, 0) - 2 * width + 1\n\n        # build hankel matrices containing pasts and futures\n        U_p = np.array([np.ravel(U[t : t + width]) for t in range(K)]).T\n        U_f = np.array([np.ravel(U[t + width : t + 2 * width]) for t in range(K)]).T\n        Y_p = np.array([np.ravel(Y[t : t + width]) for t in range(K)]).T\n        Y_f = np.array([np.ravel(Y[t + width : t + 2 * width]) for t in range(K)]).T\n\n        # solve the eigenvalue problem\n        YfUfT = np.dot(Y_f, U_f.T)\n        YfUpT = np.dot(Y_f, U_p.T)\n        YfYpT = np.dot(Y_f, Y_p.T)\n        UfUpT = np.dot(U_f, U_p.T)\n        UfYpT = np.dot(U_f, Y_p.T)\n        UpYpT = np.dot(U_p, Y_p.T)\n        F = sparse.bmat(\n            [\n                [None, YfUfT, YfUpT, YfYpT],\n                [YfUfT.T, None, UfUpT, UfYpT],\n                [YfUpT.T, UfUpT.T, None, UpYpT],\n                [YfYpT.T, UfYpT.T, UpYpT.T, None],\n            ]\n        )\n        Ginv = sparse.bmat(\n            [\n                [np.linalg.pinv(np.dot(Y_f, Y_f.T)), None, None, None],\n                [None, np.linalg.pinv(np.dot(U_f, U_f.T)), None, None],\n                [None, None, np.linalg.pinv(np.dot(U_p, U_p.T)), None],\n                [None, None, None, np.linalg.pinv(np.dot(Y_p, Y_p.T))],\n            ]\n        )\n        F = F - sparse.eye(sp.size(F, 0)) * reg\n\n        # Take smallest eigenvalues\n        _, W = sparse_linalg.eigs(Ginv.dot(F), k=statedim, which=\"SR\")\n\n        # State sequence is a weighted combination of the past\n        W_U_p = W[width * (yDim + uDim) : width * (yDim + uDim + uDim), :]\n        W_Y_p = W[width * (yDim + uDim + uDim) :, :]\n        X_hist = np.dot(W_U_p.T, U_p) + np.dot(W_Y_p.T, Y_p)\n\n        # Regress; trim inputs to match the states we retrieved\n        R = np.concatenate((X_hist[:, :-1], U[width:-width].T), 0)\n        L = np.concatenate((X_hist[:, 1:], Y[width:-width].T), 0)\n        RRi = np.linalg.pinv(np.dot(R, R.T))\n        RL = np.dot(R, L.T)\n        Sys = np.dot(RRi, RL).T\n        self.A = Sys[:statedim, :statedim]\n        self.B = Sys[:statedim, statedim:]\n        self.C = Sys[statedim:, :statedim]\n        self.D = Sys[statedim:, statedim:]\n\n    def __str__(self) -&gt; str:\n        return \"Linear Dynamical System\"\n\n    def predict(self, U: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predict output given the control inputs.\n\n        Parameters\n        ----------\n        U : np.ndarray\n            Control inputs, shape (n_samples, n_controls).\n\n        Returns\n        -------\n        np.ndarray\n            Predicted outputs, shape (n_samples, n_outputs).\n        \"\"\"\n        # If U is a vector, reshape it\n        if np.size(np.shape(U)) == 1:\n            U = np.reshape(U, (-1, 1))\n\n        # assume some random initial state\n        X = np.reshape(np.random.randn(np.size(self.A, 1)), (1, -1))\n\n        # intitial output\n        Y = np.reshape(np.dot(self.C, X[-1]) + np.dot(self.D, U[0]), (1, -1))\n\n        # generate next state\n        X = np.concatenate(\n            (X, np.reshape(np.dot(self.A, X[-1]) + np.dot(self.B, U[0]), (1, -1)))\n        )\n\n        # and so forth\n        for u in U[1:]:\n            Y = np.concatenate(\n                (Y, np.reshape(np.dot(self.C, X[-1]) + np.dot(self.D, u), (1, -1)))\n            )\n            X = np.concatenate(\n                (X, np.reshape(np.dot(self.A, X[-1]) + np.dot(self.B, u), (1, -1)))\n            )\n\n        return Y\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.SystemIdentifier.predict","title":"<code>predict(U)</code>","text":"<p>Predict output given the control inputs.</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>Control inputs, shape (n_samples, n_controls).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted outputs, shape (n_samples, n_outputs).</p> Source code in <code>neuro_py/stats/system_identifier.py</code> <pre><code>def predict(self, U: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Predict output given the control inputs.\n\n    Parameters\n    ----------\n    U : np.ndarray\n        Control inputs, shape (n_samples, n_controls).\n\n    Returns\n    -------\n    np.ndarray\n        Predicted outputs, shape (n_samples, n_outputs).\n    \"\"\"\n    # If U is a vector, reshape it\n    if np.size(np.shape(U)) == 1:\n        U = np.reshape(U, (-1, 1))\n\n    # assume some random initial state\n    X = np.reshape(np.random.randn(np.size(self.A, 1)), (1, -1))\n\n    # intitial output\n    Y = np.reshape(np.dot(self.C, X[-1]) + np.dot(self.D, U[0]), (1, -1))\n\n    # generate next state\n    X = np.concatenate(\n        (X, np.reshape(np.dot(self.A, X[-1]) + np.dot(self.B, U[0]), (1, -1)))\n    )\n\n    # and so forth\n    for u in U[1:]:\n        Y = np.concatenate(\n            (Y, np.reshape(np.dot(self.C, X[-1]) + np.dot(self.D, u), (1, -1)))\n        )\n        X = np.concatenate(\n            (X, np.reshape(np.dot(self.A, X[-1]) + np.dot(self.B, u), (1, -1)))\n        )\n\n    return Y\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.kernelReducedRankRegressor","title":"<code>kernelReducedRankRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Kernel Reduced Rank Ridge Regression.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>The rank constraint (default is 10).</p> <code>10</code> <code>reg</code> <code>float</code> <p>The regularization parameter (default is 1).</p> <code>1</code> <code>P_rr</code> <code>Optional[ndarray]</code> <p>The P matrix for reduced rank (default is None).</p> <code>None</code> <code>Q_fr</code> <code>Optional[ndarray]</code> <p>The Q matrix for fitted values (default is None).</p> <code>None</code> <code>trainX</code> <code>Optional[ndarray]</code> <p>The training features (default is None).</p> <code>None</code> References <p>Mukherjee, S. (DOI:10.1002/sam.10138) Code by Michele Svanera (2017-June).</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>class kernelReducedRankRegressor(BaseEstimator):\n    \"\"\"\n    Kernel Reduced Rank Ridge Regression.\n\n    Parameters\n    ----------\n    rank : int, optional\n        The rank constraint (default is 10).\n    reg : float, optional\n        The regularization parameter (default is 1).\n    P_rr : Optional[np.ndarray], optional\n        The P matrix for reduced rank (default is None).\n    Q_fr : Optional[np.ndarray], optional\n        The Q matrix for fitted values (default is None).\n    trainX : Optional[np.ndarray], optional\n        The training features (default is None).\n\n    References\n    ----------\n    Mukherjee, S. (DOI:10.1002/sam.10138)\n    Code by Michele Svanera (2017-June).\n    \"\"\"\n\n    def __init__(\n        self,\n        rank: int = 10,\n        reg: float = 1,\n        P_rr: Optional[np.ndarray] = None,\n        Q_fr: Optional[np.ndarray] = None,\n        trainX: Optional[np.ndarray] = None,\n    ):\n        self.rank = rank\n        self.reg = reg\n        self.P_rr = P_rr\n        self.Q_fr = Q_fr\n        self.trainX = trainX\n\n    def __str__(self) -&gt; str:\n        return \"kernel Reduced Rank Ridge Regression by Mukherjee (rank = {})\".format(\n            self.rank\n        )\n\n    def fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n        # use try/except blog with exceptions!\n        self.rank = int(self.rank)\n\n        K_X = scipy.dot(X, X.T)\n        tmp_1 = self.reg * scipy.identity(K_X.shape[0]) + K_X\n        Q_fr = np.linalg.solve(tmp_1, Y)\n        P_fr = scipy.linalg.eig(scipy.dot(Y.T, scipy.dot(K_X, Q_fr)))[1].real\n        P_rr = scipy.dot(P_fr[:, 0 : self.rank], P_fr[:, 0 : self.rank].T)\n\n        self.Q_fr = Q_fr\n        self.P_rr = P_rr\n        self.trainX = X\n\n    def predict(self, testX: np.ndarray) -&gt; np.ndarray:\n        # use try/except blog with exceptions!\n\n        K_Xx = scipy.dot(testX, self.trainX.T)\n        Yhat = scipy.dot(K_Xx, scipy.dot(self.Q_fr, self.P_rr))\n\n        return Yhat\n\n    def rrr_scorer(self, Yhat: np.ndarray, Ytest: np.ndarray) -&gt; float:\n        diag_corr = (np.diag(np.corrcoef(Ytest, Yhat))).mean()\n        return diag_corr\n\n    ## Optional\n    def get_params(self, deep: bool = True) -&gt; dict:\n        return {\"rank\": self.rank, \"reg\": self.reg}\n\n    #\n    #    def set_params(self, **parameters):\n    #        for parameter, value in parameters.items():\n    #            self.setattr(parameter, value)\n    #        return self\n\n    def mse(self, X: np.ndarray, y_true: np.ndarray) -&gt; float:\n        \"\"\"\n        Score the model on test data.\n\n        Parameters\n        ----------\n        X : np.ndarray\n            The test data features.\n        y_true : np.ndarray\n            The true target values.\n\n        Returns\n        -------\n        float\n            The mean squared error of the predictions.\n        \"\"\"\n        Yhat = self.predict(X).real\n        MSE = (np.power((y_true - Yhat), 2) / np.prod(y_true.shape)).mean()\n        return MSE\n\n    def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n        \"\"\"Score the model.\"\"\"\n\n        y_pred = self.predict(X)\n        return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.kernelReducedRankRegressor.mse","title":"<code>mse(X, y_true)</code>","text":"<p>Score the model on test data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The test data features.</p> required <code>y_true</code> <code>ndarray</code> <p>The true target values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean squared error of the predictions.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def mse(self, X: np.ndarray, y_true: np.ndarray) -&gt; float:\n    \"\"\"\n    Score the model on test data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The test data features.\n    y_true : np.ndarray\n        The true target values.\n\n    Returns\n    -------\n    float\n        The mean squared error of the predictions.\n    \"\"\"\n    Yhat = self.predict(X).real\n    MSE = (np.power((y_true - Yhat), 2) / np.prod(y_true.shape)).mean()\n    return MSE\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.kernelReducedRankRegressor.score","title":"<code>score(X, Y)</code>","text":"<p>Score the model.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n    \"\"\"Score the model.\"\"\"\n\n    y_pred = self.predict(X)\n    return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.center","title":"<code>center(*args, **kwargs)</code>","text":"<p>Centers the data on its circular mean.</p> <p>Each non-keyword argument is another data array that is centered.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>The mean is computed along this dimension (default is None). Must be used as a keyword argument!</p> required <p>Returns:</p> Type Description <code>tuple of np.ndarray</code> <p>Tuple of centered data arrays.</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>@mod2pi\ndef center(*args: np.ndarray, **kwargs: Optional[dict]) -&gt; Tuple[np.ndarray, ...]:\n    \"\"\"\n    Centers the data on its circular mean.\n\n    Each non-keyword argument is another data array that is centered.\n\n    Parameters\n    ----------\n    axis : int, optional\n        The mean is computed along this dimension (default is None).\n        **Must be used as a keyword argument!**\n\n    Returns\n    -------\n    tuple of np.ndarray\n        Tuple of centered data arrays.\n    \"\"\"\n\n    axis = kwargs.pop(\"axis\", None)\n    if axis is None:\n        axis = 0\n        args = [a.ravel() for a in args]\n\n    reshaper = tuple(\n        slice(None, None) if i != axis else np.newaxis\n        for i in range(len(args[0].shape))\n    )\n    if len(args) == 1:\n        return args[0] - mean(args[0], axis=axis)\n    else:\n        return tuple(\n            [\n                a - mean(a, axis=axis)[reshaper]\n                for a in args\n                if isinstance(a, np.ndarray)\n            ]\n        )\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.confidence_intervals","title":"<code>confidence_intervals(X, conf=0.95)</code>","text":"<p>Calculate upper and lower confidence intervals on a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A numpy ndarray of shape (n_signals, n_samples).</p> required <code>conf</code> <code>float</code> <p>Confidence level value (default is 0.95).</p> <code>0.95</code> <p>Returns:</p> Name Type Description <code>lower</code> <code>ndarray</code> <p>Lower bounds of the confidence intervals (shape: (n_signals,)).</p> <code>upper</code> <code>ndarray</code> <p>Upper bounds of the confidence intervals (shape: (n_signals,)).</p> Source code in <code>neuro_py/stats/stats.py</code> <pre><code>def confidence_intervals(\n    X: np.ndarray, conf: float = 0.95\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate upper and lower confidence intervals on a matrix.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        A numpy ndarray of shape (n_signals, n_samples).\n    conf : float, optional\n        Confidence level value (default is 0.95).\n\n    Returns\n    -------\n    lower : np.ndarray\n        Lower bounds of the confidence intervals (shape: (n_signals,)).\n    upper : np.ndarray\n        Upper bounds of the confidence intervals (shape: (n_signals,)).\n    \"\"\"\n    # compute interval for each column\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n        interval = [\n            stats.t.interval(\n                conf,\n                len(a) - 1,\n                loc=np.nanmean(a),\n                scale=stats.sem(a, nan_policy=\"omit\"),\n            )\n            for a in X.T\n        ]\n    # stack intervals into array\n    interval = np.vstack(interval)\n    # split into lower and upper\n    lower = interval[:, 0]\n    upper = interval[:, 1]\n\n    return lower, upper\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.get_significant_events","title":"<code>get_significant_events(scores, shuffled_scores, q=95, tail='both')</code>","text":"<p>Return the significant events based on percentiles, the p-values, and the standard deviation of the scores in terms of the shuffled scores.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Union[list, ndarray]</code> <p>The array of scores for which to calculate significant events.</p> required <code>shuffled_scores</code> <code>ndarray</code> <p>The array of scores obtained from randomized data (shape: (n_shuffles, n_events)).</p> required <code>q</code> <code>float</code> <p>Percentile to compute, which must be between 0 and 100 inclusive (default is 95).</p> <code>95</code> <code>tail</code> <code>str</code> <p>Tail for the test, which can be 'left', 'right', or 'both' (default is 'both').</p> <code>'both'</code> <p>Returns:</p> Name Type Description <code>sig_event_idx</code> <code>ndarray</code> <p>Indices (from 0 to n_events-1) of significant events.</p> <code>pvalues</code> <code>ndarray</code> <p>The p-values.</p> <code>stddev</code> <code>ndarray</code> <p>The standard deviation of the scores in terms of the shuffled scores.</p> Source code in <code>neuro_py/stats/stats.py</code> <pre><code>def get_significant_events(\n    scores: Union[list, np.ndarray],\n    shuffled_scores: np.ndarray,\n    q: float = 95,\n    tail: str = \"both\",\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Return the significant events based on percentiles,\n    the p-values, and the standard deviation of the scores\n    in terms of the shuffled scores.\n\n    Parameters\n    ----------\n    scores : Union[list, np.ndarray]\n        The array of scores for which to calculate significant events.\n    shuffled_scores : np.ndarray\n        The array of scores obtained from randomized data (shape: (n_shuffles, n_events)).\n    q : float, optional\n        Percentile to compute, which must be between 0 and 100 inclusive (default is 95).\n    tail : str, optional\n        Tail for the test, which can be 'left', 'right', or 'both' (default is 'both').\n\n    Returns\n    -------\n    sig_event_idx : np.ndarray\n        Indices (from 0 to n_events-1) of significant events.\n    pvalues : np.ndarray\n        The p-values.\n    stddev : np.ndarray\n        The standard deviation of the scores in terms of the shuffled scores.\n    \"\"\"\n    # check shape and correct if needed\n    if isinstance(scores, list) | isinstance(scores, np.ndarray):\n        if shuffled_scores.shape[1] != len(scores):\n            shuffled_scores = shuffled_scores.T\n\n    n = shuffled_scores.shape[0]\n    if tail == \"both\":\n        r = np.sum(np.abs(shuffled_scores) &gt;= np.abs(scores), axis=0)\n    elif tail == \"right\":\n        r = np.sum(shuffled_scores &gt;= scores, axis=0)\n    elif tail == \"left\":\n        r = np.sum(shuffled_scores &lt;= scores, axis=0)\n    else:\n        raise ValueError(\"tail must be 'left', 'right', or 'both'\")\n    pvalues = (r + 1) / (n + 1)\n\n    # set nan scores to 1\n    if isinstance(np.isnan(scores), np.ndarray):\n        pvalues[np.isnan(scores)] = 1\n\n    if tail == \"both\":\n        threshold = np.percentile(np.abs(shuffled_scores), axis=0, q=q)\n        sig_event_idx = np.where(np.abs(scores) &gt; threshold)[0]\n    elif tail == \"right\":\n        threshold = np.percentile(shuffled_scores, axis=0, q=q)\n        sig_event_idx = np.where(scores &gt; threshold)[0]\n    elif tail == \"left\":\n        threshold = np.percentile(shuffled_scores, axis=0, q=100 - q)\n        sig_event_idx = np.where(scores &lt; threshold)[0]\n\n    # calculate how many standard deviations away from shuffle\n    if tail == \"both\":\n        stddev = (np.abs(scores) - np.nanmean(np.abs(shuffled_scores), axis=0)) / np.nanstd(\n            np.abs(shuffled_scores), axis=0\n        )\n    elif tail == \"right\":\n        stddev = (scores - np.nanmean(shuffled_scores, axis=0)) / np.nanstd(\n            shuffled_scores, axis=0\n        )\n    elif tail == \"left\":\n        stddev = (np.nanmean(shuffled_scores, axis=0) - scores) / np.nanstd(\n            shuffled_scores, axis=0\n        )\n\n    return np.atleast_1d(sig_event_idx), np.atleast_1d(pvalues), np.atleast_1d(stddev)\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.get_var","title":"<code>get_var(f, varnames, args, kwargs)</code>","text":"<p>Retrieve indices of specified variables from a function's argument list.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>The function from which to retrieve variable information.</p> required <code>varnames</code> <code>list of str</code> <p>The names of the variables to retrieve.</p> required <code>args</code> <code>list</code> <p>Positional arguments passed to the function.</p> required <code>kwargs</code> <code>dict</code> <p>Keyword arguments passed to the function.</p> required <p>Returns:</p> Type Description <code>tuple of (list of int, list of str)</code> <p>A tuple containing two elements: - A list of indices of the specified variables in the function's argument list. - A list of keys for the keyword arguments that correspond to the specified variables.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a specified variable is not found in the function's argument list.</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>def get_var(f: Callable, varnames: List[str], args: List[Any], kwargs: Dict[str, Any]) -&gt; Tuple[List[int], List[str]]:\n    \"\"\"\n    Retrieve indices of specified variables from a function's argument list.\n\n    Parameters\n    ----------\n    f : Callable\n        The function from which to retrieve variable information.\n    varnames : list of str\n        The names of the variables to retrieve.\n    args : list\n        Positional arguments passed to the function.\n    kwargs : dict\n        Keyword arguments passed to the function.\n\n    Returns\n    -------\n    tuple of (list of int, list of str)\n        A tuple containing two elements:\n        - A list of indices of the specified variables in the function's argument list.\n        - A list of keys for the keyword arguments that correspond to the specified variables.\n\n    Raises\n    ------\n    ValueError\n        If a specified variable is not found in the function's argument list.\n    \"\"\"\n    fvarnames = f.__code__.co_varnames\n\n    var_idx = []\n    kwar_keys = []\n    for varname in varnames:\n        if varname in fvarnames:\n            var_pos = fvarnames.index(varname)\n        else:\n            raise ValueError(\n                \"Function %s does not have variable %s.\" % (f.__name__, varnames)\n            )\n        if len(args) &gt;= var_pos + 1:\n            var_idx.append(var_pos)\n        elif varname in kwargs:\n            kwar_keys.append(varname)\n        else:\n            raise ValueError(\"%s was not specified in  %s.\" % (varnames, f.__name__))\n\n    return var_idx, kwar_keys\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.ideal_data","title":"<code>ideal_data(num, dimX, dimY, rrank, noise=1)</code>","text":"<p>Generate low-rank data.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>int</code> <p>Number of samples.</p> required <code>dimX</code> <code>int</code> <p>Dimensionality of the input data.</p> required <code>dimY</code> <code>int</code> <p>Dimensionality of the output data.</p> required <code>rrank</code> <code>int</code> <p>Rank of the low-rank structure.</p> required <code>noise</code> <code>float</code> <p>Standard deviation of the noise added to the output data (default is 1).</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple containing: - X : np.ndarray     The generated input data of shape (num, dimX). - Y : np.ndarray     The generated output data of shape (num, dimY).</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def ideal_data(\n    num: int, dimX: int, dimY: int, rrank: int, noise: float = 1\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate low-rank data.\n\n    Parameters\n    ----------\n    num : int\n        Number of samples.\n    dimX : int\n        Dimensionality of the input data.\n    dimY : int\n        Dimensionality of the output data.\n    rrank : int\n        Rank of the low-rank structure.\n    noise : float, optional\n        Standard deviation of the noise added to the output data (default is 1).\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        A tuple containing:\n        - X : np.ndarray\n            The generated input data of shape (num, dimX).\n        - Y : np.ndarray\n            The generated output data of shape (num, dimY).\n\n    \"\"\"\n    X = np.random.randn(num, dimX)\n    W = np.random.randn(dimX, rrank) @ np.random.randn(rrank, dimY)\n    Y = X @ W + np.random.randn(num, dimY) * noise\n    return X, Y\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.mean_ci_limits","title":"<code>mean_ci_limits(alpha, ci=0.95, w=None, d=None, axis=None)</code>","text":"<p>Computes the confidence limits on the mean for circular data.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>ci</code> <code>float</code> <p>Confidence interval limits are computed. Default is 0.95.</p> <code>0.95</code> <code>w</code> <code>ndarray</code> <p>Number of incidences in case of binned angle data.</p> <code>None</code> <code>d</code> <code>float</code> <p>Spacing of bin centers for binned data. If supplied, correction factor is used to correct for bias in estimation of r, in radians.</p> <code>None</code> <code>axis</code> <code>int</code> <p>Dimension along which to compute the result. Default is None (across all dimensions).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Confidence limit width d; mean \u00b1 d yields upper/lower (1 - xi)% confidence limit.</p> References <p>[Fisher1995], [Jammalamadaka2001], [Zar2009]_</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>def mean_ci_limits(\n    alpha: np.ndarray,\n    ci: float = 0.95,\n    w: Optional[np.ndarray] = None,\n    d: Optional[float] = None,\n    axis: Optional[int] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Computes the confidence limits on the mean for circular data.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Sample of angles in radians.\n    ci : float, optional\n        Confidence interval limits are computed. Default is 0.95.\n    w : np.ndarray, optional\n        Number of incidences in case of binned angle data.\n    d : float, optional\n        Spacing of bin centers for binned data. If supplied,\n        correction factor is used to correct for bias in\n        estimation of r, in radians.\n    axis : int, optional\n        Dimension along which to compute the result. Default is None\n        (across all dimensions).\n\n    Returns\n    -------\n    np.ndarray\n        Confidence limit width d; mean \u00b1 d yields upper/lower\n        (1 - xi)% confidence limit.\n\n    References\n    ----------\n    [Fisher1995]_, [Jammalamadaka2001]_, [Zar2009]_\n    \"\"\"\n\n    if w is None:\n        w = np.ones_like(alpha)\n\n    assert alpha.shape == w.shape, \"Dimensions of data and w do not match!\"\n\n    r = np.atleast_1d(resultant_vector_length(alpha, w=w, d=d, axis=axis))\n    n = np.atleast_1d(np.sum(w, axis=axis))\n\n    R = n * r\n    c2 = stats.chi2.ppf(ci, df=1)\n\n    t = np.NaN * np.empty_like(r)\n\n    idx = (r &lt; 0.9) &amp; (r &gt; np.sqrt(c2 / 2 / n))\n    t[idx] = np.sqrt(\n        (2 * n[idx] * (2 * R[idx] ** 2 - n[idx] * c2)) / (4 * n[idx] - c2)\n    )  # eq. 26.24\n\n    idx2 = r &gt;= 0.9\n    t[idx2] = np.sqrt(\n        n[idx2] ** 2 - (n[idx2] ** 2 - R[idx2] ** 2) * np.exp(c2 / n[idx2])\n    )  # equ. 26.25\n\n    if not np.all(idx | idx2):\n        raise UserWarning(\n            \"\"\"Requirements for confidence levels not met:\n                CI limits require a certain concentration of the data around the mean\"\"\"\n        )\n\n    return np.squeeze(np.arccos(t / R))\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.percentile","title":"<code>percentile(alpha, q, q0, axis=None, ci=None, bootstrap_iter=None)</code>","text":"<p>Computes circular percentiles.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Array with circular samples.</p> required <code>q</code> <code>float or iterable of float</code> <p>Percentiles in [0, 100] (single number or iterable).</p> required <code>q0</code> <code>float</code> <p>Value of the 0 percentile.</p> required <code>axis</code> <code>int</code> <p>Percentiles will be computed along this axis.  If None, percentiles will be computed over the entire array.</p> <code>None</code> <code>ci</code> <code>float</code> <p>If not None, confidence level is bootstrapped.</p> <code>None</code> <code>bootstrap_iter</code> <code>int</code> <p>Number of bootstrap iterations (number of samples if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Computed percentiles.</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>@mod2pi\n@bootstrap(1, \"circular\")\ndef percentile(alpha, q, q0, axis=None, ci=None, bootstrap_iter=None):\n    \"\"\"\n    Computes circular percentiles.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Array with circular samples.\n    q : float or iterable of float\n        Percentiles in [0, 100] (single number or iterable).\n    q0 : float\n        Value of the 0 percentile.\n    axis : int, optional\n        Percentiles will be computed along this axis. \n        If None, percentiles will be computed over the entire array.\n    ci : float, optional\n        If not None, confidence level is bootstrapped.\n    bootstrap_iter : int, optional\n        Number of bootstrap iterations (number of samples if None).\n\n    Returns\n    -------\n    np.ndarray\n        Computed percentiles.\n    \"\"\"\n    if axis is None:\n        alpha = (alpha.ravel() - q0) % (2 * np.pi)\n    else:\n        if len(q0.shape) == len(alpha.shape) - 1:\n            reshaper = tuple(\n                slice(None, None) if i != axis else np.newaxis\n                for i in range(len(alpha.shape))\n            )\n            q0 = q0[reshaper]\n        elif not len(q0.shape) == len(alpha.shape):\n            raise ValueError(\"Dimensions of start and alpha are inconsistent!\")\n\n        alpha = (alpha - q0) % (2 * np.pi)\n\n    ret = []\n    if axis is not None:\n        selector = tuple(\n            slice(None) if i != axis else 0 for i in range(len(alpha.shape))\n        )\n        q0 = q0[selector]\n\n    for qq in np.atleast_1d(q):\n        ret.append(np.percentile(alpha, qq, axis=axis) + q0)\n\n    if not hasattr(q, \"__iter__\"):  # if q is not some sort of list, array, etc\n        return np.asarray(ret).squeeze()\n    else:\n        return np.asarray(ret)\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.rayleigh","title":"<code>rayleigh(alpha, w=None, d=None, axis=None)</code>","text":"<p>Computes Rayleigh test for non-uniformity of circular data.</p> <p>H0: the population is uniformly distributed around the circle HA: the population is not distributed uniformly around the circle</p> <p>Assumption: the distribution has maximally one mode and the data is sampled from a von Mises distribution!</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>w</code> <code>ndarray</code> <p>Number of incidences in case of binned angle data.</p> <code>None</code> <code>d</code> <code>float</code> <p>Spacing of bin centers for binned data, if supplied. Correction factor is used to correct for bias in estimation of r.</p> <code>None</code> <code>axis</code> <code>int</code> <p>Compute along this dimension, default is None; if None, the array is raveled.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>pval</code> <code>float</code> <p>Two-tailed p-value.</p> <code>z</code> <code>float</code> <p>Value of the z-statistic.</p> References <p>[Fisher1995], [Jammalamadaka2001], [Zar2009]_</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>@swap2zeroaxis([\"alpha\", \"w\"], [0, 1])\ndef rayleigh(alpha: np.ndarray, w: np.ndarray = None, d: float = None, axis: int = None) -&gt; Tuple[float, float]:\n    \"\"\"\n    Computes Rayleigh test for non-uniformity of circular data.\n\n    H0: the population is uniformly distributed around the circle\n    HA: the population is not distributed uniformly around the circle\n\n    Assumption: the distribution has maximally one mode and the data is\n    sampled from a von Mises distribution!\n\n    Parameters\n    ----------\n    alpha : ndarray\n        Sample of angles in radians.\n    w : ndarray, optional\n        Number of incidences in case of binned angle data.\n    d : float, optional\n        Spacing of bin centers for binned data, if supplied.\n        Correction factor is used to correct for bias in estimation of r.\n    axis : int, optional\n        Compute along this dimension, default is None; if None, the array is raveled.\n\n    Returns\n    -------\n    pval : float\n        Two-tailed p-value.\n    z : float\n        Value of the z-statistic.\n\n    References\n    ----------\n    [Fisher1995]_, [Jammalamadaka2001]_, [Zar2009]_\n    \"\"\"\n\n    if w is None:\n        w = np.ones_like(alpha)\n\n    assert w.shape == alpha.shape, \"Dimensions of alpha and w must match\"\n\n    r = resultant_vector_length(alpha, w=w, d=d, axis=axis)\n    n = np.sum(w, axis=axis)\n\n    # compute Rayleigh's R (equ. 27.1)\n    R = n * r\n\n    # compute Rayleigh's z (equ. 27.2)\n    z = R**2 / n\n\n    # compute p value using approxation in Zar, p. 617\n    pval = np.exp(np.sqrt(1 + 4 * n + 4 * (n**2 - R**2)) - (1 + 2 * n))\n\n    return pval, z\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.regress_out","title":"<code>regress_out(a, b)</code>","text":"<p>Regress b from a while keeping a's original mean.</p> <p>This function performs regression of variable b from variable a while preserving the original mean of a. It calculates the residual component of a that remains after removing the effect of b using ordinary least squares.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ndarray</code> <p>The variable to be regressed. Must be 1-dimensional.</p> required <code>b</code> <code>ndarray</code> <p>The variable to regress on a. Must be 1-dimensional.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The residual of a after regressing out b. Has the same shape as a.</p> Notes <p>Adapted from the seaborn function of the same name: https://github.com/mwaskom/seaborn/blob/824c102525e6a29cde9bca1ce0096d50588fda6b/seaborn/regression.py#L337</p> Source code in <code>neuro_py/stats/stats.py</code> <pre><code>def regress_out(a: np.ndarray, b: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Regress b from a while keeping a's original mean.\n\n    This function performs regression of variable b from variable a while\n    preserving the original mean of a. It calculates the residual component\n    of a that remains after removing the effect of b using ordinary least squares.\n\n    Parameters\n    ----------\n    a : np.ndarray\n        The variable to be regressed. Must be 1-dimensional.\n    b : np.ndarray\n        The variable to regress on a. Must be 1-dimensional.\n\n    Returns\n    -------\n    np.ndarray\n        The residual of a after regressing out b. Has the same shape as a.\n\n    Notes\n    -----\n    Adapted from the seaborn function of the same name:\n    https://github.com/mwaskom/seaborn/blob/824c102525e6a29cde9bca1ce0096d50588fda6b/seaborn/regression.py#L337\n    \"\"\"\n    # remove nans and infs from a and b, and make a_result vector for output\n    a_result = np.full_like(a, np.nan)\n\n    valid_mask = np.isfinite(a) &amp; np.isfinite(b)\n    a = np.asarray(a)[valid_mask]\n    b = np.asarray(b)[valid_mask]\n\n    # remove mean from a and b\n    a_mean = a.mean()\n    a = a - a_mean\n    b = b - b.mean()\n\n    # calculate regression and subtract from a to get a_prime\n    b = np.c_[b]\n    a_prime = a - b @ np.linalg.pinv(b) @ a\n\n    # add mean back to a_prime\n    a_prime = np.asarray(a_prime + a_mean).reshape(a.shape)\n\n    # put a_prime back into a_result vector to preserve nans\n    a_result[valid_mask] = a_prime\n    return a_result\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.reindex_df","title":"<code>reindex_df(df, weight_col)</code>","text":"<p>Expand the dataframe by weights.</p> <p>This function expands the dataframe to prepare for resampling, resulting in 1 row per count per sample, which is helpful when making weighted proportion plots.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas dataframe to be expanded.</p> required <code>weight_col</code> <code>str</code> <p>The column name that contains weights (should be int).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new pandas dataframe with resampling based on the weights.</p> Source code in <code>neuro_py/stats/stats.py</code> <pre><code>def reindex_df(df: pd.DataFrame, weight_col: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Expand the dataframe by weights.\n\n    This function expands the dataframe to prepare for resampling,\n    resulting in 1 row per count per sample, which is helpful\n    when making weighted proportion plots.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The pandas dataframe to be expanded.\n    weight_col : str\n        The column name that contains weights (should be int).\n\n    Returns\n    -------\n    pd.DataFrame\n        A new pandas dataframe with resampling based on the weights.\n    \"\"\"\n\n    df = df.reindex(df.index.repeat(df[weight_col])).copy()\n\n    df.reset_index(drop=True, inplace=True)\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/stats/#neuro_py.stats.resultant_vector_length","title":"<code>resultant_vector_length(alpha, w=None, d=None, axis=None, axial_correction=1, ci=None, bootstrap_iter=None)</code>","text":"<p>Computes the mean resultant vector length for circular data.</p> <p>This statistic is sometimes also called vector strength.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>w</code> <code>ndarray</code> <p>Number of incidences in case of binned angle data.</p> <code>None</code> <code>ci</code> <code>float</code> <p>Confidence limits computed via bootstrapping. Default is None.</p> <code>None</code> <code>d</code> <code>float</code> <p>Spacing of bin centers for binned data. If supplied, correction factor is used to correct for bias in estimation of r, in radians.</p> <code>None</code> <code>axis</code> <code>int</code> <p>Dimension along which to compute the result. Default is None (across all dimensions).</p> <code>None</code> <code>axial_correction</code> <code>int</code> <p>Axial correction factor (2, 3, 4,...). Default is 1.</p> <code>1</code> <code>bootstrap_iter</code> <code>int</code> <p>Number of bootstrap iterations (number of samples if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Mean resultant vector length.</p> References <p>[Fisher1995], [Jammalamadaka2001], [Zar2009]_</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>@bootstrap(1, \"linear\")\ndef resultant_vector_length(\n    alpha: np.ndarray,\n    w: Optional[np.ndarray] = None,\n    d: Optional[float] = None,\n    axis: Optional[int] = None,\n    axial_correction: int = 1,\n    ci: Optional[float] = None,\n    bootstrap_iter: Optional[int] = None\n) -&gt; float:\n    \"\"\"\n    Computes the mean resultant vector length for circular data.\n\n    This statistic is sometimes also called vector strength.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Sample of angles in radians.\n    w : np.ndarray, optional\n        Number of incidences in case of binned angle data.\n    ci : float, optional\n        Confidence limits computed via bootstrapping. Default is None.\n    d : float, optional\n        Spacing of bin centers for binned data. If supplied,\n        correction factor is used to correct for bias in\n        estimation of r, in radians.\n    axis : int, optional\n        Dimension along which to compute the result. Default is None\n        (across all dimensions).\n    axial_correction : int, optional\n        Axial correction factor (2, 3, 4,...). Default is 1.\n    bootstrap_iter : int, optional\n        Number of bootstrap iterations (number of samples if None).\n\n    Returns\n    -------\n    float\n        Mean resultant vector length.\n\n    References\n    ----------\n    [Fisher1995]_, [Jammalamadaka2001]_, [Zar2009]_\n    \"\"\"\n    if axis is None:\n        axis = 0\n        alpha = alpha.ravel()\n        if w is not None:\n            w = w.ravel()\n\n    cmean = _complex_mean(alpha, w=w, axis=axis, axial_correction=axial_correction)\n\n    # obtain length\n    r = np.abs(cmean)\n\n    # for data with known spacing, apply correction factor to correct for bias\n    # in the estimation of r (see Zar, p. 601, equ. 26.16)\n    if d is not None:\n        if axial_correction &gt; 1:\n            warnings.warn(\"Axial correction ignored for bias correction.\")\n        r *= d / 2 / np.sin(d / 2)\n    return r\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/","title":"neuro_py.stats.circ_stats","text":""},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats.bootstrap","title":"<code>bootstrap</code>","text":"<p>Decorator to implement bootstrapping. It looks for the arguments ci, axis, and bootstrap_iter to determine the proper parameters for bootstrapping. The argument scale determines whether the percentile is taken on a circular scale or on a linear scale.</p> <p>Parameters:</p> Name Type Description Default <code>no_bootstrap</code> <code>int</code> <p>The number of arguments that are bootstrapped (e.g., for correlation it would be two, for median it would be one).</p> required <code>scale</code> <code>str</code> <p>Linear or circular scale (default is 'linear').</p> <code>'linear'</code> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>class bootstrap:\n    \"\"\"\n    Decorator to implement bootstrapping. It looks for the arguments ci, axis,\n    and bootstrap_iter to determine the proper parameters for bootstrapping.\n    The argument scale determines whether the percentile is taken on a circular\n    scale or on a linear scale.\n\n    Parameters\n    ----------\n    no_bootstrap : int\n        The number of arguments that are bootstrapped\n        (e.g., for correlation it would be two, for median it would be one).\n    scale : str, optional\n        Linear or circular scale (default is 'linear').\n    \"\"\"\n\n    def __init__(self, no_bootstrap, scale=\"linear\"):\n        self.no_boostrap = no_bootstrap\n        self.scale = scale\n\n    def _get_var(self, f, what, default, args, kwargs, remove=False):\n        varnames = f.__code__.co_varnames\n\n        if what in varnames:\n            what_idx = varnames.index(what)\n        else:\n            raise ValueError(\n                \"Function %s does not have variable %s.\" % (f.__name__, what)\n            )\n\n        if len(args) &gt;= what_idx + 1:\n            val = args[what_idx]\n            if remove:\n                args[what_idx] = default\n        else:\n            val = default\n\n        return val\n\n    def __call__(self, f):\n\n        def wrapper(f, *args, **kwargs):\n            args = list(args)\n            ci = self._get_var(f, \"ci\", None, args, kwargs, remove=True)\n            bootstrap_iter = self._get_var(\n                f, \"bootstrap_iter\", None, args, kwargs, remove=True\n            )\n            axis = self._get_var(f, \"axis\", None, args, kwargs)\n\n            alpha = args[: self.no_boostrap]\n            args0 = args[self.no_boostrap :]\n\n            if bootstrap_iter is None:\n                bootstrap_iter = (\n                    alpha[0].shape[axis] if axis is not None else alpha[0].size\n                )\n\n            r0 = f(*(alpha + args0), **kwargs)\n            if ci is not None:\n                r = np.asarray(\n                    [\n                        f(*(list(a) + args0), **kwargs)\n                        for a in nd_bootstrap(\n                            alpha, bootstrap_iter, axis=axis, strip_tuple_if_one=False\n                        )\n                    ]\n                )\n\n                if self.scale == \"linear\":\n                    ci_low, ci_high = np.percentile(\n                        r, [(1 - ci) / 2 * 100, (1 + ci) / 2 * 100], axis=0\n                    )\n                elif self.scale == \"circular\":\n                    ci_low, ci_high = percentile(\n                        r,\n                        [(1 - ci) / 2 * 100, (1 + ci) / 2 * 100],\n                        q0=(r0 + np.pi) % (2 * np.pi),\n                        axis=0,\n                    )\n                else:\n                    raise ValueError(\"Scale %s not known!\" % (self.scale,))\n                return r0, CI(ci_low, ci_high)\n            else:\n                return r0\n\n        return decorator(wrapper, f)\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats.swap2zeroaxis","title":"<code>swap2zeroaxis</code>","text":"<p>Decorator to swap specified axes of input arguments to zero and swap them back in output.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>list of str</code> <p>The names of the input arguments for which the axes are swapped.</p> required <code>out_idx</code> <code>list of int</code> <p>The indices of the output arguments whose axes will be swapped back.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a specified output index is inconsistent with a single output argument.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; @swap2zeroaxis(['x', 'y'], [0, 1])\n&gt;&gt;&gt; def dummy(x, y, z, axis=None):\n&gt;&gt;&gt;    return np.mean(x[::2, ...], axis=0), np.mean(y[::2, ...], axis=0), z\n</code></pre> <p>This creates a new function that:</p> <ul> <li>Either swaps the specified axes to zero for the arguments <code>x</code> and <code>y</code>    if <code>axis</code> is specified in the wrapped function, or flattens <code>x</code> and <code>y</code>.</li> <li>Swaps back the axes from the output arguments, assuming the outputs lost    one dimension during the function (e.g., like <code>numpy.mean(x, axis=1)</code>).</li> </ul> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>class swap2zeroaxis:\n    \"\"\"\n    Decorator to swap specified axes of input arguments to zero and swap them back in output.\n\n    Parameters\n    ----------\n    inputs : list of str\n        The names of the input arguments for which the axes are swapped.\n    out_idx : list of int\n        The indices of the output arguments whose axes will be swapped back.\n\n    Raises\n    ------\n    ValueError\n        If a specified output index is inconsistent with a single output argument.\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; @swap2zeroaxis(['x', 'y'], [0, 1])\n    &gt;&gt;&gt; def dummy(x, y, z, axis=None):\n    &gt;&gt;&gt;    return np.mean(x[::2, ...], axis=0), np.mean(y[::2, ...], axis=0), z\n\n    This creates a new function that:\n\n    - Either swaps the specified axes to zero for the arguments `x` and `y` \n      if `axis` is specified in the wrapped function, or flattens `x` and `y`.\n    - Swaps back the axes from the output arguments, assuming the outputs lost \n      one dimension during the function (e.g., like `numpy.mean(x, axis=1)`).\n    \"\"\"\n\n    def __init__(self, inputs: list[str], out_idx: list[int]):\n        self.inputs = inputs\n        self.out_idx = out_idx\n\n    def __call__(self, f: callable) -&gt; callable:\n\n        def _deco(f: callable, *args: tuple, **kwargs: dict) -&gt; tuple:\n\n            to_swap_idx, to_swap_keys = get_var(f, self.inputs, args, kwargs)\n            args = list(args)\n\n            # extract axis parameter\n            try:\n                axis_idx, axis_kw = get_var(f, [\"axis\"], args, kwargs)\n                if len(axis_idx) == 0 and len(axis_kw) == 0:\n                    axis = None\n                else:\n                    if len(axis_idx) &gt; 0:\n                        axis, args[axis_idx[0]] = args[axis_idx[0]], 0\n                    else:\n                        axis, kwargs[axis_kw[0]] = kwargs[axis_kw[0]], 0\n            except ValueError:\n                axis = None\n\n            # adjust axes or flatten\n            if axis is not None:\n                for i in to_swap_idx:\n                    if args[i] is not None:\n                        args[i] = args[i].swapaxes(0, axis)\n                for k in to_swap_keys:\n                    if kwargs[k] is not None:\n                        kwargs[k] = kwargs[k].swapaxes(0, axis)\n            else:\n                for i in to_swap_idx:\n                    if args[i] is not None:\n                        args[i] = args[i].ravel()\n                for k in to_swap_keys:\n                    if kwargs[k] is not None:\n                        kwargs[k] = kwargs[k].ravel()\n\n            # compute function\n            outputs = f(*args, **kwargs)\n\n            # swap everything back into place\n            if len(self.out_idx) &gt; 0 and axis is not None:\n                if isinstance(outputs, tuple):\n                    outputs = list(outputs)\n                    for i in self.out_idx:\n                        outputs[i] = (\n                            outputs[i][np.newaxis, ...].swapaxes(0, axis).squeeze()\n                        )\n\n                    return tuple(outputs)\n                else:\n                    if self.out_idx != [0]:\n                        raise ValueError(\n                            \"Single output argument and out_idx \\\n                                         != [0] are inconsistent!\"\n                        )\n                    return outputs[np.newaxis, ...].swapaxes(0, axis).squeeze()\n            else:\n                return outputs\n\n        return decorator(_deco, f)\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats._complex_mean","title":"<code>_complex_mean(alpha, w=None, axis=None, axial_correction=1)</code>","text":"<p>Compute the weighted mean of complex values.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Array of angles (in radians) representing complex values.</p> required <code>w</code> <code>ndarray</code> <p>Array of weights corresponding to the alpha values. If None, uniform weights are used.</p> <code>None</code> <code>axis</code> <code>int</code> <p>Axis along which the mean is computed. If None, the mean is computed over the entire array.</p> <code>None</code> <code>axial_correction</code> <code>float</code> <p>Correction factor for the angles (default is 1).</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Weighted mean of the complex values.</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>def _complex_mean(alpha: np.ndarray, w: Optional[np.ndarray] = None, axis: Optional[int] = None, axial_correction: float = 1) -&gt; np.ndarray:\n    \"\"\"\n    Compute the weighted mean of complex values.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Array of angles (in radians) representing complex values.\n    w : np.ndarray, optional\n        Array of weights corresponding to the alpha values. If None, uniform weights are used.\n    axis : int, optional\n        Axis along which the mean is computed. If None, the mean is computed over the entire array.\n    axial_correction : float, optional\n        Correction factor for the angles (default is 1).\n\n    Returns\n    -------\n    np.ndarray\n        Weighted mean of the complex values.\n    \"\"\"\n    if w is None:\n        w = np.ones_like(alpha)\n    alpha = np.asarray(alpha)\n\n    assert w.shape == alpha.shape, (\n        \"Dimensions of data \"\n        + str(alpha.shape)\n        + \" and w \"\n        + str(w.shape)\n        + \" do not match!\"\n    )\n\n    return (w * np.exp(1j * alpha * axial_correction)).sum(axis=axis) / np.sum(\n        w, axis=axis\n    )\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats.center","title":"<code>center(*args, **kwargs)</code>","text":"<p>Centers the data on its circular mean.</p> <p>Each non-keyword argument is another data array that is centered.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>int</code> <p>The mean is computed along this dimension (default is None). Must be used as a keyword argument!</p> required <p>Returns:</p> Type Description <code>tuple of np.ndarray</code> <p>Tuple of centered data arrays.</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>@mod2pi\ndef center(*args: np.ndarray, **kwargs: Optional[dict]) -&gt; Tuple[np.ndarray, ...]:\n    \"\"\"\n    Centers the data on its circular mean.\n\n    Each non-keyword argument is another data array that is centered.\n\n    Parameters\n    ----------\n    axis : int, optional\n        The mean is computed along this dimension (default is None).\n        **Must be used as a keyword argument!**\n\n    Returns\n    -------\n    tuple of np.ndarray\n        Tuple of centered data arrays.\n    \"\"\"\n\n    axis = kwargs.pop(\"axis\", None)\n    if axis is None:\n        axis = 0\n        args = [a.ravel() for a in args]\n\n    reshaper = tuple(\n        slice(None, None) if i != axis else np.newaxis\n        for i in range(len(args[0].shape))\n    )\n    if len(args) == 1:\n        return args[0] - mean(args[0], axis=axis)\n    else:\n        return tuple(\n            [\n                a - mean(a, axis=axis)[reshaper]\n                for a in args\n                if isinstance(a, np.ndarray)\n            ]\n        )\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats.get_var","title":"<code>get_var(f, varnames, args, kwargs)</code>","text":"<p>Retrieve indices of specified variables from a function's argument list.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>The function from which to retrieve variable information.</p> required <code>varnames</code> <code>list of str</code> <p>The names of the variables to retrieve.</p> required <code>args</code> <code>list</code> <p>Positional arguments passed to the function.</p> required <code>kwargs</code> <code>dict</code> <p>Keyword arguments passed to the function.</p> required <p>Returns:</p> Type Description <code>tuple of (list of int, list of str)</code> <p>A tuple containing two elements: - A list of indices of the specified variables in the function's argument list. - A list of keys for the keyword arguments that correspond to the specified variables.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a specified variable is not found in the function's argument list.</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>def get_var(f: Callable, varnames: List[str], args: List[Any], kwargs: Dict[str, Any]) -&gt; Tuple[List[int], List[str]]:\n    \"\"\"\n    Retrieve indices of specified variables from a function's argument list.\n\n    Parameters\n    ----------\n    f : Callable\n        The function from which to retrieve variable information.\n    varnames : list of str\n        The names of the variables to retrieve.\n    args : list\n        Positional arguments passed to the function.\n    kwargs : dict\n        Keyword arguments passed to the function.\n\n    Returns\n    -------\n    tuple of (list of int, list of str)\n        A tuple containing two elements:\n        - A list of indices of the specified variables in the function's argument list.\n        - A list of keys for the keyword arguments that correspond to the specified variables.\n\n    Raises\n    ------\n    ValueError\n        If a specified variable is not found in the function's argument list.\n    \"\"\"\n    fvarnames = f.__code__.co_varnames\n\n    var_idx = []\n    kwar_keys = []\n    for varname in varnames:\n        if varname in fvarnames:\n            var_pos = fvarnames.index(varname)\n        else:\n            raise ValueError(\n                \"Function %s does not have variable %s.\" % (f.__name__, varnames)\n            )\n        if len(args) &gt;= var_pos + 1:\n            var_idx.append(var_pos)\n        elif varname in kwargs:\n            kwar_keys.append(varname)\n        else:\n            raise ValueError(\"%s was not specified in  %s.\" % (varnames, f.__name__))\n\n    return var_idx, kwar_keys\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats.mean","title":"<code>mean(alpha, w=None, ci=None, d=None, axis=None, axial_correction=1)</code>","text":"<p>Compute mean direction of circular data.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Circular data.</p> required <code>w</code> <code>ndarray</code> <p>Weightings in case of binned angle data.</p> <code>None</code> <code>ci</code> <code>float</code> <p>If not None, the upper and lower 100*ci% confidence interval is returned as well.</p> <code>None</code> <code>d</code> <code>float</code> <p>Spacing of bin centers for binned data. If supplied, correction factor is used to correct for bias in estimation of r, in radians.</p> <code>None</code> <code>axis</code> <code>int</code> <p>Compute along this dimension. Default is None (across all dimensions).</p> <code>None</code> <code>axial_correction</code> <code>int</code> <p>Axial correction (2,3,4,...). Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>float or Tuple[float, CI]</code> <p>Circular mean if ci is None, or circular mean as well as lower and upper confidence interval limits.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; data = 2 * np.pi * np.random.rand(10)\n&gt;&gt;&gt; mu, (ci_l, ci_u) = mean(data, ci=0.95)\n</code></pre> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>@mod2pi\ndef mean(\n    alpha: np.ndarray,\n    w: Optional[np.ndarray] = None,\n    ci: Optional[float] = None,\n    d: Optional[float] = None,\n    axis: Optional[int] = None,\n    axial_correction: int = 1\n) -&gt; Union[float, Tuple[float, CI]]:\n    \"\"\"\n    Compute mean direction of circular data.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Circular data.\n    w : np.ndarray, optional\n        Weightings in case of binned angle data.\n    ci : float, optional\n        If not None, the upper and lower 100*ci% confidence\n        interval is returned as well.\n    d : float, optional\n        Spacing of bin centers for binned data. If supplied,\n        correction factor is used to correct for bias in\n        estimation of r, in radians.\n    axis : int, optional\n        Compute along this dimension. Default is None\n        (across all dimensions).\n    axial_correction : int, optional\n        Axial correction (2,3,4,...). Default is 1.\n\n    Returns\n    -------\n    float or Tuple[float, CI]\n        Circular mean if ci is None, or circular mean as well as lower and\n        upper confidence interval limits.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; data = 2 * np.pi * np.random.rand(10)\n    &gt;&gt;&gt; mu, (ci_l, ci_u) = mean(data, ci=0.95)\n    \"\"\"\n\n    cmean = _complex_mean(alpha, w=w, axis=axis, axial_correction=axial_correction)\n\n    mu = np.angle(cmean) / axial_correction\n\n    if ci is None:\n        return mu\n    else:\n        if axial_correction &gt; 1:  # TODO: implement CI for axial correction\n            warnings.warn(\"Axial correction ignored for confidence intervals.\")\n        t = mean_ci_limits(alpha, ci=ci, w=w, d=d, axis=axis)\n        return mu, CI(mu - t, mu + t)\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats.mean_ci_limits","title":"<code>mean_ci_limits(alpha, ci=0.95, w=None, d=None, axis=None)</code>","text":"<p>Computes the confidence limits on the mean for circular data.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>ci</code> <code>float</code> <p>Confidence interval limits are computed. Default is 0.95.</p> <code>0.95</code> <code>w</code> <code>ndarray</code> <p>Number of incidences in case of binned angle data.</p> <code>None</code> <code>d</code> <code>float</code> <p>Spacing of bin centers for binned data. If supplied, correction factor is used to correct for bias in estimation of r, in radians.</p> <code>None</code> <code>axis</code> <code>int</code> <p>Dimension along which to compute the result. Default is None (across all dimensions).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Confidence limit width d; mean \u00b1 d yields upper/lower (1 - xi)% confidence limit.</p> References <p>[Fisher1995], [Jammalamadaka2001], [Zar2009]_</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>def mean_ci_limits(\n    alpha: np.ndarray,\n    ci: float = 0.95,\n    w: Optional[np.ndarray] = None,\n    d: Optional[float] = None,\n    axis: Optional[int] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Computes the confidence limits on the mean for circular data.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Sample of angles in radians.\n    ci : float, optional\n        Confidence interval limits are computed. Default is 0.95.\n    w : np.ndarray, optional\n        Number of incidences in case of binned angle data.\n    d : float, optional\n        Spacing of bin centers for binned data. If supplied,\n        correction factor is used to correct for bias in\n        estimation of r, in radians.\n    axis : int, optional\n        Dimension along which to compute the result. Default is None\n        (across all dimensions).\n\n    Returns\n    -------\n    np.ndarray\n        Confidence limit width d; mean \u00b1 d yields upper/lower\n        (1 - xi)% confidence limit.\n\n    References\n    ----------\n    [Fisher1995]_, [Jammalamadaka2001]_, [Zar2009]_\n    \"\"\"\n\n    if w is None:\n        w = np.ones_like(alpha)\n\n    assert alpha.shape == w.shape, \"Dimensions of data and w do not match!\"\n\n    r = np.atleast_1d(resultant_vector_length(alpha, w=w, d=d, axis=axis))\n    n = np.atleast_1d(np.sum(w, axis=axis))\n\n    R = n * r\n    c2 = stats.chi2.ppf(ci, df=1)\n\n    t = np.NaN * np.empty_like(r)\n\n    idx = (r &lt; 0.9) &amp; (r &gt; np.sqrt(c2 / 2 / n))\n    t[idx] = np.sqrt(\n        (2 * n[idx] * (2 * R[idx] ** 2 - n[idx] * c2)) / (4 * n[idx] - c2)\n    )  # eq. 26.24\n\n    idx2 = r &gt;= 0.9\n    t[idx2] = np.sqrt(\n        n[idx2] ** 2 - (n[idx2] ** 2 - R[idx2] ** 2) * np.exp(c2 / n[idx2])\n    )  # equ. 26.25\n\n    if not np.all(idx | idx2):\n        raise UserWarning(\n            \"\"\"Requirements for confidence levels not met:\n                CI limits require a certain concentration of the data around the mean\"\"\"\n        )\n\n    return np.squeeze(np.arccos(t / R))\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats.mod2pi","title":"<code>mod2pi(f)</code>","text":"<p>Decorator to apply modulo 2*pi on the output of the function.</p> <p>The decorated function must either return a tuple of numpy.ndarrays or a numpy.ndarray itself.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable</code> <p>The function to be decorated.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A wrapper function that applies modulo 2*pi on the output.</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>def mod2pi(f: Callable) -&gt; Callable:\n    \"\"\"\n    Decorator to apply modulo 2*pi on the output of the function.\n\n    The decorated function must either return a tuple of numpy.ndarrays or a\n    numpy.ndarray itself.\n\n    Parameters\n    ----------\n    f : Callable\n        The function to be decorated.\n\n    Returns\n    -------\n    Callable\n        A wrapper function that applies modulo 2*pi on the output.\n    \"\"\"\n\n    def wrapper(f, *args, **kwargs):\n        ret = f(*args, **kwargs)\n\n        if isinstance(ret, tuple):\n            ret2 = []\n            for r in ret:\n                if isinstance(r, np.ndarray) or np.isscalar(r):\n                    ret2.append(r % (2 * np.pi))\n                elif isinstance(r, CI):\n                    ret2.append(CI(r.lower % (2 * np.pi), r.upper % (2 * np.pi)))\n                else:\n                    raise TypeError(\"Type not known!\")\n            return tuple(ret2)\n        elif isinstance(ret, np.ndarray) or np.isscalar(ret):\n            return ret % (2 * np.pi)\n        else:\n            raise TypeError(\"Type not known!\")\n\n    return decorator(wrapper, f)\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats.nd_bootstrap","title":"<code>nd_bootstrap(data, iterations, axis=None, strip_tuple_if_one=True)</code>","text":"<p>Bootstrap iterator for several n-dimensional data arrays.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Iterable[ndarray]</code> <p>Iterable containing the data arrays.</p> required <code>iterations</code> <code>int</code> <p>Number of bootstrap iterations.</p> required <code>axis</code> <code>Union[int, None]</code> <p>Bootstrapping is performed along this axis. If None, the data is flattened.</p> <code>None</code> <code>strip_tuple_if_one</code> <code>bool</code> <p>If True, return a single array without tuple if only one data array is provided.</p> <code>True</code> <p>Yields:</p> Type Description <code>Tuple[ndarray, ...]</code> <p>Bootstrapped data arrays for each iteration.</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>def nd_bootstrap(\n    data: Iterable[np.ndarray], \n    iterations: int, \n    axis: Union[int, None] = None, \n    strip_tuple_if_one: bool = True\n) -&gt; Generator[Union[np.ndarray, Tuple[np.ndarray, ...]], None, None]:\n    \"\"\"\n    Bootstrap iterator for several n-dimensional data arrays.\n\n    Parameters\n    ----------\n    data : Iterable[np.ndarray]\n        Iterable containing the data arrays.\n    iterations : int\n        Number of bootstrap iterations.\n    axis : Union[int, None], optional\n        Bootstrapping is performed along this axis. If None, the data is flattened.\n    strip_tuple_if_one : bool, optional\n        If True, return a single array without tuple if only one data array is provided.\n\n    Yields\n    ------\n    Tuple[np.ndarray, ...]\n        Bootstrapped data arrays for each iteration.\n    \"\"\"\n    shape0 = data[0].shape\n    if axis is None:\n        axis = 0\n        data = [d.ravel() for d in data]\n\n    n = len(data[0].shape)\n    K = len(data)\n    data0 = []\n\n    if axis is not None:\n        m = data[0].shape[axis]\n        to = tuple([axis]) + tuple(range(axis)) + tuple(range(axis + 1, n))\n        fro = tuple(range(1, axis + 1)) + (0,) + tuple(range(axis + 1, n))\n        for i in range(K):\n            data0.append(data[i].transpose(to))\n\n        for i in range(iterations):\n            idx = np.random.randint(m, size=(m,))\n            if len(data) == 1 and strip_tuple_if_one:\n                yield (\n                    data0[0][np.ix_(idx), ...].squeeze().transpose(fro).reshape(shape0)\n                )\n            else:\n                yield tuple(\n                    a[np.ix_(idx), ...].squeeze().transpose(fro).reshape(shape0)\n                    for a in data0\n                )\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats.percentile","title":"<code>percentile(alpha, q, q0, axis=None, ci=None, bootstrap_iter=None)</code>","text":"<p>Computes circular percentiles.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Array with circular samples.</p> required <code>q</code> <code>float or iterable of float</code> <p>Percentiles in [0, 100] (single number or iterable).</p> required <code>q0</code> <code>float</code> <p>Value of the 0 percentile.</p> required <code>axis</code> <code>int</code> <p>Percentiles will be computed along this axis.  If None, percentiles will be computed over the entire array.</p> <code>None</code> <code>ci</code> <code>float</code> <p>If not None, confidence level is bootstrapped.</p> <code>None</code> <code>bootstrap_iter</code> <code>int</code> <p>Number of bootstrap iterations (number of samples if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Computed percentiles.</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>@mod2pi\n@bootstrap(1, \"circular\")\ndef percentile(alpha, q, q0, axis=None, ci=None, bootstrap_iter=None):\n    \"\"\"\n    Computes circular percentiles.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Array with circular samples.\n    q : float or iterable of float\n        Percentiles in [0, 100] (single number or iterable).\n    q0 : float\n        Value of the 0 percentile.\n    axis : int, optional\n        Percentiles will be computed along this axis. \n        If None, percentiles will be computed over the entire array.\n    ci : float, optional\n        If not None, confidence level is bootstrapped.\n    bootstrap_iter : int, optional\n        Number of bootstrap iterations (number of samples if None).\n\n    Returns\n    -------\n    np.ndarray\n        Computed percentiles.\n    \"\"\"\n    if axis is None:\n        alpha = (alpha.ravel() - q0) % (2 * np.pi)\n    else:\n        if len(q0.shape) == len(alpha.shape) - 1:\n            reshaper = tuple(\n                slice(None, None) if i != axis else np.newaxis\n                for i in range(len(alpha.shape))\n            )\n            q0 = q0[reshaper]\n        elif not len(q0.shape) == len(alpha.shape):\n            raise ValueError(\"Dimensions of start and alpha are inconsistent!\")\n\n        alpha = (alpha - q0) % (2 * np.pi)\n\n    ret = []\n    if axis is not None:\n        selector = tuple(\n            slice(None) if i != axis else 0 for i in range(len(alpha.shape))\n        )\n        q0 = q0[selector]\n\n    for qq in np.atleast_1d(q):\n        ret.append(np.percentile(alpha, qq, axis=axis) + q0)\n\n    if not hasattr(q, \"__iter__\"):  # if q is not some sort of list, array, etc\n        return np.asarray(ret).squeeze()\n    else:\n        return np.asarray(ret)\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats.rayleigh","title":"<code>rayleigh(alpha, w=None, d=None, axis=None)</code>","text":"<p>Computes Rayleigh test for non-uniformity of circular data.</p> <p>H0: the population is uniformly distributed around the circle HA: the population is not distributed uniformly around the circle</p> <p>Assumption: the distribution has maximally one mode and the data is sampled from a von Mises distribution!</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>w</code> <code>ndarray</code> <p>Number of incidences in case of binned angle data.</p> <code>None</code> <code>d</code> <code>float</code> <p>Spacing of bin centers for binned data, if supplied. Correction factor is used to correct for bias in estimation of r.</p> <code>None</code> <code>axis</code> <code>int</code> <p>Compute along this dimension, default is None; if None, the array is raveled.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>pval</code> <code>float</code> <p>Two-tailed p-value.</p> <code>z</code> <code>float</code> <p>Value of the z-statistic.</p> References <p>[Fisher1995], [Jammalamadaka2001], [Zar2009]_</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>@swap2zeroaxis([\"alpha\", \"w\"], [0, 1])\ndef rayleigh(alpha: np.ndarray, w: np.ndarray = None, d: float = None, axis: int = None) -&gt; Tuple[float, float]:\n    \"\"\"\n    Computes Rayleigh test for non-uniformity of circular data.\n\n    H0: the population is uniformly distributed around the circle\n    HA: the population is not distributed uniformly around the circle\n\n    Assumption: the distribution has maximally one mode and the data is\n    sampled from a von Mises distribution!\n\n    Parameters\n    ----------\n    alpha : ndarray\n        Sample of angles in radians.\n    w : ndarray, optional\n        Number of incidences in case of binned angle data.\n    d : float, optional\n        Spacing of bin centers for binned data, if supplied.\n        Correction factor is used to correct for bias in estimation of r.\n    axis : int, optional\n        Compute along this dimension, default is None; if None, the array is raveled.\n\n    Returns\n    -------\n    pval : float\n        Two-tailed p-value.\n    z : float\n        Value of the z-statistic.\n\n    References\n    ----------\n    [Fisher1995]_, [Jammalamadaka2001]_, [Zar2009]_\n    \"\"\"\n\n    if w is None:\n        w = np.ones_like(alpha)\n\n    assert w.shape == alpha.shape, \"Dimensions of alpha and w must match\"\n\n    r = resultant_vector_length(alpha, w=w, d=d, axis=axis)\n    n = np.sum(w, axis=axis)\n\n    # compute Rayleigh's R (equ. 27.1)\n    R = n * r\n\n    # compute Rayleigh's z (equ. 27.2)\n    z = R**2 / n\n\n    # compute p value using approxation in Zar, p. 617\n    pval = np.exp(np.sqrt(1 + 4 * n + 4 * (n**2 - R**2)) - (1 + 2 * n))\n\n    return pval, z\n</code></pre>"},{"location":"reference/neuro_py/stats/circ_stats/#neuro_py.stats.circ_stats.resultant_vector_length","title":"<code>resultant_vector_length(alpha, w=None, d=None, axis=None, axial_correction=1, ci=None, bootstrap_iter=None)</code>","text":"<p>Computes the mean resultant vector length for circular data.</p> <p>This statistic is sometimes also called vector strength.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>ndarray</code> <p>Sample of angles in radians.</p> required <code>w</code> <code>ndarray</code> <p>Number of incidences in case of binned angle data.</p> <code>None</code> <code>ci</code> <code>float</code> <p>Confidence limits computed via bootstrapping. Default is None.</p> <code>None</code> <code>d</code> <code>float</code> <p>Spacing of bin centers for binned data. If supplied, correction factor is used to correct for bias in estimation of r, in radians.</p> <code>None</code> <code>axis</code> <code>int</code> <p>Dimension along which to compute the result. Default is None (across all dimensions).</p> <code>None</code> <code>axial_correction</code> <code>int</code> <p>Axial correction factor (2, 3, 4,...). Default is 1.</p> <code>1</code> <code>bootstrap_iter</code> <code>int</code> <p>Number of bootstrap iterations (number of samples if None).</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Mean resultant vector length.</p> References <p>[Fisher1995], [Jammalamadaka2001], [Zar2009]_</p> Source code in <code>neuro_py/stats/circ_stats.py</code> <pre><code>@bootstrap(1, \"linear\")\ndef resultant_vector_length(\n    alpha: np.ndarray,\n    w: Optional[np.ndarray] = None,\n    d: Optional[float] = None,\n    axis: Optional[int] = None,\n    axial_correction: int = 1,\n    ci: Optional[float] = None,\n    bootstrap_iter: Optional[int] = None\n) -&gt; float:\n    \"\"\"\n    Computes the mean resultant vector length for circular data.\n\n    This statistic is sometimes also called vector strength.\n\n    Parameters\n    ----------\n    alpha : np.ndarray\n        Sample of angles in radians.\n    w : np.ndarray, optional\n        Number of incidences in case of binned angle data.\n    ci : float, optional\n        Confidence limits computed via bootstrapping. Default is None.\n    d : float, optional\n        Spacing of bin centers for binned data. If supplied,\n        correction factor is used to correct for bias in\n        estimation of r, in radians.\n    axis : int, optional\n        Dimension along which to compute the result. Default is None\n        (across all dimensions).\n    axial_correction : int, optional\n        Axial correction factor (2, 3, 4,...). Default is 1.\n    bootstrap_iter : int, optional\n        Number of bootstrap iterations (number of samples if None).\n\n    Returns\n    -------\n    float\n        Mean resultant vector length.\n\n    References\n    ----------\n    [Fisher1995]_, [Jammalamadaka2001]_, [Zar2009]_\n    \"\"\"\n    if axis is None:\n        axis = 0\n        alpha = alpha.ravel()\n        if w is not None:\n            w = w.ravel()\n\n    cmean = _complex_mean(alpha, w=w, axis=axis, axial_correction=axial_correction)\n\n    # obtain length\n    r = np.abs(cmean)\n\n    # for data with known spacing, apply correction factor to correct for bias\n    # in the estimation of r (see Zar, p. 601, equ. 26.16)\n    if d is not None:\n        if axial_correction &gt; 1:\n            warnings.warn(\"Axial correction ignored for bias correction.\")\n        r *= d / 2 / np.sin(d / 2)\n    return r\n</code></pre>"},{"location":"reference/neuro_py/stats/regression/","title":"neuro_py.stats.regression","text":""},{"location":"reference/neuro_py/stats/regression/#neuro_py.stats.regression.MultivariateRegressor","title":"<code>MultivariateRegressor</code>","text":"<p>               Bases: <code>object</code></p> <p>Multivariate Linear Regressor.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>An n-by-d matrix of features.</p> required <code>Y</code> <code>ndarray</code> <p>An n-by-D matrix of targets.</p> required <code>reg</code> <code>Optional[float]</code> <p>A regularization parameter (default is None).</p> <code>None</code> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>class MultivariateRegressor(object):\n    \"\"\"\n    Multivariate Linear Regressor.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        An n-by-d matrix of features.\n    Y : np.ndarray\n        An n-by-D matrix of targets.\n    reg : Optional[float], optional\n        A regularization parameter (default is None).\n    \"\"\"\n\n    def __init__(self, X: np.ndarray, Y: np.ndarray, reg: Optional[float] = None):\n        if np.size(np.shape(X)) == 1:\n            X = np.reshape(X, (-1, 1))\n        if np.size(np.shape(Y)) == 1:\n            Y = np.reshape(Y, (-1, 1))\n        if reg is None:\n            reg = 0\n\n        W1 = np.linalg.pinv(np.dot(X.T, X) + reg * sparse.eye(np.size(X, 1)))\n        W2 = np.dot(X, W1)\n        self.W = np.dot(Y.T, W2)\n\n    def __str__(self) -&gt; str:\n        return \"Multivariate Linear Regression\"\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Return the predicted Y for input X.\"\"\"\n        if np.size(np.shape(X)) == 1:\n            X = np.reshape(X, (-1, 1))\n        return np.array(np.dot(X, self.W.T))\n\n    def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n        \"\"\"Return the coefficient of determination R^2 of the prediction.\"\"\"\n        y_pred = self.predict(X)\n        return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/regression/#neuro_py.stats.regression.MultivariateRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Return the predicted Y for input X.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return the predicted Y for input X.\"\"\"\n    if np.size(np.shape(X)) == 1:\n        X = np.reshape(X, (-1, 1))\n    return np.array(np.dot(X, self.W.T))\n</code></pre>"},{"location":"reference/neuro_py/stats/regression/#neuro_py.stats.regression.MultivariateRegressor.score","title":"<code>score(X, Y)</code>","text":"<p>Return the coefficient of determination R^2 of the prediction.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n    \"\"\"Return the coefficient of determination R^2 of the prediction.\"\"\"\n    y_pred = self.predict(X)\n    return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/regression/#neuro_py.stats.regression.ReducedRankRegressor","title":"<code>ReducedRankRegressor</code>","text":"<p>               Bases: <code>object</code></p> <p>Reduced Rank Regressor (linear 'bottlenecking' or 'multitask learning').</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>An n-by-d matrix of features.</p> required <code>Y</code> <code>ndarray</code> <p>An n-by-D matrix of targets.</p> required <code>rank</code> <code>int</code> <p>A rank constraint.</p> required <code>reg</code> <code>Optional[float]</code> <p>A regularization parameter (default is None).</p> <code>None</code> References <p>Implemented by Chris Rayner (2015). dchrisrayner AT gmail DOT com</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>class ReducedRankRegressor(object):\n    \"\"\"\n    Reduced Rank Regressor (linear 'bottlenecking' or 'multitask learning').\n\n    Parameters\n    ----------\n    X : np.ndarray\n        An n-by-d matrix of features.\n    Y : np.ndarray\n        An n-by-D matrix of targets.\n    rank : int\n        A rank constraint.\n    reg : Optional[float], optional\n        A regularization parameter (default is None).\n\n    References\n    ----\n    Implemented by Chris Rayner (2015).\n    dchrisrayner AT gmail DOT com\n    \"\"\"\n\n    def __init__(\n        self, X: np.ndarray, Y: np.ndarray, rank: int, reg: Optional[float] = None\n    ):\n        if np.size(np.shape(X)) == 1:\n            X = np.reshape(X, (-1, 1))\n        if np.size(np.shape(Y)) == 1:\n            Y = np.reshape(Y, (-1, 1))\n        if reg is None:\n            reg = 0\n        self.rank = rank\n\n        CXX = X.T @ X + reg * sparse.eye(np.size(X, 1))\n        CXY = X.T @ Y\n        _U, _S, V = np.linalg.svd(CXY.T @ (np.linalg.pinv(CXX) @ CXY))\n        self.W = V[0:rank, :].T\n        self.A = (np.linalg.pinv(CXX) @ (CXY @ self.W)).T\n\n    def __str__(self) -&gt; str:\n        return \"Reduced Rank Regressor (rank = {})\".format(self.rank)\n\n    def predict(self, X: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Predict Y from X.\"\"\"\n        if np.size(np.shape(X)) == 1:\n            X = np.reshape(X, (-1, 1))\n        return X @ (self.A.T @ self.W.T)\n\n    def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n        \"\"\"Score the model.\"\"\"\n        if np.size(np.shape(X)) == 1:\n            X = np.reshape(X, (-1, 1))\n        if np.size(np.shape(Y)) == 1:\n            Y = np.reshape(Y, (-1, 1))\n\n        y_pred = self.predict(X)\n        return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/regression/#neuro_py.stats.regression.ReducedRankRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predict Y from X.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def predict(self, X: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Predict Y from X.\"\"\"\n    if np.size(np.shape(X)) == 1:\n        X = np.reshape(X, (-1, 1))\n    return X @ (self.A.T @ self.W.T)\n</code></pre>"},{"location":"reference/neuro_py/stats/regression/#neuro_py.stats.regression.ReducedRankRegressor.score","title":"<code>score(X, Y)</code>","text":"<p>Score the model.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n    \"\"\"Score the model.\"\"\"\n    if np.size(np.shape(X)) == 1:\n        X = np.reshape(X, (-1, 1))\n    if np.size(np.shape(Y)) == 1:\n        Y = np.reshape(Y, (-1, 1))\n\n    y_pred = self.predict(X)\n    return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/regression/#neuro_py.stats.regression.kernelReducedRankRegressor","title":"<code>kernelReducedRankRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Kernel Reduced Rank Ridge Regression.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>The rank constraint (default is 10).</p> <code>10</code> <code>reg</code> <code>float</code> <p>The regularization parameter (default is 1).</p> <code>1</code> <code>P_rr</code> <code>Optional[ndarray]</code> <p>The P matrix for reduced rank (default is None).</p> <code>None</code> <code>Q_fr</code> <code>Optional[ndarray]</code> <p>The Q matrix for fitted values (default is None).</p> <code>None</code> <code>trainX</code> <code>Optional[ndarray]</code> <p>The training features (default is None).</p> <code>None</code> References <p>Mukherjee, S. (DOI:10.1002/sam.10138) Code by Michele Svanera (2017-June).</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>class kernelReducedRankRegressor(BaseEstimator):\n    \"\"\"\n    Kernel Reduced Rank Ridge Regression.\n\n    Parameters\n    ----------\n    rank : int, optional\n        The rank constraint (default is 10).\n    reg : float, optional\n        The regularization parameter (default is 1).\n    P_rr : Optional[np.ndarray], optional\n        The P matrix for reduced rank (default is None).\n    Q_fr : Optional[np.ndarray], optional\n        The Q matrix for fitted values (default is None).\n    trainX : Optional[np.ndarray], optional\n        The training features (default is None).\n\n    References\n    ----------\n    Mukherjee, S. (DOI:10.1002/sam.10138)\n    Code by Michele Svanera (2017-June).\n    \"\"\"\n\n    def __init__(\n        self,\n        rank: int = 10,\n        reg: float = 1,\n        P_rr: Optional[np.ndarray] = None,\n        Q_fr: Optional[np.ndarray] = None,\n        trainX: Optional[np.ndarray] = None,\n    ):\n        self.rank = rank\n        self.reg = reg\n        self.P_rr = P_rr\n        self.Q_fr = Q_fr\n        self.trainX = trainX\n\n    def __str__(self) -&gt; str:\n        return \"kernel Reduced Rank Ridge Regression by Mukherjee (rank = {})\".format(\n            self.rank\n        )\n\n    def fit(self, X: np.ndarray, Y: np.ndarray) -&gt; None:\n        # use try/except blog with exceptions!\n        self.rank = int(self.rank)\n\n        K_X = scipy.dot(X, X.T)\n        tmp_1 = self.reg * scipy.identity(K_X.shape[0]) + K_X\n        Q_fr = np.linalg.solve(tmp_1, Y)\n        P_fr = scipy.linalg.eig(scipy.dot(Y.T, scipy.dot(K_X, Q_fr)))[1].real\n        P_rr = scipy.dot(P_fr[:, 0 : self.rank], P_fr[:, 0 : self.rank].T)\n\n        self.Q_fr = Q_fr\n        self.P_rr = P_rr\n        self.trainX = X\n\n    def predict(self, testX: np.ndarray) -&gt; np.ndarray:\n        # use try/except blog with exceptions!\n\n        K_Xx = scipy.dot(testX, self.trainX.T)\n        Yhat = scipy.dot(K_Xx, scipy.dot(self.Q_fr, self.P_rr))\n\n        return Yhat\n\n    def rrr_scorer(self, Yhat: np.ndarray, Ytest: np.ndarray) -&gt; float:\n        diag_corr = (np.diag(np.corrcoef(Ytest, Yhat))).mean()\n        return diag_corr\n\n    ## Optional\n    def get_params(self, deep: bool = True) -&gt; dict:\n        return {\"rank\": self.rank, \"reg\": self.reg}\n\n    #\n    #    def set_params(self, **parameters):\n    #        for parameter, value in parameters.items():\n    #            self.setattr(parameter, value)\n    #        return self\n\n    def mse(self, X: np.ndarray, y_true: np.ndarray) -&gt; float:\n        \"\"\"\n        Score the model on test data.\n\n        Parameters\n        ----------\n        X : np.ndarray\n            The test data features.\n        y_true : np.ndarray\n            The true target values.\n\n        Returns\n        -------\n        float\n            The mean squared error of the predictions.\n        \"\"\"\n        Yhat = self.predict(X).real\n        MSE = (np.power((y_true - Yhat), 2) / np.prod(y_true.shape)).mean()\n        return MSE\n\n    def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n        \"\"\"Score the model.\"\"\"\n\n        y_pred = self.predict(X)\n        return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/regression/#neuro_py.stats.regression.kernelReducedRankRegressor.mse","title":"<code>mse(X, y_true)</code>","text":"<p>Score the model on test data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The test data features.</p> required <code>y_true</code> <code>ndarray</code> <p>The true target values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The mean squared error of the predictions.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def mse(self, X: np.ndarray, y_true: np.ndarray) -&gt; float:\n    \"\"\"\n    Score the model on test data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The test data features.\n    y_true : np.ndarray\n        The true target values.\n\n    Returns\n    -------\n    float\n        The mean squared error of the predictions.\n    \"\"\"\n    Yhat = self.predict(X).real\n    MSE = (np.power((y_true - Yhat), 2) / np.prod(y_true.shape)).mean()\n    return MSE\n</code></pre>"},{"location":"reference/neuro_py/stats/regression/#neuro_py.stats.regression.kernelReducedRankRegressor.score","title":"<code>score(X, Y)</code>","text":"<p>Score the model.</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def score(self, X: np.ndarray, Y: np.ndarray) -&gt; float:\n    \"\"\"Score the model.\"\"\"\n\n    y_pred = self.predict(X)\n    return r2_score(Y, y_pred)\n</code></pre>"},{"location":"reference/neuro_py/stats/regression/#neuro_py.stats.regression.ideal_data","title":"<code>ideal_data(num, dimX, dimY, rrank, noise=1)</code>","text":"<p>Generate low-rank data.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>int</code> <p>Number of samples.</p> required <code>dimX</code> <code>int</code> <p>Dimensionality of the input data.</p> required <code>dimY</code> <code>int</code> <p>Dimensionality of the output data.</p> required <code>rrank</code> <code>int</code> <p>Rank of the low-rank structure.</p> required <code>noise</code> <code>float</code> <p>Standard deviation of the noise added to the output data (default is 1).</p> <code>1</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple containing: - X : np.ndarray     The generated input data of shape (num, dimX). - Y : np.ndarray     The generated output data of shape (num, dimY).</p> Source code in <code>neuro_py/stats/regression.py</code> <pre><code>def ideal_data(\n    num: int, dimX: int, dimY: int, rrank: int, noise: float = 1\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generate low-rank data.\n\n    Parameters\n    ----------\n    num : int\n        Number of samples.\n    dimX : int\n        Dimensionality of the input data.\n    dimY : int\n        Dimensionality of the output data.\n    rrank : int\n        Rank of the low-rank structure.\n    noise : float, optional\n        Standard deviation of the noise added to the output data (default is 1).\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray]\n        A tuple containing:\n        - X : np.ndarray\n            The generated input data of shape (num, dimX).\n        - Y : np.ndarray\n            The generated output data of shape (num, dimY).\n\n    \"\"\"\n    X = np.random.randn(num, dimX)\n    W = np.random.randn(dimX, rrank) @ np.random.randn(rrank, dimY)\n    Y = X @ W + np.random.randn(num, dimY) * noise\n    return X, Y\n</code></pre>"},{"location":"reference/neuro_py/stats/stats/","title":"neuro_py.stats.stats","text":""},{"location":"reference/neuro_py/stats/stats/#neuro_py.stats.stats.confidence_intervals","title":"<code>confidence_intervals(X, conf=0.95)</code>","text":"<p>Calculate upper and lower confidence intervals on a matrix.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A numpy ndarray of shape (n_signals, n_samples).</p> required <code>conf</code> <code>float</code> <p>Confidence level value (default is 0.95).</p> <code>0.95</code> <p>Returns:</p> Name Type Description <code>lower</code> <code>ndarray</code> <p>Lower bounds of the confidence intervals (shape: (n_signals,)).</p> <code>upper</code> <code>ndarray</code> <p>Upper bounds of the confidence intervals (shape: (n_signals,)).</p> Source code in <code>neuro_py/stats/stats.py</code> <pre><code>def confidence_intervals(\n    X: np.ndarray, conf: float = 0.95\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate upper and lower confidence intervals on a matrix.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        A numpy ndarray of shape (n_signals, n_samples).\n    conf : float, optional\n        Confidence level value (default is 0.95).\n\n    Returns\n    -------\n    lower : np.ndarray\n        Lower bounds of the confidence intervals (shape: (n_signals,)).\n    upper : np.ndarray\n        Upper bounds of the confidence intervals (shape: (n_signals,)).\n    \"\"\"\n    # compute interval for each column\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n        interval = [\n            stats.t.interval(\n                conf,\n                len(a) - 1,\n                loc=np.nanmean(a),\n                scale=stats.sem(a, nan_policy=\"omit\"),\n            )\n            for a in X.T\n        ]\n    # stack intervals into array\n    interval = np.vstack(interval)\n    # split into lower and upper\n    lower = interval[:, 0]\n    upper = interval[:, 1]\n\n    return lower, upper\n</code></pre>"},{"location":"reference/neuro_py/stats/stats/#neuro_py.stats.stats.get_significant_events","title":"<code>get_significant_events(scores, shuffled_scores, q=95, tail='both')</code>","text":"<p>Return the significant events based on percentiles, the p-values, and the standard deviation of the scores in terms of the shuffled scores.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Union[list, ndarray]</code> <p>The array of scores for which to calculate significant events.</p> required <code>shuffled_scores</code> <code>ndarray</code> <p>The array of scores obtained from randomized data (shape: (n_shuffles, n_events)).</p> required <code>q</code> <code>float</code> <p>Percentile to compute, which must be between 0 and 100 inclusive (default is 95).</p> <code>95</code> <code>tail</code> <code>str</code> <p>Tail for the test, which can be 'left', 'right', or 'both' (default is 'both').</p> <code>'both'</code> <p>Returns:</p> Name Type Description <code>sig_event_idx</code> <code>ndarray</code> <p>Indices (from 0 to n_events-1) of significant events.</p> <code>pvalues</code> <code>ndarray</code> <p>The p-values.</p> <code>stddev</code> <code>ndarray</code> <p>The standard deviation of the scores in terms of the shuffled scores.</p> Source code in <code>neuro_py/stats/stats.py</code> <pre><code>def get_significant_events(\n    scores: Union[list, np.ndarray],\n    shuffled_scores: np.ndarray,\n    q: float = 95,\n    tail: str = \"both\",\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Return the significant events based on percentiles,\n    the p-values, and the standard deviation of the scores\n    in terms of the shuffled scores.\n\n    Parameters\n    ----------\n    scores : Union[list, np.ndarray]\n        The array of scores for which to calculate significant events.\n    shuffled_scores : np.ndarray\n        The array of scores obtained from randomized data (shape: (n_shuffles, n_events)).\n    q : float, optional\n        Percentile to compute, which must be between 0 and 100 inclusive (default is 95).\n    tail : str, optional\n        Tail for the test, which can be 'left', 'right', or 'both' (default is 'both').\n\n    Returns\n    -------\n    sig_event_idx : np.ndarray\n        Indices (from 0 to n_events-1) of significant events.\n    pvalues : np.ndarray\n        The p-values.\n    stddev : np.ndarray\n        The standard deviation of the scores in terms of the shuffled scores.\n    \"\"\"\n    # check shape and correct if needed\n    if isinstance(scores, list) | isinstance(scores, np.ndarray):\n        if shuffled_scores.shape[1] != len(scores):\n            shuffled_scores = shuffled_scores.T\n\n    n = shuffled_scores.shape[0]\n    if tail == \"both\":\n        r = np.sum(np.abs(shuffled_scores) &gt;= np.abs(scores), axis=0)\n    elif tail == \"right\":\n        r = np.sum(shuffled_scores &gt;= scores, axis=0)\n    elif tail == \"left\":\n        r = np.sum(shuffled_scores &lt;= scores, axis=0)\n    else:\n        raise ValueError(\"tail must be 'left', 'right', or 'both'\")\n    pvalues = (r + 1) / (n + 1)\n\n    # set nan scores to 1\n    if isinstance(np.isnan(scores), np.ndarray):\n        pvalues[np.isnan(scores)] = 1\n\n    if tail == \"both\":\n        threshold = np.percentile(np.abs(shuffled_scores), axis=0, q=q)\n        sig_event_idx = np.where(np.abs(scores) &gt; threshold)[0]\n    elif tail == \"right\":\n        threshold = np.percentile(shuffled_scores, axis=0, q=q)\n        sig_event_idx = np.where(scores &gt; threshold)[0]\n    elif tail == \"left\":\n        threshold = np.percentile(shuffled_scores, axis=0, q=100 - q)\n        sig_event_idx = np.where(scores &lt; threshold)[0]\n\n    # calculate how many standard deviations away from shuffle\n    if tail == \"both\":\n        stddev = (np.abs(scores) - np.nanmean(np.abs(shuffled_scores), axis=0)) / np.nanstd(\n            np.abs(shuffled_scores), axis=0\n        )\n    elif tail == \"right\":\n        stddev = (scores - np.nanmean(shuffled_scores, axis=0)) / np.nanstd(\n            shuffled_scores, axis=0\n        )\n    elif tail == \"left\":\n        stddev = (np.nanmean(shuffled_scores, axis=0) - scores) / np.nanstd(\n            shuffled_scores, axis=0\n        )\n\n    return np.atleast_1d(sig_event_idx), np.atleast_1d(pvalues), np.atleast_1d(stddev)\n</code></pre>"},{"location":"reference/neuro_py/stats/stats/#neuro_py.stats.stats.regress_out","title":"<code>regress_out(a, b)</code>","text":"<p>Regress b from a while keeping a's original mean.</p> <p>This function performs regression of variable b from variable a while preserving the original mean of a. It calculates the residual component of a that remains after removing the effect of b using ordinary least squares.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ndarray</code> <p>The variable to be regressed. Must be 1-dimensional.</p> required <code>b</code> <code>ndarray</code> <p>The variable to regress on a. Must be 1-dimensional.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The residual of a after regressing out b. Has the same shape as a.</p> Notes <p>Adapted from the seaborn function of the same name: https://github.com/mwaskom/seaborn/blob/824c102525e6a29cde9bca1ce0096d50588fda6b/seaborn/regression.py#L337</p> Source code in <code>neuro_py/stats/stats.py</code> <pre><code>def regress_out(a: np.ndarray, b: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Regress b from a while keeping a's original mean.\n\n    This function performs regression of variable b from variable a while\n    preserving the original mean of a. It calculates the residual component\n    of a that remains after removing the effect of b using ordinary least squares.\n\n    Parameters\n    ----------\n    a : np.ndarray\n        The variable to be regressed. Must be 1-dimensional.\n    b : np.ndarray\n        The variable to regress on a. Must be 1-dimensional.\n\n    Returns\n    -------\n    np.ndarray\n        The residual of a after regressing out b. Has the same shape as a.\n\n    Notes\n    -----\n    Adapted from the seaborn function of the same name:\n    https://github.com/mwaskom/seaborn/blob/824c102525e6a29cde9bca1ce0096d50588fda6b/seaborn/regression.py#L337\n    \"\"\"\n    # remove nans and infs from a and b, and make a_result vector for output\n    a_result = np.full_like(a, np.nan)\n\n    valid_mask = np.isfinite(a) &amp; np.isfinite(b)\n    a = np.asarray(a)[valid_mask]\n    b = np.asarray(b)[valid_mask]\n\n    # remove mean from a and b\n    a_mean = a.mean()\n    a = a - a_mean\n    b = b - b.mean()\n\n    # calculate regression and subtract from a to get a_prime\n    b = np.c_[b]\n    a_prime = a - b @ np.linalg.pinv(b) @ a\n\n    # add mean back to a_prime\n    a_prime = np.asarray(a_prime + a_mean).reshape(a.shape)\n\n    # put a_prime back into a_result vector to preserve nans\n    a_result[valid_mask] = a_prime\n    return a_result\n</code></pre>"},{"location":"reference/neuro_py/stats/stats/#neuro_py.stats.stats.reindex_df","title":"<code>reindex_df(df, weight_col)</code>","text":"<p>Expand the dataframe by weights.</p> <p>This function expands the dataframe to prepare for resampling, resulting in 1 row per count per sample, which is helpful when making weighted proportion plots.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas dataframe to be expanded.</p> required <code>weight_col</code> <code>str</code> <p>The column name that contains weights (should be int).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new pandas dataframe with resampling based on the weights.</p> Source code in <code>neuro_py/stats/stats.py</code> <pre><code>def reindex_df(df: pd.DataFrame, weight_col: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Expand the dataframe by weights.\n\n    This function expands the dataframe to prepare for resampling,\n    resulting in 1 row per count per sample, which is helpful\n    when making weighted proportion plots.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The pandas dataframe to be expanded.\n    weight_col : str\n        The column name that contains weights (should be int).\n\n    Returns\n    -------\n    pd.DataFrame\n        A new pandas dataframe with resampling based on the weights.\n    \"\"\"\n\n    df = df.reindex(df.index.repeat(df[weight_col])).copy()\n\n    df.reset_index(drop=True, inplace=True)\n\n    return df\n</code></pre>"},{"location":"reference/neuro_py/stats/system_identifier/","title":"neuro_py.stats.system_identifier","text":"<p>Subspace system identification with regularization. Requires scipy to be installed.</p> <p>Implemented by Chris Rayner (2015) dchrisrayner AT gmail DOT com</p> <p>Based on a talk on Subspace System Identification by Tijl De Bie (2005):</p> <p>Assume every output (y_i) is a function of the input (u_i) and the current state x_i of the system, i.e.,    y_i = dot(C, x_i) + dot(D, u_i) Also assume the system state evolves after every input:    x_(i+1) = dot(A, x_i) + dot(B, u_i) This is a linear dynamical system.</p>"},{"location":"reference/neuro_py/stats/system_identifier/#neuro_py.stats.system_identifier.SystemIdentifier","title":"<code>SystemIdentifier</code>","text":"<p>               Bases: <code>object</code></p> <p>Simple Subspace System Identifier.</p> <p>This class identifies a linear dynamical system based on given input and output data using subspace methods.</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>An n-by-d matrix of control inputs.</p> required <code>Y</code> <code>ndarray</code> <p>An n-by-D matrix of output observations.</p> required <code>statedim</code> <code>int</code> <p>The dimension of the internal state variable.</p> required <code>reg</code> <code>float</code> <p>Regularization parameter (default is None, which is set to 0).</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>A</code> <code>ndarray</code> <p>State transition matrix.</p> <code>B</code> <code>ndarray</code> <p>Control input matrix.</p> <code>C</code> <code>ndarray</code> <p>Output matrix.</p> <code>D</code> <code>ndarray</code> <p>Feedforward matrix.</p> Source code in <code>neuro_py/stats/system_identifier.py</code> <pre><code>class SystemIdentifier(object):\n    \"\"\"\n    Simple Subspace System Identifier.\n\n    This class identifies a linear dynamical system based on given input and output data using subspace methods.\n\n    Parameters\n    ----------\n    U : np.ndarray\n        An n-by-d matrix of control inputs.\n    Y : np.ndarray\n        An n-by-D matrix of output observations.\n    statedim : int\n        The dimension of the internal state variable.\n    reg : float, optional\n        Regularization parameter (default is None, which is set to 0).\n\n    Attributes\n    ----------\n    A : np.ndarray\n        State transition matrix.\n    B : np.ndarray\n        Control input matrix.\n    C : np.ndarray\n        Output matrix.\n    D : np.ndarray\n        Feedforward matrix.\n    \"\"\"\n\n    def __init__(self, U: np.ndarray, Y: np.ndarray, statedim: int, reg: Union[float, None] = None):\n        if np.size(np.shape(U)) == 1:\n            U = np.reshape(U, (-1, 1))\n        if np.size(np.shape(Y)) == 1:\n            Y = np.reshape(Y, (-1, 1))\n        if reg is None:\n            reg = 0\n\n        yDim = np.size(Y, 1)\n        uDim = np.size(U, 1)\n\n        self.output_size = np.size(Y, 1)  # placeholder\n\n        # number of samples of past/future we'll mash together into a 'state'\n        width = 1\n        # total number of past/future pairings we get as a result\n        K = np.size(U, 0) - 2 * width + 1\n\n        # build hankel matrices containing pasts and futures\n        U_p = np.array([np.ravel(U[t : t + width]) for t in range(K)]).T\n        U_f = np.array([np.ravel(U[t + width : t + 2 * width]) for t in range(K)]).T\n        Y_p = np.array([np.ravel(Y[t : t + width]) for t in range(K)]).T\n        Y_f = np.array([np.ravel(Y[t + width : t + 2 * width]) for t in range(K)]).T\n\n        # solve the eigenvalue problem\n        YfUfT = np.dot(Y_f, U_f.T)\n        YfUpT = np.dot(Y_f, U_p.T)\n        YfYpT = np.dot(Y_f, Y_p.T)\n        UfUpT = np.dot(U_f, U_p.T)\n        UfYpT = np.dot(U_f, Y_p.T)\n        UpYpT = np.dot(U_p, Y_p.T)\n        F = sparse.bmat(\n            [\n                [None, YfUfT, YfUpT, YfYpT],\n                [YfUfT.T, None, UfUpT, UfYpT],\n                [YfUpT.T, UfUpT.T, None, UpYpT],\n                [YfYpT.T, UfYpT.T, UpYpT.T, None],\n            ]\n        )\n        Ginv = sparse.bmat(\n            [\n                [np.linalg.pinv(np.dot(Y_f, Y_f.T)), None, None, None],\n                [None, np.linalg.pinv(np.dot(U_f, U_f.T)), None, None],\n                [None, None, np.linalg.pinv(np.dot(U_p, U_p.T)), None],\n                [None, None, None, np.linalg.pinv(np.dot(Y_p, Y_p.T))],\n            ]\n        )\n        F = F - sparse.eye(sp.size(F, 0)) * reg\n\n        # Take smallest eigenvalues\n        _, W = sparse_linalg.eigs(Ginv.dot(F), k=statedim, which=\"SR\")\n\n        # State sequence is a weighted combination of the past\n        W_U_p = W[width * (yDim + uDim) : width * (yDim + uDim + uDim), :]\n        W_Y_p = W[width * (yDim + uDim + uDim) :, :]\n        X_hist = np.dot(W_U_p.T, U_p) + np.dot(W_Y_p.T, Y_p)\n\n        # Regress; trim inputs to match the states we retrieved\n        R = np.concatenate((X_hist[:, :-1], U[width:-width].T), 0)\n        L = np.concatenate((X_hist[:, 1:], Y[width:-width].T), 0)\n        RRi = np.linalg.pinv(np.dot(R, R.T))\n        RL = np.dot(R, L.T)\n        Sys = np.dot(RRi, RL).T\n        self.A = Sys[:statedim, :statedim]\n        self.B = Sys[:statedim, statedim:]\n        self.C = Sys[statedim:, :statedim]\n        self.D = Sys[statedim:, statedim:]\n\n    def __str__(self) -&gt; str:\n        return \"Linear Dynamical System\"\n\n    def predict(self, U: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Predict output given the control inputs.\n\n        Parameters\n        ----------\n        U : np.ndarray\n            Control inputs, shape (n_samples, n_controls).\n\n        Returns\n        -------\n        np.ndarray\n            Predicted outputs, shape (n_samples, n_outputs).\n        \"\"\"\n        # If U is a vector, reshape it\n        if np.size(np.shape(U)) == 1:\n            U = np.reshape(U, (-1, 1))\n\n        # assume some random initial state\n        X = np.reshape(np.random.randn(np.size(self.A, 1)), (1, -1))\n\n        # intitial output\n        Y = np.reshape(np.dot(self.C, X[-1]) + np.dot(self.D, U[0]), (1, -1))\n\n        # generate next state\n        X = np.concatenate(\n            (X, np.reshape(np.dot(self.A, X[-1]) + np.dot(self.B, U[0]), (1, -1)))\n        )\n\n        # and so forth\n        for u in U[1:]:\n            Y = np.concatenate(\n                (Y, np.reshape(np.dot(self.C, X[-1]) + np.dot(self.D, u), (1, -1)))\n            )\n            X = np.concatenate(\n                (X, np.reshape(np.dot(self.A, X[-1]) + np.dot(self.B, u), (1, -1)))\n            )\n\n        return Y\n</code></pre>"},{"location":"reference/neuro_py/stats/system_identifier/#neuro_py.stats.system_identifier.SystemIdentifier.predict","title":"<code>predict(U)</code>","text":"<p>Predict output given the control inputs.</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>Control inputs, shape (n_samples, n_controls).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted outputs, shape (n_samples, n_outputs).</p> Source code in <code>neuro_py/stats/system_identifier.py</code> <pre><code>def predict(self, U: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Predict output given the control inputs.\n\n    Parameters\n    ----------\n    U : np.ndarray\n        Control inputs, shape (n_samples, n_controls).\n\n    Returns\n    -------\n    np.ndarray\n        Predicted outputs, shape (n_samples, n_outputs).\n    \"\"\"\n    # If U is a vector, reshape it\n    if np.size(np.shape(U)) == 1:\n        U = np.reshape(U, (-1, 1))\n\n    # assume some random initial state\n    X = np.reshape(np.random.randn(np.size(self.A, 1)), (1, -1))\n\n    # intitial output\n    Y = np.reshape(np.dot(self.C, X[-1]) + np.dot(self.D, U[0]), (1, -1))\n\n    # generate next state\n    X = np.concatenate(\n        (X, np.reshape(np.dot(self.A, X[-1]) + np.dot(self.B, U[0]), (1, -1)))\n    )\n\n    # and so forth\n    for u in U[1:]:\n        Y = np.concatenate(\n            (Y, np.reshape(np.dot(self.C, X[-1]) + np.dot(self.D, u), (1, -1)))\n        )\n        X = np.concatenate(\n            (X, np.reshape(np.dot(self.A, X[-1]) + np.dot(self.B, u), (1, -1)))\n        )\n\n    return Y\n</code></pre>"},{"location":"reference/neuro_py/stats/system_identifier/#neuro_py.stats.system_identifier.ideal_data","title":"<code>ideal_data(num, dimU, dimY, dimX, noise=1.0)</code>","text":"<p>Generate linear system data.</p> <p>This function creates randomized linear system matrices and simulates a linear system with specified dimensions. The resulting output data includes added noise.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>int</code> <p>Number of time points (samples).</p> required <code>dimU</code> <code>int</code> <p>Dimensionality of the input (control inputs).</p> required <code>dimY</code> <code>int</code> <p>Dimensionality of the output.</p> required <code>dimX</code> <code>int</code> <p>Dimensionality of the state.</p> required <code>noise</code> <code>float</code> <p>Standard deviation of the noise added to the output (default: 1.0).</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>U : np.ndarray     Random input data of shape (num, dimU). Y : np.ndarray     Output data of shape (num, dimY) with added noise.</p> Source code in <code>neuro_py/stats/system_identifier.py</code> <pre><code>def ideal_data(num: int, dimU: int, dimY: int, dimX: int, noise: float = 1.0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generate linear system data.\n\n    This function creates randomized linear system matrices and simulates a linear\n    system with specified dimensions. The resulting output data includes added noise.\n\n    Parameters\n    ----------\n    num : int\n        Number of time points (samples).\n    dimU : int\n        Dimensionality of the input (control inputs).\n    dimY : int\n        Dimensionality of the output.\n    dimX : int\n        Dimensionality of the state.\n    noise : float\n        Standard deviation of the noise added to the output (default: 1.0).\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        U : np.ndarray\n            Random input data of shape (num, dimU).\n        Y : np.ndarray\n            Output data of shape (num, dimY) with added noise.\n    \"\"\"\n    # generate randomized linear system matrices\n    A = np.random.randn(dimX, dimX)\n    B = np.random.randn(dimX, dimU)\n    C = np.random.randn(dimY, dimX)\n    D = np.random.randn(dimY, dimU)\n\n    # make sure state evolution is stable\n    U, S, V = np.linalg.svd(A)\n    A = np.dot(U, np.dot(np.lib.diag(S / max(S)), V))\n    U, S, V = np.linalg.svd(B)\n    S2 = np.zeros((np.size(U, 1), np.size(V, 0)))\n    S2[:, : np.size(U, 1)] = np.lib.diag(S / max(S))\n    B = np.dot(U, np.dot(S2, V))\n\n    # random input\n    U = np.random.randn(num, dimU)\n\n    # initial state\n    X = np.reshape(np.random.randn(dimX), (1, -1))\n\n    # initial output\n    Y = np.reshape(np.dot(C, X[-1]) + np.dot(D, U[0]), (1, -1))\n\n    # generate next state\n    X = np.concatenate((X, np.reshape(np.dot(A, X[-1]) + np.dot(B, U[0]), (1, -1))))\n\n    # and so forth\n    for u in U[1:]:\n        Y = np.concatenate((Y, np.reshape(np.dot(C, X[-1]) + np.dot(D, u), (1, -1))))\n        X = np.concatenate((X, np.reshape(np.dot(A, X[-1]) + np.dot(B, u), (1, -1))))\n\n    return U, Y + np.random.randn(num, dimY) * noise\n</code></pre>"},{"location":"reference/neuro_py/tuning/","title":"neuro_py.tuning","text":""},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.SpatialMap","title":"<code>SpatialMap</code>","text":"<p>               Bases: <code>object</code></p> <p>SpatialMap: make a spatial map tuning curve     maps timestamps or continuous signals onto positions</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>object</code> <p>Position data (nelpy.AnalogSignal or nel.PositionArray).</p> required <code>st</code> <code>object</code> <p>Spike train data (nelpy.SpikeTrain or nelpy.AnalogSignal).</p> required <code>speed</code> <code>Optional[object]</code> <p>Speed data (nelpy.AnalogSignal), recommended input: from non-epoched data.</p> <code>None</code> <code>dim</code> <code>Optional[int]</code> <p>Dimension of the map (1 or 2) deprecated.</p> <code>None</code> <code>dir_epoch</code> <code>Optional[object]</code> <p>Epochs of the running direction, for linear data (nelpy.Epoch) deprecated.</p> <code>None</code> <code>speed_thres</code> <code>Union[int, float]</code> <p>Speed threshold for running. Default is 4.</p> <code>4</code> <code>s_binsize</code> <code>Union[int, float]</code> <p>Bin size for the spatial map. Default is 3.</p> <code>3</code> <code>x_minmax</code> <code>Optional[List[Union[int, float]]]</code> <p>Min and max x values for the spatial map.</p> <code>None</code> <code>y_minmax</code> <code>Optional[List[Union[int, float]]]</code> <p>Min and max y values for the spatial map.</p> <code>None</code> <code>tuning_curve_sigma</code> <code>Union[int, float]</code> <p>Sigma for the tuning curve. Default is 3.</p> <code>3</code> <code>smooth_mode</code> <code>str</code> <p>Mode for smoothing curve (str) reflect, constant, nearest, mirror, wrap. Default is \"reflect\".</p> <code>'reflect'</code> <code>min_duration</code> <code>float</code> <p>Minimum duration for a tuning curve. Default is 0.1.</p> <code>0.1</code> <code>minbgrate</code> <code>Union[int, float]</code> <p>Minimum firing rate for tuning curve; will set to this if lower. Default is 0.</p> <code>0</code> <code>n_shuff</code> <code>int</code> <p>Number of position shuffles for spatial information. Default is 500.</p> <code>500</code> <code>parallel_shuff</code> <code>bool</code> <p>Parallelize shuffling. Default is True.</p> <code>True</code> <code>place_field_thres</code> <code>Union[int, float]</code> <p>Percent of continuous region of peak firing rate. Default is 0.2.</p> <code>0.2</code> <code>place_field_min_size</code> <code>Optional[Union[int, float]]</code> <p>Minimum size of place field (cm).</p> <code>None</code> <code>place_field_max_size</code> <code>Optional[Union[int, float]]</code> <p>Maximum size of place field (cm).</p> <code>None</code> <code>place_field_min_peak</code> <code>Union[int, float]</code> <p>Minimum peak rate of place field. Default is 3.</p> <code>3</code> <code>place_field_sigma</code> <code>Union[int, float]</code> <p>Extra smoothing sigma to apply before field detection. Default is 2.</p> <code>2</code> <p>Attributes:</p> Name Type Description <code>tc</code> <code>TuningCurve</code> <p>Tuning curves.</p> <code>st_run</code> <code>SpikeTrain</code> <p>Spike train restricted to running epochs.</p> <code>bst_run</code> <code>binnedSpikeTrain</code> <p>Binned spike train restricted to running epochs.</p> <code>speed</code> <code>Optional[AnalogSignal]</code> <p>Speed data.</p> <code>run_epochs</code> <code>EpochArray</code> <p>Running epochs.</p> Notes <p>Place field detector (.find_fields()) is sensitive to many parameters. For 2D, it is highly recommended to have good environmental sampling. In brief testing with 300cm linear track, optimal 1D parameters were:     place_field_min_size=15     place_field_max_size=None     place_field_min_peak=3     place_field_sigma=None     place_field_thres=.33</p> TODO <p>Place field detector currently collects field width and peak rate for peak place field. In the future, these should be stored for all sub fields.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>class SpatialMap(object):\n    \"\"\"\n    SpatialMap: make a spatial map tuning curve\n        maps timestamps or continuous signals onto positions\n\n    Parameters\n    ----------\n    pos : object\n        Position data (nelpy.AnalogSignal or nel.PositionArray).\n    st : object\n        Spike train data (nelpy.SpikeTrain or nelpy.AnalogSignal).\n    speed : Optional[object]\n        Speed data (nelpy.AnalogSignal), recommended input: from non-epoched data.\n    dim : Optional[int]\n        Dimension of the map (1 or 2) *deprecated*.\n    dir_epoch : Optional[object]\n        Epochs of the running direction, for linear data (nelpy.Epoch) *deprecated*.\n    speed_thres : Union[int, float], optional\n        Speed threshold for running. Default is 4.\n    s_binsize : Union[int, float], optional\n        Bin size for the spatial map. Default is 3.\n    x_minmax : Optional[List[Union[int, float]]], optional\n        Min and max x values for the spatial map.\n    y_minmax : Optional[List[Union[int, float]]], optional\n        Min and max y values for the spatial map.\n    tuning_curve_sigma : Union[int, float], optional\n        Sigma for the tuning curve. Default is 3.\n    smooth_mode : str, optional\n        Mode for smoothing curve (str) reflect, constant, nearest, mirror, wrap. Default is \"reflect\".\n    min_duration : float, optional\n        Minimum duration for a tuning curve. Default is 0.1.\n    minbgrate : Union[int, float], optional\n        Minimum firing rate for tuning curve; will set to this if lower. Default is 0.\n    n_shuff : int, optional\n        Number of position shuffles for spatial information. Default is 500.\n    parallel_shuff : bool, optional\n        Parallelize shuffling. Default is True.\n    place_field_thres : Union[int, float], optional\n        Percent of continuous region of peak firing rate. Default is 0.2.\n    place_field_min_size : Optional[Union[int, float]]\n        Minimum size of place field (cm).\n    place_field_max_size : Optional[Union[int, float]]\n        Maximum size of place field (cm).\n    place_field_min_peak : Union[int, float], optional\n        Minimum peak rate of place field. Default is 3.\n    place_field_sigma : Union[int, float], optional\n        Extra smoothing sigma to apply before field detection. Default is 2.\n\n    Attributes\n    ----------\n    tc : nelpy.TuningCurve\n        Tuning curves.\n    st_run : nelpy.SpikeTrain\n        Spike train restricted to running epochs.\n    bst_run : nelpy.binnedSpikeTrain\n        Binned spike train restricted to running epochs.\n    speed : Optional[nnelpy.AnalogSignal]\n        Speed data.\n    run_epochs : nelpy.EpochArray\n        Running epochs.\n\n    Notes\n    -----\n    Place field detector (.find_fields()) is sensitive to many parameters.\n    For 2D, it is highly recommended to have good environmental sampling.\n    In brief testing with 300cm linear track, optimal 1D parameters were:\n        place_field_min_size=15\n        place_field_max_size=None\n        place_field_min_peak=3\n        place_field_sigma=None\n        place_field_thres=.33\n\n    TODO\n    ----\n    Place field detector currently collects field width and peak rate for peak place field.\n    In the future, these should be stored for all sub fields.\n    \"\"\"\n\n    def __init__(\n        self,\n        pos: object,\n        st: object,\n        speed: Optional[object] = None,\n        dim: Optional[int] = None,  # deprecated\n        dir_epoch: Optional[object] = None,  # deprecated\n        speed_thres: Union[int, float] = 4,\n        s_binsize: Union[int, float] = 3,\n        tuning_curve_sigma: Union[int, float] = 3,\n        x_minmax: Optional[List[Union[int, float]]] = None,\n        y_minmax: Optional[List[Union[int, float]]] = None,\n        smooth_mode: str = \"reflect\",\n        min_duration: float = 0.1,\n        minbgrate: Union[int, float] = 0,\n        n_shuff: int = 500,\n        parallel_shuff: bool = True,\n        place_field_thres: Union[int, float] = 0.2,\n        place_field_min_size: Optional[Union[int, float]] = None,\n        place_field_max_size: Optional[Union[int, float]] = None,\n        place_field_min_peak: Union[int, float] = 3,\n        place_field_sigma: Union[int, float] = 2,\n    ) -&gt; None:\n        # add all the inputs to self\n        self.__dict__.update(locals())\n        del self.__dict__[\"self\"]\n\n        # Verify inputs: make sure pos and st are nelpy objects\n        if not isinstance(\n            pos, (nel.core._analogsignalarray.AnalogSignalArray, nel.core.PositionArray)\n        ):\n            raise TypeError(\"pos must be nelpy.AnalogSignal or nelpy.PositionArray\")\n        if not isinstance(\n            st,\n            (\n                nel.core._eventarray.SpikeTrainArray,\n                nel.core._analogsignalarray.AnalogSignalArray,\n            ),\n        ):\n            raise TypeError(\n                \"st must be nelpy.SpikeTrain or nelpy.BinnedSpikeTrainArray\"\n            )\n\n        # check data is not empty\n        if pos.isempty or st.isempty:\n            raise ValueError(\"pos and st must not be empty\")\n\n        # check if pos all nan\n        if np.all(np.isnan(pos.data)):\n            raise ValueError(\"Position data cannot contain all NaN values\")\n\n        # get speed and running epochs (highly recommended you calculate\n        #   speed before hand on non epoched data)\n        if speed_thres &gt; 0:\n            if self.speed is None:\n                self.speed = nel.utils.ddt_asa(\n                    self.pos, smooth=True, sigma=0.1, norm=True\n                )\n\n            self.run_epochs = nel.utils.get_run_epochs(\n                self.speed, v1=self.speed_thres, v2=self.speed_thres\n            ).merge()\n        else:\n            self.run_epochs = self.pos.support.copy()\n\n        # calculate maps, 1d or 2d\n        self.dim = pos.n_signals\n        if pos.n_signals == 2:\n            self.tc, self.st_run = self.map_2d()\n        elif pos.n_signals == 1:\n            self.tc, self.st_run = self.map_1d()\n        else:\n            raise ValueError(\"pos dims must be 1 or 2\")\n\n        # find place fields. Currently only collects metrics from peak field\n        # self.find_fields()\n\n    def map_1d(self, pos: Optional[object] = None) -&gt; tuple:\n        \"\"\"Maps 1D data for the spatial tuning curve.\n\n        Parameters\n        ----------\n        pos : Optional[object]\n            Position data for shuffling.\n\n        Returns\n        -------\n        tuple\n            A tuple containing the tuning curve and restricted spike train.\n        \"\"\"\n        # dir_epoch is deprecated input\n        if self.dir_epoch is not None:\n            # warn user\n            logging.warning(\n                \"dir_epoch is deprecated and will be removed. Epoch data by direction prior to calling SpatialMap\"\n            )\n            self.st = self.st[self.dir_epoch]\n            self.pos = self.pos[self.dir_epoch]\n\n        # restrict spike trains to those epochs during which the animal was running\n        st_run = self.st[self.run_epochs]\n\n        # log warning if st_run is empty following restriction\n        if st_run.isempty:\n            logging.warning(\n                \"No spike trains during running epochs\"\n            )  # This will log it but not raise a warning\n            warnings.warn(\"No spike trains during running epochs\", UserWarning)\n\n        # take pos as input for case of shuffling\n        if pos is not None:\n            pos_run = pos[self.run_epochs]\n        else:\n            pos_run = self.pos[self.run_epochs]\n\n        if self.x_minmax is None:\n            x_max = np.ceil(np.nanmax(self.pos.data))\n            x_min = np.floor(np.nanmin(self.pos.data))\n        else:\n            x_min, x_max = self.x_minmax\n\n        self.x_edges = np.arange(x_min, x_max + self.s_binsize, self.s_binsize)\n\n        # compute occupancy\n        occupancy = self.compute_occupancy_1d(pos_run)\n\n        # compute ratemap (in Hz)\n        ratemap = self.compute_ratemap_1d(st_run, pos_run, occupancy)\n\n        # enforce minimum background firing rate\n        # background firing rate of xx Hz\n        ratemap[ratemap &lt; self.minbgrate] = self.minbgrate\n\n        # enforce minimum background occupancy\n        for uu in range(st_run.data.shape[0]):\n            ratemap[uu][occupancy &lt; self.min_duration] = 0\n\n        # add to nelpy tuning curve class\n        tc = nel.TuningCurve1D(\n            ratemap=ratemap,\n            extmin=x_min,\n            extmax=x_max,\n        )\n\n        tc._occupancy = occupancy\n\n        if self.tuning_curve_sigma is not None:\n            if self.tuning_curve_sigma &gt; 0:\n                tc.smooth(\n                    sigma=self.tuning_curve_sigma, inplace=True, mode=self.smooth_mode\n                )\n\n        return tc, st_run\n\n    def compute_occupancy_1d(self, pos_run: object) -&gt; np.ndarray:\n        \"\"\"Computes the occupancy for 1D position data.\n\n        Parameters\n        ----------\n        pos_run : object\n            Restricted position data for running.\n\n        Returns\n        -------\n        np.ndarray\n            Occupancy values per bin.\n        \"\"\"\n        occupancy, _ = np.histogram(pos_run.data[0, :], bins=self.x_edges)\n        return occupancy / pos_run.fs\n\n    def compute_ratemap_1d(\n        self, st_run: object, pos_run: object, occupancy: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Computes the ratemap for 1D data.\n\n        Parameters\n        ----------\n        st_run : object\n            Spike train data restricted to running epochs.\n        pos_run : object\n            Position data restricted to running epochs.\n        occupancy : np.ndarray\n            Occupancy values per bin.\n\n        Returns\n        -------\n        np.ndarray\n            Ratemap values for the given spike and position data.\n        \"\"\"\n        # initialize ratemap\n        ratemap = np.zeros((st_run.data.shape[0], occupancy.shape[0]))\n\n        if st_run.isempty:\n            return ratemap\n\n        mask = ~np.isnan(pos_run.data).any(axis=0)\n        x_pos, ts = (\n            pos_run.data[0, mask],\n            pos_run.abscissa_vals[mask],\n        )\n        # if data to map is spike train (point process)\n        if isinstance(st_run, nel.core._eventarray.SpikeTrainArray):\n            for i in range(st_run.data.shape[0]):\n                # get spike counts in each bin\n                (\n                    ratemap[i, : len(self.x_edges)],\n                    _,\n                ) = np.histogram(\n                    np.interp(st_run.data[i], ts, x_pos),\n                    bins=self.x_edges,\n                )\n\n        # if data to map is analog signal (continuous)\n        elif isinstance(st_run, nel.core._analogsignalarray.AnalogSignalArray):\n            # get x location for every bin center\n            x = np.interp(st_run.abscissa_vals, ts, x_pos)\n            # get indices location within bin edges\n            ext_bin_idx = np.squeeze(np.digitize(x, self.x_edges, right=True))\n            # iterate over each time step and add data values to ratemap\n            for tt, bidx in enumerate(ext_bin_idx):\n                ratemap[:, bidx - 1] += st_run.data[:, tt]\n            # divide by sampling rate\n            ratemap = ratemap * st_run.fs\n\n        # divide by occupancy\n        np.divide(ratemap, occupancy, where=occupancy != 0, out=ratemap)\n\n        # remove nans and infs\n        bad_idx = np.isnan(ratemap) | np.isinf(ratemap)\n        ratemap[bad_idx] = 0\n\n        return ratemap\n\n    def map_2d(self, pos: Optional[object] = None) -&gt; tuple:\n        \"\"\"Maps 2D data for the spatial tuning curve.\n\n        Parameters\n        ----------\n        pos : Optional[object]\n            Position data for shuffling.\n\n        Returns\n        -------\n        tuple\n            A tuple containing the tuning curve and restricted spike train.\n        \"\"\"\n        # restrict spike trains to those epochs during which the animal was running\n        st_run = self.st[self.run_epochs]\n\n        # log warning if st_run is empty following restriction\n        if st_run.isempty:\n            logging.warning(\n                \"No spike trains during running epochs\"\n            )  # This will log it but not raise a warning\n            warnings.warn(\"No spike trains during running epochs\", UserWarning)\n\n        # take pos as input for case of shuffling\n        if pos is not None:\n            pos_run = pos[self.run_epochs]\n        else:\n            pos_run = self.pos[self.run_epochs]\n\n        # get xy max min\n        if self.x_minmax is None:\n            ext_xmin, ext_xmax = (\n                np.floor(np.nanmin(self.pos.data[0, :])),\n                np.ceil(np.nanmax(self.pos.data[0, :])),\n            )\n        else:\n            ext_xmin, ext_xmax = self.x_minmax\n\n        if self.y_minmax is None:\n            ext_ymin, ext_ymax = (\n                np.floor(np.nanmin(self.pos.data[1, :])),\n                np.ceil(np.nanmax(self.pos.data[1, :])),\n            )\n        else:\n            ext_ymin, ext_ymax = self.y_minmax\n\n        # create bin edges\n        self.x_edges = np.arange(ext_xmin, ext_xmax + self.s_binsize, self.s_binsize)\n        self.y_edges = np.arange(ext_ymin, ext_ymax + self.s_binsize, self.s_binsize)\n\n        # number of bins in each dimension\n        ext_nx, ext_ny = len(self.x_edges), len(self.y_edges)\n\n        # compute occupancy\n        occupancy = self.compute_occupancy_2d(pos_run)\n\n        # compute ratemap (in Hz)\n        ratemap = self.compute_ratemap_2d(st_run, pos_run, occupancy)\n\n        # enforce minimum background occupancy\n        for uu in range(st_run.data.shape[0]):\n            ratemap[uu][occupancy &lt; self.min_duration] = 0\n\n        # enforce minimum background firing rate\n        # background firing rate of xx Hz\n        ratemap[ratemap &lt; self.minbgrate] = self.minbgrate\n\n        tc = nel.TuningCurve2D(\n            ratemap=ratemap,\n            ext_xmin=ext_xmin,\n            ext_ymin=ext_ymin,\n            ext_xmax=ext_xmax,\n            ext_ymax=ext_ymax,\n            ext_ny=ext_ny,\n            ext_nx=ext_nx,\n        )\n        tc._occupancy = occupancy\n\n        if self.tuning_curve_sigma is not None:\n            if self.tuning_curve_sigma &gt; 0:\n                tc.smooth(\n                    sigma=self.tuning_curve_sigma, inplace=True, mode=self.smooth_mode\n                )\n\n        return tc, st_run\n\n    def compute_occupancy_2d(self, pos_run: object) -&gt; np.ndarray:\n        \"\"\"Computes the occupancy for 2D position data.\n\n        Parameters\n        ----------\n        pos_run : object\n            Restricted position data for running.\n\n        Returns\n        -------\n        np.ndarray\n            Occupancy values per bin.\n        \"\"\"\n        occupancy, _, _ = np.histogram2d(\n            pos_run.data[0, :], pos_run.data[1, :], bins=(self.x_edges, self.y_edges)\n        )\n        return occupancy / pos_run.fs\n\n    def compute_ratemap_2d(\n        self, st_run: object, pos_run: object, occupancy: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Computes the ratemap for 2D data.\n\n        Parameters\n        ----------\n        st_run : object\n            Spike train data restricted to running epochs.\n        pos_run : object\n            Position data restricted to running epochs.\n        occupancy : np.ndarray\n            Occupancy values per bin.\n\n        Returns\n        -------\n        np.ndarray\n            Ratemap values for the given spike and position data.\n        \"\"\"\n        ratemap = np.zeros(\n            (st_run.data.shape[0], occupancy.shape[0], occupancy.shape[1])\n        )\n        if st_run.isempty:\n            return ratemap\n\n        # remove nans from position data for interpolation\n        mask = ~np.isnan(pos_run.data).any(axis=0)\n        x_pos, y_pos, ts = (\n            pos_run.data[0, mask],\n            pos_run.data[1, mask],\n            pos_run.abscissa_vals[mask],\n        )\n\n        if isinstance(st_run, nel.core._eventarray.SpikeTrainArray):\n            for i in range(st_run.data.shape[0]):\n                ratemap[i, : len(self.x_edges), : len(self.y_edges)], _, _ = (\n                    np.histogram2d(\n                        np.interp(st_run.data[i], ts, x_pos),\n                        np.interp(st_run.data[i], ts, y_pos),\n                        bins=(self.x_edges, self.y_edges),\n                    )\n                )\n\n        elif isinstance(st_run, nel.core._analogsignalarray.AnalogSignalArray):\n            x = np.interp(st_run.abscissa_vals, ts, x_pos)\n            y = np.interp(st_run.abscissa_vals, ts, y_pos)\n            ext_bin_idx_x = np.squeeze(np.digitize(x, self.x_edges, right=True))\n            ext_bin_idx_y = np.squeeze(np.digitize(y, self.y_edges, right=True))\n            for tt, (bidxx, bidxy) in enumerate(zip(ext_bin_idx_x, ext_bin_idx_y)):\n                ratemap[:, bidxx - 1, bidxy - 1] += st_run.data[:, tt]\n            ratemap = ratemap * st_run.fs\n\n        np.divide(ratemap, occupancy, where=occupancy != 0, out=ratemap)\n\n        bad_idx = np.isnan(ratemap) | np.isinf(ratemap)\n        ratemap[bad_idx] = 0\n\n        return ratemap\n\n    def shuffle_spatial_information(self) -&gt; np.ndarray:\n        \"\"\"Shuffle spatial information and compute p-values for observed vs. null.\n\n        This method creates shuffled coordinates of the position data and computes\n        spatial information for each shuffle. The p-values for the observed\n        spatial information against the null distribution are calculated.\n\n        Returns\n        -------\n        np.ndarray\n            P-values for the spatial information.\n        \"\"\"\n\n        def create_shuffled_coordinates(\n            X: np.ndarray, n_shuff: int = 500\n        ) -&gt; List[np.ndarray]:\n            \"\"\"Create shuffled coordinates by rolling the original coordinates.\n\n            Parameters\n            ----------\n            X : np.ndarray\n                Original position data.\n            n_shuff : int, optional\n                Number of shuffles to create (default is 500).\n\n            Returns\n            -------\n            List[np.ndarray]\n                List of shuffled coordinates.\n            \"\"\"\n            range_ = X.shape[1]\n\n            # if fewer coordinates then shuffles, reduce number of shuffles to n coords\n            n_shuff = np.min([range_, n_shuff])\n\n            surrogate = np.random.choice(\n                np.arange(-range_, range_), size=n_shuff, replace=False\n            )\n            x_temp = []\n            for n in surrogate:\n                x_temp.append(np.roll(X, n, axis=1))\n\n            return x_temp\n\n        def get_spatial_infos(pos_shuff: np.ndarray, ts: np.ndarray, dim: int) -&gt; float:\n            \"\"\"Get spatial information for shuffled position data.\n\n            Parameters\n            ----------\n            pos_shuff : np.ndarray\n                Shuffled position data.\n            ts : np.ndarray\n                Timestamps corresponding to the shuffled data.\n            dim : int\n                Dimension of the spatial data (1 or 2).\n\n            Returns\n            -------\n            float\n                Spatial information calculated from the tuning curve.\n            \"\"\"\n            pos_shuff = nel.AnalogSignalArray(\n                data=pos_shuff,\n                timestamps=ts,\n            )\n            if dim == 1:\n                tc, _ = self.map_1d(pos_shuff)\n                return tc.spatial_information()\n            elif dim == 2:\n                tc, _ = self.map_2d(pos_shuff)\n                return tc.spatial_information()\n\n        pos_data_shuff = create_shuffled_coordinates(\n            self.pos.data, n_shuff=self.n_shuff\n        )\n\n        # construct tuning curves for each position shuffle\n        if self.parallel_shuff:\n            num_cores = multiprocessing.cpu_count()\n            shuffle_spatial_info = Parallel(n_jobs=num_cores)(\n                delayed(get_spatial_infos)(\n                    pos_data_shuff[i], self.pos.abscissa_vals, self.dim\n                )\n                for i in range(self.n_shuff)\n            )\n        else:\n            shuffle_spatial_info = [\n                get_spatial_infos(pos_data_shuff[i], self.pos.abscissa_vals, self.dim)\n                for i in range(self.n_shuff)\n            ]\n\n        # calculate p values for the obs vs null\n        _, self.spatial_information_pvalues, self.spatial_information_zscore = (\n            get_significant_events(\n                self.tc.spatial_information(), np.array(shuffle_spatial_info)\n            )\n        )\n\n        return self.spatial_information_pvalues\n\n    def find_fields(self) -&gt; None:\n        \"\"\"Find place fields in the spatial maps.\n\n        This method detects place fields from the spatial maps and calculates\n        their properties, including width, peak firing rate, and a mask for\n        each detected field.\n        \"\"\"\n        from skimage import measure\n\n        field_width = []\n        peak_rate = []\n        mask = []\n\n        if self.place_field_max_size is None and self.dim == 1:\n            self.place_field_max_size = self.tc.n_bins * self.s_binsize\n        elif self.place_field_max_size is None and self.dim == 2:\n            self.place_field_max_size = self.tc.n_bins * self.s_binsize\n\n        if self.dim == 1:\n            for ratemap_ in self.tc.ratemap:\n                map_fields = fields.map_stats2(\n                    ratemap_,\n                    threshold=self.place_field_thres,\n                    min_size=self.place_field_min_size / self.s_binsize,\n                    max_size=self.place_field_max_size / self.s_binsize,\n                    min_peak=self.place_field_min_peak,\n                    sigma=self.place_field_sigma,\n                )\n                if len(map_fields[\"sizes\"]) == 0:\n                    field_width.append(np.nan)\n                    peak_rate.append(np.nan)\n                    mask.append(map_fields[\"fields\"])\n                else:\n                    field_width.append(\n                        np.array(map_fields[\"sizes\"]).max()\n                        * len(ratemap_)\n                        * self.s_binsize\n                    )\n                    peak_rate.append(np.array(map_fields[\"peaks\"]).max())\n                    mask.append(map_fields[\"fields\"])\n\n        if self.dim == 2:\n            for ratemap_ in self.tc.ratemap:\n                peaks = fields.compute_2d_place_fields(\n                    ratemap_,\n                    min_firing_rate=self.place_field_min_peak,\n                    thresh=self.place_field_thres,\n                    min_size=(self.place_field_min_size / self.s_binsize),\n                    max_size=(self.place_field_max_size / self.s_binsize),\n                    sigma=self.place_field_sigma,\n                )\n                # field coords of fields using contours\n                bc = measure.find_contours(\n                    peaks, 0, fully_connected=\"low\", positive_orientation=\"low\"\n                )\n                if len(bc) == 0:\n                    field_width.append(np.nan)\n                    peak_rate.append(np.nan)\n                    mask.append(peaks)\n                elif np.vstack(bc).shape[0] &lt; 3:\n                    field_width.append(np.nan)\n                    peak_rate.append(np.nan)\n                    mask.append(peaks)\n                else:\n                    field_width.append(\n                        np.max(pdist(bc[0], \"euclidean\")) * self.s_binsize\n                    )\n                    # field_ids = np.unique(peaks)\n                    peak_rate.append(ratemap_[peaks == 1].max())\n                    mask.append(peaks)\n\n        self.tc.field_width = np.array(field_width)\n        self.tc.field_peak_rate = np.array(peak_rate)\n        self.tc.field_mask = np.array(mask)\n        self.tc.n_fields = np.array(\n            [len(np.unique(mask_)) - 1 for mask_ in self.tc.field_mask]\n        )\n\n    def save_mat_file(self, basepath: str, UID: Optional[Any] = None) -&gt; None:\n        \"\"\"Save firing rate map data to a .mat file in MATLAB format.\n\n        The saved file will contain the following variables:\n        - map: a 1xN cell array containing the ratemaps, where N is the number of ratemaps.\n        - field: a 1xN cell array containing the field masks, if they exist.\n        - n_fields: the number of fields detected.\n        - size: the width of the detected fields.\n        - peak: the peak firing rate of the detected fields.\n        - occupancy: the occupancy map.\n        - spatial_information: the spatial information of the ratemaps.\n        - spatial_sparsity: the spatial sparsity of the ratemaps.\n        - x_bins: the bin edges for the x-axis of the ratemaps.\n        - y_bins: the bin edges for the y-axis of the ratemaps.\n        - run_epochs: the time points at which the animal was running.\n        - speed: the speed data.\n        - timestamps: the timestamps for the speed data.\n        - pos: the position data.\n\n        The file will be saved to a .mat file with the name `basepath.ratemap.firingRateMap.mat`,\n        where `basepath` is the base path of the data.\n\n        Parameters\n        ----------\n        basepath : str\n            The base path for saving the .mat file.\n        UID : Optional[Any], optional\n            A unique identifier for the data (default is None).\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if self.dim == 1:\n            raise ValueError(\"1d storeage not implemented\")\n\n        # set up dict\n        firingRateMap = {}\n\n        # store UID if exist\n        if UID is not None:\n            firingRateMap[\"UID\"] = UID.tolist()\n\n        # set up empty fields for conversion to matlab cell array\n        firingRateMap[\"map\"] = np.empty(self.tc.ratemap.shape[0], dtype=object)\n        firingRateMap[\"field\"] = np.empty(self.tc.ratemap.shape[0], dtype=object)\n\n        # Iterate over the ratemaps and store each one in a cell of the cell array\n        for i, ratemap in enumerate(self.tc.ratemap):\n            firingRateMap[\"map\"][i] = ratemap\n\n        # store occupancy\n        firingRateMap[\"occupancy\"] = self.tc.occupancy\n\n        # store bin edges\n        firingRateMap[\"x_bins\"] = self.tc.xbins.tolist()\n        firingRateMap[\"y_bins\"] = self.tc.ybins.tolist()\n\n        # store field mask if exist\n        if hasattr(self.tc, \"field_mask\"):\n            for i, field_mask in enumerate(self.tc.field_mask):\n                firingRateMap[\"field\"][i] = field_mask\n\n            # store field finding info\n            firingRateMap[\"n_fields\"] = self.tc.n_fields.tolist()\n            firingRateMap[\"size\"] = self.tc.field_width.tolist()\n            firingRateMap[\"peak\"] = self.tc.field_peak_rate.tolist()\n\n        # store spatial metrics\n        firingRateMap[\"spatial_information\"] = self.tc.spatial_information().tolist()\n        if hasattr(self, \"spatial_information_pvalues\"):\n            firingRateMap[\"spatial_information_pvalues\"] = (\n                self.spatial_information_pvalues.tolist()\n            )\n        firingRateMap[\"spatial_sparsity\"] = self.tc.spatial_sparsity().tolist()\n\n        # store position speed and timestamps\n        firingRateMap[\"timestamps\"] = self.speed.abscissa_vals.tolist()\n        firingRateMap[\"pos\"] = self.pos.data\n        firingRateMap[\"speed\"] = self.speed.data.tolist()\n        firingRateMap[\"run_epochs\"] = self.run_epochs.time.tolist()\n\n        # store epoch interval\n        firingRateMap[\"epoch_interval\"] = [\n            self.pos.support.start,\n            self.pos.support.stop,\n        ]\n\n        # save matlab file\n        savemat(\n            os.path.join(\n                basepath, os.path.basename(basepath) + \".ratemap.firingRateMap.mat\"\n            ),\n            {\"firingRateMap\": firingRateMap},\n        )\n\n    def _unit_subset(self, unit_list):\n        newtuningcurve = copy.copy(self)\n        newtuningcurve.st = newtuningcurve.st._unit_subset(unit_list)\n        newtuningcurve.st_run = newtuningcurve.st_run._unit_subset(unit_list)\n        newtuningcurve.tc = self.tc._unit_subset(unit_list)\n        return newtuningcurve\n\n    @property\n    def is2d(self):\n        return self.tc.is2d\n\n    @property\n    def occupancy(self):\n        return self.tc._occupancy\n\n    @property\n    def n_units(self):\n        return self.tc.n_units\n\n    @property\n    def shape(self):\n        return self.tc.shape\n\n    def __repr__(self):\n        return self.tc.__repr__()\n\n    @property\n    def isempty(self):\n        return self.tc.isempty\n\n    @property\n    def ratemap(self):\n        return self.tc.ratemap\n\n    def __len__(self):\n        return self.tc.__len__()\n\n    def smooth(self, **kwargs):\n        return self.tc.smooth(**kwargs)\n\n    @property\n    def mean(self):\n        return self.tc.mean\n\n    @property\n    def std(self):\n        return self.tc.std\n\n    @property\n    def max(self):\n        return self.tc.max\n\n    @property\n    def min(self):\n        return self.tc.min\n\n    @property\n    def mask(self):\n        return self.tc.mask\n\n    @property\n    def n_bins(self):\n        return self.tc.n_bins\n\n    @property\n    def n_xbins(self):\n        return self.tc.n_xbins\n\n    @property\n    def n_ybins(self):\n        return self.tc.n_ybins\n\n    @property\n    def xbins(self):\n        return self.tc.xbins\n\n    @property\n    def ybins(self):\n        return self.tc.ybins\n\n    @property\n    def xbin_centers(self):\n        return self.tc.xbin_centers\n\n    @property\n    def ybin_centers(self):\n        return self.tc.ybin_centers\n\n    @property\n    def bin_centers(self):\n        return self.tc.bin_centers\n\n    @property\n    def bins(self):\n        return self.tc.bins\n\n    def normalize(self, **kwargs):\n        return self.tc.normalize(**kwargs)\n\n    @property\n    def spatial_sparsity(self):\n        return self.tc.spatial_sparsity\n\n    @property\n    def spatial_information(self):\n        return self.tc.spatial_information\n\n    @property\n    def information_rate(self):\n        return self.tc.information_rate\n\n    @property\n    def spatial_selectivity(self):\n        return self.tc.spatial_selectivity\n\n    def __sub__(self, other):\n        return self.tc.__sub__(other)\n\n    def __mul__(self, other):\n        return self.tc.__mul__(other)\n\n    def __rmul__(self, other):\n        return self.tc.__rmul__(other)\n\n    def __truediv__(self, other):\n        return self.tc.__truediv__(other)\n\n    def __iter__(self):\n        return self.tc.__iter__()\n\n    def __next__(self):\n        return self.tc.__next__()\n\n    def __getitem__(self, *idx):\n        return self.tc.__getitem__(*idx)\n\n    def _get_peak_firing_order_idx(self):\n        return self.tc._get_peak_firing_order_idx()\n\n    def get_peak_firing_order_ids(self):\n        return self.tc.get_peak_firing_order_ids()\n\n    def _reorder_units_by_idx(self):\n        return self.tc._reorder_units_by_idx()\n\n    def reorder_units_by_ids(self):\n        return self.tc.reorder_units_by_ids()\n\n    def reorder_units(self):\n        return self.tc.reorder_units()\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.SpatialMap.compute_occupancy_1d","title":"<code>compute_occupancy_1d(pos_run)</code>","text":"<p>Computes the occupancy for 1D position data.</p> <p>Parameters:</p> Name Type Description Default <code>pos_run</code> <code>object</code> <p>Restricted position data for running.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Occupancy values per bin.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def compute_occupancy_1d(self, pos_run: object) -&gt; np.ndarray:\n    \"\"\"Computes the occupancy for 1D position data.\n\n    Parameters\n    ----------\n    pos_run : object\n        Restricted position data for running.\n\n    Returns\n    -------\n    np.ndarray\n        Occupancy values per bin.\n    \"\"\"\n    occupancy, _ = np.histogram(pos_run.data[0, :], bins=self.x_edges)\n    return occupancy / pos_run.fs\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.SpatialMap.compute_occupancy_2d","title":"<code>compute_occupancy_2d(pos_run)</code>","text":"<p>Computes the occupancy for 2D position data.</p> <p>Parameters:</p> Name Type Description Default <code>pos_run</code> <code>object</code> <p>Restricted position data for running.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Occupancy values per bin.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def compute_occupancy_2d(self, pos_run: object) -&gt; np.ndarray:\n    \"\"\"Computes the occupancy for 2D position data.\n\n    Parameters\n    ----------\n    pos_run : object\n        Restricted position data for running.\n\n    Returns\n    -------\n    np.ndarray\n        Occupancy values per bin.\n    \"\"\"\n    occupancy, _, _ = np.histogram2d(\n        pos_run.data[0, :], pos_run.data[1, :], bins=(self.x_edges, self.y_edges)\n    )\n    return occupancy / pos_run.fs\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.SpatialMap.compute_ratemap_1d","title":"<code>compute_ratemap_1d(st_run, pos_run, occupancy)</code>","text":"<p>Computes the ratemap for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>st_run</code> <code>object</code> <p>Spike train data restricted to running epochs.</p> required <code>pos_run</code> <code>object</code> <p>Position data restricted to running epochs.</p> required <code>occupancy</code> <code>ndarray</code> <p>Occupancy values per bin.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Ratemap values for the given spike and position data.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def compute_ratemap_1d(\n    self, st_run: object, pos_run: object, occupancy: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"Computes the ratemap for 1D data.\n\n    Parameters\n    ----------\n    st_run : object\n        Spike train data restricted to running epochs.\n    pos_run : object\n        Position data restricted to running epochs.\n    occupancy : np.ndarray\n        Occupancy values per bin.\n\n    Returns\n    -------\n    np.ndarray\n        Ratemap values for the given spike and position data.\n    \"\"\"\n    # initialize ratemap\n    ratemap = np.zeros((st_run.data.shape[0], occupancy.shape[0]))\n\n    if st_run.isempty:\n        return ratemap\n\n    mask = ~np.isnan(pos_run.data).any(axis=0)\n    x_pos, ts = (\n        pos_run.data[0, mask],\n        pos_run.abscissa_vals[mask],\n    )\n    # if data to map is spike train (point process)\n    if isinstance(st_run, nel.core._eventarray.SpikeTrainArray):\n        for i in range(st_run.data.shape[0]):\n            # get spike counts in each bin\n            (\n                ratemap[i, : len(self.x_edges)],\n                _,\n            ) = np.histogram(\n                np.interp(st_run.data[i], ts, x_pos),\n                bins=self.x_edges,\n            )\n\n    # if data to map is analog signal (continuous)\n    elif isinstance(st_run, nel.core._analogsignalarray.AnalogSignalArray):\n        # get x location for every bin center\n        x = np.interp(st_run.abscissa_vals, ts, x_pos)\n        # get indices location within bin edges\n        ext_bin_idx = np.squeeze(np.digitize(x, self.x_edges, right=True))\n        # iterate over each time step and add data values to ratemap\n        for tt, bidx in enumerate(ext_bin_idx):\n            ratemap[:, bidx - 1] += st_run.data[:, tt]\n        # divide by sampling rate\n        ratemap = ratemap * st_run.fs\n\n    # divide by occupancy\n    np.divide(ratemap, occupancy, where=occupancy != 0, out=ratemap)\n\n    # remove nans and infs\n    bad_idx = np.isnan(ratemap) | np.isinf(ratemap)\n    ratemap[bad_idx] = 0\n\n    return ratemap\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.SpatialMap.compute_ratemap_2d","title":"<code>compute_ratemap_2d(st_run, pos_run, occupancy)</code>","text":"<p>Computes the ratemap for 2D data.</p> <p>Parameters:</p> Name Type Description Default <code>st_run</code> <code>object</code> <p>Spike train data restricted to running epochs.</p> required <code>pos_run</code> <code>object</code> <p>Position data restricted to running epochs.</p> required <code>occupancy</code> <code>ndarray</code> <p>Occupancy values per bin.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Ratemap values for the given spike and position data.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def compute_ratemap_2d(\n    self, st_run: object, pos_run: object, occupancy: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"Computes the ratemap for 2D data.\n\n    Parameters\n    ----------\n    st_run : object\n        Spike train data restricted to running epochs.\n    pos_run : object\n        Position data restricted to running epochs.\n    occupancy : np.ndarray\n        Occupancy values per bin.\n\n    Returns\n    -------\n    np.ndarray\n        Ratemap values for the given spike and position data.\n    \"\"\"\n    ratemap = np.zeros(\n        (st_run.data.shape[0], occupancy.shape[0], occupancy.shape[1])\n    )\n    if st_run.isempty:\n        return ratemap\n\n    # remove nans from position data for interpolation\n    mask = ~np.isnan(pos_run.data).any(axis=0)\n    x_pos, y_pos, ts = (\n        pos_run.data[0, mask],\n        pos_run.data[1, mask],\n        pos_run.abscissa_vals[mask],\n    )\n\n    if isinstance(st_run, nel.core._eventarray.SpikeTrainArray):\n        for i in range(st_run.data.shape[0]):\n            ratemap[i, : len(self.x_edges), : len(self.y_edges)], _, _ = (\n                np.histogram2d(\n                    np.interp(st_run.data[i], ts, x_pos),\n                    np.interp(st_run.data[i], ts, y_pos),\n                    bins=(self.x_edges, self.y_edges),\n                )\n            )\n\n    elif isinstance(st_run, nel.core._analogsignalarray.AnalogSignalArray):\n        x = np.interp(st_run.abscissa_vals, ts, x_pos)\n        y = np.interp(st_run.abscissa_vals, ts, y_pos)\n        ext_bin_idx_x = np.squeeze(np.digitize(x, self.x_edges, right=True))\n        ext_bin_idx_y = np.squeeze(np.digitize(y, self.y_edges, right=True))\n        for tt, (bidxx, bidxy) in enumerate(zip(ext_bin_idx_x, ext_bin_idx_y)):\n            ratemap[:, bidxx - 1, bidxy - 1] += st_run.data[:, tt]\n        ratemap = ratemap * st_run.fs\n\n    np.divide(ratemap, occupancy, where=occupancy != 0, out=ratemap)\n\n    bad_idx = np.isnan(ratemap) | np.isinf(ratemap)\n    ratemap[bad_idx] = 0\n\n    return ratemap\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.SpatialMap.find_fields","title":"<code>find_fields()</code>","text":"<p>Find place fields in the spatial maps.</p> <p>This method detects place fields from the spatial maps and calculates their properties, including width, peak firing rate, and a mask for each detected field.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def find_fields(self) -&gt; None:\n    \"\"\"Find place fields in the spatial maps.\n\n    This method detects place fields from the spatial maps and calculates\n    their properties, including width, peak firing rate, and a mask for\n    each detected field.\n    \"\"\"\n    from skimage import measure\n\n    field_width = []\n    peak_rate = []\n    mask = []\n\n    if self.place_field_max_size is None and self.dim == 1:\n        self.place_field_max_size = self.tc.n_bins * self.s_binsize\n    elif self.place_field_max_size is None and self.dim == 2:\n        self.place_field_max_size = self.tc.n_bins * self.s_binsize\n\n    if self.dim == 1:\n        for ratemap_ in self.tc.ratemap:\n            map_fields = fields.map_stats2(\n                ratemap_,\n                threshold=self.place_field_thres,\n                min_size=self.place_field_min_size / self.s_binsize,\n                max_size=self.place_field_max_size / self.s_binsize,\n                min_peak=self.place_field_min_peak,\n                sigma=self.place_field_sigma,\n            )\n            if len(map_fields[\"sizes\"]) == 0:\n                field_width.append(np.nan)\n                peak_rate.append(np.nan)\n                mask.append(map_fields[\"fields\"])\n            else:\n                field_width.append(\n                    np.array(map_fields[\"sizes\"]).max()\n                    * len(ratemap_)\n                    * self.s_binsize\n                )\n                peak_rate.append(np.array(map_fields[\"peaks\"]).max())\n                mask.append(map_fields[\"fields\"])\n\n    if self.dim == 2:\n        for ratemap_ in self.tc.ratemap:\n            peaks = fields.compute_2d_place_fields(\n                ratemap_,\n                min_firing_rate=self.place_field_min_peak,\n                thresh=self.place_field_thres,\n                min_size=(self.place_field_min_size / self.s_binsize),\n                max_size=(self.place_field_max_size / self.s_binsize),\n                sigma=self.place_field_sigma,\n            )\n            # field coords of fields using contours\n            bc = measure.find_contours(\n                peaks, 0, fully_connected=\"low\", positive_orientation=\"low\"\n            )\n            if len(bc) == 0:\n                field_width.append(np.nan)\n                peak_rate.append(np.nan)\n                mask.append(peaks)\n            elif np.vstack(bc).shape[0] &lt; 3:\n                field_width.append(np.nan)\n                peak_rate.append(np.nan)\n                mask.append(peaks)\n            else:\n                field_width.append(\n                    np.max(pdist(bc[0], \"euclidean\")) * self.s_binsize\n                )\n                # field_ids = np.unique(peaks)\n                peak_rate.append(ratemap_[peaks == 1].max())\n                mask.append(peaks)\n\n    self.tc.field_width = np.array(field_width)\n    self.tc.field_peak_rate = np.array(peak_rate)\n    self.tc.field_mask = np.array(mask)\n    self.tc.n_fields = np.array(\n        [len(np.unique(mask_)) - 1 for mask_ in self.tc.field_mask]\n    )\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.SpatialMap.map_1d","title":"<code>map_1d(pos=None)</code>","text":"<p>Maps 1D data for the spatial tuning curve.</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>Optional[object]</code> <p>Position data for shuffling.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the tuning curve and restricted spike train.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def map_1d(self, pos: Optional[object] = None) -&gt; tuple:\n    \"\"\"Maps 1D data for the spatial tuning curve.\n\n    Parameters\n    ----------\n    pos : Optional[object]\n        Position data for shuffling.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the tuning curve and restricted spike train.\n    \"\"\"\n    # dir_epoch is deprecated input\n    if self.dir_epoch is not None:\n        # warn user\n        logging.warning(\n            \"dir_epoch is deprecated and will be removed. Epoch data by direction prior to calling SpatialMap\"\n        )\n        self.st = self.st[self.dir_epoch]\n        self.pos = self.pos[self.dir_epoch]\n\n    # restrict spike trains to those epochs during which the animal was running\n    st_run = self.st[self.run_epochs]\n\n    # log warning if st_run is empty following restriction\n    if st_run.isempty:\n        logging.warning(\n            \"No spike trains during running epochs\"\n        )  # This will log it but not raise a warning\n        warnings.warn(\"No spike trains during running epochs\", UserWarning)\n\n    # take pos as input for case of shuffling\n    if pos is not None:\n        pos_run = pos[self.run_epochs]\n    else:\n        pos_run = self.pos[self.run_epochs]\n\n    if self.x_minmax is None:\n        x_max = np.ceil(np.nanmax(self.pos.data))\n        x_min = np.floor(np.nanmin(self.pos.data))\n    else:\n        x_min, x_max = self.x_minmax\n\n    self.x_edges = np.arange(x_min, x_max + self.s_binsize, self.s_binsize)\n\n    # compute occupancy\n    occupancy = self.compute_occupancy_1d(pos_run)\n\n    # compute ratemap (in Hz)\n    ratemap = self.compute_ratemap_1d(st_run, pos_run, occupancy)\n\n    # enforce minimum background firing rate\n    # background firing rate of xx Hz\n    ratemap[ratemap &lt; self.minbgrate] = self.minbgrate\n\n    # enforce minimum background occupancy\n    for uu in range(st_run.data.shape[0]):\n        ratemap[uu][occupancy &lt; self.min_duration] = 0\n\n    # add to nelpy tuning curve class\n    tc = nel.TuningCurve1D(\n        ratemap=ratemap,\n        extmin=x_min,\n        extmax=x_max,\n    )\n\n    tc._occupancy = occupancy\n\n    if self.tuning_curve_sigma is not None:\n        if self.tuning_curve_sigma &gt; 0:\n            tc.smooth(\n                sigma=self.tuning_curve_sigma, inplace=True, mode=self.smooth_mode\n            )\n\n    return tc, st_run\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.SpatialMap.map_2d","title":"<code>map_2d(pos=None)</code>","text":"<p>Maps 2D data for the spatial tuning curve.</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>Optional[object]</code> <p>Position data for shuffling.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the tuning curve and restricted spike train.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def map_2d(self, pos: Optional[object] = None) -&gt; tuple:\n    \"\"\"Maps 2D data for the spatial tuning curve.\n\n    Parameters\n    ----------\n    pos : Optional[object]\n        Position data for shuffling.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the tuning curve and restricted spike train.\n    \"\"\"\n    # restrict spike trains to those epochs during which the animal was running\n    st_run = self.st[self.run_epochs]\n\n    # log warning if st_run is empty following restriction\n    if st_run.isempty:\n        logging.warning(\n            \"No spike trains during running epochs\"\n        )  # This will log it but not raise a warning\n        warnings.warn(\"No spike trains during running epochs\", UserWarning)\n\n    # take pos as input for case of shuffling\n    if pos is not None:\n        pos_run = pos[self.run_epochs]\n    else:\n        pos_run = self.pos[self.run_epochs]\n\n    # get xy max min\n    if self.x_minmax is None:\n        ext_xmin, ext_xmax = (\n            np.floor(np.nanmin(self.pos.data[0, :])),\n            np.ceil(np.nanmax(self.pos.data[0, :])),\n        )\n    else:\n        ext_xmin, ext_xmax = self.x_minmax\n\n    if self.y_minmax is None:\n        ext_ymin, ext_ymax = (\n            np.floor(np.nanmin(self.pos.data[1, :])),\n            np.ceil(np.nanmax(self.pos.data[1, :])),\n        )\n    else:\n        ext_ymin, ext_ymax = self.y_minmax\n\n    # create bin edges\n    self.x_edges = np.arange(ext_xmin, ext_xmax + self.s_binsize, self.s_binsize)\n    self.y_edges = np.arange(ext_ymin, ext_ymax + self.s_binsize, self.s_binsize)\n\n    # number of bins in each dimension\n    ext_nx, ext_ny = len(self.x_edges), len(self.y_edges)\n\n    # compute occupancy\n    occupancy = self.compute_occupancy_2d(pos_run)\n\n    # compute ratemap (in Hz)\n    ratemap = self.compute_ratemap_2d(st_run, pos_run, occupancy)\n\n    # enforce minimum background occupancy\n    for uu in range(st_run.data.shape[0]):\n        ratemap[uu][occupancy &lt; self.min_duration] = 0\n\n    # enforce minimum background firing rate\n    # background firing rate of xx Hz\n    ratemap[ratemap &lt; self.minbgrate] = self.minbgrate\n\n    tc = nel.TuningCurve2D(\n        ratemap=ratemap,\n        ext_xmin=ext_xmin,\n        ext_ymin=ext_ymin,\n        ext_xmax=ext_xmax,\n        ext_ymax=ext_ymax,\n        ext_ny=ext_ny,\n        ext_nx=ext_nx,\n    )\n    tc._occupancy = occupancy\n\n    if self.tuning_curve_sigma is not None:\n        if self.tuning_curve_sigma &gt; 0:\n            tc.smooth(\n                sigma=self.tuning_curve_sigma, inplace=True, mode=self.smooth_mode\n            )\n\n    return tc, st_run\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.SpatialMap.save_mat_file","title":"<code>save_mat_file(basepath, UID=None)</code>","text":"<p>Save firing rate map data to a .mat file in MATLAB format.</p> <p>The saved file will contain the following variables: - map: a 1xN cell array containing the ratemaps, where N is the number of ratemaps. - field: a 1xN cell array containing the field masks, if they exist. - n_fields: the number of fields detected. - size: the width of the detected fields. - peak: the peak firing rate of the detected fields. - occupancy: the occupancy map. - spatial_information: the spatial information of the ratemaps. - spatial_sparsity: the spatial sparsity of the ratemaps. - x_bins: the bin edges for the x-axis of the ratemaps. - y_bins: the bin edges for the y-axis of the ratemaps. - run_epochs: the time points at which the animal was running. - speed: the speed data. - timestamps: the timestamps for the speed data. - pos: the position data.</p> <p>The file will be saved to a .mat file with the name <code>basepath.ratemap.firingRateMap.mat</code>, where <code>basepath</code> is the base path of the data.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path for saving the .mat file.</p> required <code>UID</code> <code>Optional[Any]</code> <p>A unique identifier for the data (default is None).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def save_mat_file(self, basepath: str, UID: Optional[Any] = None) -&gt; None:\n    \"\"\"Save firing rate map data to a .mat file in MATLAB format.\n\n    The saved file will contain the following variables:\n    - map: a 1xN cell array containing the ratemaps, where N is the number of ratemaps.\n    - field: a 1xN cell array containing the field masks, if they exist.\n    - n_fields: the number of fields detected.\n    - size: the width of the detected fields.\n    - peak: the peak firing rate of the detected fields.\n    - occupancy: the occupancy map.\n    - spatial_information: the spatial information of the ratemaps.\n    - spatial_sparsity: the spatial sparsity of the ratemaps.\n    - x_bins: the bin edges for the x-axis of the ratemaps.\n    - y_bins: the bin edges for the y-axis of the ratemaps.\n    - run_epochs: the time points at which the animal was running.\n    - speed: the speed data.\n    - timestamps: the timestamps for the speed data.\n    - pos: the position data.\n\n    The file will be saved to a .mat file with the name `basepath.ratemap.firingRateMap.mat`,\n    where `basepath` is the base path of the data.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path for saving the .mat file.\n    UID : Optional[Any], optional\n        A unique identifier for the data (default is None).\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if self.dim == 1:\n        raise ValueError(\"1d storeage not implemented\")\n\n    # set up dict\n    firingRateMap = {}\n\n    # store UID if exist\n    if UID is not None:\n        firingRateMap[\"UID\"] = UID.tolist()\n\n    # set up empty fields for conversion to matlab cell array\n    firingRateMap[\"map\"] = np.empty(self.tc.ratemap.shape[0], dtype=object)\n    firingRateMap[\"field\"] = np.empty(self.tc.ratemap.shape[0], dtype=object)\n\n    # Iterate over the ratemaps and store each one in a cell of the cell array\n    for i, ratemap in enumerate(self.tc.ratemap):\n        firingRateMap[\"map\"][i] = ratemap\n\n    # store occupancy\n    firingRateMap[\"occupancy\"] = self.tc.occupancy\n\n    # store bin edges\n    firingRateMap[\"x_bins\"] = self.tc.xbins.tolist()\n    firingRateMap[\"y_bins\"] = self.tc.ybins.tolist()\n\n    # store field mask if exist\n    if hasattr(self.tc, \"field_mask\"):\n        for i, field_mask in enumerate(self.tc.field_mask):\n            firingRateMap[\"field\"][i] = field_mask\n\n        # store field finding info\n        firingRateMap[\"n_fields\"] = self.tc.n_fields.tolist()\n        firingRateMap[\"size\"] = self.tc.field_width.tolist()\n        firingRateMap[\"peak\"] = self.tc.field_peak_rate.tolist()\n\n    # store spatial metrics\n    firingRateMap[\"spatial_information\"] = self.tc.spatial_information().tolist()\n    if hasattr(self, \"spatial_information_pvalues\"):\n        firingRateMap[\"spatial_information_pvalues\"] = (\n            self.spatial_information_pvalues.tolist()\n        )\n    firingRateMap[\"spatial_sparsity\"] = self.tc.spatial_sparsity().tolist()\n\n    # store position speed and timestamps\n    firingRateMap[\"timestamps\"] = self.speed.abscissa_vals.tolist()\n    firingRateMap[\"pos\"] = self.pos.data\n    firingRateMap[\"speed\"] = self.speed.data.tolist()\n    firingRateMap[\"run_epochs\"] = self.run_epochs.time.tolist()\n\n    # store epoch interval\n    firingRateMap[\"epoch_interval\"] = [\n        self.pos.support.start,\n        self.pos.support.stop,\n    ]\n\n    # save matlab file\n    savemat(\n        os.path.join(\n            basepath, os.path.basename(basepath) + \".ratemap.firingRateMap.mat\"\n        ),\n        {\"firingRateMap\": firingRateMap},\n    )\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.SpatialMap.shuffle_spatial_information","title":"<code>shuffle_spatial_information()</code>","text":"<p>Shuffle spatial information and compute p-values for observed vs. null.</p> <p>This method creates shuffled coordinates of the position data and computes spatial information for each shuffle. The p-values for the observed spatial information against the null distribution are calculated.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>P-values for the spatial information.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def shuffle_spatial_information(self) -&gt; np.ndarray:\n    \"\"\"Shuffle spatial information and compute p-values for observed vs. null.\n\n    This method creates shuffled coordinates of the position data and computes\n    spatial information for each shuffle. The p-values for the observed\n    spatial information against the null distribution are calculated.\n\n    Returns\n    -------\n    np.ndarray\n        P-values for the spatial information.\n    \"\"\"\n\n    def create_shuffled_coordinates(\n        X: np.ndarray, n_shuff: int = 500\n    ) -&gt; List[np.ndarray]:\n        \"\"\"Create shuffled coordinates by rolling the original coordinates.\n\n        Parameters\n        ----------\n        X : np.ndarray\n            Original position data.\n        n_shuff : int, optional\n            Number of shuffles to create (default is 500).\n\n        Returns\n        -------\n        List[np.ndarray]\n            List of shuffled coordinates.\n        \"\"\"\n        range_ = X.shape[1]\n\n        # if fewer coordinates then shuffles, reduce number of shuffles to n coords\n        n_shuff = np.min([range_, n_shuff])\n\n        surrogate = np.random.choice(\n            np.arange(-range_, range_), size=n_shuff, replace=False\n        )\n        x_temp = []\n        for n in surrogate:\n            x_temp.append(np.roll(X, n, axis=1))\n\n        return x_temp\n\n    def get_spatial_infos(pos_shuff: np.ndarray, ts: np.ndarray, dim: int) -&gt; float:\n        \"\"\"Get spatial information for shuffled position data.\n\n        Parameters\n        ----------\n        pos_shuff : np.ndarray\n            Shuffled position data.\n        ts : np.ndarray\n            Timestamps corresponding to the shuffled data.\n        dim : int\n            Dimension of the spatial data (1 or 2).\n\n        Returns\n        -------\n        float\n            Spatial information calculated from the tuning curve.\n        \"\"\"\n        pos_shuff = nel.AnalogSignalArray(\n            data=pos_shuff,\n            timestamps=ts,\n        )\n        if dim == 1:\n            tc, _ = self.map_1d(pos_shuff)\n            return tc.spatial_information()\n        elif dim == 2:\n            tc, _ = self.map_2d(pos_shuff)\n            return tc.spatial_information()\n\n    pos_data_shuff = create_shuffled_coordinates(\n        self.pos.data, n_shuff=self.n_shuff\n    )\n\n    # construct tuning curves for each position shuffle\n    if self.parallel_shuff:\n        num_cores = multiprocessing.cpu_count()\n        shuffle_spatial_info = Parallel(n_jobs=num_cores)(\n            delayed(get_spatial_infos)(\n                pos_data_shuff[i], self.pos.abscissa_vals, self.dim\n            )\n            for i in range(self.n_shuff)\n        )\n    else:\n        shuffle_spatial_info = [\n            get_spatial_infos(pos_data_shuff[i], self.pos.abscissa_vals, self.dim)\n            for i in range(self.n_shuff)\n        ]\n\n    # calculate p values for the obs vs null\n    _, self.spatial_information_pvalues, self.spatial_information_zscore = (\n        get_significant_events(\n            self.tc.spatial_information(), np.array(shuffle_spatial_info)\n        )\n    )\n\n    return self.spatial_information_pvalues\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.calculate_field_centers","title":"<code>calculate_field_centers(rate_map, labels, center_method='maxima')</code>","text":"<p>Finds center of fields at labels.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>2D array representing firing rate in each bin.</p> required <code>labels</code> <code>ndarray</code> <p>Labeled fields.</p> required <code>center_method</code> <code>str</code> <p>Method to calculate the center; either 'maxima' or 'center_of_mass'. Default is 'maxima'.</p> <code>'maxima'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinates of the center for each field.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid center_method is provided.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def calculate_field_centers(\n    rate_map: np.ndarray,\n    labels: np.ndarray,\n    center_method: str = \"maxima\"\n) -&gt; np.ndarray:\n    \"\"\"\n    Finds center of fields at labels.\n\n    Parameters\n    ----------\n    rate_map : np.ndarray\n        2D array representing firing rate in each bin.\n    labels : np.ndarray\n        Labeled fields.\n    center_method : str\n        Method to calculate the center; either 'maxima' or 'center_of_mass'. Default is 'maxima'.\n\n    Returns\n    -------\n    np.ndarray\n        Coordinates of the center for each field.\n\n    Raises\n    ------\n    ValueError\n        If an invalid center_method is provided.\n    \"\"\"\n    indices = np.arange(1, np.max(labels) + 1)\n    if center_method == \"maxima\":\n        bc = ndimage.maximum_position(rate_map, labels=labels, index=indices)\n    elif center_method == \"center_of_mass\":\n        bc = ndimage.center_of_mass(rate_map, labels=labels, index=indices)\n    else:\n        raise ValueError(\"invalid center_method flag '{}'\".format(center_method))\n\n    if not bc:\n        # empty list\n        return bc\n\n    bc = np.array(bc)\n    bc[:, [0, 1]] = bc[:, [1, 0]]  # y, x -&gt; x, y\n    return bc\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.compute_2d_place_fields","title":"<code>compute_2d_place_fields(firing_rate, min_firing_rate=1, thresh=0.2, min_size=100, max_size=200, sigma=None)</code>","text":"<p>Compute place fields from the firing rate.</p> <p>Parameters:</p> Name Type Description Default <code>firing_rate</code> <code>ndarray</code> <p>2D array of firing rates (NxN).</p> required <code>min_firing_rate</code> <code>float</code> <p>Minimum firing rate in Hz. Default is 1.</p> <code>1</code> <code>thresh</code> <code>float</code> <p>Percentage of local max. Default is 0.2.</p> <code>0.2</code> <code>min_size</code> <code>int</code> <p>Minimum size of place field in pixels. Default is 100.</p> <code>100</code> <code>max_size</code> <code>int</code> <p>Maximum size of place field in pixels. Default is 200.</p> <code>200</code> <code>sigma</code> <code>Optional[float]</code> <p>Standard deviation for Gaussian smoothing. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>2D array of receptive fields labeled with unique integers.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def compute_2d_place_fields(\n    firing_rate: np.ndarray,\n    min_firing_rate: float = 1,\n    thresh: float = 0.2,\n    min_size: int = 100,\n    max_size: int = 200,\n    sigma: Optional[float] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute place fields from the firing rate.\n\n    Parameters\n    ----------\n    firing_rate : np.ndarray\n        2D array of firing rates (NxN).\n    min_firing_rate : float, optional\n        Minimum firing rate in Hz. Default is 1.\n    thresh : float, optional\n        Percentage of local max. Default is 0.2.\n    min_size : int, optional\n        Minimum size of place field in pixels. Default is 100.\n    max_size : int, optional\n        Maximum size of place field in pixels. Default is 200.\n    sigma : Optional[float], optional\n        Standard deviation for Gaussian smoothing. Default is None.\n\n    Returns\n    -------\n    np.ndarray\n        2D array of receptive fields labeled with unique integers.\n    \"\"\"\n    firing_rate_orig = firing_rate.copy()\n\n    if sigma is not None:\n        firing_rate = gaussian_filter(firing_rate, sigma)\n\n    local_maxima_inds = firing_rate == maximum_filter(firing_rate, 3)\n    receptive_fields = np.zeros(firing_rate.shape, dtype=int)\n    n_receptive_fields = 0\n    firing_rate = firing_rate.copy()\n    for local_max in np.flipud(np.sort(firing_rate[local_maxima_inds])):\n        labeled_image, num_labels = label(\n            firing_rate &gt; max(local_max * thresh, min_firing_rate)\n        )\n\n        if not num_labels:  # nothing above min_firing_thresh\n            continue\n        for i in range(1, num_labels + 1):\n            image_label = labeled_image == i\n            if local_max in firing_rate[image_label]:\n                break\n            if np.sum(image_label) &gt;= min_size:\n                n_receptive_fields += 1\n                receptive_fields[image_label] = n_receptive_fields\n                firing_rate[image_label] = 0\n\n    receptive_fields = remove_fields_by_area(\n        receptive_fields, int(min_size), maximum_field_area=max_size\n    )\n    if n_receptive_fields &gt; 0:\n        receptive_fields = sort_fields_by_rate(\n            firing_rate_orig, receptive_fields, func=np.max\n        )\n    return receptive_fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.compute_crossings","title":"<code>compute_crossings(field_indices)</code>","text":"<p>Compute indices at which a field is entered or exited.</p> <p>Parameters:</p> Name Type Description Default <code>field_indices</code> <code>ndarray</code> <p>1D array, typically obtained with in_field.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Indices at which fields are entered and exited.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def compute_crossings(field_indices: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute indices at which a field is entered or exited.\n\n    Parameters\n    ----------\n    field_indices : np.ndarray\n        1D array, typically obtained with in_field.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        Indices at which fields are entered and exited.\n    \"\"\"\n    # make sure to start and end outside fields\n    field_indices = np.concatenate(([0], field_indices.astype(bool).astype(int), [0]))\n    (enter,) = np.where(np.diff(field_indices) == 1)\n    (exit,) = np.where(np.diff(field_indices) == -1)\n    assert len(enter) == len(exit), (len(enter), len(exit))\n    return enter, exit\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.compute_linear_place_fields","title":"<code>compute_linear_place_fields(firing_rate, min_window_size=5, min_firing_rate=1.0, thresh=0.5)</code>","text":"<p>Find consecutive bins where all are &gt;= threshold of local max firing rate and the local max is &gt; min_firing_rate.</p> <p>Parameters:</p> Name Type Description Default <code>firing_rate</code> <code>ndarray</code> <p>Array of firing rates.</p> required <code>min_window_size</code> <code>int</code> <p>Minimum size of the window. Default is 5.</p> <code>5</code> <code>min_firing_rate</code> <code>float</code> <p>Minimum firing rate to consider a bin. Default is 1.0.</p> <code>1.0</code> <code>thresh</code> <code>float</code> <p>Threshold percentage of local max. Default is 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean array indicating place fields.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def compute_linear_place_fields(\n    firing_rate: np.ndarray,\n    min_window_size: int = 5,\n    min_firing_rate: float = 1.0,\n    thresh: float = 0.5\n) -&gt; np.ndarray:\n    \"\"\"\n    Find consecutive bins where all are &gt;= threshold of local max firing rate\n    and the local max is &gt; min_firing_rate.\n\n    Parameters\n    ----------\n    firing_rate : np.ndarray\n        Array of firing rates.\n    min_window_size : int, optional\n        Minimum size of the window. Default is 5.\n    min_firing_rate : float, optional\n        Minimum firing rate to consider a bin. Default is 1.0.\n    thresh : float, optional\n        Threshold percentage of local max. Default is 0.5.\n\n    Returns\n    -------\n    np.ndarray\n        Boolean array indicating place fields.\n    \"\"\"\n    is_place_field = np.zeros(len(firing_rate), dtype=\"bool\")\n    for start in range(len(firing_rate) - min_window_size):\n        for fin in range(start + min_window_size, len(firing_rate)):\n            window = firing_rate[start:fin]\n            mm = max(window)\n            if mm &gt; min_firing_rate and all(window &gt; thresh * mm):\n                is_place_field[start:fin] = True\n            else:\n                break\n\n    return is_place_field\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.consecutive","title":"<code>consecutive(array, stepsize)</code>","text":"<p>Splits array when distance between neighboring points is further than the stepsize.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Array to be split.</p> required <code>stepsize</code> <code>float</code> <p>Minimum distance to consider points as separate.</p> required <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List of arrays, split when jump greater than stepsize.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def consecutive(array: np.ndarray, stepsize: float) -&gt; List[np.ndarray]:\n    \"\"\"\n    Splits array when distance between neighboring points is further than the stepsize.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Array to be split.\n    stepsize : float\n        Minimum distance to consider points as separate.\n\n    Returns\n    -------\n    List[np.ndarray]\n        List of arrays, split when jump greater than stepsize.\n    \"\"\"\n    return np.split(array, np.where(np.diff(array) &gt; stepsize)[0] + 1)\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.detect_firing_fields","title":"<code>detect_firing_fields(image_gray, max_sigma=30, log_num_sigma=10, log_thres=0.1, dog_thres=0.1, doh_thres=0.01)</code>","text":"<p>Detect firing fields in a grayscale image using different blob detection methods.</p> <p>Parameters:</p> Name Type Description Default <code>image_gray</code> <code>ndarray</code> <p>Grayscale image to analyze.</p> required <code>max_sigma</code> <code>int</code> <p>The maximum standard deviation for Gaussian filter.</p> <code>30</code> <code>log_num_sigma</code> <code>int</code> <p>The number of sigma values for the Laplacian of Gaussian.</p> <code>10</code> <code>log_thres</code> <code>float</code> <p>Threshold for Laplacian of Gaussian blobs.</p> <code>0.1</code> <code>dog_thres</code> <code>float</code> <p>Threshold for Difference of Gaussian blobs.</p> <code>0.1</code> <code>doh_thres</code> <code>float</code> <p>Threshold for Determinant of Hessian blobs.</p> <code>0.01</code> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def detect_firing_fields(\n    image_gray: np.ndarray,\n    max_sigma: int = 30,\n    log_num_sigma: int = 10,\n    log_thres: float = 0.1,\n    dog_thres: float = 0.1,\n    doh_thres: float = 0.01,\n) -&gt; None:\n    \"\"\"\n    Detect firing fields in a grayscale image using different blob detection methods.\n\n    Parameters\n    ----------\n    image_gray : np.ndarray\n        Grayscale image to analyze.\n    max_sigma : int, optional\n        The maximum standard deviation for Gaussian filter.\n    log_num_sigma : int, optional\n        The number of sigma values for the Laplacian of Gaussian.\n    log_thres : float, optional\n        Threshold for Laplacian of Gaussian blobs.\n    dog_thres : float, optional\n        Threshold for Difference of Gaussian blobs.\n    doh_thres : float, optional\n        Threshold for Determinant of Hessian blobs.\n    \"\"\"\n    from skimage.feature import blob_dog, blob_doh, blob_log\n    plt.imshow(image_gray, origin=\"lower\")\n\n    blobs_log = blob_log(\n        image_gray, max_sigma=max_sigma, num_sigma=log_num_sigma, threshold=log_thres\n    )\n    # Compute radii in the 3rd column.\n    blobs_log[:, 2] = blobs_log[:, 2] * sqrt(2)\n\n    blobs_dog = blob_dog(image_gray, max_sigma=max_sigma, threshold=dog_thres)\n    blobs_dog[:, 2] = blobs_dog[:, 2] * sqrt(2)\n\n    blobs_doh = blob_doh(image_gray, max_sigma=max_sigma, threshold=doh_thres)\n\n    blobs_list = [blobs_log, blobs_dog, blobs_doh]\n    colors = [\"yellow\", \"lime\", \"red\"]\n    titles = [\n        \"Laplacian of Gaussian\",\n        \"Difference of Gaussian\",\n        \"Determinant of Hessian\",\n    ]\n    sequence = zip(blobs_list, colors, titles)\n\n    fig, axes = plt.subplots(1, 3, figsize=(9, 3), sharex=True, sharey=True)\n    ax = axes.ravel()\n\n    for idx, (blobs, color, title) in enumerate(sequence):\n        ax[idx].set_title(title)\n        ax[idx].imshow(image_gray, interpolation=\"nearest\", origin=\"lower\")\n        for blob in blobs:\n            y, x, r = blob\n            c = plt.Circle((x, y), r, color=color, linewidth=2, fill=False)\n            ax[idx].add_patch(c)\n        ax[idx].set_axis_off()\n\n    plt.tight_layout()\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.distance_to_edge_function","title":"<code>distance_to_edge_function(x_c, y_c, field, box_size, interpolation='linear')</code>","text":"<p>Returns a function which, for a given angle, returns the distance to the edge of the field from the center.</p> <p>Parameters:</p> Name Type Description Default <code>x_c</code> <code>float</code> <p>X-coordinate of the center.</p> required <code>y_c</code> <code>float</code> <p>Y-coordinate of the center.</p> required <code>field</code> <code>ndarray</code> <p>2D array with ones at field bins and zeros elsewhere.</p> required <code>box_size</code> <code>Tuple[float, float]</code> <p>Size of the box (arena).</p> required <code>interpolation</code> <code>str</code> <p>Type of interpolation to use. Default is \"linear\".</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>Callable[[float], float]</code> <p>A function that takes an angle and returns the distance to the edge of the field.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def distance_to_edge_function(\n    x_c: float,\n    y_c: float,\n    field: np.ndarray,\n    box_size: Tuple[float, float],\n    interpolation: str = \"linear\"\n) -&gt; Callable[[float], float]:\n    \"\"\"\n    Returns a function which, for a given angle, returns the distance to\n    the edge of the field from the center.\n\n    Parameters\n    ----------\n    x_c : float\n        X-coordinate of the center.\n    y_c : float\n        Y-coordinate of the center.\n    field : np.ndarray\n        2D array with ones at field bins and zeros elsewhere.\n    box_size : Tuple[float, float]\n        Size of the box (arena).\n    interpolation : str, optional\n        Type of interpolation to use. Default is \"linear\".\n\n    Returns\n    -------\n    Callable[[float], float]\n        A function that takes an angle and returns the distance to the edge of the field.\n    \"\"\"\n    from skimage import measure\n\n    contours = measure.find_contours(field, 0.8)\n\n    box_dim = np.array(box_size)\n    edge_x, edge_y = (contours[0] * box_dim / (np.array(field.shape) - (1, 1))).T\n\n    # # angle between 0 and 2\\pi\n    angles = np.arctan2((edge_y - y_c), (edge_x - x_c)) % (2 * np.pi)\n    a_sort = np.argsort(angles)\n    angles = angles[a_sort]\n    edge_x = edge_x[a_sort]\n    edge_y = edge_y[a_sort]\n\n    # # Fill in edge values for the interpolation\n    pad_a = np.pad(angles, 2, mode=\"linear_ramp\", end_values=(0, 2 * np.pi))\n    ev_x = (edge_x[0] + edge_x[-1]) / 2\n    pad_x = np.pad(edge_x, 2, mode=\"linear_ramp\", end_values=ev_x)\n    ev_y = (edge_y[0] + edge_y[-1]) / 2\n    pad_y = np.pad(edge_y, 2, mode=\"linear_ramp\", end_values=ev_y)\n\n    if interpolation == \"cubic\":\n        mask = np.where(np.diff(pad_a) == 0)\n        pad_a = np.delete(pad_a, mask)\n        pad_x = np.delete(pad_x, mask)\n        pad_y = np.delete(pad_y, mask)\n\n    x_func = interp1d(pad_a, pad_x, kind=interpolation)\n    y_func = interp1d(pad_a, pad_y, kind=interpolation)\n\n    def dist_func(angle: float) -&gt; float:\n        \"\"\"\n        Computes the distance from the center to the edge of the field at a given angle.\n\n        Parameters\n        ----------\n        angle : float\n            Angle in radians.\n\n        Returns\n        -------\n        float\n            Distance to the edge of the field from the center.\n        \"\"\"\n        x = x_func(angle)\n        y = y_func(angle)\n        dist = np.sqrt((x - x_c) ** 2 + (y - y_c) ** 2)\n        return dist\n\n    return dist_func\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.find_field","title":"<code>find_field(firing_rate, threshold)</code>","text":"<p>Find the field in the firing rate that exceeds the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>firing_rate</code> <code>ndarray</code> <p>Array of firing rates.</p> required <code>threshold</code> <code>float</code> <p>Threshold for detection.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple containing the image label and the same label.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def find_field(\n    firing_rate: np.ndarray,\n    threshold: float\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Find the field in the firing rate that exceeds the threshold.\n\n    Parameters\n    ----------\n    firing_rate : np.ndarray\n        Array of firing rates.\n    threshold : float\n        Threshold for detection.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        Tuple containing the image label and the same label.\n    \"\"\"\n    mm = np.max(firing_rate)\n\n    labeled_image, num_labels = label(firing_rate &gt; threshold)\n    for i in range(1, num_labels + 1):\n        image_label = labeled_image == i\n        if mm in firing_rate[image_label]:\n            return image_label, image_label\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.find_field2","title":"<code>find_field2(firing_rate, thresh)</code>","text":"<p>Find the field in a 1D firing rate array that exceeds the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>firing_rate</code> <code>ndarray</code> <p>1D array of firing rates.</p> required <code>thresh</code> <code>float</code> <p>Threshold for detection.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple containing two boolean arrays:  the first indicates the buffer area and the second indicates the field.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def find_field2(\n    firing_rate: np.ndarray,\n    thresh: float\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Find the field in a 1D firing rate array that exceeds the threshold.\n\n    Parameters\n    ----------\n    firing_rate : np.ndarray\n        1D array of firing rates.\n    thresh : float\n        Threshold for detection.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        Tuple containing two boolean arrays: \n        the first indicates the buffer area and the second indicates the field.\n    \"\"\"\n    firing_rate = np.array(firing_rate)\n    imm = np.argmax(firing_rate)\n    mm = np.max(firing_rate)\n    # TODO: make more efficient by using argmax instead of where()[0]\n    first = np.where(np.diff(firing_rate[:imm]) &lt; 0)[0]\n    if len(first) == 0:\n        first = 0\n    else:\n        first = first[-1] + 2\n\n    last = np.where(np.diff(firing_rate[imm:]) &gt; 0)[0]\n\n    if len(last) == 0:\n        last = len(firing_rate)\n    else:\n        last = last[0] + imm + 1\n    field_buffer = np.zeros(firing_rate.shape, dtype=\"bool\")\n    field_buffer[first:last] = True\n    field = field_buffer &amp; (firing_rate &gt; thresh * mm)\n\n    return field_buffer, field\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.find_fields_1d","title":"<code>find_fields_1d(tuning, hz_thresh=5, min_length=1, max_length=20, max_mean_firing=10)</code>","text":"<p>Finds the location of maximum spiking.</p> <p>Parameters:</p> Name Type Description Default <code>tuning</code> <code>List[ndarray]</code> <p>Each inner array contains the tuning curves for an individual neuron.</p> required <code>hz_thresh</code> <code>float</code> <p>Any bin with firing above this value is considered to be part of a field. Default is 5.</p> <code>5</code> <code>min_length</code> <code>int</code> <p>Minimum length of field (in tuning curve bin units). Default is 1.</p> <code>1</code> <code>max_length</code> <code>int</code> <p>Maximum length of field (in tuning curve bin units). Default is 20.</p> <code>20</code> <code>max_mean_firing</code> <code>float</code> <p>Only neurons with a mean firing rate less than this amount are considered for having place fields. Default is 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>dict</code> <p>Where the key is the neuron number (int), and the value is a list of arrays (int) that are indices into the tuning curve where the field occurs. Each inner array contains the indices for a given place field.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def find_fields_1d(\n    tuning: List[np.ndarray],\n    hz_thresh: float = 5,\n    min_length: int = 1,\n    max_length: int = 20,\n    max_mean_firing: float = 10\n) -&gt; dict:\n    \"\"\"\n    Finds the location of maximum spiking.\n\n    Parameters\n    ----------\n    tuning : List[np.ndarray]\n        Each inner array contains the tuning curves for an individual neuron.\n    hz_thresh : float, optional\n        Any bin with firing above this value is considered to be part of a field. Default is 5.\n    min_length : int, optional\n        Minimum length of field (in tuning curve bin units). Default is 1.\n    max_length : int, optional\n        Maximum length of field (in tuning curve bin units). Default is 20.\n    max_mean_firing : float, optional\n        Only neurons with a mean firing rate less than this amount are considered for\n        having place fields. Default is 10.\n\n    Returns\n    -------\n    dict\n        Where the key is the neuron number (int), and the value is a list of arrays (int)\n        that are indices into the tuning curve where the field occurs.\n        Each inner array contains the indices for a given place field.\n    \"\"\"\n    fields = []\n    for neuron_tc in tuning:\n        if np.mean(neuron_tc) &lt; max_mean_firing:\n            neuron_field = np.zeros(neuron_tc.shape[0])\n            for i, this_bin in enumerate(neuron_tc):\n                if this_bin &gt; hz_thresh:\n                    neuron_field[i] = 1\n            fields.append(neuron_field)\n        else:\n            fields.append(np.array([]))\n\n    fields_idx = dict()\n    for i, neuron_fields in enumerate(fields):\n        field_idx = np.nonzero(neuron_fields)[0]\n        fields_idx[i] = consecutive(field_idx, stepsize=1)\n\n    with_fields = dict()\n    for key in fields_idx:\n        for field in fields_idx[key]:\n            if len(field) &gt; max_length:\n                continue\n            elif min_length &lt;= len(field):\n                with_fields[key] = fields_idx[key]\n                continue\n    return with_fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.find_peaks","title":"<code>find_peaks(image)</code>","text":"<p>Find peaks sorted by distance from the center of the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinates for peaks in the image as [row, column].</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def find_peaks(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Find peaks sorted by distance from the center of the image.\n\n    Parameters\n    ----------\n    image : np.ndarray\n        The input image.\n\n    Returns\n    -------\n    np.ndarray\n        Coordinates for peaks in the image as [row, column].\n    \"\"\"\n    image = image.copy()\n    image[~np.isfinite(image)] = 0\n    image_max = filters.maximum_filter(image, 3)\n    is_maxima = image == image_max\n    labels, num_objects = ndimage.label(is_maxima)\n    indices = np.arange(1, num_objects + 1)\n    peaks = ndimage.maximum_position(image, labels=labels, index=indices)\n    peaks = np.array(peaks)\n    center = (np.array(image.shape) - 1) / 2\n    distances = np.linalg.norm(peaks - center, axis=1)\n    peaks = peaks[distances.argsort()]\n    return peaks\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.map_pass_to_unit_circle","title":"<code>map_pass_to_unit_circle(x, y, t, x_c, y_c, field=None, box_size=None, dist_func=None)</code>","text":"<p>Uses three vectors {v, p, q} to map the passes to the unit circle. v is the average velocity vector of the pass, p is the vector from the position (x, y) to the center of the field, and q is the vector from the center to the edge through (x, y).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>X-coordinates.</p> required <code>y</code> <code>ndarray</code> <p>Y-coordinates.</p> required <code>t</code> <code>ndarray</code> <p>Time data.</p> required <code>x_c</code> <code>float</code> <p>X-coordinate of the center of the field.</p> required <code>y_c</code> <code>float</code> <p>Y-coordinate of the center of the field.</p> required <code>field</code> <code>Optional[ndarray]</code> <p>2D array indicating the location of the field.</p> <code>None</code> <code>box_size</code> <code>Optional[Tuple[float, float]]</code> <p>Size of the box (arena).</p> <code>None</code> <code>dist_func</code> <code>Optional[Callable[[float], float]]</code> <p>Function that computes distance to bump edge from center. Default is distance_to_edge_function with linear interpolation.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray, ndarray]</code> <p>r : Array of distances to origin on unit circle. theta : Array of angles to axis defined by mean velocity vector. pdcd : Array of distances to peak projected onto the current direction. pdmd : Array of distances to peak projected onto the mean direction.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If neither dist_func nor both field and box_size are provided.</p> References: <p>.. [1] Jeewajee A, Barry C, Douchamps V, Manson D, Lever C, Burgess N. Theta        phase precession of grid and place cell firing in open environments.        Philos Trans R Soc Lond B Biol Sci. 2013 Dec 23;369(1635):20120532</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def map_pass_to_unit_circle(\n    x: np.ndarray,\n    y: np.ndarray,\n    t: np.ndarray,\n    x_c: float,\n    y_c: float,\n    field: Optional[np.ndarray] = None,\n    box_size: Optional[Tuple[float, float]] = None,\n    dist_func: Optional[Callable[[float], float]] = None\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Uses three vectors {v, p, q} to map the passes to the unit circle. v\n    is the average velocity vector of the pass, p is the vector from the\n    position (x, y) to the center of the field, and q is the vector from the\n    center to the edge through (x, y).\n\n    Parameters\n    ----------\n    x : np.ndarray\n        X-coordinates.\n    y : np.ndarray\n        Y-coordinates.\n    t : np.ndarray\n        Time data.\n    x_c : float\n        X-coordinate of the center of the field.\n    y_c : float\n        Y-coordinate of the center of the field.\n    field : Optional[np.ndarray], optional\n        2D array indicating the location of the field.\n    box_size : Optional[Tuple[float, float]], optional\n        Size of the box (arena).\n    dist_func : Optional[Callable[[float], float]], optional\n        Function that computes distance to bump edge from center. Default is\n        distance_to_edge_function with linear interpolation.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n        r : Array of distances to origin on unit circle.\n        theta : Array of angles to axis defined by mean velocity vector.\n        pdcd : Array of distances to peak projected onto the current direction.\n        pdmd : Array of distances to peak projected onto the mean direction.\n\n    Raises\n    ------\n    AssertionError\n        If neither dist_func nor both field and box_size are provided.\n\n    References:\n    -----------\n    .. [1] Jeewajee A, Barry C, Douchamps V, Manson D, Lever C, Burgess N. Theta\n           phase precession of grid and place cell firing in open environments.\n           Philos Trans R Soc Lond B Biol Sci. 2013 Dec 23;369(1635):20120532\n    \"\"\"\n    if dist_func is None:\n        assert (\n            field is not None and box_size is not None\n        ), 'either provide \"dist_func\" or \"field\" and \"box_size\"'\n        dist_func = distance_to_edge_function(\n            x_c, y_c, field, box_size, interpolation=\"linear\"\n        )\n    pos = np.array((x, y))\n\n    # vector from pos to center p\n    p_vec = ((x_c, y_c) - pos.T).T\n    # angle between x-axis and negative vector p\n    angle = (np.arctan2(p_vec[1], p_vec[0]) + np.pi) % (2 * np.pi)\n    # distance from center to edge at each angle\n    q = dist_func(angle)\n    # distance from center to pos\n    p = np.linalg.norm(p_vec, axis=0)\n    # r-coordinate on unit circle\n    r = p / q\n\n    dpos = np.gradient(pos, axis=1)\n    dt = np.gradient(t)\n    velocity = np.divide(dpos, dt)\n\n    # mean velocity vector v\n    mean_velocity = np.average(velocity, axis=1)\n    # angle on unit circle, run is rotated such that mean velocity vector\n    # is toward positive x\n    theta = (angle - np.arctan2(mean_velocity[1], mean_velocity[0])) % (2 * np.pi)\n\n    w_pdcd = angle - np.arctan2(velocity[1], velocity[0])\n    pdcd = r * np.cos(w_pdcd)\n\n    w_pdmd = angle - np.arctan2(mean_velocity[1], mean_velocity[0])\n    pdmd = r * np.cos(w_pdmd)\n    return r, theta, pdcd, pdmd\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.map_stats2","title":"<code>map_stats2(firing_rate, threshold=0.1, min_size=5, max_size=None, min_peak=1.0, sigma=None)</code>","text":"<p>Map statistics of firing rate fields.</p> <p>Parameters:</p> Name Type Description Default <code>firing_rate</code> <code>ndarray</code> <p>1D array of firing rates.</p> required <code>threshold</code> <code>float</code> <p>Threshold for field detection. Default is 0.1.</p> <code>0.1</code> <code>min_size</code> <code>int</code> <p>Minimum size of detected fields. Default is 5.</p> <code>5</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of detected fields. Default is None, which sets it to the length of firing_rate.</p> <code>None</code> <code>min_peak</code> <code>float</code> <p>Minimum peak firing rate to consider a field valid. Default is 1.0.</p> <code>1.0</code> <code>sigma</code> <code>Optional[float]</code> <p>Standard deviation for Gaussian smoothing. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, List[float]]</code> <p>A dictionary containing the sizes, peaks, means, and fields of detected firing rate fields.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def map_stats2(\n    firing_rate: np.ndarray,\n    threshold: float = 0.1,\n    min_size: int = 5,\n    max_size: Optional[int] = None,\n    min_peak: float = 1.0,\n    sigma: Optional[float] = None\n) -&gt; Dict[str, List[float]]:\n    \"\"\"\n    Map statistics of firing rate fields.\n\n    Parameters\n    ----------\n    firing_rate : np.ndarray\n        1D array of firing rates.\n    threshold : float, optional\n        Threshold for field detection. Default is 0.1.\n    min_size : int, optional\n        Minimum size of detected fields. Default is 5.\n    max_size : Optional[int], optional\n        Maximum size of detected fields. Default is None, which sets it to the length of firing_rate.\n    min_peak : float, optional\n        Minimum peak firing rate to consider a field valid. Default is 1.0.\n    sigma : Optional[float], optional\n        Standard deviation for Gaussian smoothing. Default is None.\n\n    Returns\n    -------\n    Dict[str, List[float]]\n        A dictionary containing the sizes, peaks, means, and fields of detected firing rate fields.\n    \"\"\"\n    if sigma is not None:\n        firing_rate = gaussian_filter1d(firing_rate, sigma)\n\n    if max_size is None:\n        max_size = len(firing_rate)\n\n    firing_rate = firing_rate.copy()\n    firing_rate = firing_rate - np.min(firing_rate)\n    out = dict(sizes=list(), peaks=list(), means=list())\n    out[\"fields\"] = np.zeros(firing_rate.shape)\n    field_counter = 1\n    while True:\n        peak = np.max(firing_rate)\n        if peak &lt; min_peak:\n            break\n        field_buffer, field = find_field(firing_rate, threshold)\n        field_size = np.sum(field)\n        if (\n            (field_size &gt; min_size)\n            and (field_size &lt; max_size)\n            and (np.max(firing_rate[field]) &gt; (2 * np.min(firing_rate[field_buffer])))\n        ):\n            out[\"fields\"][field] = field_counter\n            out[\"sizes\"].append(float(field_size) / len(firing_rate))\n            out[\"peaks\"].append(peak)\n            out[\"means\"].append(np.mean(firing_rate[field]))\n            field_counter += 1\n        firing_rate[field_buffer] = 0\n\n    return out\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.remove_fields_by_area","title":"<code>remove_fields_by_area(fields, minimum_field_area, maximum_field_area=None)</code>","text":"<p>Sets fields below minimum area to zero, measured as the number of bins in a field.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>ndarray</code> <p>The fields.</p> required <code>minimum_field_area</code> <code>int</code> <p>Minimum field area (number of bins in a field).</p> required <code>maximum_field_area</code> <code>Optional[int]</code> <p>Maximum field area (number of bins in a field). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Fields with number of bins below minimum_field_area are set to zero.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If minimum_field_area is not an integer.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def remove_fields_by_area(\n    fields: np.ndarray,\n    minimum_field_area: int,\n    maximum_field_area: Optional[int] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Sets fields below minimum area to zero, measured as the number of bins in a field.\n\n    Parameters\n    ----------\n    fields : np.ndarray\n        The fields.\n    minimum_field_area : int\n        Minimum field area (number of bins in a field).\n    maximum_field_area : Optional[int]\n        Maximum field area (number of bins in a field). Default is None.\n\n    Returns\n    -------\n    np.ndarray\n        Fields with number of bins below minimum_field_area are set to zero.\n\n    Raises\n    ------\n    ValueError\n        If minimum_field_area is not an integer.\n    \"\"\"\n    if not isinstance(minimum_field_area, (int, np.integer)):\n        raise ValueError(\"'minimum_field_area' should be int\")\n\n    if maximum_field_area is None:\n        maximum_field_area = len(fields.flatten())\n    ## variant\n    # fields_areas = scipy.ndimage.measurements.sum(\n    #     np.ones_like(fields), fields, index=np.arange(fields.max() + 1))\n    # fields_area = fields_areas[fields]\n    # fields[fields_area &lt; minimum_field_area] = 0\n\n    labels, counts = np.unique(fields, return_counts=True)\n    for lab, count in zip(labels, counts):\n        if lab != 0:\n            if (count &lt; minimum_field_area) | (count &gt; maximum_field_area):\n                fields[fields == lab] = 0\n    return fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.separate_fields_by_dilation","title":"<code>separate_fields_by_dilation(rate_map, seed=2.5, sigma=2.5, minimum_field_area=None)</code>","text":"<p>Separates fields by the Laplace of Gaussian (LoG) on the rate map subtracted by a reconstruction of the rate map using dilation.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>2D array representing firing rate in each bin.</p> required <code>seed</code> <code>float</code> <p>Magnitude of dilation.</p> <code>2.5</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian to separate fields. Default is 2.5.</p> <code>2.5</code> <code>minimum_field_area</code> <code>Optional[int]</code> <p>Minimum number of bins to consider it a field. Default is None (all fields are kept).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Labels with areas filled with the same value, corresponding to fields in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the field (sum of all field values) with 0 elsewhere.</p> References <p>.. [1] https://scikit-image.org/docs/stable/auto_examples/color_exposure/plot_regional_maxima.html</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def separate_fields_by_dilation(\n    rate_map: np.ndarray,\n    seed: float = 2.5,\n    sigma: float = 2.5,\n    minimum_field_area: Optional[int] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Separates fields by the Laplace of Gaussian (LoG)\n    on the rate map subtracted by a reconstruction of the rate map using\n    dilation.\n\n    Parameters\n    ----------\n    rate_map : np.ndarray\n        2D array representing firing rate in each bin.\n    seed : float\n        Magnitude of dilation.\n    sigma : float\n        Standard deviation of Gaussian to separate fields. Default is 2.5.\n    minimum_field_area : Optional[int]\n        Minimum number of bins to consider it a field. Default is None (all fields are kept).\n\n    Returns\n    -------\n    np.ndarray\n        Labels with areas filled with the same value, corresponding to fields\n        in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the\n        field (sum of all field values) with 0 elsewhere.\n\n    References\n    ----------\n    .. [1] https://scikit-image.org/docs/stable/auto_examples/color_exposure/plot_regional_maxima.html\n    \"\"\"\n    from skimage.morphology import reconstruction\n\n    rate_map_norm = (rate_map - rate_map.mean()) / rate_map.std()\n    dilated = reconstruction(rate_map_norm - seed, rate_map_norm, method=\"dilation\")\n    rate_map_reconstructed = rate_map_norm - dilated\n\n    laplacian = ndimage.gaussian_laplace(rate_map_reconstructed, sigma)\n    laplacian[laplacian &gt; 0] = 0\n    fields, _ = ndimage.label(laplacian)\n    fields = sort_fields_by_rate(rate_map, fields)\n    if minimum_field_area is not None:\n        fields = remove_fields_by_area(fields, minimum_field_area)\n    return fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.separate_fields_by_laplace","title":"<code>separate_fields_by_laplace(rate_map, threshold=0, minimum_field_area=None)</code>","text":"<p>Separates fields using the Laplacian to identify fields separated by a negative second derivative.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>2D array representing firing rate in each bin.</p> required <code>threshold</code> <code>float</code> <p>Value of Laplacian to separate fields by relative to the minima. Should be on the interval 0 to 1, where 0 cuts off at 0 and 1 cuts off at min(laplace(rate_map)). Default is 0.</p> <code>0</code> <code>minimum_field_area</code> <code>Optional[int]</code> <p>Minimum number of bins to consider it a field. Default is None (all fields are kept).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Labels with areas filled with the same value, corresponding to fields in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the field (sum of all field values) with 0 elsewhere.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def separate_fields_by_laplace(\n    rate_map: np.ndarray,\n    threshold: float = 0,\n    minimum_field_area: Optional[int] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Separates fields using the Laplacian to identify fields separated by\n    a negative second derivative.\n\n    Parameters\n    ----------\n    rate_map : np.ndarray\n        2D array representing firing rate in each bin.\n    threshold : float\n        Value of Laplacian to separate fields by relative to the minima.\n        Should be on the interval 0 to 1, where 0 cuts off at 0 and\n        1 cuts off at min(laplace(rate_map)). Default is 0.\n    minimum_field_area : Optional[int]\n        Minimum number of bins to consider it a field. Default is None (all fields are kept).\n\n    Returns\n    -------\n    np.ndarray\n        Labels with areas filled with the same value, corresponding to fields\n        in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the\n        field (sum of all field values) with 0 elsewhere.\n    \"\"\"\n\n    laplacian = ndimage.laplace(rate_map)\n\n    laplacian[laplacian &gt; threshold * np.min(laplacian)] = 0\n\n    # Labels areas of the laplacian not connected by values &gt; 0.\n    fields, _ = ndimage.label(laplacian)\n    fields = sort_fields_by_rate(rate_map, fields)\n    if minimum_field_area is not None:\n        fields = remove_fields_by_area(fields, minimum_field_area)\n    return fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.separate_fields_by_laplace_of_gaussian","title":"<code>separate_fields_by_laplace_of_gaussian(rate_map, sigma=2, minimum_field_area=None)</code>","text":"<p>Separates fields using the Laplace of Gaussian (LoG) to identify fields separated by a negative second derivative. Works best if no smoothing is applied to the rate map, preferably with interpolated NaNs.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>2D array representing firing rate in each bin.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian to separate fields. Default is 2.</p> <code>2</code> <code>minimum_field_area</code> <code>Optional[int]</code> <p>Minimum number of bins to consider it a field. Default is None (all fields are kept).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Labels with areas filled with the same value, corresponding to fields in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the field (sum of all field values) with 0 elsewhere.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def separate_fields_by_laplace_of_gaussian(\n    rate_map: np.ndarray,\n    sigma: float = 2,\n    minimum_field_area: Optional[int] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Separates fields using the Laplace of Gaussian (LoG) to identify fields\n    separated by a negative second derivative. Works best if no smoothing is\n    applied to the rate map, preferably with interpolated NaNs.\n\n    Parameters\n    ----------\n    rate_map : np.ndarray\n        2D array representing firing rate in each bin.\n    sigma : float\n        Standard deviation of Gaussian to separate fields. Default is 2.\n    minimum_field_area : Optional[int]\n        Minimum number of bins to consider it a field. Default is None (all fields are kept).\n\n    Returns\n    -------\n    np.ndarray\n        Labels with areas filled with the same value, corresponding to fields\n        in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the\n        field (sum of all field values) with 0 elsewhere.\n    \"\"\"\n    laplacian = ndimage.gaussian_laplace(rate_map, sigma)\n    laplacian[laplacian &gt; 0] = 0\n\n    # Labels areas of the laplacian not connected by values &gt; 0.\n    fields, _ = ndimage.label(laplacian)\n\n    fields = sort_fields_by_rate(rate_map, fields)\n    if minimum_field_area is not None:\n        fields = remove_fields_by_area(fields, minimum_field_area)\n    return fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.sort_fields_by_rate","title":"<code>sort_fields_by_rate(rate_map, fields, func=None)</code>","text":"<p>Sort fields by the rate value of each field.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>The rate map.</p> required <code>fields</code> <code>ndarray</code> <p>The fields of the same shape as rate_map.</p> required <code>func</code> <code>Callable</code> <p>Function returning value to sort after, default is np.max.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Sorted fields.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def sort_fields_by_rate(\n    rate_map: np.ndarray,\n    fields: np.ndarray,\n    func: Optional[Callable[[np.ndarray], Any]] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Sort fields by the rate value of each field.\n\n    Parameters\n    ----------\n    rate_map : np.ndarray\n        The rate map.\n    fields : np.ndarray\n        The fields of the same shape as rate_map.\n    func : Callable, optional\n        Function returning value to sort after, default is np.max.\n\n    Returns\n    -------\n    np.ndarray\n        Sorted fields.\n    \"\"\"\n    indx = np.sort(np.unique(fields.ravel()))\n    func = func or np.max\n    # Sort by largest peak\n    rate_means = ndimage.labeled_comprehension(\n        rate_map, fields, indx, func, np.float64, 0\n    )\n    sort = np.argsort(rate_means)[::-1]\n\n    sorted_fields = np.zeros_like(fields)\n    for indx_i, indx_ in enumerate(indx[sort]):\n        if indx_ == 0:\n            continue\n        sorted_fields[fields == indx_] = np.max(sorted_fields) + 1\n\n    # new rate map with fields &gt; min_size, sorted\n    # sorted_fields = np.zeros_like(fields)\n    # for i in range(indx.max() + 1):\n    #     sorted_fields[fields == sort[i] + 1] = i + 1\n\n    return sorted_fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/#neuro_py.tuning.which_field","title":"<code>which_field(x, y, fields, box_size)</code>","text":"<p>Returns which spatial field each (x, y) position is in.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>X-coordinates.</p> required <code>y</code> <code>ndarray</code> <p>Y-coordinates, must have the same length as x.</p> required <code>fields</code> <code>ndarray</code> <p>Labeled fields, where each field is defined by an area separated by zeros. The fields are labeled with indices from [1:].</p> required <code>box_size</code> <code>List[float]</code> <p>Extents of the arena.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array-like x and y with fields-labeled indices.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If x and y do not have the same length.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def which_field(\n    x: np.ndarray,\n    y: np.ndarray,\n    fields: np.ndarray,\n    box_size: List[float]\n) -&gt; np.ndarray:\n    \"\"\"\n    Returns which spatial field each (x, y) position is in.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        X-coordinates.\n    y : np.ndarray\n        Y-coordinates, must have the same length as x.\n    fields : np.ndarray\n        Labeled fields, where each field is defined by an area separated by\n        zeros. The fields are labeled with indices from [1:].\n    box_size : List[float]\n        Extents of the arena.\n\n    Returns\n    -------\n    np.ndarray\n        Array-like x and y with fields-labeled indices.\n\n    Raises\n    ------\n    ValueError\n        If x and y do not have the same length.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"x and y must have same length\")\n\n    sx, sy = fields.shape\n    # bin sizes\n    dx = box_size[0] / sx\n    dy = box_size[1] / sy\n    x_bins = dx + np.arange(0, box_size[0] + dx, dx)\n    y_bins = dy + np.arange(0, box_size[1] + dx, dy)\n    # x_bins = np.arange(0, box_size[0] + dx, dx)\n    # y_bins = np.arange(0, box_size[1] + dx, dy)\n    ix = np.digitize(x, x_bins)\n    iy = np.digitize(y, y_bins)\n\n    # fix for boundaries:\n    ix[ix == sx] = sx - 1\n    iy[iy == sy] = sy - 1\n    return np.array(fields[ix, iy])\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/","title":"neuro_py.tuning.fields","text":""},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.calculate_field_centers","title":"<code>calculate_field_centers(rate_map, labels, center_method='maxima')</code>","text":"<p>Finds center of fields at labels.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>2D array representing firing rate in each bin.</p> required <code>labels</code> <code>ndarray</code> <p>Labeled fields.</p> required <code>center_method</code> <code>str</code> <p>Method to calculate the center; either 'maxima' or 'center_of_mass'. Default is 'maxima'.</p> <code>'maxima'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinates of the center for each field.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid center_method is provided.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def calculate_field_centers(\n    rate_map: np.ndarray,\n    labels: np.ndarray,\n    center_method: str = \"maxima\"\n) -&gt; np.ndarray:\n    \"\"\"\n    Finds center of fields at labels.\n\n    Parameters\n    ----------\n    rate_map : np.ndarray\n        2D array representing firing rate in each bin.\n    labels : np.ndarray\n        Labeled fields.\n    center_method : str\n        Method to calculate the center; either 'maxima' or 'center_of_mass'. Default is 'maxima'.\n\n    Returns\n    -------\n    np.ndarray\n        Coordinates of the center for each field.\n\n    Raises\n    ------\n    ValueError\n        If an invalid center_method is provided.\n    \"\"\"\n    indices = np.arange(1, np.max(labels) + 1)\n    if center_method == \"maxima\":\n        bc = ndimage.maximum_position(rate_map, labels=labels, index=indices)\n    elif center_method == \"center_of_mass\":\n        bc = ndimage.center_of_mass(rate_map, labels=labels, index=indices)\n    else:\n        raise ValueError(\"invalid center_method flag '{}'\".format(center_method))\n\n    if not bc:\n        # empty list\n        return bc\n\n    bc = np.array(bc)\n    bc[:, [0, 1]] = bc[:, [1, 0]]  # y, x -&gt; x, y\n    return bc\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.compute_2d_place_fields","title":"<code>compute_2d_place_fields(firing_rate, min_firing_rate=1, thresh=0.2, min_size=100, max_size=200, sigma=None)</code>","text":"<p>Compute place fields from the firing rate.</p> <p>Parameters:</p> Name Type Description Default <code>firing_rate</code> <code>ndarray</code> <p>2D array of firing rates (NxN).</p> required <code>min_firing_rate</code> <code>float</code> <p>Minimum firing rate in Hz. Default is 1.</p> <code>1</code> <code>thresh</code> <code>float</code> <p>Percentage of local max. Default is 0.2.</p> <code>0.2</code> <code>min_size</code> <code>int</code> <p>Minimum size of place field in pixels. Default is 100.</p> <code>100</code> <code>max_size</code> <code>int</code> <p>Maximum size of place field in pixels. Default is 200.</p> <code>200</code> <code>sigma</code> <code>Optional[float]</code> <p>Standard deviation for Gaussian smoothing. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>2D array of receptive fields labeled with unique integers.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def compute_2d_place_fields(\n    firing_rate: np.ndarray,\n    min_firing_rate: float = 1,\n    thresh: float = 0.2,\n    min_size: int = 100,\n    max_size: int = 200,\n    sigma: Optional[float] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute place fields from the firing rate.\n\n    Parameters\n    ----------\n    firing_rate : np.ndarray\n        2D array of firing rates (NxN).\n    min_firing_rate : float, optional\n        Minimum firing rate in Hz. Default is 1.\n    thresh : float, optional\n        Percentage of local max. Default is 0.2.\n    min_size : int, optional\n        Minimum size of place field in pixels. Default is 100.\n    max_size : int, optional\n        Maximum size of place field in pixels. Default is 200.\n    sigma : Optional[float], optional\n        Standard deviation for Gaussian smoothing. Default is None.\n\n    Returns\n    -------\n    np.ndarray\n        2D array of receptive fields labeled with unique integers.\n    \"\"\"\n    firing_rate_orig = firing_rate.copy()\n\n    if sigma is not None:\n        firing_rate = gaussian_filter(firing_rate, sigma)\n\n    local_maxima_inds = firing_rate == maximum_filter(firing_rate, 3)\n    receptive_fields = np.zeros(firing_rate.shape, dtype=int)\n    n_receptive_fields = 0\n    firing_rate = firing_rate.copy()\n    for local_max in np.flipud(np.sort(firing_rate[local_maxima_inds])):\n        labeled_image, num_labels = label(\n            firing_rate &gt; max(local_max * thresh, min_firing_rate)\n        )\n\n        if not num_labels:  # nothing above min_firing_thresh\n            continue\n        for i in range(1, num_labels + 1):\n            image_label = labeled_image == i\n            if local_max in firing_rate[image_label]:\n                break\n            if np.sum(image_label) &gt;= min_size:\n                n_receptive_fields += 1\n                receptive_fields[image_label] = n_receptive_fields\n                firing_rate[image_label] = 0\n\n    receptive_fields = remove_fields_by_area(\n        receptive_fields, int(min_size), maximum_field_area=max_size\n    )\n    if n_receptive_fields &gt; 0:\n        receptive_fields = sort_fields_by_rate(\n            firing_rate_orig, receptive_fields, func=np.max\n        )\n    return receptive_fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.compute_crossings","title":"<code>compute_crossings(field_indices)</code>","text":"<p>Compute indices at which a field is entered or exited.</p> <p>Parameters:</p> Name Type Description Default <code>field_indices</code> <code>ndarray</code> <p>1D array, typically obtained with in_field.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Indices at which fields are entered and exited.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def compute_crossings(field_indices: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute indices at which a field is entered or exited.\n\n    Parameters\n    ----------\n    field_indices : np.ndarray\n        1D array, typically obtained with in_field.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        Indices at which fields are entered and exited.\n    \"\"\"\n    # make sure to start and end outside fields\n    field_indices = np.concatenate(([0], field_indices.astype(bool).astype(int), [0]))\n    (enter,) = np.where(np.diff(field_indices) == 1)\n    (exit,) = np.where(np.diff(field_indices) == -1)\n    assert len(enter) == len(exit), (len(enter), len(exit))\n    return enter, exit\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.compute_linear_place_fields","title":"<code>compute_linear_place_fields(firing_rate, min_window_size=5, min_firing_rate=1.0, thresh=0.5)</code>","text":"<p>Find consecutive bins where all are &gt;= threshold of local max firing rate and the local max is &gt; min_firing_rate.</p> <p>Parameters:</p> Name Type Description Default <code>firing_rate</code> <code>ndarray</code> <p>Array of firing rates.</p> required <code>min_window_size</code> <code>int</code> <p>Minimum size of the window. Default is 5.</p> <code>5</code> <code>min_firing_rate</code> <code>float</code> <p>Minimum firing rate to consider a bin. Default is 1.0.</p> <code>1.0</code> <code>thresh</code> <code>float</code> <p>Threshold percentage of local max. Default is 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean array indicating place fields.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def compute_linear_place_fields(\n    firing_rate: np.ndarray,\n    min_window_size: int = 5,\n    min_firing_rate: float = 1.0,\n    thresh: float = 0.5\n) -&gt; np.ndarray:\n    \"\"\"\n    Find consecutive bins where all are &gt;= threshold of local max firing rate\n    and the local max is &gt; min_firing_rate.\n\n    Parameters\n    ----------\n    firing_rate : np.ndarray\n        Array of firing rates.\n    min_window_size : int, optional\n        Minimum size of the window. Default is 5.\n    min_firing_rate : float, optional\n        Minimum firing rate to consider a bin. Default is 1.0.\n    thresh : float, optional\n        Threshold percentage of local max. Default is 0.5.\n\n    Returns\n    -------\n    np.ndarray\n        Boolean array indicating place fields.\n    \"\"\"\n    is_place_field = np.zeros(len(firing_rate), dtype=\"bool\")\n    for start in range(len(firing_rate) - min_window_size):\n        for fin in range(start + min_window_size, len(firing_rate)):\n            window = firing_rate[start:fin]\n            mm = max(window)\n            if mm &gt; min_firing_rate and all(window &gt; thresh * mm):\n                is_place_field[start:fin] = True\n            else:\n                break\n\n    return is_place_field\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.consecutive","title":"<code>consecutive(array, stepsize)</code>","text":"<p>Splits array when distance between neighboring points is further than the stepsize.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Array to be split.</p> required <code>stepsize</code> <code>float</code> <p>Minimum distance to consider points as separate.</p> required <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List of arrays, split when jump greater than stepsize.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def consecutive(array: np.ndarray, stepsize: float) -&gt; List[np.ndarray]:\n    \"\"\"\n    Splits array when distance between neighboring points is further than the stepsize.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Array to be split.\n    stepsize : float\n        Minimum distance to consider points as separate.\n\n    Returns\n    -------\n    List[np.ndarray]\n        List of arrays, split when jump greater than stepsize.\n    \"\"\"\n    return np.split(array, np.where(np.diff(array) &gt; stepsize)[0] + 1)\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.detect_firing_fields","title":"<code>detect_firing_fields(image_gray, max_sigma=30, log_num_sigma=10, log_thres=0.1, dog_thres=0.1, doh_thres=0.01)</code>","text":"<p>Detect firing fields in a grayscale image using different blob detection methods.</p> <p>Parameters:</p> Name Type Description Default <code>image_gray</code> <code>ndarray</code> <p>Grayscale image to analyze.</p> required <code>max_sigma</code> <code>int</code> <p>The maximum standard deviation for Gaussian filter.</p> <code>30</code> <code>log_num_sigma</code> <code>int</code> <p>The number of sigma values for the Laplacian of Gaussian.</p> <code>10</code> <code>log_thres</code> <code>float</code> <p>Threshold for Laplacian of Gaussian blobs.</p> <code>0.1</code> <code>dog_thres</code> <code>float</code> <p>Threshold for Difference of Gaussian blobs.</p> <code>0.1</code> <code>doh_thres</code> <code>float</code> <p>Threshold for Determinant of Hessian blobs.</p> <code>0.01</code> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def detect_firing_fields(\n    image_gray: np.ndarray,\n    max_sigma: int = 30,\n    log_num_sigma: int = 10,\n    log_thres: float = 0.1,\n    dog_thres: float = 0.1,\n    doh_thres: float = 0.01,\n) -&gt; None:\n    \"\"\"\n    Detect firing fields in a grayscale image using different blob detection methods.\n\n    Parameters\n    ----------\n    image_gray : np.ndarray\n        Grayscale image to analyze.\n    max_sigma : int, optional\n        The maximum standard deviation for Gaussian filter.\n    log_num_sigma : int, optional\n        The number of sigma values for the Laplacian of Gaussian.\n    log_thres : float, optional\n        Threshold for Laplacian of Gaussian blobs.\n    dog_thres : float, optional\n        Threshold for Difference of Gaussian blobs.\n    doh_thres : float, optional\n        Threshold for Determinant of Hessian blobs.\n    \"\"\"\n    from skimage.feature import blob_dog, blob_doh, blob_log\n    plt.imshow(image_gray, origin=\"lower\")\n\n    blobs_log = blob_log(\n        image_gray, max_sigma=max_sigma, num_sigma=log_num_sigma, threshold=log_thres\n    )\n    # Compute radii in the 3rd column.\n    blobs_log[:, 2] = blobs_log[:, 2] * sqrt(2)\n\n    blobs_dog = blob_dog(image_gray, max_sigma=max_sigma, threshold=dog_thres)\n    blobs_dog[:, 2] = blobs_dog[:, 2] * sqrt(2)\n\n    blobs_doh = blob_doh(image_gray, max_sigma=max_sigma, threshold=doh_thres)\n\n    blobs_list = [blobs_log, blobs_dog, blobs_doh]\n    colors = [\"yellow\", \"lime\", \"red\"]\n    titles = [\n        \"Laplacian of Gaussian\",\n        \"Difference of Gaussian\",\n        \"Determinant of Hessian\",\n    ]\n    sequence = zip(blobs_list, colors, titles)\n\n    fig, axes = plt.subplots(1, 3, figsize=(9, 3), sharex=True, sharey=True)\n    ax = axes.ravel()\n\n    for idx, (blobs, color, title) in enumerate(sequence):\n        ax[idx].set_title(title)\n        ax[idx].imshow(image_gray, interpolation=\"nearest\", origin=\"lower\")\n        for blob in blobs:\n            y, x, r = blob\n            c = plt.Circle((x, y), r, color=color, linewidth=2, fill=False)\n            ax[idx].add_patch(c)\n        ax[idx].set_axis_off()\n\n    plt.tight_layout()\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.distance_to_edge_function","title":"<code>distance_to_edge_function(x_c, y_c, field, box_size, interpolation='linear')</code>","text":"<p>Returns a function which, for a given angle, returns the distance to the edge of the field from the center.</p> <p>Parameters:</p> Name Type Description Default <code>x_c</code> <code>float</code> <p>X-coordinate of the center.</p> required <code>y_c</code> <code>float</code> <p>Y-coordinate of the center.</p> required <code>field</code> <code>ndarray</code> <p>2D array with ones at field bins and zeros elsewhere.</p> required <code>box_size</code> <code>Tuple[float, float]</code> <p>Size of the box (arena).</p> required <code>interpolation</code> <code>str</code> <p>Type of interpolation to use. Default is \"linear\".</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>Callable[[float], float]</code> <p>A function that takes an angle and returns the distance to the edge of the field.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def distance_to_edge_function(\n    x_c: float,\n    y_c: float,\n    field: np.ndarray,\n    box_size: Tuple[float, float],\n    interpolation: str = \"linear\"\n) -&gt; Callable[[float], float]:\n    \"\"\"\n    Returns a function which, for a given angle, returns the distance to\n    the edge of the field from the center.\n\n    Parameters\n    ----------\n    x_c : float\n        X-coordinate of the center.\n    y_c : float\n        Y-coordinate of the center.\n    field : np.ndarray\n        2D array with ones at field bins and zeros elsewhere.\n    box_size : Tuple[float, float]\n        Size of the box (arena).\n    interpolation : str, optional\n        Type of interpolation to use. Default is \"linear\".\n\n    Returns\n    -------\n    Callable[[float], float]\n        A function that takes an angle and returns the distance to the edge of the field.\n    \"\"\"\n    from skimage import measure\n\n    contours = measure.find_contours(field, 0.8)\n\n    box_dim = np.array(box_size)\n    edge_x, edge_y = (contours[0] * box_dim / (np.array(field.shape) - (1, 1))).T\n\n    # # angle between 0 and 2\\pi\n    angles = np.arctan2((edge_y - y_c), (edge_x - x_c)) % (2 * np.pi)\n    a_sort = np.argsort(angles)\n    angles = angles[a_sort]\n    edge_x = edge_x[a_sort]\n    edge_y = edge_y[a_sort]\n\n    # # Fill in edge values for the interpolation\n    pad_a = np.pad(angles, 2, mode=\"linear_ramp\", end_values=(0, 2 * np.pi))\n    ev_x = (edge_x[0] + edge_x[-1]) / 2\n    pad_x = np.pad(edge_x, 2, mode=\"linear_ramp\", end_values=ev_x)\n    ev_y = (edge_y[0] + edge_y[-1]) / 2\n    pad_y = np.pad(edge_y, 2, mode=\"linear_ramp\", end_values=ev_y)\n\n    if interpolation == \"cubic\":\n        mask = np.where(np.diff(pad_a) == 0)\n        pad_a = np.delete(pad_a, mask)\n        pad_x = np.delete(pad_x, mask)\n        pad_y = np.delete(pad_y, mask)\n\n    x_func = interp1d(pad_a, pad_x, kind=interpolation)\n    y_func = interp1d(pad_a, pad_y, kind=interpolation)\n\n    def dist_func(angle: float) -&gt; float:\n        \"\"\"\n        Computes the distance from the center to the edge of the field at a given angle.\n\n        Parameters\n        ----------\n        angle : float\n            Angle in radians.\n\n        Returns\n        -------\n        float\n            Distance to the edge of the field from the center.\n        \"\"\"\n        x = x_func(angle)\n        y = y_func(angle)\n        dist = np.sqrt((x - x_c) ** 2 + (y - y_c) ** 2)\n        return dist\n\n    return dist_func\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.find_field","title":"<code>find_field(firing_rate, threshold)</code>","text":"<p>Find the field in the firing rate that exceeds the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>firing_rate</code> <code>ndarray</code> <p>Array of firing rates.</p> required <code>threshold</code> <code>float</code> <p>Threshold for detection.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple containing the image label and the same label.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def find_field(\n    firing_rate: np.ndarray,\n    threshold: float\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Find the field in the firing rate that exceeds the threshold.\n\n    Parameters\n    ----------\n    firing_rate : np.ndarray\n        Array of firing rates.\n    threshold : float\n        Threshold for detection.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        Tuple containing the image label and the same label.\n    \"\"\"\n    mm = np.max(firing_rate)\n\n    labeled_image, num_labels = label(firing_rate &gt; threshold)\n    for i in range(1, num_labels + 1):\n        image_label = labeled_image == i\n        if mm in firing_rate[image_label]:\n            return image_label, image_label\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.find_field2","title":"<code>find_field2(firing_rate, thresh)</code>","text":"<p>Find the field in a 1D firing rate array that exceeds the threshold.</p> <p>Parameters:</p> Name Type Description Default <code>firing_rate</code> <code>ndarray</code> <p>1D array of firing rates.</p> required <code>thresh</code> <code>float</code> <p>Threshold for detection.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple containing two boolean arrays:  the first indicates the buffer area and the second indicates the field.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def find_field2(\n    firing_rate: np.ndarray,\n    thresh: float\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Find the field in a 1D firing rate array that exceeds the threshold.\n\n    Parameters\n    ----------\n    firing_rate : np.ndarray\n        1D array of firing rates.\n    thresh : float\n        Threshold for detection.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        Tuple containing two boolean arrays: \n        the first indicates the buffer area and the second indicates the field.\n    \"\"\"\n    firing_rate = np.array(firing_rate)\n    imm = np.argmax(firing_rate)\n    mm = np.max(firing_rate)\n    # TODO: make more efficient by using argmax instead of where()[0]\n    first = np.where(np.diff(firing_rate[:imm]) &lt; 0)[0]\n    if len(first) == 0:\n        first = 0\n    else:\n        first = first[-1] + 2\n\n    last = np.where(np.diff(firing_rate[imm:]) &gt; 0)[0]\n\n    if len(last) == 0:\n        last = len(firing_rate)\n    else:\n        last = last[0] + imm + 1\n    field_buffer = np.zeros(firing_rate.shape, dtype=\"bool\")\n    field_buffer[first:last] = True\n    field = field_buffer &amp; (firing_rate &gt; thresh * mm)\n\n    return field_buffer, field\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.find_fields_1d","title":"<code>find_fields_1d(tuning, hz_thresh=5, min_length=1, max_length=20, max_mean_firing=10)</code>","text":"<p>Finds the location of maximum spiking.</p> <p>Parameters:</p> Name Type Description Default <code>tuning</code> <code>List[ndarray]</code> <p>Each inner array contains the tuning curves for an individual neuron.</p> required <code>hz_thresh</code> <code>float</code> <p>Any bin with firing above this value is considered to be part of a field. Default is 5.</p> <code>5</code> <code>min_length</code> <code>int</code> <p>Minimum length of field (in tuning curve bin units). Default is 1.</p> <code>1</code> <code>max_length</code> <code>int</code> <p>Maximum length of field (in tuning curve bin units). Default is 20.</p> <code>20</code> <code>max_mean_firing</code> <code>float</code> <p>Only neurons with a mean firing rate less than this amount are considered for having place fields. Default is 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>dict</code> <p>Where the key is the neuron number (int), and the value is a list of arrays (int) that are indices into the tuning curve where the field occurs. Each inner array contains the indices for a given place field.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def find_fields_1d(\n    tuning: List[np.ndarray],\n    hz_thresh: float = 5,\n    min_length: int = 1,\n    max_length: int = 20,\n    max_mean_firing: float = 10\n) -&gt; dict:\n    \"\"\"\n    Finds the location of maximum spiking.\n\n    Parameters\n    ----------\n    tuning : List[np.ndarray]\n        Each inner array contains the tuning curves for an individual neuron.\n    hz_thresh : float, optional\n        Any bin with firing above this value is considered to be part of a field. Default is 5.\n    min_length : int, optional\n        Minimum length of field (in tuning curve bin units). Default is 1.\n    max_length : int, optional\n        Maximum length of field (in tuning curve bin units). Default is 20.\n    max_mean_firing : float, optional\n        Only neurons with a mean firing rate less than this amount are considered for\n        having place fields. Default is 10.\n\n    Returns\n    -------\n    dict\n        Where the key is the neuron number (int), and the value is a list of arrays (int)\n        that are indices into the tuning curve where the field occurs.\n        Each inner array contains the indices for a given place field.\n    \"\"\"\n    fields = []\n    for neuron_tc in tuning:\n        if np.mean(neuron_tc) &lt; max_mean_firing:\n            neuron_field = np.zeros(neuron_tc.shape[0])\n            for i, this_bin in enumerate(neuron_tc):\n                if this_bin &gt; hz_thresh:\n                    neuron_field[i] = 1\n            fields.append(neuron_field)\n        else:\n            fields.append(np.array([]))\n\n    fields_idx = dict()\n    for i, neuron_fields in enumerate(fields):\n        field_idx = np.nonzero(neuron_fields)[0]\n        fields_idx[i] = consecutive(field_idx, stepsize=1)\n\n    with_fields = dict()\n    for key in fields_idx:\n        for field in fields_idx[key]:\n            if len(field) &gt; max_length:\n                continue\n            elif min_length &lt;= len(field):\n                with_fields[key] = fields_idx[key]\n                continue\n    return with_fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.find_peaks","title":"<code>find_peaks(image)</code>","text":"<p>Find peaks sorted by distance from the center of the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Coordinates for peaks in the image as [row, column].</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def find_peaks(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Find peaks sorted by distance from the center of the image.\n\n    Parameters\n    ----------\n    image : np.ndarray\n        The input image.\n\n    Returns\n    -------\n    np.ndarray\n        Coordinates for peaks in the image as [row, column].\n    \"\"\"\n    image = image.copy()\n    image[~np.isfinite(image)] = 0\n    image_max = filters.maximum_filter(image, 3)\n    is_maxima = image == image_max\n    labels, num_objects = ndimage.label(is_maxima)\n    indices = np.arange(1, num_objects + 1)\n    peaks = ndimage.maximum_position(image, labels=labels, index=indices)\n    peaks = np.array(peaks)\n    center = (np.array(image.shape) - 1) / 2\n    distances = np.linalg.norm(peaks - center, axis=1)\n    peaks = peaks[distances.argsort()]\n    return peaks\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.map_pass_to_unit_circle","title":"<code>map_pass_to_unit_circle(x, y, t, x_c, y_c, field=None, box_size=None, dist_func=None)</code>","text":"<p>Uses three vectors {v, p, q} to map the passes to the unit circle. v is the average velocity vector of the pass, p is the vector from the position (x, y) to the center of the field, and q is the vector from the center to the edge through (x, y).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>X-coordinates.</p> required <code>y</code> <code>ndarray</code> <p>Y-coordinates.</p> required <code>t</code> <code>ndarray</code> <p>Time data.</p> required <code>x_c</code> <code>float</code> <p>X-coordinate of the center of the field.</p> required <code>y_c</code> <code>float</code> <p>Y-coordinate of the center of the field.</p> required <code>field</code> <code>Optional[ndarray]</code> <p>2D array indicating the location of the field.</p> <code>None</code> <code>box_size</code> <code>Optional[Tuple[float, float]]</code> <p>Size of the box (arena).</p> <code>None</code> <code>dist_func</code> <code>Optional[Callable[[float], float]]</code> <p>Function that computes distance to bump edge from center. Default is distance_to_edge_function with linear interpolation.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray, ndarray]</code> <p>r : Array of distances to origin on unit circle. theta : Array of angles to axis defined by mean velocity vector. pdcd : Array of distances to peak projected onto the current direction. pdmd : Array of distances to peak projected onto the mean direction.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If neither dist_func nor both field and box_size are provided.</p> References: <p>.. [1] Jeewajee A, Barry C, Douchamps V, Manson D, Lever C, Burgess N. Theta        phase precession of grid and place cell firing in open environments.        Philos Trans R Soc Lond B Biol Sci. 2013 Dec 23;369(1635):20120532</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def map_pass_to_unit_circle(\n    x: np.ndarray,\n    y: np.ndarray,\n    t: np.ndarray,\n    x_c: float,\n    y_c: float,\n    field: Optional[np.ndarray] = None,\n    box_size: Optional[Tuple[float, float]] = None,\n    dist_func: Optional[Callable[[float], float]] = None\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Uses three vectors {v, p, q} to map the passes to the unit circle. v\n    is the average velocity vector of the pass, p is the vector from the\n    position (x, y) to the center of the field, and q is the vector from the\n    center to the edge through (x, y).\n\n    Parameters\n    ----------\n    x : np.ndarray\n        X-coordinates.\n    y : np.ndarray\n        Y-coordinates.\n    t : np.ndarray\n        Time data.\n    x_c : float\n        X-coordinate of the center of the field.\n    y_c : float\n        Y-coordinate of the center of the field.\n    field : Optional[np.ndarray], optional\n        2D array indicating the location of the field.\n    box_size : Optional[Tuple[float, float]], optional\n        Size of the box (arena).\n    dist_func : Optional[Callable[[float], float]], optional\n        Function that computes distance to bump edge from center. Default is\n        distance_to_edge_function with linear interpolation.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n        r : Array of distances to origin on unit circle.\n        theta : Array of angles to axis defined by mean velocity vector.\n        pdcd : Array of distances to peak projected onto the current direction.\n        pdmd : Array of distances to peak projected onto the mean direction.\n\n    Raises\n    ------\n    AssertionError\n        If neither dist_func nor both field and box_size are provided.\n\n    References:\n    -----------\n    .. [1] Jeewajee A, Barry C, Douchamps V, Manson D, Lever C, Burgess N. Theta\n           phase precession of grid and place cell firing in open environments.\n           Philos Trans R Soc Lond B Biol Sci. 2013 Dec 23;369(1635):20120532\n    \"\"\"\n    if dist_func is None:\n        assert (\n            field is not None and box_size is not None\n        ), 'either provide \"dist_func\" or \"field\" and \"box_size\"'\n        dist_func = distance_to_edge_function(\n            x_c, y_c, field, box_size, interpolation=\"linear\"\n        )\n    pos = np.array((x, y))\n\n    # vector from pos to center p\n    p_vec = ((x_c, y_c) - pos.T).T\n    # angle between x-axis and negative vector p\n    angle = (np.arctan2(p_vec[1], p_vec[0]) + np.pi) % (2 * np.pi)\n    # distance from center to edge at each angle\n    q = dist_func(angle)\n    # distance from center to pos\n    p = np.linalg.norm(p_vec, axis=0)\n    # r-coordinate on unit circle\n    r = p / q\n\n    dpos = np.gradient(pos, axis=1)\n    dt = np.gradient(t)\n    velocity = np.divide(dpos, dt)\n\n    # mean velocity vector v\n    mean_velocity = np.average(velocity, axis=1)\n    # angle on unit circle, run is rotated such that mean velocity vector\n    # is toward positive x\n    theta = (angle - np.arctan2(mean_velocity[1], mean_velocity[0])) % (2 * np.pi)\n\n    w_pdcd = angle - np.arctan2(velocity[1], velocity[0])\n    pdcd = r * np.cos(w_pdcd)\n\n    w_pdmd = angle - np.arctan2(mean_velocity[1], mean_velocity[0])\n    pdmd = r * np.cos(w_pdmd)\n    return r, theta, pdcd, pdmd\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.map_stats2","title":"<code>map_stats2(firing_rate, threshold=0.1, min_size=5, max_size=None, min_peak=1.0, sigma=None)</code>","text":"<p>Map statistics of firing rate fields.</p> <p>Parameters:</p> Name Type Description Default <code>firing_rate</code> <code>ndarray</code> <p>1D array of firing rates.</p> required <code>threshold</code> <code>float</code> <p>Threshold for field detection. Default is 0.1.</p> <code>0.1</code> <code>min_size</code> <code>int</code> <p>Minimum size of detected fields. Default is 5.</p> <code>5</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of detected fields. Default is None, which sets it to the length of firing_rate.</p> <code>None</code> <code>min_peak</code> <code>float</code> <p>Minimum peak firing rate to consider a field valid. Default is 1.0.</p> <code>1.0</code> <code>sigma</code> <code>Optional[float]</code> <p>Standard deviation for Gaussian smoothing. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, List[float]]</code> <p>A dictionary containing the sizes, peaks, means, and fields of detected firing rate fields.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def map_stats2(\n    firing_rate: np.ndarray,\n    threshold: float = 0.1,\n    min_size: int = 5,\n    max_size: Optional[int] = None,\n    min_peak: float = 1.0,\n    sigma: Optional[float] = None\n) -&gt; Dict[str, List[float]]:\n    \"\"\"\n    Map statistics of firing rate fields.\n\n    Parameters\n    ----------\n    firing_rate : np.ndarray\n        1D array of firing rates.\n    threshold : float, optional\n        Threshold for field detection. Default is 0.1.\n    min_size : int, optional\n        Minimum size of detected fields. Default is 5.\n    max_size : Optional[int], optional\n        Maximum size of detected fields. Default is None, which sets it to the length of firing_rate.\n    min_peak : float, optional\n        Minimum peak firing rate to consider a field valid. Default is 1.0.\n    sigma : Optional[float], optional\n        Standard deviation for Gaussian smoothing. Default is None.\n\n    Returns\n    -------\n    Dict[str, List[float]]\n        A dictionary containing the sizes, peaks, means, and fields of detected firing rate fields.\n    \"\"\"\n    if sigma is not None:\n        firing_rate = gaussian_filter1d(firing_rate, sigma)\n\n    if max_size is None:\n        max_size = len(firing_rate)\n\n    firing_rate = firing_rate.copy()\n    firing_rate = firing_rate - np.min(firing_rate)\n    out = dict(sizes=list(), peaks=list(), means=list())\n    out[\"fields\"] = np.zeros(firing_rate.shape)\n    field_counter = 1\n    while True:\n        peak = np.max(firing_rate)\n        if peak &lt; min_peak:\n            break\n        field_buffer, field = find_field(firing_rate, threshold)\n        field_size = np.sum(field)\n        if (\n            (field_size &gt; min_size)\n            and (field_size &lt; max_size)\n            and (np.max(firing_rate[field]) &gt; (2 * np.min(firing_rate[field_buffer])))\n        ):\n            out[\"fields\"][field] = field_counter\n            out[\"sizes\"].append(float(field_size) / len(firing_rate))\n            out[\"peaks\"].append(peak)\n            out[\"means\"].append(np.mean(firing_rate[field]))\n            field_counter += 1\n        firing_rate[field_buffer] = 0\n\n    return out\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.remove_fields_by_area","title":"<code>remove_fields_by_area(fields, minimum_field_area, maximum_field_area=None)</code>","text":"<p>Sets fields below minimum area to zero, measured as the number of bins in a field.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>ndarray</code> <p>The fields.</p> required <code>minimum_field_area</code> <code>int</code> <p>Minimum field area (number of bins in a field).</p> required <code>maximum_field_area</code> <code>Optional[int]</code> <p>Maximum field area (number of bins in a field). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Fields with number of bins below minimum_field_area are set to zero.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If minimum_field_area is not an integer.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def remove_fields_by_area(\n    fields: np.ndarray,\n    minimum_field_area: int,\n    maximum_field_area: Optional[int] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Sets fields below minimum area to zero, measured as the number of bins in a field.\n\n    Parameters\n    ----------\n    fields : np.ndarray\n        The fields.\n    minimum_field_area : int\n        Minimum field area (number of bins in a field).\n    maximum_field_area : Optional[int]\n        Maximum field area (number of bins in a field). Default is None.\n\n    Returns\n    -------\n    np.ndarray\n        Fields with number of bins below minimum_field_area are set to zero.\n\n    Raises\n    ------\n    ValueError\n        If minimum_field_area is not an integer.\n    \"\"\"\n    if not isinstance(minimum_field_area, (int, np.integer)):\n        raise ValueError(\"'minimum_field_area' should be int\")\n\n    if maximum_field_area is None:\n        maximum_field_area = len(fields.flatten())\n    ## variant\n    # fields_areas = scipy.ndimage.measurements.sum(\n    #     np.ones_like(fields), fields, index=np.arange(fields.max() + 1))\n    # fields_area = fields_areas[fields]\n    # fields[fields_area &lt; minimum_field_area] = 0\n\n    labels, counts = np.unique(fields, return_counts=True)\n    for lab, count in zip(labels, counts):\n        if lab != 0:\n            if (count &lt; minimum_field_area) | (count &gt; maximum_field_area):\n                fields[fields == lab] = 0\n    return fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.separate_fields_by_dilation","title":"<code>separate_fields_by_dilation(rate_map, seed=2.5, sigma=2.5, minimum_field_area=None)</code>","text":"<p>Separates fields by the Laplace of Gaussian (LoG) on the rate map subtracted by a reconstruction of the rate map using dilation.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>2D array representing firing rate in each bin.</p> required <code>seed</code> <code>float</code> <p>Magnitude of dilation.</p> <code>2.5</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian to separate fields. Default is 2.5.</p> <code>2.5</code> <code>minimum_field_area</code> <code>Optional[int]</code> <p>Minimum number of bins to consider it a field. Default is None (all fields are kept).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Labels with areas filled with the same value, corresponding to fields in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the field (sum of all field values) with 0 elsewhere.</p> References <p>.. [1] https://scikit-image.org/docs/stable/auto_examples/color_exposure/plot_regional_maxima.html</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def separate_fields_by_dilation(\n    rate_map: np.ndarray,\n    seed: float = 2.5,\n    sigma: float = 2.5,\n    minimum_field_area: Optional[int] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Separates fields by the Laplace of Gaussian (LoG)\n    on the rate map subtracted by a reconstruction of the rate map using\n    dilation.\n\n    Parameters\n    ----------\n    rate_map : np.ndarray\n        2D array representing firing rate in each bin.\n    seed : float\n        Magnitude of dilation.\n    sigma : float\n        Standard deviation of Gaussian to separate fields. Default is 2.5.\n    minimum_field_area : Optional[int]\n        Minimum number of bins to consider it a field. Default is None (all fields are kept).\n\n    Returns\n    -------\n    np.ndarray\n        Labels with areas filled with the same value, corresponding to fields\n        in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the\n        field (sum of all field values) with 0 elsewhere.\n\n    References\n    ----------\n    .. [1] https://scikit-image.org/docs/stable/auto_examples/color_exposure/plot_regional_maxima.html\n    \"\"\"\n    from skimage.morphology import reconstruction\n\n    rate_map_norm = (rate_map - rate_map.mean()) / rate_map.std()\n    dilated = reconstruction(rate_map_norm - seed, rate_map_norm, method=\"dilation\")\n    rate_map_reconstructed = rate_map_norm - dilated\n\n    laplacian = ndimage.gaussian_laplace(rate_map_reconstructed, sigma)\n    laplacian[laplacian &gt; 0] = 0\n    fields, _ = ndimage.label(laplacian)\n    fields = sort_fields_by_rate(rate_map, fields)\n    if minimum_field_area is not None:\n        fields = remove_fields_by_area(fields, minimum_field_area)\n    return fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.separate_fields_by_laplace","title":"<code>separate_fields_by_laplace(rate_map, threshold=0, minimum_field_area=None)</code>","text":"<p>Separates fields using the Laplacian to identify fields separated by a negative second derivative.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>2D array representing firing rate in each bin.</p> required <code>threshold</code> <code>float</code> <p>Value of Laplacian to separate fields by relative to the minima. Should be on the interval 0 to 1, where 0 cuts off at 0 and 1 cuts off at min(laplace(rate_map)). Default is 0.</p> <code>0</code> <code>minimum_field_area</code> <code>Optional[int]</code> <p>Minimum number of bins to consider it a field. Default is None (all fields are kept).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Labels with areas filled with the same value, corresponding to fields in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the field (sum of all field values) with 0 elsewhere.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def separate_fields_by_laplace(\n    rate_map: np.ndarray,\n    threshold: float = 0,\n    minimum_field_area: Optional[int] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Separates fields using the Laplacian to identify fields separated by\n    a negative second derivative.\n\n    Parameters\n    ----------\n    rate_map : np.ndarray\n        2D array representing firing rate in each bin.\n    threshold : float\n        Value of Laplacian to separate fields by relative to the minima.\n        Should be on the interval 0 to 1, where 0 cuts off at 0 and\n        1 cuts off at min(laplace(rate_map)). Default is 0.\n    minimum_field_area : Optional[int]\n        Minimum number of bins to consider it a field. Default is None (all fields are kept).\n\n    Returns\n    -------\n    np.ndarray\n        Labels with areas filled with the same value, corresponding to fields\n        in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the\n        field (sum of all field values) with 0 elsewhere.\n    \"\"\"\n\n    laplacian = ndimage.laplace(rate_map)\n\n    laplacian[laplacian &gt; threshold * np.min(laplacian)] = 0\n\n    # Labels areas of the laplacian not connected by values &gt; 0.\n    fields, _ = ndimage.label(laplacian)\n    fields = sort_fields_by_rate(rate_map, fields)\n    if minimum_field_area is not None:\n        fields = remove_fields_by_area(fields, minimum_field_area)\n    return fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.separate_fields_by_laplace_of_gaussian","title":"<code>separate_fields_by_laplace_of_gaussian(rate_map, sigma=2, minimum_field_area=None)</code>","text":"<p>Separates fields using the Laplace of Gaussian (LoG) to identify fields separated by a negative second derivative. Works best if no smoothing is applied to the rate map, preferably with interpolated NaNs.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>2D array representing firing rate in each bin.</p> required <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian to separate fields. Default is 2.</p> <code>2</code> <code>minimum_field_area</code> <code>Optional[int]</code> <p>Minimum number of bins to consider it a field. Default is None (all fields are kept).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Labels with areas filled with the same value, corresponding to fields in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the field (sum of all field values) with 0 elsewhere.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def separate_fields_by_laplace_of_gaussian(\n    rate_map: np.ndarray,\n    sigma: float = 2,\n    minimum_field_area: Optional[int] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Separates fields using the Laplace of Gaussian (LoG) to identify fields\n    separated by a negative second derivative. Works best if no smoothing is\n    applied to the rate map, preferably with interpolated NaNs.\n\n    Parameters\n    ----------\n    rate_map : np.ndarray\n        2D array representing firing rate in each bin.\n    sigma : float\n        Standard deviation of Gaussian to separate fields. Default is 2.\n    minimum_field_area : Optional[int]\n        Minimum number of bins to consider it a field. Default is None (all fields are kept).\n\n    Returns\n    -------\n    np.ndarray\n        Labels with areas filled with the same value, corresponding to fields\n        in rate_map. The fill values are in range(1, nFields + 1), sorted by size of the\n        field (sum of all field values) with 0 elsewhere.\n    \"\"\"\n    laplacian = ndimage.gaussian_laplace(rate_map, sigma)\n    laplacian[laplacian &gt; 0] = 0\n\n    # Labels areas of the laplacian not connected by values &gt; 0.\n    fields, _ = ndimage.label(laplacian)\n\n    fields = sort_fields_by_rate(rate_map, fields)\n    if minimum_field_area is not None:\n        fields = remove_fields_by_area(fields, minimum_field_area)\n    return fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.sort_fields_by_rate","title":"<code>sort_fields_by_rate(rate_map, fields, func=None)</code>","text":"<p>Sort fields by the rate value of each field.</p> <p>Parameters:</p> Name Type Description Default <code>rate_map</code> <code>ndarray</code> <p>The rate map.</p> required <code>fields</code> <code>ndarray</code> <p>The fields of the same shape as rate_map.</p> required <code>func</code> <code>Callable</code> <p>Function returning value to sort after, default is np.max.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Sorted fields.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def sort_fields_by_rate(\n    rate_map: np.ndarray,\n    fields: np.ndarray,\n    func: Optional[Callable[[np.ndarray], Any]] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Sort fields by the rate value of each field.\n\n    Parameters\n    ----------\n    rate_map : np.ndarray\n        The rate map.\n    fields : np.ndarray\n        The fields of the same shape as rate_map.\n    func : Callable, optional\n        Function returning value to sort after, default is np.max.\n\n    Returns\n    -------\n    np.ndarray\n        Sorted fields.\n    \"\"\"\n    indx = np.sort(np.unique(fields.ravel()))\n    func = func or np.max\n    # Sort by largest peak\n    rate_means = ndimage.labeled_comprehension(\n        rate_map, fields, indx, func, np.float64, 0\n    )\n    sort = np.argsort(rate_means)[::-1]\n\n    sorted_fields = np.zeros_like(fields)\n    for indx_i, indx_ in enumerate(indx[sort]):\n        if indx_ == 0:\n            continue\n        sorted_fields[fields == indx_] = np.max(sorted_fields) + 1\n\n    # new rate map with fields &gt; min_size, sorted\n    # sorted_fields = np.zeros_like(fields)\n    # for i in range(indx.max() + 1):\n    #     sorted_fields[fields == sort[i] + 1] = i + 1\n\n    return sorted_fields\n</code></pre>"},{"location":"reference/neuro_py/tuning/fields/#neuro_py.tuning.fields.which_field","title":"<code>which_field(x, y, fields, box_size)</code>","text":"<p>Returns which spatial field each (x, y) position is in.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>X-coordinates.</p> required <code>y</code> <code>ndarray</code> <p>Y-coordinates, must have the same length as x.</p> required <code>fields</code> <code>ndarray</code> <p>Labeled fields, where each field is defined by an area separated by zeros. The fields are labeled with indices from [1:].</p> required <code>box_size</code> <code>List[float]</code> <p>Extents of the arena.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array-like x and y with fields-labeled indices.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If x and y do not have the same length.</p> Source code in <code>neuro_py/tuning/fields.py</code> <pre><code>def which_field(\n    x: np.ndarray,\n    y: np.ndarray,\n    fields: np.ndarray,\n    box_size: List[float]\n) -&gt; np.ndarray:\n    \"\"\"\n    Returns which spatial field each (x, y) position is in.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        X-coordinates.\n    y : np.ndarray\n        Y-coordinates, must have the same length as x.\n    fields : np.ndarray\n        Labeled fields, where each field is defined by an area separated by\n        zeros. The fields are labeled with indices from [1:].\n    box_size : List[float]\n        Extents of the arena.\n\n    Returns\n    -------\n    np.ndarray\n        Array-like x and y with fields-labeled indices.\n\n    Raises\n    ------\n    ValueError\n        If x and y do not have the same length.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"x and y must have same length\")\n\n    sx, sy = fields.shape\n    # bin sizes\n    dx = box_size[0] / sx\n    dy = box_size[1] / sy\n    x_bins = dx + np.arange(0, box_size[0] + dx, dx)\n    y_bins = dy + np.arange(0, box_size[1] + dx, dy)\n    # x_bins = np.arange(0, box_size[0] + dx, dx)\n    # y_bins = np.arange(0, box_size[1] + dx, dy)\n    ix = np.digitize(x, x_bins)\n    iy = np.digitize(y, y_bins)\n\n    # fix for boundaries:\n    ix[ix == sx] = sx - 1\n    iy[iy == sy] = sy - 1\n    return np.array(fields[ix, iy])\n</code></pre>"},{"location":"reference/neuro_py/tuning/maps/","title":"neuro_py.tuning.maps","text":""},{"location":"reference/neuro_py/tuning/maps/#neuro_py.tuning.maps.SpatialMap","title":"<code>SpatialMap</code>","text":"<p>               Bases: <code>object</code></p> <p>SpatialMap: make a spatial map tuning curve     maps timestamps or continuous signals onto positions</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>object</code> <p>Position data (nelpy.AnalogSignal or nel.PositionArray).</p> required <code>st</code> <code>object</code> <p>Spike train data (nelpy.SpikeTrain or nelpy.AnalogSignal).</p> required <code>speed</code> <code>Optional[object]</code> <p>Speed data (nelpy.AnalogSignal), recommended input: from non-epoched data.</p> <code>None</code> <code>dim</code> <code>Optional[int]</code> <p>Dimension of the map (1 or 2) deprecated.</p> <code>None</code> <code>dir_epoch</code> <code>Optional[object]</code> <p>Epochs of the running direction, for linear data (nelpy.Epoch) deprecated.</p> <code>None</code> <code>speed_thres</code> <code>Union[int, float]</code> <p>Speed threshold for running. Default is 4.</p> <code>4</code> <code>s_binsize</code> <code>Union[int, float]</code> <p>Bin size for the spatial map. Default is 3.</p> <code>3</code> <code>x_minmax</code> <code>Optional[List[Union[int, float]]]</code> <p>Min and max x values for the spatial map.</p> <code>None</code> <code>y_minmax</code> <code>Optional[List[Union[int, float]]]</code> <p>Min and max y values for the spatial map.</p> <code>None</code> <code>tuning_curve_sigma</code> <code>Union[int, float]</code> <p>Sigma for the tuning curve. Default is 3.</p> <code>3</code> <code>smooth_mode</code> <code>str</code> <p>Mode for smoothing curve (str) reflect, constant, nearest, mirror, wrap. Default is \"reflect\".</p> <code>'reflect'</code> <code>min_duration</code> <code>float</code> <p>Minimum duration for a tuning curve. Default is 0.1.</p> <code>0.1</code> <code>minbgrate</code> <code>Union[int, float]</code> <p>Minimum firing rate for tuning curve; will set to this if lower. Default is 0.</p> <code>0</code> <code>n_shuff</code> <code>int</code> <p>Number of position shuffles for spatial information. Default is 500.</p> <code>500</code> <code>parallel_shuff</code> <code>bool</code> <p>Parallelize shuffling. Default is True.</p> <code>True</code> <code>place_field_thres</code> <code>Union[int, float]</code> <p>Percent of continuous region of peak firing rate. Default is 0.2.</p> <code>0.2</code> <code>place_field_min_size</code> <code>Optional[Union[int, float]]</code> <p>Minimum size of place field (cm).</p> <code>None</code> <code>place_field_max_size</code> <code>Optional[Union[int, float]]</code> <p>Maximum size of place field (cm).</p> <code>None</code> <code>place_field_min_peak</code> <code>Union[int, float]</code> <p>Minimum peak rate of place field. Default is 3.</p> <code>3</code> <code>place_field_sigma</code> <code>Union[int, float]</code> <p>Extra smoothing sigma to apply before field detection. Default is 2.</p> <code>2</code> <p>Attributes:</p> Name Type Description <code>tc</code> <code>TuningCurve</code> <p>Tuning curves.</p> <code>st_run</code> <code>SpikeTrain</code> <p>Spike train restricted to running epochs.</p> <code>bst_run</code> <code>binnedSpikeTrain</code> <p>Binned spike train restricted to running epochs.</p> <code>speed</code> <code>Optional[AnalogSignal]</code> <p>Speed data.</p> <code>run_epochs</code> <code>EpochArray</code> <p>Running epochs.</p> Notes <p>Place field detector (.find_fields()) is sensitive to many parameters. For 2D, it is highly recommended to have good environmental sampling. In brief testing with 300cm linear track, optimal 1D parameters were:     place_field_min_size=15     place_field_max_size=None     place_field_min_peak=3     place_field_sigma=None     place_field_thres=.33</p> TODO <p>Place field detector currently collects field width and peak rate for peak place field. In the future, these should be stored for all sub fields.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>class SpatialMap(object):\n    \"\"\"\n    SpatialMap: make a spatial map tuning curve\n        maps timestamps or continuous signals onto positions\n\n    Parameters\n    ----------\n    pos : object\n        Position data (nelpy.AnalogSignal or nel.PositionArray).\n    st : object\n        Spike train data (nelpy.SpikeTrain or nelpy.AnalogSignal).\n    speed : Optional[object]\n        Speed data (nelpy.AnalogSignal), recommended input: from non-epoched data.\n    dim : Optional[int]\n        Dimension of the map (1 or 2) *deprecated*.\n    dir_epoch : Optional[object]\n        Epochs of the running direction, for linear data (nelpy.Epoch) *deprecated*.\n    speed_thres : Union[int, float], optional\n        Speed threshold for running. Default is 4.\n    s_binsize : Union[int, float], optional\n        Bin size for the spatial map. Default is 3.\n    x_minmax : Optional[List[Union[int, float]]], optional\n        Min and max x values for the spatial map.\n    y_minmax : Optional[List[Union[int, float]]], optional\n        Min and max y values for the spatial map.\n    tuning_curve_sigma : Union[int, float], optional\n        Sigma for the tuning curve. Default is 3.\n    smooth_mode : str, optional\n        Mode for smoothing curve (str) reflect, constant, nearest, mirror, wrap. Default is \"reflect\".\n    min_duration : float, optional\n        Minimum duration for a tuning curve. Default is 0.1.\n    minbgrate : Union[int, float], optional\n        Minimum firing rate for tuning curve; will set to this if lower. Default is 0.\n    n_shuff : int, optional\n        Number of position shuffles for spatial information. Default is 500.\n    parallel_shuff : bool, optional\n        Parallelize shuffling. Default is True.\n    place_field_thres : Union[int, float], optional\n        Percent of continuous region of peak firing rate. Default is 0.2.\n    place_field_min_size : Optional[Union[int, float]]\n        Minimum size of place field (cm).\n    place_field_max_size : Optional[Union[int, float]]\n        Maximum size of place field (cm).\n    place_field_min_peak : Union[int, float], optional\n        Minimum peak rate of place field. Default is 3.\n    place_field_sigma : Union[int, float], optional\n        Extra smoothing sigma to apply before field detection. Default is 2.\n\n    Attributes\n    ----------\n    tc : nelpy.TuningCurve\n        Tuning curves.\n    st_run : nelpy.SpikeTrain\n        Spike train restricted to running epochs.\n    bst_run : nelpy.binnedSpikeTrain\n        Binned spike train restricted to running epochs.\n    speed : Optional[nnelpy.AnalogSignal]\n        Speed data.\n    run_epochs : nelpy.EpochArray\n        Running epochs.\n\n    Notes\n    -----\n    Place field detector (.find_fields()) is sensitive to many parameters.\n    For 2D, it is highly recommended to have good environmental sampling.\n    In brief testing with 300cm linear track, optimal 1D parameters were:\n        place_field_min_size=15\n        place_field_max_size=None\n        place_field_min_peak=3\n        place_field_sigma=None\n        place_field_thres=.33\n\n    TODO\n    ----\n    Place field detector currently collects field width and peak rate for peak place field.\n    In the future, these should be stored for all sub fields.\n    \"\"\"\n\n    def __init__(\n        self,\n        pos: object,\n        st: object,\n        speed: Optional[object] = None,\n        dim: Optional[int] = None,  # deprecated\n        dir_epoch: Optional[object] = None,  # deprecated\n        speed_thres: Union[int, float] = 4,\n        s_binsize: Union[int, float] = 3,\n        tuning_curve_sigma: Union[int, float] = 3,\n        x_minmax: Optional[List[Union[int, float]]] = None,\n        y_minmax: Optional[List[Union[int, float]]] = None,\n        smooth_mode: str = \"reflect\",\n        min_duration: float = 0.1,\n        minbgrate: Union[int, float] = 0,\n        n_shuff: int = 500,\n        parallel_shuff: bool = True,\n        place_field_thres: Union[int, float] = 0.2,\n        place_field_min_size: Optional[Union[int, float]] = None,\n        place_field_max_size: Optional[Union[int, float]] = None,\n        place_field_min_peak: Union[int, float] = 3,\n        place_field_sigma: Union[int, float] = 2,\n    ) -&gt; None:\n        # add all the inputs to self\n        self.__dict__.update(locals())\n        del self.__dict__[\"self\"]\n\n        # Verify inputs: make sure pos and st are nelpy objects\n        if not isinstance(\n            pos, (nel.core._analogsignalarray.AnalogSignalArray, nel.core.PositionArray)\n        ):\n            raise TypeError(\"pos must be nelpy.AnalogSignal or nelpy.PositionArray\")\n        if not isinstance(\n            st,\n            (\n                nel.core._eventarray.SpikeTrainArray,\n                nel.core._analogsignalarray.AnalogSignalArray,\n            ),\n        ):\n            raise TypeError(\n                \"st must be nelpy.SpikeTrain or nelpy.BinnedSpikeTrainArray\"\n            )\n\n        # check data is not empty\n        if pos.isempty or st.isempty:\n            raise ValueError(\"pos and st must not be empty\")\n\n        # check if pos all nan\n        if np.all(np.isnan(pos.data)):\n            raise ValueError(\"Position data cannot contain all NaN values\")\n\n        # get speed and running epochs (highly recommended you calculate\n        #   speed before hand on non epoched data)\n        if speed_thres &gt; 0:\n            if self.speed is None:\n                self.speed = nel.utils.ddt_asa(\n                    self.pos, smooth=True, sigma=0.1, norm=True\n                )\n\n            self.run_epochs = nel.utils.get_run_epochs(\n                self.speed, v1=self.speed_thres, v2=self.speed_thres\n            ).merge()\n        else:\n            self.run_epochs = self.pos.support.copy()\n\n        # calculate maps, 1d or 2d\n        self.dim = pos.n_signals\n        if pos.n_signals == 2:\n            self.tc, self.st_run = self.map_2d()\n        elif pos.n_signals == 1:\n            self.tc, self.st_run = self.map_1d()\n        else:\n            raise ValueError(\"pos dims must be 1 or 2\")\n\n        # find place fields. Currently only collects metrics from peak field\n        # self.find_fields()\n\n    def map_1d(self, pos: Optional[object] = None) -&gt; tuple:\n        \"\"\"Maps 1D data for the spatial tuning curve.\n\n        Parameters\n        ----------\n        pos : Optional[object]\n            Position data for shuffling.\n\n        Returns\n        -------\n        tuple\n            A tuple containing the tuning curve and restricted spike train.\n        \"\"\"\n        # dir_epoch is deprecated input\n        if self.dir_epoch is not None:\n            # warn user\n            logging.warning(\n                \"dir_epoch is deprecated and will be removed. Epoch data by direction prior to calling SpatialMap\"\n            )\n            self.st = self.st[self.dir_epoch]\n            self.pos = self.pos[self.dir_epoch]\n\n        # restrict spike trains to those epochs during which the animal was running\n        st_run = self.st[self.run_epochs]\n\n        # log warning if st_run is empty following restriction\n        if st_run.isempty:\n            logging.warning(\n                \"No spike trains during running epochs\"\n            )  # This will log it but not raise a warning\n            warnings.warn(\"No spike trains during running epochs\", UserWarning)\n\n        # take pos as input for case of shuffling\n        if pos is not None:\n            pos_run = pos[self.run_epochs]\n        else:\n            pos_run = self.pos[self.run_epochs]\n\n        if self.x_minmax is None:\n            x_max = np.ceil(np.nanmax(self.pos.data))\n            x_min = np.floor(np.nanmin(self.pos.data))\n        else:\n            x_min, x_max = self.x_minmax\n\n        self.x_edges = np.arange(x_min, x_max + self.s_binsize, self.s_binsize)\n\n        # compute occupancy\n        occupancy = self.compute_occupancy_1d(pos_run)\n\n        # compute ratemap (in Hz)\n        ratemap = self.compute_ratemap_1d(st_run, pos_run, occupancy)\n\n        # enforce minimum background firing rate\n        # background firing rate of xx Hz\n        ratemap[ratemap &lt; self.minbgrate] = self.minbgrate\n\n        # enforce minimum background occupancy\n        for uu in range(st_run.data.shape[0]):\n            ratemap[uu][occupancy &lt; self.min_duration] = 0\n\n        # add to nelpy tuning curve class\n        tc = nel.TuningCurve1D(\n            ratemap=ratemap,\n            extmin=x_min,\n            extmax=x_max,\n        )\n\n        tc._occupancy = occupancy\n\n        if self.tuning_curve_sigma is not None:\n            if self.tuning_curve_sigma &gt; 0:\n                tc.smooth(\n                    sigma=self.tuning_curve_sigma, inplace=True, mode=self.smooth_mode\n                )\n\n        return tc, st_run\n\n    def compute_occupancy_1d(self, pos_run: object) -&gt; np.ndarray:\n        \"\"\"Computes the occupancy for 1D position data.\n\n        Parameters\n        ----------\n        pos_run : object\n            Restricted position data for running.\n\n        Returns\n        -------\n        np.ndarray\n            Occupancy values per bin.\n        \"\"\"\n        occupancy, _ = np.histogram(pos_run.data[0, :], bins=self.x_edges)\n        return occupancy / pos_run.fs\n\n    def compute_ratemap_1d(\n        self, st_run: object, pos_run: object, occupancy: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Computes the ratemap for 1D data.\n\n        Parameters\n        ----------\n        st_run : object\n            Spike train data restricted to running epochs.\n        pos_run : object\n            Position data restricted to running epochs.\n        occupancy : np.ndarray\n            Occupancy values per bin.\n\n        Returns\n        -------\n        np.ndarray\n            Ratemap values for the given spike and position data.\n        \"\"\"\n        # initialize ratemap\n        ratemap = np.zeros((st_run.data.shape[0], occupancy.shape[0]))\n\n        if st_run.isempty:\n            return ratemap\n\n        mask = ~np.isnan(pos_run.data).any(axis=0)\n        x_pos, ts = (\n            pos_run.data[0, mask],\n            pos_run.abscissa_vals[mask],\n        )\n        # if data to map is spike train (point process)\n        if isinstance(st_run, nel.core._eventarray.SpikeTrainArray):\n            for i in range(st_run.data.shape[0]):\n                # get spike counts in each bin\n                (\n                    ratemap[i, : len(self.x_edges)],\n                    _,\n                ) = np.histogram(\n                    np.interp(st_run.data[i], ts, x_pos),\n                    bins=self.x_edges,\n                )\n\n        # if data to map is analog signal (continuous)\n        elif isinstance(st_run, nel.core._analogsignalarray.AnalogSignalArray):\n            # get x location for every bin center\n            x = np.interp(st_run.abscissa_vals, ts, x_pos)\n            # get indices location within bin edges\n            ext_bin_idx = np.squeeze(np.digitize(x, self.x_edges, right=True))\n            # iterate over each time step and add data values to ratemap\n            for tt, bidx in enumerate(ext_bin_idx):\n                ratemap[:, bidx - 1] += st_run.data[:, tt]\n            # divide by sampling rate\n            ratemap = ratemap * st_run.fs\n\n        # divide by occupancy\n        np.divide(ratemap, occupancy, where=occupancy != 0, out=ratemap)\n\n        # remove nans and infs\n        bad_idx = np.isnan(ratemap) | np.isinf(ratemap)\n        ratemap[bad_idx] = 0\n\n        return ratemap\n\n    def map_2d(self, pos: Optional[object] = None) -&gt; tuple:\n        \"\"\"Maps 2D data for the spatial tuning curve.\n\n        Parameters\n        ----------\n        pos : Optional[object]\n            Position data for shuffling.\n\n        Returns\n        -------\n        tuple\n            A tuple containing the tuning curve and restricted spike train.\n        \"\"\"\n        # restrict spike trains to those epochs during which the animal was running\n        st_run = self.st[self.run_epochs]\n\n        # log warning if st_run is empty following restriction\n        if st_run.isempty:\n            logging.warning(\n                \"No spike trains during running epochs\"\n            )  # This will log it but not raise a warning\n            warnings.warn(\"No spike trains during running epochs\", UserWarning)\n\n        # take pos as input for case of shuffling\n        if pos is not None:\n            pos_run = pos[self.run_epochs]\n        else:\n            pos_run = self.pos[self.run_epochs]\n\n        # get xy max min\n        if self.x_minmax is None:\n            ext_xmin, ext_xmax = (\n                np.floor(np.nanmin(self.pos.data[0, :])),\n                np.ceil(np.nanmax(self.pos.data[0, :])),\n            )\n        else:\n            ext_xmin, ext_xmax = self.x_minmax\n\n        if self.y_minmax is None:\n            ext_ymin, ext_ymax = (\n                np.floor(np.nanmin(self.pos.data[1, :])),\n                np.ceil(np.nanmax(self.pos.data[1, :])),\n            )\n        else:\n            ext_ymin, ext_ymax = self.y_minmax\n\n        # create bin edges\n        self.x_edges = np.arange(ext_xmin, ext_xmax + self.s_binsize, self.s_binsize)\n        self.y_edges = np.arange(ext_ymin, ext_ymax + self.s_binsize, self.s_binsize)\n\n        # number of bins in each dimension\n        ext_nx, ext_ny = len(self.x_edges), len(self.y_edges)\n\n        # compute occupancy\n        occupancy = self.compute_occupancy_2d(pos_run)\n\n        # compute ratemap (in Hz)\n        ratemap = self.compute_ratemap_2d(st_run, pos_run, occupancy)\n\n        # enforce minimum background occupancy\n        for uu in range(st_run.data.shape[0]):\n            ratemap[uu][occupancy &lt; self.min_duration] = 0\n\n        # enforce minimum background firing rate\n        # background firing rate of xx Hz\n        ratemap[ratemap &lt; self.minbgrate] = self.minbgrate\n\n        tc = nel.TuningCurve2D(\n            ratemap=ratemap,\n            ext_xmin=ext_xmin,\n            ext_ymin=ext_ymin,\n            ext_xmax=ext_xmax,\n            ext_ymax=ext_ymax,\n            ext_ny=ext_ny,\n            ext_nx=ext_nx,\n        )\n        tc._occupancy = occupancy\n\n        if self.tuning_curve_sigma is not None:\n            if self.tuning_curve_sigma &gt; 0:\n                tc.smooth(\n                    sigma=self.tuning_curve_sigma, inplace=True, mode=self.smooth_mode\n                )\n\n        return tc, st_run\n\n    def compute_occupancy_2d(self, pos_run: object) -&gt; np.ndarray:\n        \"\"\"Computes the occupancy for 2D position data.\n\n        Parameters\n        ----------\n        pos_run : object\n            Restricted position data for running.\n\n        Returns\n        -------\n        np.ndarray\n            Occupancy values per bin.\n        \"\"\"\n        occupancy, _, _ = np.histogram2d(\n            pos_run.data[0, :], pos_run.data[1, :], bins=(self.x_edges, self.y_edges)\n        )\n        return occupancy / pos_run.fs\n\n    def compute_ratemap_2d(\n        self, st_run: object, pos_run: object, occupancy: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Computes the ratemap for 2D data.\n\n        Parameters\n        ----------\n        st_run : object\n            Spike train data restricted to running epochs.\n        pos_run : object\n            Position data restricted to running epochs.\n        occupancy : np.ndarray\n            Occupancy values per bin.\n\n        Returns\n        -------\n        np.ndarray\n            Ratemap values for the given spike and position data.\n        \"\"\"\n        ratemap = np.zeros(\n            (st_run.data.shape[0], occupancy.shape[0], occupancy.shape[1])\n        )\n        if st_run.isempty:\n            return ratemap\n\n        # remove nans from position data for interpolation\n        mask = ~np.isnan(pos_run.data).any(axis=0)\n        x_pos, y_pos, ts = (\n            pos_run.data[0, mask],\n            pos_run.data[1, mask],\n            pos_run.abscissa_vals[mask],\n        )\n\n        if isinstance(st_run, nel.core._eventarray.SpikeTrainArray):\n            for i in range(st_run.data.shape[0]):\n                ratemap[i, : len(self.x_edges), : len(self.y_edges)], _, _ = (\n                    np.histogram2d(\n                        np.interp(st_run.data[i], ts, x_pos),\n                        np.interp(st_run.data[i], ts, y_pos),\n                        bins=(self.x_edges, self.y_edges),\n                    )\n                )\n\n        elif isinstance(st_run, nel.core._analogsignalarray.AnalogSignalArray):\n            x = np.interp(st_run.abscissa_vals, ts, x_pos)\n            y = np.interp(st_run.abscissa_vals, ts, y_pos)\n            ext_bin_idx_x = np.squeeze(np.digitize(x, self.x_edges, right=True))\n            ext_bin_idx_y = np.squeeze(np.digitize(y, self.y_edges, right=True))\n            for tt, (bidxx, bidxy) in enumerate(zip(ext_bin_idx_x, ext_bin_idx_y)):\n                ratemap[:, bidxx - 1, bidxy - 1] += st_run.data[:, tt]\n            ratemap = ratemap * st_run.fs\n\n        np.divide(ratemap, occupancy, where=occupancy != 0, out=ratemap)\n\n        bad_idx = np.isnan(ratemap) | np.isinf(ratemap)\n        ratemap[bad_idx] = 0\n\n        return ratemap\n\n    def shuffle_spatial_information(self) -&gt; np.ndarray:\n        \"\"\"Shuffle spatial information and compute p-values for observed vs. null.\n\n        This method creates shuffled coordinates of the position data and computes\n        spatial information for each shuffle. The p-values for the observed\n        spatial information against the null distribution are calculated.\n\n        Returns\n        -------\n        np.ndarray\n            P-values for the spatial information.\n        \"\"\"\n\n        def create_shuffled_coordinates(\n            X: np.ndarray, n_shuff: int = 500\n        ) -&gt; List[np.ndarray]:\n            \"\"\"Create shuffled coordinates by rolling the original coordinates.\n\n            Parameters\n            ----------\n            X : np.ndarray\n                Original position data.\n            n_shuff : int, optional\n                Number of shuffles to create (default is 500).\n\n            Returns\n            -------\n            List[np.ndarray]\n                List of shuffled coordinates.\n            \"\"\"\n            range_ = X.shape[1]\n\n            # if fewer coordinates then shuffles, reduce number of shuffles to n coords\n            n_shuff = np.min([range_, n_shuff])\n\n            surrogate = np.random.choice(\n                np.arange(-range_, range_), size=n_shuff, replace=False\n            )\n            x_temp = []\n            for n in surrogate:\n                x_temp.append(np.roll(X, n, axis=1))\n\n            return x_temp\n\n        def get_spatial_infos(pos_shuff: np.ndarray, ts: np.ndarray, dim: int) -&gt; float:\n            \"\"\"Get spatial information for shuffled position data.\n\n            Parameters\n            ----------\n            pos_shuff : np.ndarray\n                Shuffled position data.\n            ts : np.ndarray\n                Timestamps corresponding to the shuffled data.\n            dim : int\n                Dimension of the spatial data (1 or 2).\n\n            Returns\n            -------\n            float\n                Spatial information calculated from the tuning curve.\n            \"\"\"\n            pos_shuff = nel.AnalogSignalArray(\n                data=pos_shuff,\n                timestamps=ts,\n            )\n            if dim == 1:\n                tc, _ = self.map_1d(pos_shuff)\n                return tc.spatial_information()\n            elif dim == 2:\n                tc, _ = self.map_2d(pos_shuff)\n                return tc.spatial_information()\n\n        pos_data_shuff = create_shuffled_coordinates(\n            self.pos.data, n_shuff=self.n_shuff\n        )\n\n        # construct tuning curves for each position shuffle\n        if self.parallel_shuff:\n            num_cores = multiprocessing.cpu_count()\n            shuffle_spatial_info = Parallel(n_jobs=num_cores)(\n                delayed(get_spatial_infos)(\n                    pos_data_shuff[i], self.pos.abscissa_vals, self.dim\n                )\n                for i in range(self.n_shuff)\n            )\n        else:\n            shuffle_spatial_info = [\n                get_spatial_infos(pos_data_shuff[i], self.pos.abscissa_vals, self.dim)\n                for i in range(self.n_shuff)\n            ]\n\n        # calculate p values for the obs vs null\n        _, self.spatial_information_pvalues, self.spatial_information_zscore = (\n            get_significant_events(\n                self.tc.spatial_information(), np.array(shuffle_spatial_info)\n            )\n        )\n\n        return self.spatial_information_pvalues\n\n    def find_fields(self) -&gt; None:\n        \"\"\"Find place fields in the spatial maps.\n\n        This method detects place fields from the spatial maps and calculates\n        their properties, including width, peak firing rate, and a mask for\n        each detected field.\n        \"\"\"\n        from skimage import measure\n\n        field_width = []\n        peak_rate = []\n        mask = []\n\n        if self.place_field_max_size is None and self.dim == 1:\n            self.place_field_max_size = self.tc.n_bins * self.s_binsize\n        elif self.place_field_max_size is None and self.dim == 2:\n            self.place_field_max_size = self.tc.n_bins * self.s_binsize\n\n        if self.dim == 1:\n            for ratemap_ in self.tc.ratemap:\n                map_fields = fields.map_stats2(\n                    ratemap_,\n                    threshold=self.place_field_thres,\n                    min_size=self.place_field_min_size / self.s_binsize,\n                    max_size=self.place_field_max_size / self.s_binsize,\n                    min_peak=self.place_field_min_peak,\n                    sigma=self.place_field_sigma,\n                )\n                if len(map_fields[\"sizes\"]) == 0:\n                    field_width.append(np.nan)\n                    peak_rate.append(np.nan)\n                    mask.append(map_fields[\"fields\"])\n                else:\n                    field_width.append(\n                        np.array(map_fields[\"sizes\"]).max()\n                        * len(ratemap_)\n                        * self.s_binsize\n                    )\n                    peak_rate.append(np.array(map_fields[\"peaks\"]).max())\n                    mask.append(map_fields[\"fields\"])\n\n        if self.dim == 2:\n            for ratemap_ in self.tc.ratemap:\n                peaks = fields.compute_2d_place_fields(\n                    ratemap_,\n                    min_firing_rate=self.place_field_min_peak,\n                    thresh=self.place_field_thres,\n                    min_size=(self.place_field_min_size / self.s_binsize),\n                    max_size=(self.place_field_max_size / self.s_binsize),\n                    sigma=self.place_field_sigma,\n                )\n                # field coords of fields using contours\n                bc = measure.find_contours(\n                    peaks, 0, fully_connected=\"low\", positive_orientation=\"low\"\n                )\n                if len(bc) == 0:\n                    field_width.append(np.nan)\n                    peak_rate.append(np.nan)\n                    mask.append(peaks)\n                elif np.vstack(bc).shape[0] &lt; 3:\n                    field_width.append(np.nan)\n                    peak_rate.append(np.nan)\n                    mask.append(peaks)\n                else:\n                    field_width.append(\n                        np.max(pdist(bc[0], \"euclidean\")) * self.s_binsize\n                    )\n                    # field_ids = np.unique(peaks)\n                    peak_rate.append(ratemap_[peaks == 1].max())\n                    mask.append(peaks)\n\n        self.tc.field_width = np.array(field_width)\n        self.tc.field_peak_rate = np.array(peak_rate)\n        self.tc.field_mask = np.array(mask)\n        self.tc.n_fields = np.array(\n            [len(np.unique(mask_)) - 1 for mask_ in self.tc.field_mask]\n        )\n\n    def save_mat_file(self, basepath: str, UID: Optional[Any] = None) -&gt; None:\n        \"\"\"Save firing rate map data to a .mat file in MATLAB format.\n\n        The saved file will contain the following variables:\n        - map: a 1xN cell array containing the ratemaps, where N is the number of ratemaps.\n        - field: a 1xN cell array containing the field masks, if they exist.\n        - n_fields: the number of fields detected.\n        - size: the width of the detected fields.\n        - peak: the peak firing rate of the detected fields.\n        - occupancy: the occupancy map.\n        - spatial_information: the spatial information of the ratemaps.\n        - spatial_sparsity: the spatial sparsity of the ratemaps.\n        - x_bins: the bin edges for the x-axis of the ratemaps.\n        - y_bins: the bin edges for the y-axis of the ratemaps.\n        - run_epochs: the time points at which the animal was running.\n        - speed: the speed data.\n        - timestamps: the timestamps for the speed data.\n        - pos: the position data.\n\n        The file will be saved to a .mat file with the name `basepath.ratemap.firingRateMap.mat`,\n        where `basepath` is the base path of the data.\n\n        Parameters\n        ----------\n        basepath : str\n            The base path for saving the .mat file.\n        UID : Optional[Any], optional\n            A unique identifier for the data (default is None).\n\n        Returns\n        -------\n        None\n        \"\"\"\n        if self.dim == 1:\n            raise ValueError(\"1d storeage not implemented\")\n\n        # set up dict\n        firingRateMap = {}\n\n        # store UID if exist\n        if UID is not None:\n            firingRateMap[\"UID\"] = UID.tolist()\n\n        # set up empty fields for conversion to matlab cell array\n        firingRateMap[\"map\"] = np.empty(self.tc.ratemap.shape[0], dtype=object)\n        firingRateMap[\"field\"] = np.empty(self.tc.ratemap.shape[0], dtype=object)\n\n        # Iterate over the ratemaps and store each one in a cell of the cell array\n        for i, ratemap in enumerate(self.tc.ratemap):\n            firingRateMap[\"map\"][i] = ratemap\n\n        # store occupancy\n        firingRateMap[\"occupancy\"] = self.tc.occupancy\n\n        # store bin edges\n        firingRateMap[\"x_bins\"] = self.tc.xbins.tolist()\n        firingRateMap[\"y_bins\"] = self.tc.ybins.tolist()\n\n        # store field mask if exist\n        if hasattr(self.tc, \"field_mask\"):\n            for i, field_mask in enumerate(self.tc.field_mask):\n                firingRateMap[\"field\"][i] = field_mask\n\n            # store field finding info\n            firingRateMap[\"n_fields\"] = self.tc.n_fields.tolist()\n            firingRateMap[\"size\"] = self.tc.field_width.tolist()\n            firingRateMap[\"peak\"] = self.tc.field_peak_rate.tolist()\n\n        # store spatial metrics\n        firingRateMap[\"spatial_information\"] = self.tc.spatial_information().tolist()\n        if hasattr(self, \"spatial_information_pvalues\"):\n            firingRateMap[\"spatial_information_pvalues\"] = (\n                self.spatial_information_pvalues.tolist()\n            )\n        firingRateMap[\"spatial_sparsity\"] = self.tc.spatial_sparsity().tolist()\n\n        # store position speed and timestamps\n        firingRateMap[\"timestamps\"] = self.speed.abscissa_vals.tolist()\n        firingRateMap[\"pos\"] = self.pos.data\n        firingRateMap[\"speed\"] = self.speed.data.tolist()\n        firingRateMap[\"run_epochs\"] = self.run_epochs.time.tolist()\n\n        # store epoch interval\n        firingRateMap[\"epoch_interval\"] = [\n            self.pos.support.start,\n            self.pos.support.stop,\n        ]\n\n        # save matlab file\n        savemat(\n            os.path.join(\n                basepath, os.path.basename(basepath) + \".ratemap.firingRateMap.mat\"\n            ),\n            {\"firingRateMap\": firingRateMap},\n        )\n\n    def _unit_subset(self, unit_list):\n        newtuningcurve = copy.copy(self)\n        newtuningcurve.st = newtuningcurve.st._unit_subset(unit_list)\n        newtuningcurve.st_run = newtuningcurve.st_run._unit_subset(unit_list)\n        newtuningcurve.tc = self.tc._unit_subset(unit_list)\n        return newtuningcurve\n\n    @property\n    def is2d(self):\n        return self.tc.is2d\n\n    @property\n    def occupancy(self):\n        return self.tc._occupancy\n\n    @property\n    def n_units(self):\n        return self.tc.n_units\n\n    @property\n    def shape(self):\n        return self.tc.shape\n\n    def __repr__(self):\n        return self.tc.__repr__()\n\n    @property\n    def isempty(self):\n        return self.tc.isempty\n\n    @property\n    def ratemap(self):\n        return self.tc.ratemap\n\n    def __len__(self):\n        return self.tc.__len__()\n\n    def smooth(self, **kwargs):\n        return self.tc.smooth(**kwargs)\n\n    @property\n    def mean(self):\n        return self.tc.mean\n\n    @property\n    def std(self):\n        return self.tc.std\n\n    @property\n    def max(self):\n        return self.tc.max\n\n    @property\n    def min(self):\n        return self.tc.min\n\n    @property\n    def mask(self):\n        return self.tc.mask\n\n    @property\n    def n_bins(self):\n        return self.tc.n_bins\n\n    @property\n    def n_xbins(self):\n        return self.tc.n_xbins\n\n    @property\n    def n_ybins(self):\n        return self.tc.n_ybins\n\n    @property\n    def xbins(self):\n        return self.tc.xbins\n\n    @property\n    def ybins(self):\n        return self.tc.ybins\n\n    @property\n    def xbin_centers(self):\n        return self.tc.xbin_centers\n\n    @property\n    def ybin_centers(self):\n        return self.tc.ybin_centers\n\n    @property\n    def bin_centers(self):\n        return self.tc.bin_centers\n\n    @property\n    def bins(self):\n        return self.tc.bins\n\n    def normalize(self, **kwargs):\n        return self.tc.normalize(**kwargs)\n\n    @property\n    def spatial_sparsity(self):\n        return self.tc.spatial_sparsity\n\n    @property\n    def spatial_information(self):\n        return self.tc.spatial_information\n\n    @property\n    def information_rate(self):\n        return self.tc.information_rate\n\n    @property\n    def spatial_selectivity(self):\n        return self.tc.spatial_selectivity\n\n    def __sub__(self, other):\n        return self.tc.__sub__(other)\n\n    def __mul__(self, other):\n        return self.tc.__mul__(other)\n\n    def __rmul__(self, other):\n        return self.tc.__rmul__(other)\n\n    def __truediv__(self, other):\n        return self.tc.__truediv__(other)\n\n    def __iter__(self):\n        return self.tc.__iter__()\n\n    def __next__(self):\n        return self.tc.__next__()\n\n    def __getitem__(self, *idx):\n        return self.tc.__getitem__(*idx)\n\n    def _get_peak_firing_order_idx(self):\n        return self.tc._get_peak_firing_order_idx()\n\n    def get_peak_firing_order_ids(self):\n        return self.tc.get_peak_firing_order_ids()\n\n    def _reorder_units_by_idx(self):\n        return self.tc._reorder_units_by_idx()\n\n    def reorder_units_by_ids(self):\n        return self.tc.reorder_units_by_ids()\n\n    def reorder_units(self):\n        return self.tc.reorder_units()\n</code></pre>"},{"location":"reference/neuro_py/tuning/maps/#neuro_py.tuning.maps.SpatialMap.compute_occupancy_1d","title":"<code>compute_occupancy_1d(pos_run)</code>","text":"<p>Computes the occupancy for 1D position data.</p> <p>Parameters:</p> Name Type Description Default <code>pos_run</code> <code>object</code> <p>Restricted position data for running.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Occupancy values per bin.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def compute_occupancy_1d(self, pos_run: object) -&gt; np.ndarray:\n    \"\"\"Computes the occupancy for 1D position data.\n\n    Parameters\n    ----------\n    pos_run : object\n        Restricted position data for running.\n\n    Returns\n    -------\n    np.ndarray\n        Occupancy values per bin.\n    \"\"\"\n    occupancy, _ = np.histogram(pos_run.data[0, :], bins=self.x_edges)\n    return occupancy / pos_run.fs\n</code></pre>"},{"location":"reference/neuro_py/tuning/maps/#neuro_py.tuning.maps.SpatialMap.compute_occupancy_2d","title":"<code>compute_occupancy_2d(pos_run)</code>","text":"<p>Computes the occupancy for 2D position data.</p> <p>Parameters:</p> Name Type Description Default <code>pos_run</code> <code>object</code> <p>Restricted position data for running.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Occupancy values per bin.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def compute_occupancy_2d(self, pos_run: object) -&gt; np.ndarray:\n    \"\"\"Computes the occupancy for 2D position data.\n\n    Parameters\n    ----------\n    pos_run : object\n        Restricted position data for running.\n\n    Returns\n    -------\n    np.ndarray\n        Occupancy values per bin.\n    \"\"\"\n    occupancy, _, _ = np.histogram2d(\n        pos_run.data[0, :], pos_run.data[1, :], bins=(self.x_edges, self.y_edges)\n    )\n    return occupancy / pos_run.fs\n</code></pre>"},{"location":"reference/neuro_py/tuning/maps/#neuro_py.tuning.maps.SpatialMap.compute_ratemap_1d","title":"<code>compute_ratemap_1d(st_run, pos_run, occupancy)</code>","text":"<p>Computes the ratemap for 1D data.</p> <p>Parameters:</p> Name Type Description Default <code>st_run</code> <code>object</code> <p>Spike train data restricted to running epochs.</p> required <code>pos_run</code> <code>object</code> <p>Position data restricted to running epochs.</p> required <code>occupancy</code> <code>ndarray</code> <p>Occupancy values per bin.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Ratemap values for the given spike and position data.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def compute_ratemap_1d(\n    self, st_run: object, pos_run: object, occupancy: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"Computes the ratemap for 1D data.\n\n    Parameters\n    ----------\n    st_run : object\n        Spike train data restricted to running epochs.\n    pos_run : object\n        Position data restricted to running epochs.\n    occupancy : np.ndarray\n        Occupancy values per bin.\n\n    Returns\n    -------\n    np.ndarray\n        Ratemap values for the given spike and position data.\n    \"\"\"\n    # initialize ratemap\n    ratemap = np.zeros((st_run.data.shape[0], occupancy.shape[0]))\n\n    if st_run.isempty:\n        return ratemap\n\n    mask = ~np.isnan(pos_run.data).any(axis=0)\n    x_pos, ts = (\n        pos_run.data[0, mask],\n        pos_run.abscissa_vals[mask],\n    )\n    # if data to map is spike train (point process)\n    if isinstance(st_run, nel.core._eventarray.SpikeTrainArray):\n        for i in range(st_run.data.shape[0]):\n            # get spike counts in each bin\n            (\n                ratemap[i, : len(self.x_edges)],\n                _,\n            ) = np.histogram(\n                np.interp(st_run.data[i], ts, x_pos),\n                bins=self.x_edges,\n            )\n\n    # if data to map is analog signal (continuous)\n    elif isinstance(st_run, nel.core._analogsignalarray.AnalogSignalArray):\n        # get x location for every bin center\n        x = np.interp(st_run.abscissa_vals, ts, x_pos)\n        # get indices location within bin edges\n        ext_bin_idx = np.squeeze(np.digitize(x, self.x_edges, right=True))\n        # iterate over each time step and add data values to ratemap\n        for tt, bidx in enumerate(ext_bin_idx):\n            ratemap[:, bidx - 1] += st_run.data[:, tt]\n        # divide by sampling rate\n        ratemap = ratemap * st_run.fs\n\n    # divide by occupancy\n    np.divide(ratemap, occupancy, where=occupancy != 0, out=ratemap)\n\n    # remove nans and infs\n    bad_idx = np.isnan(ratemap) | np.isinf(ratemap)\n    ratemap[bad_idx] = 0\n\n    return ratemap\n</code></pre>"},{"location":"reference/neuro_py/tuning/maps/#neuro_py.tuning.maps.SpatialMap.compute_ratemap_2d","title":"<code>compute_ratemap_2d(st_run, pos_run, occupancy)</code>","text":"<p>Computes the ratemap for 2D data.</p> <p>Parameters:</p> Name Type Description Default <code>st_run</code> <code>object</code> <p>Spike train data restricted to running epochs.</p> required <code>pos_run</code> <code>object</code> <p>Position data restricted to running epochs.</p> required <code>occupancy</code> <code>ndarray</code> <p>Occupancy values per bin.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Ratemap values for the given spike and position data.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def compute_ratemap_2d(\n    self, st_run: object, pos_run: object, occupancy: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"Computes the ratemap for 2D data.\n\n    Parameters\n    ----------\n    st_run : object\n        Spike train data restricted to running epochs.\n    pos_run : object\n        Position data restricted to running epochs.\n    occupancy : np.ndarray\n        Occupancy values per bin.\n\n    Returns\n    -------\n    np.ndarray\n        Ratemap values for the given spike and position data.\n    \"\"\"\n    ratemap = np.zeros(\n        (st_run.data.shape[0], occupancy.shape[0], occupancy.shape[1])\n    )\n    if st_run.isempty:\n        return ratemap\n\n    # remove nans from position data for interpolation\n    mask = ~np.isnan(pos_run.data).any(axis=0)\n    x_pos, y_pos, ts = (\n        pos_run.data[0, mask],\n        pos_run.data[1, mask],\n        pos_run.abscissa_vals[mask],\n    )\n\n    if isinstance(st_run, nel.core._eventarray.SpikeTrainArray):\n        for i in range(st_run.data.shape[0]):\n            ratemap[i, : len(self.x_edges), : len(self.y_edges)], _, _ = (\n                np.histogram2d(\n                    np.interp(st_run.data[i], ts, x_pos),\n                    np.interp(st_run.data[i], ts, y_pos),\n                    bins=(self.x_edges, self.y_edges),\n                )\n            )\n\n    elif isinstance(st_run, nel.core._analogsignalarray.AnalogSignalArray):\n        x = np.interp(st_run.abscissa_vals, ts, x_pos)\n        y = np.interp(st_run.abscissa_vals, ts, y_pos)\n        ext_bin_idx_x = np.squeeze(np.digitize(x, self.x_edges, right=True))\n        ext_bin_idx_y = np.squeeze(np.digitize(y, self.y_edges, right=True))\n        for tt, (bidxx, bidxy) in enumerate(zip(ext_bin_idx_x, ext_bin_idx_y)):\n            ratemap[:, bidxx - 1, bidxy - 1] += st_run.data[:, tt]\n        ratemap = ratemap * st_run.fs\n\n    np.divide(ratemap, occupancy, where=occupancy != 0, out=ratemap)\n\n    bad_idx = np.isnan(ratemap) | np.isinf(ratemap)\n    ratemap[bad_idx] = 0\n\n    return ratemap\n</code></pre>"},{"location":"reference/neuro_py/tuning/maps/#neuro_py.tuning.maps.SpatialMap.find_fields","title":"<code>find_fields()</code>","text":"<p>Find place fields in the spatial maps.</p> <p>This method detects place fields from the spatial maps and calculates their properties, including width, peak firing rate, and a mask for each detected field.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def find_fields(self) -&gt; None:\n    \"\"\"Find place fields in the spatial maps.\n\n    This method detects place fields from the spatial maps and calculates\n    their properties, including width, peak firing rate, and a mask for\n    each detected field.\n    \"\"\"\n    from skimage import measure\n\n    field_width = []\n    peak_rate = []\n    mask = []\n\n    if self.place_field_max_size is None and self.dim == 1:\n        self.place_field_max_size = self.tc.n_bins * self.s_binsize\n    elif self.place_field_max_size is None and self.dim == 2:\n        self.place_field_max_size = self.tc.n_bins * self.s_binsize\n\n    if self.dim == 1:\n        for ratemap_ in self.tc.ratemap:\n            map_fields = fields.map_stats2(\n                ratemap_,\n                threshold=self.place_field_thres,\n                min_size=self.place_field_min_size / self.s_binsize,\n                max_size=self.place_field_max_size / self.s_binsize,\n                min_peak=self.place_field_min_peak,\n                sigma=self.place_field_sigma,\n            )\n            if len(map_fields[\"sizes\"]) == 0:\n                field_width.append(np.nan)\n                peak_rate.append(np.nan)\n                mask.append(map_fields[\"fields\"])\n            else:\n                field_width.append(\n                    np.array(map_fields[\"sizes\"]).max()\n                    * len(ratemap_)\n                    * self.s_binsize\n                )\n                peak_rate.append(np.array(map_fields[\"peaks\"]).max())\n                mask.append(map_fields[\"fields\"])\n\n    if self.dim == 2:\n        for ratemap_ in self.tc.ratemap:\n            peaks = fields.compute_2d_place_fields(\n                ratemap_,\n                min_firing_rate=self.place_field_min_peak,\n                thresh=self.place_field_thres,\n                min_size=(self.place_field_min_size / self.s_binsize),\n                max_size=(self.place_field_max_size / self.s_binsize),\n                sigma=self.place_field_sigma,\n            )\n            # field coords of fields using contours\n            bc = measure.find_contours(\n                peaks, 0, fully_connected=\"low\", positive_orientation=\"low\"\n            )\n            if len(bc) == 0:\n                field_width.append(np.nan)\n                peak_rate.append(np.nan)\n                mask.append(peaks)\n            elif np.vstack(bc).shape[0] &lt; 3:\n                field_width.append(np.nan)\n                peak_rate.append(np.nan)\n                mask.append(peaks)\n            else:\n                field_width.append(\n                    np.max(pdist(bc[0], \"euclidean\")) * self.s_binsize\n                )\n                # field_ids = np.unique(peaks)\n                peak_rate.append(ratemap_[peaks == 1].max())\n                mask.append(peaks)\n\n    self.tc.field_width = np.array(field_width)\n    self.tc.field_peak_rate = np.array(peak_rate)\n    self.tc.field_mask = np.array(mask)\n    self.tc.n_fields = np.array(\n        [len(np.unique(mask_)) - 1 for mask_ in self.tc.field_mask]\n    )\n</code></pre>"},{"location":"reference/neuro_py/tuning/maps/#neuro_py.tuning.maps.SpatialMap.map_1d","title":"<code>map_1d(pos=None)</code>","text":"<p>Maps 1D data for the spatial tuning curve.</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>Optional[object]</code> <p>Position data for shuffling.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the tuning curve and restricted spike train.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def map_1d(self, pos: Optional[object] = None) -&gt; tuple:\n    \"\"\"Maps 1D data for the spatial tuning curve.\n\n    Parameters\n    ----------\n    pos : Optional[object]\n        Position data for shuffling.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the tuning curve and restricted spike train.\n    \"\"\"\n    # dir_epoch is deprecated input\n    if self.dir_epoch is not None:\n        # warn user\n        logging.warning(\n            \"dir_epoch is deprecated and will be removed. Epoch data by direction prior to calling SpatialMap\"\n        )\n        self.st = self.st[self.dir_epoch]\n        self.pos = self.pos[self.dir_epoch]\n\n    # restrict spike trains to those epochs during which the animal was running\n    st_run = self.st[self.run_epochs]\n\n    # log warning if st_run is empty following restriction\n    if st_run.isempty:\n        logging.warning(\n            \"No spike trains during running epochs\"\n        )  # This will log it but not raise a warning\n        warnings.warn(\"No spike trains during running epochs\", UserWarning)\n\n    # take pos as input for case of shuffling\n    if pos is not None:\n        pos_run = pos[self.run_epochs]\n    else:\n        pos_run = self.pos[self.run_epochs]\n\n    if self.x_minmax is None:\n        x_max = np.ceil(np.nanmax(self.pos.data))\n        x_min = np.floor(np.nanmin(self.pos.data))\n    else:\n        x_min, x_max = self.x_minmax\n\n    self.x_edges = np.arange(x_min, x_max + self.s_binsize, self.s_binsize)\n\n    # compute occupancy\n    occupancy = self.compute_occupancy_1d(pos_run)\n\n    # compute ratemap (in Hz)\n    ratemap = self.compute_ratemap_1d(st_run, pos_run, occupancy)\n\n    # enforce minimum background firing rate\n    # background firing rate of xx Hz\n    ratemap[ratemap &lt; self.minbgrate] = self.minbgrate\n\n    # enforce minimum background occupancy\n    for uu in range(st_run.data.shape[0]):\n        ratemap[uu][occupancy &lt; self.min_duration] = 0\n\n    # add to nelpy tuning curve class\n    tc = nel.TuningCurve1D(\n        ratemap=ratemap,\n        extmin=x_min,\n        extmax=x_max,\n    )\n\n    tc._occupancy = occupancy\n\n    if self.tuning_curve_sigma is not None:\n        if self.tuning_curve_sigma &gt; 0:\n            tc.smooth(\n                sigma=self.tuning_curve_sigma, inplace=True, mode=self.smooth_mode\n            )\n\n    return tc, st_run\n</code></pre>"},{"location":"reference/neuro_py/tuning/maps/#neuro_py.tuning.maps.SpatialMap.map_2d","title":"<code>map_2d(pos=None)</code>","text":"<p>Maps 2D data for the spatial tuning curve.</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>Optional[object]</code> <p>Position data for shuffling.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the tuning curve and restricted spike train.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def map_2d(self, pos: Optional[object] = None) -&gt; tuple:\n    \"\"\"Maps 2D data for the spatial tuning curve.\n\n    Parameters\n    ----------\n    pos : Optional[object]\n        Position data for shuffling.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the tuning curve and restricted spike train.\n    \"\"\"\n    # restrict spike trains to those epochs during which the animal was running\n    st_run = self.st[self.run_epochs]\n\n    # log warning if st_run is empty following restriction\n    if st_run.isempty:\n        logging.warning(\n            \"No spike trains during running epochs\"\n        )  # This will log it but not raise a warning\n        warnings.warn(\"No spike trains during running epochs\", UserWarning)\n\n    # take pos as input for case of shuffling\n    if pos is not None:\n        pos_run = pos[self.run_epochs]\n    else:\n        pos_run = self.pos[self.run_epochs]\n\n    # get xy max min\n    if self.x_minmax is None:\n        ext_xmin, ext_xmax = (\n            np.floor(np.nanmin(self.pos.data[0, :])),\n            np.ceil(np.nanmax(self.pos.data[0, :])),\n        )\n    else:\n        ext_xmin, ext_xmax = self.x_minmax\n\n    if self.y_minmax is None:\n        ext_ymin, ext_ymax = (\n            np.floor(np.nanmin(self.pos.data[1, :])),\n            np.ceil(np.nanmax(self.pos.data[1, :])),\n        )\n    else:\n        ext_ymin, ext_ymax = self.y_minmax\n\n    # create bin edges\n    self.x_edges = np.arange(ext_xmin, ext_xmax + self.s_binsize, self.s_binsize)\n    self.y_edges = np.arange(ext_ymin, ext_ymax + self.s_binsize, self.s_binsize)\n\n    # number of bins in each dimension\n    ext_nx, ext_ny = len(self.x_edges), len(self.y_edges)\n\n    # compute occupancy\n    occupancy = self.compute_occupancy_2d(pos_run)\n\n    # compute ratemap (in Hz)\n    ratemap = self.compute_ratemap_2d(st_run, pos_run, occupancy)\n\n    # enforce minimum background occupancy\n    for uu in range(st_run.data.shape[0]):\n        ratemap[uu][occupancy &lt; self.min_duration] = 0\n\n    # enforce minimum background firing rate\n    # background firing rate of xx Hz\n    ratemap[ratemap &lt; self.minbgrate] = self.minbgrate\n\n    tc = nel.TuningCurve2D(\n        ratemap=ratemap,\n        ext_xmin=ext_xmin,\n        ext_ymin=ext_ymin,\n        ext_xmax=ext_xmax,\n        ext_ymax=ext_ymax,\n        ext_ny=ext_ny,\n        ext_nx=ext_nx,\n    )\n    tc._occupancy = occupancy\n\n    if self.tuning_curve_sigma is not None:\n        if self.tuning_curve_sigma &gt; 0:\n            tc.smooth(\n                sigma=self.tuning_curve_sigma, inplace=True, mode=self.smooth_mode\n            )\n\n    return tc, st_run\n</code></pre>"},{"location":"reference/neuro_py/tuning/maps/#neuro_py.tuning.maps.SpatialMap.save_mat_file","title":"<code>save_mat_file(basepath, UID=None)</code>","text":"<p>Save firing rate map data to a .mat file in MATLAB format.</p> <p>The saved file will contain the following variables: - map: a 1xN cell array containing the ratemaps, where N is the number of ratemaps. - field: a 1xN cell array containing the field masks, if they exist. - n_fields: the number of fields detected. - size: the width of the detected fields. - peak: the peak firing rate of the detected fields. - occupancy: the occupancy map. - spatial_information: the spatial information of the ratemaps. - spatial_sparsity: the spatial sparsity of the ratemaps. - x_bins: the bin edges for the x-axis of the ratemaps. - y_bins: the bin edges for the y-axis of the ratemaps. - run_epochs: the time points at which the animal was running. - speed: the speed data. - timestamps: the timestamps for the speed data. - pos: the position data.</p> <p>The file will be saved to a .mat file with the name <code>basepath.ratemap.firingRateMap.mat</code>, where <code>basepath</code> is the base path of the data.</p> <p>Parameters:</p> Name Type Description Default <code>basepath</code> <code>str</code> <p>The base path for saving the .mat file.</p> required <code>UID</code> <code>Optional[Any]</code> <p>A unique identifier for the data (default is None).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def save_mat_file(self, basepath: str, UID: Optional[Any] = None) -&gt; None:\n    \"\"\"Save firing rate map data to a .mat file in MATLAB format.\n\n    The saved file will contain the following variables:\n    - map: a 1xN cell array containing the ratemaps, where N is the number of ratemaps.\n    - field: a 1xN cell array containing the field masks, if they exist.\n    - n_fields: the number of fields detected.\n    - size: the width of the detected fields.\n    - peak: the peak firing rate of the detected fields.\n    - occupancy: the occupancy map.\n    - spatial_information: the spatial information of the ratemaps.\n    - spatial_sparsity: the spatial sparsity of the ratemaps.\n    - x_bins: the bin edges for the x-axis of the ratemaps.\n    - y_bins: the bin edges for the y-axis of the ratemaps.\n    - run_epochs: the time points at which the animal was running.\n    - speed: the speed data.\n    - timestamps: the timestamps for the speed data.\n    - pos: the position data.\n\n    The file will be saved to a .mat file with the name `basepath.ratemap.firingRateMap.mat`,\n    where `basepath` is the base path of the data.\n\n    Parameters\n    ----------\n    basepath : str\n        The base path for saving the .mat file.\n    UID : Optional[Any], optional\n        A unique identifier for the data (default is None).\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if self.dim == 1:\n        raise ValueError(\"1d storeage not implemented\")\n\n    # set up dict\n    firingRateMap = {}\n\n    # store UID if exist\n    if UID is not None:\n        firingRateMap[\"UID\"] = UID.tolist()\n\n    # set up empty fields for conversion to matlab cell array\n    firingRateMap[\"map\"] = np.empty(self.tc.ratemap.shape[0], dtype=object)\n    firingRateMap[\"field\"] = np.empty(self.tc.ratemap.shape[0], dtype=object)\n\n    # Iterate over the ratemaps and store each one in a cell of the cell array\n    for i, ratemap in enumerate(self.tc.ratemap):\n        firingRateMap[\"map\"][i] = ratemap\n\n    # store occupancy\n    firingRateMap[\"occupancy\"] = self.tc.occupancy\n\n    # store bin edges\n    firingRateMap[\"x_bins\"] = self.tc.xbins.tolist()\n    firingRateMap[\"y_bins\"] = self.tc.ybins.tolist()\n\n    # store field mask if exist\n    if hasattr(self.tc, \"field_mask\"):\n        for i, field_mask in enumerate(self.tc.field_mask):\n            firingRateMap[\"field\"][i] = field_mask\n\n        # store field finding info\n        firingRateMap[\"n_fields\"] = self.tc.n_fields.tolist()\n        firingRateMap[\"size\"] = self.tc.field_width.tolist()\n        firingRateMap[\"peak\"] = self.tc.field_peak_rate.tolist()\n\n    # store spatial metrics\n    firingRateMap[\"spatial_information\"] = self.tc.spatial_information().tolist()\n    if hasattr(self, \"spatial_information_pvalues\"):\n        firingRateMap[\"spatial_information_pvalues\"] = (\n            self.spatial_information_pvalues.tolist()\n        )\n    firingRateMap[\"spatial_sparsity\"] = self.tc.spatial_sparsity().tolist()\n\n    # store position speed and timestamps\n    firingRateMap[\"timestamps\"] = self.speed.abscissa_vals.tolist()\n    firingRateMap[\"pos\"] = self.pos.data\n    firingRateMap[\"speed\"] = self.speed.data.tolist()\n    firingRateMap[\"run_epochs\"] = self.run_epochs.time.tolist()\n\n    # store epoch interval\n    firingRateMap[\"epoch_interval\"] = [\n        self.pos.support.start,\n        self.pos.support.stop,\n    ]\n\n    # save matlab file\n    savemat(\n        os.path.join(\n            basepath, os.path.basename(basepath) + \".ratemap.firingRateMap.mat\"\n        ),\n        {\"firingRateMap\": firingRateMap},\n    )\n</code></pre>"},{"location":"reference/neuro_py/tuning/maps/#neuro_py.tuning.maps.SpatialMap.shuffle_spatial_information","title":"<code>shuffle_spatial_information()</code>","text":"<p>Shuffle spatial information and compute p-values for observed vs. null.</p> <p>This method creates shuffled coordinates of the position data and computes spatial information for each shuffle. The p-values for the observed spatial information against the null distribution are calculated.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>P-values for the spatial information.</p> Source code in <code>neuro_py/tuning/maps.py</code> <pre><code>def shuffle_spatial_information(self) -&gt; np.ndarray:\n    \"\"\"Shuffle spatial information and compute p-values for observed vs. null.\n\n    This method creates shuffled coordinates of the position data and computes\n    spatial information for each shuffle. The p-values for the observed\n    spatial information against the null distribution are calculated.\n\n    Returns\n    -------\n    np.ndarray\n        P-values for the spatial information.\n    \"\"\"\n\n    def create_shuffled_coordinates(\n        X: np.ndarray, n_shuff: int = 500\n    ) -&gt; List[np.ndarray]:\n        \"\"\"Create shuffled coordinates by rolling the original coordinates.\n\n        Parameters\n        ----------\n        X : np.ndarray\n            Original position data.\n        n_shuff : int, optional\n            Number of shuffles to create (default is 500).\n\n        Returns\n        -------\n        List[np.ndarray]\n            List of shuffled coordinates.\n        \"\"\"\n        range_ = X.shape[1]\n\n        # if fewer coordinates then shuffles, reduce number of shuffles to n coords\n        n_shuff = np.min([range_, n_shuff])\n\n        surrogate = np.random.choice(\n            np.arange(-range_, range_), size=n_shuff, replace=False\n        )\n        x_temp = []\n        for n in surrogate:\n            x_temp.append(np.roll(X, n, axis=1))\n\n        return x_temp\n\n    def get_spatial_infos(pos_shuff: np.ndarray, ts: np.ndarray, dim: int) -&gt; float:\n        \"\"\"Get spatial information for shuffled position data.\n\n        Parameters\n        ----------\n        pos_shuff : np.ndarray\n            Shuffled position data.\n        ts : np.ndarray\n            Timestamps corresponding to the shuffled data.\n        dim : int\n            Dimension of the spatial data (1 or 2).\n\n        Returns\n        -------\n        float\n            Spatial information calculated from the tuning curve.\n        \"\"\"\n        pos_shuff = nel.AnalogSignalArray(\n            data=pos_shuff,\n            timestamps=ts,\n        )\n        if dim == 1:\n            tc, _ = self.map_1d(pos_shuff)\n            return tc.spatial_information()\n        elif dim == 2:\n            tc, _ = self.map_2d(pos_shuff)\n            return tc.spatial_information()\n\n    pos_data_shuff = create_shuffled_coordinates(\n        self.pos.data, n_shuff=self.n_shuff\n    )\n\n    # construct tuning curves for each position shuffle\n    if self.parallel_shuff:\n        num_cores = multiprocessing.cpu_count()\n        shuffle_spatial_info = Parallel(n_jobs=num_cores)(\n            delayed(get_spatial_infos)(\n                pos_data_shuff[i], self.pos.abscissa_vals, self.dim\n            )\n            for i in range(self.n_shuff)\n        )\n    else:\n        shuffle_spatial_info = [\n            get_spatial_infos(pos_data_shuff[i], self.pos.abscissa_vals, self.dim)\n            for i in range(self.n_shuff)\n        ]\n\n    # calculate p values for the obs vs null\n    _, self.spatial_information_pvalues, self.spatial_information_zscore = (\n        get_significant_events(\n            self.tc.spatial_information(), np.array(shuffle_spatial_info)\n        )\n    )\n\n    return self.spatial_information_pvalues\n</code></pre>"},{"location":"reference/neuro_py/util/","title":"neuro_py.util","text":""},{"location":"reference/neuro_py/util/#neuro_py.util._check_dependency","title":"<code>_check_dependency(module_name, extra)</code>","text":"<p>Check if a module is installed, and raise an ImportError with a helpful message if not.</p> <p>Parameters:</p> Name Type Description Default <code>module_name</code> <code>str</code> <p>The name of the module to check.</p> required <code>extra</code> <code>str</code> <p>The name of the extra requirement group (e.g., 'csd') to suggest in the error message.</p> required Source code in <code>neuro_py/util/_dependencies.py</code> <pre><code>def _check_dependency(module_name: str, extra: str) -&gt; None:\n    \"\"\"\n    Check if a module is installed, and raise an ImportError with a helpful message if not.\n\n    Parameters\n    ----------\n    module_name : str\n        The name of the module to check.\n    extra : str\n        The name of the extra requirement group (e.g., 'csd') to suggest in the error message.\n    \"\"\"\n    try:\n        __import__(module_name)\n    except ImportError:\n        raise ImportError(\n            f\"{module_name} is not installed. Please install it to use this function. \"\n            f\"Run: pip install -e .[{extra}]\"\n        )\n</code></pre>"},{"location":"reference/neuro_py/util/#neuro_py.util.find_terminal_masked_indices","title":"<code>find_terminal_masked_indices(mask, axis)</code>","text":"<p>Find the first and last indices of non-masked values along an axis.</p> <p>Only tested upto 2D arrays. If <code>mask</code> is empty along <code>axis</code>, the first and last indices are set to 0 and the last index along the axis, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>Mask of <code>arr</code>.</p> required <code>axis</code> <code>int</code> <p>Axis along which to find the first and last indices.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>First index of non-masked values along <code>axis</code>.</p> <code>ndarray</code> <p>Last index of non-masked values along <code>axis</code>.</p> <p>Examples:</p> <p>1D Example:</p> <pre><code>&gt;&gt;&gt; mask = np.array([0, 0, 1, 1, 0])\n&gt;&gt;&gt; find_terminal_masked_indices(mask, axis=0)\n(2, 3)\n</code></pre> <p>2D Example (along rows):</p> <pre><code>&gt;&gt;&gt; mask = np.array([[0, 0, 1],\n...                  [1, 1, 0],\n...                  [0, 0, 0]])\n&gt;&gt;&gt; find_terminal_masked_indices(mask, axis=1)\n(array([2, 0, 0]), array([2, 1, -1]))\n</code></pre> <p>2D Example (along columns):</p> <pre><code>&gt;&gt;&gt; find_terminal_masked_indices(mask, axis=0)\n(array([1, 1, 0]), array([1, 1, 0]))\n</code></pre> Source code in <code>neuro_py/util/array.py</code> <pre><code>def find_terminal_masked_indices(\n    mask: np.ndarray, axis: int\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Find the first and last indices of non-masked values along an axis.\n\n    Only tested upto 2D arrays. If `mask` is empty along `axis`, the first and\n    last indices are set to 0 and the last index along the axis, respectively.\n\n    Parameters\n    ----------\n    mask : np.ndarray\n        Mask of `arr`.\n    axis : int\n        Axis along which to find the first and last indices.\n\n    Returns\n    -------\n    np.ndarray\n        First index of non-masked values along `axis`.\n    np.ndarray\n        Last index of non-masked values along `axis`.\n\n    Examples\n    --------\n    1D Example:\n    &gt;&gt;&gt; mask = np.array([0, 0, 1, 1, 0])\n    &gt;&gt;&gt; find_terminal_masked_indices(mask, axis=0)\n    (2, 3)\n\n    2D Example (along rows):\n    &gt;&gt;&gt; mask = np.array([[0, 0, 1],\n    ...                  [1, 1, 0],\n    ...                  [0, 0, 0]])\n    &gt;&gt;&gt; find_terminal_masked_indices(mask, axis=1)\n    (array([2, 0, 0]), array([2, 1, -1]))\n\n    2D Example (along columns):\n    &gt;&gt;&gt; find_terminal_masked_indices(mask, axis=0)\n    (array([1, 1, 0]), array([1, 1, 0]))\n    \"\"\"\n    first_idx = np.argmax(mask, axis=axis)\n    reversed_mask = np.flip(mask, axis=axis)\n    last_idx = mask.shape[axis] - np.argmax(reversed_mask, axis=axis) - 1\n\n    return first_idx, last_idx\n</code></pre>"},{"location":"reference/neuro_py/util/#neuro_py.util.is_nested","title":"<code>is_nested(array)</code>","text":"<p>Check if an array is nested.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the array is nested, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_nested(np.array([1, 2, 3]))\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; is_nested(np.array([[1, 2], [3, 4]]))\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; is_nested(np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]))\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; is_nested(np.array([np.array([1, 2]), np.array([3, 4, 5])], dtype=object))\nTrue\n</code></pre> Source code in <code>neuro_py/util/array.py</code> <pre><code>def is_nested(array: np.ndarray) -&gt; bool:\n    \"\"\"\n    Check if an array is nested.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Input array.\n\n    Returns\n    -------\n    bool\n        True if the array is nested, False otherwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; is_nested(np.array([1, 2, 3]))\n    False\n\n    &gt;&gt;&gt; is_nested(np.array([[1, 2], [3, 4]]))\n    False\n\n    &gt;&gt;&gt; is_nested(np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]))\n    False\n\n    &gt;&gt;&gt; is_nested(np.array([np.array([1, 2]), np.array([3, 4, 5])], dtype=object))\n    True\n    \"\"\"\n    if array.dtype != object:\n        return False\n    return any(isinstance(item, np.ndarray) for item in array)\n</code></pre>"},{"location":"reference/neuro_py/util/#neuro_py.util.replace_border_zeros_with_nan","title":"<code>replace_border_zeros_with_nan(arr)</code>","text":"<p>Replace zero values at the borders of each dimension of a n-dimensional array with NaN.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array with zero values at the borders replaced with NaN.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = np.arange(27).reshape(3, 3, 3)\n&gt;&gt;&gt; arr[0, 2] = arr[2, 2] = arr[2, 0, 0] = arr[1, 1, 1] = 0\n&gt;&gt;&gt; replace_border_zeros_with_nan(arr)\narray([[[nan,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [nan, nan, nan]],\n</code></pre> <pre><code>   [[ 9., 10., 11.],\n    [12.,  0., 14.],\n    [15., 16., 17.]],\n\n   [[nan, 19., 20.],\n    [21., 22., 23.],\n    [nan, nan, nan]]])\n</code></pre> Source code in <code>neuro_py/util/array.py</code> <pre><code>def replace_border_zeros_with_nan(arr: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Replace zero values at the borders of each dimension of a n-dimensional\n    array with NaN.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input array.\n\n    Returns\n    -------\n    np.ndarray\n        Array with zero values at the borders replaced with NaN.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = np.arange(27).reshape(3, 3, 3)\n    &gt;&gt;&gt; arr[0, 2] = arr[2, 2] = arr[2, 0, 0] = arr[1, 1, 1] = 0\n    &gt;&gt;&gt; replace_border_zeros_with_nan(arr)\n    array([[[nan,  1.,  2.],\n            [ 3.,  4.,  5.],\n            [nan, nan, nan]],\n\n           [[ 9., 10., 11.],\n            [12.,  0., 14.],\n            [15., 16., 17.]],\n\n           [[nan, 19., 20.],\n            [21., 22., 23.],\n            [nan, nan, nan]]])\n    \"\"\"\n    arr = np.array(arr, dtype=float)\n    dims = arr.shape\n\n    for axis in range(len(dims)):\n        # Find indices where zero values start and stop\n        for idx in np.ndindex(*[dims[i] for i in range(len(dims)) if i != axis]):\n            slicer = list(idx)\n            slicer.insert(\n                axis, slice(None)\n            )  # Insert the full slice along the current axis\n\n            # Check for first sequence of zeros\n            subarray = arr[tuple(slicer)]\n            first_zero_indices = np.where(np.cumsum(subarray != 0) == 0)[0]\n            if len(first_zero_indices) &gt; 0:\n                subarray[first_zero_indices] = np.nan\n\n            # Check for last sequence of zeros\n            last_zero_indices = np.where(np.cumsum(subarray[::-1] != 0) == 0)[0]\n            if len(last_zero_indices) &gt; 0:\n                subarray[-last_zero_indices - 1] = np.nan\n\n            arr[tuple(slicer)] = subarray  # Replace modified subarray\n\n    return arr\n</code></pre>"},{"location":"reference/neuro_py/util/array/","title":"neuro_py.util.array","text":""},{"location":"reference/neuro_py/util/array/#neuro_py.util.array.find_terminal_masked_indices","title":"<code>find_terminal_masked_indices(mask, axis)</code>","text":"<p>Find the first and last indices of non-masked values along an axis.</p> <p>Only tested upto 2D arrays. If <code>mask</code> is empty along <code>axis</code>, the first and last indices are set to 0 and the last index along the axis, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>Mask of <code>arr</code>.</p> required <code>axis</code> <code>int</code> <p>Axis along which to find the first and last indices.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>First index of non-masked values along <code>axis</code>.</p> <code>ndarray</code> <p>Last index of non-masked values along <code>axis</code>.</p> <p>Examples:</p> <p>1D Example:</p> <pre><code>&gt;&gt;&gt; mask = np.array([0, 0, 1, 1, 0])\n&gt;&gt;&gt; find_terminal_masked_indices(mask, axis=0)\n(2, 3)\n</code></pre> <p>2D Example (along rows):</p> <pre><code>&gt;&gt;&gt; mask = np.array([[0, 0, 1],\n...                  [1, 1, 0],\n...                  [0, 0, 0]])\n&gt;&gt;&gt; find_terminal_masked_indices(mask, axis=1)\n(array([2, 0, 0]), array([2, 1, -1]))\n</code></pre> <p>2D Example (along columns):</p> <pre><code>&gt;&gt;&gt; find_terminal_masked_indices(mask, axis=0)\n(array([1, 1, 0]), array([1, 1, 0]))\n</code></pre> Source code in <code>neuro_py/util/array.py</code> <pre><code>def find_terminal_masked_indices(\n    mask: np.ndarray, axis: int\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Find the first and last indices of non-masked values along an axis.\n\n    Only tested upto 2D arrays. If `mask` is empty along `axis`, the first and\n    last indices are set to 0 and the last index along the axis, respectively.\n\n    Parameters\n    ----------\n    mask : np.ndarray\n        Mask of `arr`.\n    axis : int\n        Axis along which to find the first and last indices.\n\n    Returns\n    -------\n    np.ndarray\n        First index of non-masked values along `axis`.\n    np.ndarray\n        Last index of non-masked values along `axis`.\n\n    Examples\n    --------\n    1D Example:\n    &gt;&gt;&gt; mask = np.array([0, 0, 1, 1, 0])\n    &gt;&gt;&gt; find_terminal_masked_indices(mask, axis=0)\n    (2, 3)\n\n    2D Example (along rows):\n    &gt;&gt;&gt; mask = np.array([[0, 0, 1],\n    ...                  [1, 1, 0],\n    ...                  [0, 0, 0]])\n    &gt;&gt;&gt; find_terminal_masked_indices(mask, axis=1)\n    (array([2, 0, 0]), array([2, 1, -1]))\n\n    2D Example (along columns):\n    &gt;&gt;&gt; find_terminal_masked_indices(mask, axis=0)\n    (array([1, 1, 0]), array([1, 1, 0]))\n    \"\"\"\n    first_idx = np.argmax(mask, axis=axis)\n    reversed_mask = np.flip(mask, axis=axis)\n    last_idx = mask.shape[axis] - np.argmax(reversed_mask, axis=axis) - 1\n\n    return first_idx, last_idx\n</code></pre>"},{"location":"reference/neuro_py/util/array/#neuro_py.util.array.is_nested","title":"<code>is_nested(array)</code>","text":"<p>Check if an array is nested.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the array is nested, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_nested(np.array([1, 2, 3]))\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; is_nested(np.array([[1, 2], [3, 4]]))\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; is_nested(np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]))\nFalse\n</code></pre> <pre><code>&gt;&gt;&gt; is_nested(np.array([np.array([1, 2]), np.array([3, 4, 5])], dtype=object))\nTrue\n</code></pre> Source code in <code>neuro_py/util/array.py</code> <pre><code>def is_nested(array: np.ndarray) -&gt; bool:\n    \"\"\"\n    Check if an array is nested.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Input array.\n\n    Returns\n    -------\n    bool\n        True if the array is nested, False otherwise.\n\n    Examples\n    --------\n    &gt;&gt;&gt; is_nested(np.array([1, 2, 3]))\n    False\n\n    &gt;&gt;&gt; is_nested(np.array([[1, 2], [3, 4]]))\n    False\n\n    &gt;&gt;&gt; is_nested(np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]))\n    False\n\n    &gt;&gt;&gt; is_nested(np.array([np.array([1, 2]), np.array([3, 4, 5])], dtype=object))\n    True\n    \"\"\"\n    if array.dtype != object:\n        return False\n    return any(isinstance(item, np.ndarray) for item in array)\n</code></pre>"},{"location":"reference/neuro_py/util/array/#neuro_py.util.array.replace_border_zeros_with_nan","title":"<code>replace_border_zeros_with_nan(arr)</code>","text":"<p>Replace zero values at the borders of each dimension of a n-dimensional array with NaN.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array with zero values at the borders replaced with NaN.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; arr = np.arange(27).reshape(3, 3, 3)\n&gt;&gt;&gt; arr[0, 2] = arr[2, 2] = arr[2, 0, 0] = arr[1, 1, 1] = 0\n&gt;&gt;&gt; replace_border_zeros_with_nan(arr)\narray([[[nan,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [nan, nan, nan]],\n</code></pre> <pre><code>   [[ 9., 10., 11.],\n    [12.,  0., 14.],\n    [15., 16., 17.]],\n\n   [[nan, 19., 20.],\n    [21., 22., 23.],\n    [nan, nan, nan]]])\n</code></pre> Source code in <code>neuro_py/util/array.py</code> <pre><code>def replace_border_zeros_with_nan(arr: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Replace zero values at the borders of each dimension of a n-dimensional\n    array with NaN.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input array.\n\n    Returns\n    -------\n    np.ndarray\n        Array with zero values at the borders replaced with NaN.\n\n    Examples\n    --------\n    &gt;&gt;&gt; arr = np.arange(27).reshape(3, 3, 3)\n    &gt;&gt;&gt; arr[0, 2] = arr[2, 2] = arr[2, 0, 0] = arr[1, 1, 1] = 0\n    &gt;&gt;&gt; replace_border_zeros_with_nan(arr)\n    array([[[nan,  1.,  2.],\n            [ 3.,  4.,  5.],\n            [nan, nan, nan]],\n\n           [[ 9., 10., 11.],\n            [12.,  0., 14.],\n            [15., 16., 17.]],\n\n           [[nan, 19., 20.],\n            [21., 22., 23.],\n            [nan, nan, nan]]])\n    \"\"\"\n    arr = np.array(arr, dtype=float)\n    dims = arr.shape\n\n    for axis in range(len(dims)):\n        # Find indices where zero values start and stop\n        for idx in np.ndindex(*[dims[i] for i in range(len(dims)) if i != axis]):\n            slicer = list(idx)\n            slicer.insert(\n                axis, slice(None)\n            )  # Insert the full slice along the current axis\n\n            # Check for first sequence of zeros\n            subarray = arr[tuple(slicer)]\n            first_zero_indices = np.where(np.cumsum(subarray != 0) == 0)[0]\n            if len(first_zero_indices) &gt; 0:\n                subarray[first_zero_indices] = np.nan\n\n            # Check for last sequence of zeros\n            last_zero_indices = np.where(np.cumsum(subarray[::-1] != 0) == 0)[0]\n            if len(last_zero_indices) &gt; 0:\n                subarray[-last_zero_indices - 1] = np.nan\n\n            arr[tuple(slicer)] = subarray  # Replace modified subarray\n\n    return arr\n</code></pre>"},{"location":"tutorials/attractor_landscape/","title":"Attractor Estimation","text":"In\u00a0[\u00a0]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nimport napari\nimport neuro_py as npy\nimport scipy\nimport sklearn\nimport sklearn.decomposition\n\nfrom IPython.display import HTML\nfrom matplotlib.animation import FuncAnimation\nfrom napari_animation import Animation\n\nwarnings.simplefilter(\"ignore\", category=RuntimeWarning)\n</pre> %reload_ext autoreload %autoreload 2 import warnings  import numpy as np import matplotlib.pyplot as plt import ipywidgets as widgets import napari import neuro_py as npy import scipy import sklearn import sklearn.decomposition  from IPython.display import HTML from matplotlib.animation import FuncAnimation from napari_animation import Animation  warnings.simplefilter(\"ignore\", category=RuntimeWarning) In\u00a0[\u00a0]: Copied! <pre>def gaussian(X, X0, sig, A):\n    G = A * np.exp(-((X - X0) ** 2) / (2 * sig**2))\n    dG = G * (X0 - X) / sig**2\n    return -G, -dG\n\n\ndef mix_gaussians(X, Xp, a):\n    u1, du1 = gaussian(X, Xp[0], 1, a[0])\n    u2, du2 = gaussian(X, Xp[1], 1, a[1])\n    u3, du3 = gaussian(X, Xp[2], 1, a[2])\n    u = u1 + u2 + u3\n    du = du1 + du2 + du3\n    return u, du\n\n\ndef simulate_trials(Xp, a, num_trials, iterations, dt, noise_fac):\n    \"\"\"Simulate trials using Langevin dynamics with noise\"\"\"\n    lan_fac = noise_fac * np.sqrt(dt)\n    X_dyn = np.empty((num_trials, iterations))\n    for i in range(num_trials):\n        # random initial condition\n        X_dyn[i, 0] = np.random.choice(Xp) + np.random.randn() * lan_fac\n        for ii in range(iterations - 1):\n            # energy and gradient at current position\n            E, dE = mix_gaussians(X_dyn[i, ii], Xp, a)\n            X_dyn[i, ii + 1] = X_dyn[i, ii] - dt * dE + lan_fac * np.random.randn()\n    return X_dyn\n</pre> def gaussian(X, X0, sig, A):     G = A * np.exp(-((X - X0) ** 2) / (2 * sig**2))     dG = G * (X0 - X) / sig**2     return -G, -dG   def mix_gaussians(X, Xp, a):     u1, du1 = gaussian(X, Xp[0], 1, a[0])     u2, du2 = gaussian(X, Xp[1], 1, a[1])     u3, du3 = gaussian(X, Xp[2], 1, a[2])     u = u1 + u2 + u3     du = du1 + du2 + du3     return u, du   def simulate_trials(Xp, a, num_trials, iterations, dt, noise_fac):     \"\"\"Simulate trials using Langevin dynamics with noise\"\"\"     lan_fac = noise_fac * np.sqrt(dt)     X_dyn = np.empty((num_trials, iterations))     for i in range(num_trials):         # random initial condition         X_dyn[i, 0] = np.random.choice(Xp) + np.random.randn() * lan_fac         for ii in range(iterations - 1):             # energy and gradient at current position             E, dE = mix_gaussians(X_dyn[i, ii], Xp, a)             X_dyn[i, ii + 1] = X_dyn[i, ii] - dt * dE + lan_fac * np.random.randn()     return X_dyn <p>Set parameters for the simulation generating the synthetic data influenced by multiple bump attractors.</p> In\u00a0[\u00a0]: Copied! <pre>Xp = [-2.5, 0, 2.5]  # positions of the minima of bump attractors\nattractordepths = [0.25, 0.2, 0.25]\n\nntrials = 100\niterations = 5000\ndomain_bins = proj_bins = 100\ndt = 0.1  # time step\nnoise_fac = 0.15  # noise factor\n\n# generate data\nX_dyn = simulate_trials(Xp, attractordepths, ntrials, iterations, dt, noise_fac)\n</pre> Xp = [-2.5, 0, 2.5]  # positions of the minima of bump attractors attractordepths = [0.25, 0.2, 0.25]  ntrials = 100 iterations = 5000 domain_bins = proj_bins = 100 dt = 0.1  # time step noise_fac = 0.15  # noise factor  # generate data X_dyn = simulate_trials(Xp, attractordepths, ntrials, iterations, dt, noise_fac) In\u00a0[\u00a0]: Copied! <pre>potential_pos_t, grad_pos_t_svm, H, latentedges, domainedges = (\n    npy.ensemble.potential_landscape(X_dyn, proj_bins, domain_bins)\n)\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nX = np.arange(-10, 10, 0.0025)\n\nE, dE = mix_gaussians(X, Xp, attractordepths)\naxes[0].plot(X, E, linewidth=2, label=\"Potential\")\naxes[0].set_title(\"Potential Landscape\")\naxes[0].set_xlabel(\"Position\")\naxes[0].set_ylabel(\"Energy\")\n\nfor i in range(len(Xp)):\n    axes[0].plot(\n        X, gaussian(X, Xp[i], 1, attractordepths[i])[0], label=f\"Component {i}\"\n    )\naxes[0].legend()\n\nT = np.arange(0, iterations * dt, dt)\naxes[1].plot(T, X_dyn[0], linewidth=2, label=\"Trial 1\")\naxes[1].plot(T, X_dyn[-1], linewidth=2, label=f\"Trial {ntrials}\")\naxes[1].set_title(\"Dynamics of trials\")\naxes[1].set_xlabel(\"Time\")\naxes[1].set_ylabel(\"Position (X)\")\naxes[1].legend()\n\naxes[2].plot(X, E, linewidth=2, label=\"Potential\")\naxes[2].plot(latentedges[:-1], np.nanmean(potential_pos_t, axis=1), label=\"Fit\")\naxes[2].set_title(\"Potential Landscape Estimation\")\naxes[2].set_xlabel(\"Position\")\naxes[2].set_ylabel(\"Energy\")\naxes[2].legend()\n\nplt.show()\n</pre> potential_pos_t, grad_pos_t_svm, H, latentedges, domainedges = (     npy.ensemble.potential_landscape(X_dyn, proj_bins, domain_bins) )  # Visualize fig, axes = plt.subplots(1, 3, figsize=(15, 4)) X = np.arange(-10, 10, 0.0025)  E, dE = mix_gaussians(X, Xp, attractordepths) axes[0].plot(X, E, linewidth=2, label=\"Potential\") axes[0].set_title(\"Potential Landscape\") axes[0].set_xlabel(\"Position\") axes[0].set_ylabel(\"Energy\")  for i in range(len(Xp)):     axes[0].plot(         X, gaussian(X, Xp[i], 1, attractordepths[i])[0], label=f\"Component {i}\"     ) axes[0].legend()  T = np.arange(0, iterations * dt, dt) axes[1].plot(T, X_dyn[0], linewidth=2, label=\"Trial 1\") axes[1].plot(T, X_dyn[-1], linewidth=2, label=f\"Trial {ntrials}\") axes[1].set_title(\"Dynamics of trials\") axes[1].set_xlabel(\"Time\") axes[1].set_ylabel(\"Position (X)\") axes[1].legend()  axes[2].plot(X, E, linewidth=2, label=\"Potential\") axes[2].plot(latentedges[:-1], np.nanmean(potential_pos_t, axis=1), label=\"Fit\") axes[2].set_title(\"Potential Landscape Estimation\") axes[2].set_xlabel(\"Position\") axes[2].set_ylabel(\"Energy\") axes[2].legend()  plt.show() In\u00a0[5]: Copied! <pre>def gaussian_nd(X, X0, sig, A):  # -&gt; tuple:\n    \"\"\"n-dimensional Gaussian function\n\n    Parameters\n    ----------\n    X : np.ndarray\n        n-dimensional grid space. Shape: (n_points, ndim).\n    X0 : np.ndarray\n        Centers of the Gaussian for each dimension (same shape as X).\n    sig : np.ndarray or float\n        Standard deviations for each dimension (either same shape as X or a single float).\n    A : float\n        Amplitude of the Gaussian.\n\n    Returns\n    -------\n    G : np.ndarray\n        Value of the n-dimensional Gaussian function at X.\n    dG : np.ndarray\n        Derivative of the n-dimensional Gaussian function along each dimension.\n    \"\"\"\n    X = np.atleast_2d(X)\n    X0 = np.atleast_1d(X0)\n    sig = np.atleast_1d(sig)\n\n    # Ensure X0 and sig are compatible with each dimension of the grid\n    # assert len(X) == len(X0), \"X0 should have the same number of dimensions as X\"\n    # assert len(X) == len(sig), \"sig should have the same number of dimensions as X\"\n\n    # Compute the Gaussian function values across the grid\n    exponent = -np.sum((X - X0) ** 2 / (2 * sig**2), axis=-1)\n    G = A * np.exp(exponent)\n\n    # Use np.gradient to compute the derivative along each axis\n    dG = (((X0 - X) / sig**2).T * G).T  # shape: (n_points, ndim)\n\n    return -G, -dG\n\n\ndef mix_functions(X, Xp, a, std=1, func=gaussian_nd):\n    \"\"\"Sum of Gaussians.\n\n    Parameters\n    ----------\n    X : float\n        Position\n    Xp : list\n        Centers of the Gaussians\n    a : list\n        Amplitudes of the Gaussians\n    n : int\n        Number of Gaussians\n\n    Returns\n    -------\n    U : float\n        Mixture of Gaussians\n    dU : float\n        Derivative of the mixture of Gaussians\n    \"\"\"\n    for i, xp in enumerate(Xp):\n        if i == 0:\n            U, dU = func(X, xp, std, a[i])\n        else:\n            u, du = func(X, xp, std, a[i])\n            U += u\n            dU += du\n    return U, dU\n\n\ndef simulate_trials(\n    Xp,\n    a,\n    ntrials,\n    iterations,\n    dt,\n    noise_fac,\n    std=1,\n    ngaussians=3,\n    xstarts=None,\n    func=gaussian_nd,\n):\n    \"\"\"Simulate trials\n\n    Parameters\n    ----------\n    Xp : list\n        Centers of the Gaussians\n    a : list\n        Amplitudes of the Gaussians\n    ntrials : int\n        Number of trials\n    iterations : int\n        Number of iterations\n    dt : float\n        Time step\n    noise_fac : float\n        Noise factor\n\n    Returns\n    -------\n    X_dyn : array\n        Trajectories of the trials\n    \"\"\"\n    nnrns = len(np.atleast_1d(Xp[0]))\n    lan_fac = noise_fac * np.sqrt(dt)  # Langevin factor\n    X_dyn = np.empty((ntrials, iterations, nnrns))  # Trajectories of the trials\n    for i in range(ntrials):\n        sel_ix = np.random.randint(len(Xp))\n        if xstarts is None or len(xstarts) &lt;= i:\n            startstate = (\n                Xp[sel_ix] + np.random.randn(*X_dyn[i, 0].shape) * lan_fac\n            )  # random initial condition\n        else:\n            startstate = xstarts[i]\n        X_dyn[i, 0] = startstate\n        for ii in range(iterations - 1):\n            E, dE = mix_functions(X_dyn[i, ii], Xp, a, std=std, func=func)\n            X_dyn[i, ii + 1] = (\n                X_dyn[i, ii] - dt * dE + lan_fac * np.random.randn(*dE.shape)\n            )\n    return X_dyn\n</pre> def gaussian_nd(X, X0, sig, A):  # -&gt; tuple:     \"\"\"n-dimensional Gaussian function      Parameters     ----------     X : np.ndarray         n-dimensional grid space. Shape: (n_points, ndim).     X0 : np.ndarray         Centers of the Gaussian for each dimension (same shape as X).     sig : np.ndarray or float         Standard deviations for each dimension (either same shape as X or a single float).     A : float         Amplitude of the Gaussian.      Returns     -------     G : np.ndarray         Value of the n-dimensional Gaussian function at X.     dG : np.ndarray         Derivative of the n-dimensional Gaussian function along each dimension.     \"\"\"     X = np.atleast_2d(X)     X0 = np.atleast_1d(X0)     sig = np.atleast_1d(sig)      # Ensure X0 and sig are compatible with each dimension of the grid     # assert len(X) == len(X0), \"X0 should have the same number of dimensions as X\"     # assert len(X) == len(sig), \"sig should have the same number of dimensions as X\"      # Compute the Gaussian function values across the grid     exponent = -np.sum((X - X0) ** 2 / (2 * sig**2), axis=-1)     G = A * np.exp(exponent)      # Use np.gradient to compute the derivative along each axis     dG = (((X0 - X) / sig**2).T * G).T  # shape: (n_points, ndim)      return -G, -dG   def mix_functions(X, Xp, a, std=1, func=gaussian_nd):     \"\"\"Sum of Gaussians.      Parameters     ----------     X : float         Position     Xp : list         Centers of the Gaussians     a : list         Amplitudes of the Gaussians     n : int         Number of Gaussians      Returns     -------     U : float         Mixture of Gaussians     dU : float         Derivative of the mixture of Gaussians     \"\"\"     for i, xp in enumerate(Xp):         if i == 0:             U, dU = func(X, xp, std, a[i])         else:             u, du = func(X, xp, std, a[i])             U += u             dU += du     return U, dU   def simulate_trials(     Xp,     a,     ntrials,     iterations,     dt,     noise_fac,     std=1,     ngaussians=3,     xstarts=None,     func=gaussian_nd, ):     \"\"\"Simulate trials      Parameters     ----------     Xp : list         Centers of the Gaussians     a : list         Amplitudes of the Gaussians     ntrials : int         Number of trials     iterations : int         Number of iterations     dt : float         Time step     noise_fac : float         Noise factor      Returns     -------     X_dyn : array         Trajectories of the trials     \"\"\"     nnrns = len(np.atleast_1d(Xp[0]))     lan_fac = noise_fac * np.sqrt(dt)  # Langevin factor     X_dyn = np.empty((ntrials, iterations, nnrns))  # Trajectories of the trials     for i in range(ntrials):         sel_ix = np.random.randint(len(Xp))         if xstarts is None or len(xstarts) &lt;= i:             startstate = (                 Xp[sel_ix] + np.random.randn(*X_dyn[i, 0].shape) * lan_fac             )  # random initial condition         else:             startstate = xstarts[i]         X_dyn[i, 0] = startstate         for ii in range(iterations - 1):             E, dE = mix_functions(X_dyn[i, ii], Xp, a, std=std, func=func)             X_dyn[i, ii + 1] = (                 X_dyn[i, ii] - dt * dE + lan_fac * np.random.randn(*dE.shape)             )     return X_dyn In\u00a0[\u00a0]: Copied! <pre>Xp_1D = [0]  # Center of Gaussian\nvar_1D = 0.5  # Variance\nX_1D = np.linspace(-5, 5, 100).reshape(-1, 1)  # 1D input points\nA_1D = 1.0  # Amplitude\n\nG_1D, dG_1D = gaussian_nd(X_1D, Xp_1D, var_1D, A_1D)\n\nXp_2D = [0, 0]\nvar_2D = 1\nX_2D = np.random.randn(5000, 2)  # 2D input points\nA_2D = 1.0  # Amplitude\n\nG_2D, dG_2D = gaussian_nd(X_2D, Xp_2D, var_2D, A_2D)\n\n\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\n\naxes[0].plot(X_1D, G_1D, label=\"Gaussian\")\naxes[0].plot(X_1D, dG_1D, label=\"Derivative\")\naxes[0].set_title(\"1D Gaussian\")\naxes[0].set_xlabel(\"Position\")\naxes[0].set_ylabel(\"Potential\")\naxes[0].legend()\n\naxes[1].scatter(X_2D[:, 0], X_2D[:, 1], c=G_2D, cmap=\"viridis\", s=5)\n\naxes[1].set_title(\"2D Gaussian\")\naxes[1].set_xlabel(\"X\")\naxes[1].set_ylabel(\"Y\")\n\naxes[2].scatter(X_2D[:, 0], X_2D[:, 1], c=dG_2D[:, 0], cmap=\"viridis\", s=5)\naxes[2].set_title(\"2D Gaussian Derivative (X)\")\naxes[2].set_xlabel(\"X\")\naxes[2].set_ylabel(\"Y\")\n\naxes[3].scatter(X_2D[:, 0], X_2D[:, 1], c=dG_2D[:, 1], cmap=\"viridis\", s=5)\naxes[3].set_title(\"2D Gaussian Derivative (Y)\")\naxes[3].set_xlabel(\"X\")\naxes[3].set_ylabel(\"Y\")\n\nplt.show()\n</pre> Xp_1D = [0]  # Center of Gaussian var_1D = 0.5  # Variance X_1D = np.linspace(-5, 5, 100).reshape(-1, 1)  # 1D input points A_1D = 1.0  # Amplitude  G_1D, dG_1D = gaussian_nd(X_1D, Xp_1D, var_1D, A_1D)  Xp_2D = [0, 0] var_2D = 1 X_2D = np.random.randn(5000, 2)  # 2D input points A_2D = 1.0  # Amplitude  G_2D, dG_2D = gaussian_nd(X_2D, Xp_2D, var_2D, A_2D)   fig, axes = plt.subplots(1, 4, figsize=(20, 5))  axes[0].plot(X_1D, G_1D, label=\"Gaussian\") axes[0].plot(X_1D, dG_1D, label=\"Derivative\") axes[0].set_title(\"1D Gaussian\") axes[0].set_xlabel(\"Position\") axes[0].set_ylabel(\"Potential\") axes[0].legend()  axes[1].scatter(X_2D[:, 0], X_2D[:, 1], c=G_2D, cmap=\"viridis\", s=5)  axes[1].set_title(\"2D Gaussian\") axes[1].set_xlabel(\"X\") axes[1].set_ylabel(\"Y\")  axes[2].scatter(X_2D[:, 0], X_2D[:, 1], c=dG_2D[:, 0], cmap=\"viridis\", s=5) axes[2].set_title(\"2D Gaussian Derivative (X)\") axes[2].set_xlabel(\"X\") axes[2].set_ylabel(\"Y\")  axes[3].scatter(X_2D[:, 0], X_2D[:, 1], c=dG_2D[:, 1], cmap=\"viridis\", s=5) axes[3].set_title(\"2D Gaussian Derivative (Y)\") axes[3].set_xlabel(\"X\") axes[3].set_ylabel(\"Y\")  plt.show() <p>Set parameters for the simulation generating the synthetic data influenced by multiple bump attractors.</p> <p>Note: We artificially create <code>xstarts</code> throughout different simulations to set some of the initial states of the simulated trajectories to span the entire space for better coverage, which is not necessary in real data and intelligent initialization strategies can be used for simulations.</p> In\u00a0[\u00a0]: Copied! <pre>Xp = np.asarray([[-2, -2], [0, 0], [2, 2]])\nattractordepths = [0.25, 0.2, 0.25]\n\nntrials = 3000\niterations = 500\ndomain_bins = 20\nproj_bins = 25\n\ndt = 0.1  # time step\nnoise_fac = 0.1  # noise factor\n\n# Generate starting points\nposxstarts = np.linspace(-3.5, 3.5, 15)\nxstarts = np.asarray(np.meshgrid(posxstarts, posxstarts)).T.reshape(-1, 2)\nxstarts = np.repeat(xstarts, 5, axis=0)\nnp.random.shuffle(xstarts)\n\n# Simulate dynamics\nX_dyn = simulate_trials(\n    Xp,\n    attractordepths,\n    ntrials,\n    iterations,\n    dt,\n    noise_fac,\n    ngaussians=len(Xp),\n    xstarts=xstarts,\n    std=1,\n)\n</pre> Xp = np.asarray([[-2, -2], [0, 0], [2, 2]]) attractordepths = [0.25, 0.2, 0.25]  ntrials = 3000 iterations = 500 domain_bins = 20 proj_bins = 25  dt = 0.1  # time step noise_fac = 0.1  # noise factor  # Generate starting points posxstarts = np.linspace(-3.5, 3.5, 15) xstarts = np.asarray(np.meshgrid(posxstarts, posxstarts)).T.reshape(-1, 2) xstarts = np.repeat(xstarts, 5, axis=0) np.random.shuffle(xstarts)  # Simulate dynamics X_dyn = simulate_trials(     Xp,     attractordepths,     ntrials,     iterations,     dt,     noise_fac,     ngaussians=len(Xp),     xstarts=xstarts,     std=1, ) In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\npotential_pos, potential_pos_t_nrns, grad_pos_t_svm, H, latentedges, domainedges = (\n    npy.ensemble.potential_landscape_nd(\n        X_dyn,\n        [np.linspace(-3.5, 3.5, proj_bins), np.linspace(-3.5, 3.5, proj_bins)],\n        domain_bins,\n    )\n)\n\nX = np.linspace(-3.5, 3.5, proj_bins)\ny = np.linspace(-3.5, 3.5, proj_bins)\nX, Y = np.meshgrid(X, y)\nX = np.stack((X, Y), axis=-1)\nX = X.reshape(-1, 2)\n\nE, dE = mix_functions(X, Xp, attractordepths, std=1, func=gaussian_nd)\n\ngaussians_nd = []\nfor i in range(len(Xp)):\n    gaussian = gaussian_nd(X, Xp[i], 1, attractordepths[i])[0].reshape(\n        proj_bins, proj_bins\n    )\n    gaussian = (gaussian - np.min(gaussian)) / (np.max(gaussian) - np.min(gaussian))\n    gaussians_nd.append(gaussian)\ngaussians_nd = np.stack(gaussians_nd, axis=-1)\n\n\ndef simulate(t=0):\n    fig = plt.figure(figsize=(17, 10))\n    axes = []\n\n    # make axes[0] 2D\n    axes.append(fig.add_subplot(2, 3, 1))\n    axes[-1].set_title(\"Potential Energy Landscape\")\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n\n    axes[-1].imshow(gaussians_nd[::-1])\n    # add color label for each attractor outside the plot colored: red, green, blue\n    colors = [\"cyan\", \"magenta\", \"yellow\"]\n    for i, txt in enumerate([\"Bump 1\", \"Bump 2\", \"Bump 3\"]):\n        axes[-1].text(gaussians_nd.shape[0], 2 * i, txt, color=colors[i])\n\n    # make axes[1] 3D\n    axes.append(fig.add_subplot(2, 3, 4, projection=\"3d\"))\n    X, Y = np.meshgrid(\n        np.linspace(-3.5, 3.5, proj_bins), np.linspace(-3.5, 3.5, proj_bins)\n    )\n    axes[-1].plot_surface(X, Y, E.reshape(proj_bins, proj_bins), cmap=\"turbo\")\n    # plot the xstarts over the surface\n    axes[-1].scatter(xstarts[:, 0], xstarts[:, 1], c=\"b\", marker=\".\", alpha=0.05)\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n    axes[-1].set_zlabel(\"Energy\")\n\n    axes.append(fig.add_subplot(2, 3, 2))\n    for i in range(ntrials):\n        axes[-1].plot(*(X_dyn[i].T), color=\"black\", alpha=0.02)\n    axes[-1].plot(*(X_dyn[0].T), linewidth=1, label=\"Trial 1\", alpha=0.5)\n    # start with green and end with red\n    axes[-1].plot(*X_dyn[0, 0], \"go\", label=\"Start\")\n    axes[-1].plot(*X_dyn[0, -1], \"rx\", label=\"End\")\n    axes[-1].plot(*(X_dyn[-1].T), linewidth=1, label=f\"Trial {ntrials}\", alpha=0.5)\n    axes[-1].plot(*X_dyn[-1, 0], \"go\")\n    axes[-1].plot(*X_dyn[-1, -1], \"rx\")\n\n    axes[-1].set_title(\"Dynamics of trials\")\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n    axes[-1].legend()\n\n    axes.append(fig.add_subplot(2, 3, 3, projection=\"3d\"))\n    X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])\n    axes[-1].plot_surface(X, Y, np.nanmean(potential_pos, axis=0), cmap=\"turbo\")\n    axes[-1].set_title(\"Potential Estimation\")\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n    axes[-1].set_zlabel(\"Energy\")\n\n    axes.append(fig.add_subplot(2, 3, 5))\n    # plot vector field\n    X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])\n    U = grad_pos_t_svm[:, :, t, 0]\n    V = grad_pos_t_svm[:, :, t, 1]\n    axes[-1].quiver(X, Y, U, V)\n    axes[-1].set_title(\"Phase plane\")\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n\n    axes.append(fig.add_subplot(2, 3, 6, projection=\"3d\"))\n    axes[-1].plot_surface(\n        X, Y, np.nanmean(potential_pos_t_nrns[:, :, t], axis=-1), cmap=\"turbo\"\n    )\n    axes[-1].set_title(\"Time-resolved Potential Estimation\")\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n    axes[-1].set_zlabel(\"Energy\")\n\n    plt.show()\n\n\n_ = widgets.interact(simulate, t=(0, domain_bins - 1))\n</pre> %matplotlib inline potential_pos, potential_pos_t_nrns, grad_pos_t_svm, H, latentedges, domainedges = (     npy.ensemble.potential_landscape_nd(         X_dyn,         [np.linspace(-3.5, 3.5, proj_bins), np.linspace(-3.5, 3.5, proj_bins)],         domain_bins,     ) )  X = np.linspace(-3.5, 3.5, proj_bins) y = np.linspace(-3.5, 3.5, proj_bins) X, Y = np.meshgrid(X, y) X = np.stack((X, Y), axis=-1) X = X.reshape(-1, 2)  E, dE = mix_functions(X, Xp, attractordepths, std=1, func=gaussian_nd)  gaussians_nd = [] for i in range(len(Xp)):     gaussian = gaussian_nd(X, Xp[i], 1, attractordepths[i])[0].reshape(         proj_bins, proj_bins     )     gaussian = (gaussian - np.min(gaussian)) / (np.max(gaussian) - np.min(gaussian))     gaussians_nd.append(gaussian) gaussians_nd = np.stack(gaussians_nd, axis=-1)   def simulate(t=0):     fig = plt.figure(figsize=(17, 10))     axes = []      # make axes[0] 2D     axes.append(fig.add_subplot(2, 3, 1))     axes[-1].set_title(\"Potential Energy Landscape\")     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")      axes[-1].imshow(gaussians_nd[::-1])     # add color label for each attractor outside the plot colored: red, green, blue     colors = [\"cyan\", \"magenta\", \"yellow\"]     for i, txt in enumerate([\"Bump 1\", \"Bump 2\", \"Bump 3\"]):         axes[-1].text(gaussians_nd.shape[0], 2 * i, txt, color=colors[i])      # make axes[1] 3D     axes.append(fig.add_subplot(2, 3, 4, projection=\"3d\"))     X, Y = np.meshgrid(         np.linspace(-3.5, 3.5, proj_bins), np.linspace(-3.5, 3.5, proj_bins)     )     axes[-1].plot_surface(X, Y, E.reshape(proj_bins, proj_bins), cmap=\"turbo\")     # plot the xstarts over the surface     axes[-1].scatter(xstarts[:, 0], xstarts[:, 1], c=\"b\", marker=\".\", alpha=0.05)     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")     axes[-1].set_zlabel(\"Energy\")      axes.append(fig.add_subplot(2, 3, 2))     for i in range(ntrials):         axes[-1].plot(*(X_dyn[i].T), color=\"black\", alpha=0.02)     axes[-1].plot(*(X_dyn[0].T), linewidth=1, label=\"Trial 1\", alpha=0.5)     # start with green and end with red     axes[-1].plot(*X_dyn[0, 0], \"go\", label=\"Start\")     axes[-1].plot(*X_dyn[0, -1], \"rx\", label=\"End\")     axes[-1].plot(*(X_dyn[-1].T), linewidth=1, label=f\"Trial {ntrials}\", alpha=0.5)     axes[-1].plot(*X_dyn[-1, 0], \"go\")     axes[-1].plot(*X_dyn[-1, -1], \"rx\")      axes[-1].set_title(\"Dynamics of trials\")     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")     axes[-1].legend()      axes.append(fig.add_subplot(2, 3, 3, projection=\"3d\"))     X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])     axes[-1].plot_surface(X, Y, np.nanmean(potential_pos, axis=0), cmap=\"turbo\")     axes[-1].set_title(\"Potential Estimation\")     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")     axes[-1].set_zlabel(\"Energy\")      axes.append(fig.add_subplot(2, 3, 5))     # plot vector field     X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])     U = grad_pos_t_svm[:, :, t, 0]     V = grad_pos_t_svm[:, :, t, 1]     axes[-1].quiver(X, Y, U, V)     axes[-1].set_title(\"Phase plane\")     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")      axes.append(fig.add_subplot(2, 3, 6, projection=\"3d\"))     axes[-1].plot_surface(         X, Y, np.nanmean(potential_pos_t_nrns[:, :, t], axis=-1), cmap=\"turbo\"     )     axes[-1].set_title(\"Time-resolved Potential Estimation\")     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")     axes[-1].set_zlabel(\"Energy\")      plt.show()   _ = widgets.interact(simulate, t=(0, domain_bins - 1)) <pre>interactive(children=(IntSlider(value=0, description='t', max=19), Output()), _dom_classes=('widget-interact',\u2026</pre> <p>Visualize the time-resolved potential estimation to observe how the landscape evolves over time.</p> In\u00a0[\u00a0]: Copied! <pre>t_steps = domain_bins  # Number of time steps\n\n\ndef update_surface(t, ax, surf):\n    \"\"\"\n    Update the surface plot for time `t`.\n    \"\"\"\n    X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])\n    Z = -np.nanmean(\n        np.asarray(\n            [np.nancumsum(grad_pos_t_svm[:, :, t, nrn], axis=nrn) for nrn in range(2)]\n        ),\n        axis=0,\n    )\n\n    surf[0].remove()  # Remove old surface\n    surf[0] = ax.plot_surface(X, Y, Z, cmap=\"viridis\", edgecolor=\"none\")\n\n\ndef animate(t):\n    \"\"\"\n    Update the plot for frame t and rotate the view.\n    \"\"\"\n    update_surface(t, ax, surf)\n    ax.view_init(elev=30, azim=t * 360 / t_steps)  # Rotate azimuth over time\n\n\n# Create figure and 3D plot\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111, projection=\"3d\")\nX, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])\nZ = np.nanmean(potential_pos_t_nrns[:, :, 0], axis=-1)\n\nsurf = [ax.plot_surface(X, Y, Z, cmap=\"viridis\", edgecolor=\"none\")]\nax.set_xlabel(\"Position X\")\nax.set_ylabel(\"Position Y\")\nax.set_zlabel(\"Potential Energy\")\n\n# Animate\nanim = FuncAnimation(fig, animate, frames=t_steps, interval=100, blit=False)\nHTML(anim.to_jshtml())\n</pre> t_steps = domain_bins  # Number of time steps   def update_surface(t, ax, surf):     \"\"\"     Update the surface plot for time `t`.     \"\"\"     X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])     Z = -np.nanmean(         np.asarray(             [np.nancumsum(grad_pos_t_svm[:, :, t, nrn], axis=nrn) for nrn in range(2)]         ),         axis=0,     )      surf[0].remove()  # Remove old surface     surf[0] = ax.plot_surface(X, Y, Z, cmap=\"viridis\", edgecolor=\"none\")   def animate(t):     \"\"\"     Update the plot for frame t and rotate the view.     \"\"\"     update_surface(t, ax, surf)     ax.view_init(elev=30, azim=t * 360 / t_steps)  # Rotate azimuth over time   # Create figure and 3D plot fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(111, projection=\"3d\") X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0]) Z = np.nanmean(potential_pos_t_nrns[:, :, 0], axis=-1)  surf = [ax.plot_surface(X, Y, Z, cmap=\"viridis\", edgecolor=\"none\")] ax.set_xlabel(\"Position X\") ax.set_ylabel(\"Position Y\") ax.set_zlabel(\"Potential Energy\")  # Animate anim = FuncAnimation(fig, animate, frames=t_steps, interval=100, blit=False) HTML(anim.to_jshtml()) Out[\u00a0]: Once Loop Reflect <p>We simulate a 2D system with a ring attractor, a special case where attractors form a ring structure, defined by:</p> <p>$$ \\psi(x, y) = \\frac{1}{\\pi \\sigma^4} (1 - \\frac{x^2 + y^2}{2 _ \\sigma^2}) {\\rm e}^{-\\frac{x^2 + y^2}{2 _ \\sigma^2}}$$</p> <p>where $\\sigma$ is the standard deviation of the repulsive potential.</p> <p>Goal:</p> <ul> <li>To understand how ring attractors influence particle dynamics.</li> </ul> <p>Key Insights:</p> <ul> <li>Particles converge to the ring attractor, forming a stable ring structure.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>def mexican_hat_nd(X, X0, sig, A):\n    \"\"\"n-dimensional Mexican hat function\n\n    Negative normalized second derivative of a Gaussian function.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        n-dimensional grid space. Shape: (n_points, ndim).\n    X0 : np.ndarray\n        Centers of the Mexican hat for each dimension (same shape as X).\n    sig : np.ndarray or float\n        Standard deviations for each dimension (either same shape as X or a single float).\n    A : float\n        Amplitude of the Mexican hat.\n\n    Returns\n    -------\n    G : np.ndarray\n        Value of the n-dimensional Mexican hat function at X.\n    dG : np.ndarray\n        Derivative of the n-dimensional Mexican hat function along each dimension.\n    \"\"\"\n    X = np.atleast_2d(X)\n    X0 = np.atleast_1d(X0)\n    sig = np.atleast_1d(sig)\n\n    # Compute the Mexican hat function values across the grid\n    exponent = np.sum((X - X0) ** 2 / (2 * sig**2), axis=-1)\n    M = A * (1 - exponent) * np.exp(-exponent) / sig**4\n\n    dM = -(\n        ((X - X0) / (np.pi * sig**6)).T * np.exp(-exponent) * (2 - exponent)\n    ).T  # shape: (n_points, ndim)\n\n    return M, dM\n</pre> def mexican_hat_nd(X, X0, sig, A):     \"\"\"n-dimensional Mexican hat function      Negative normalized second derivative of a Gaussian function.      Parameters     ----------     X : np.ndarray         n-dimensional grid space. Shape: (n_points, ndim).     X0 : np.ndarray         Centers of the Mexican hat for each dimension (same shape as X).     sig : np.ndarray or float         Standard deviations for each dimension (either same shape as X or a single float).     A : float         Amplitude of the Mexican hat.      Returns     -------     G : np.ndarray         Value of the n-dimensional Mexican hat function at X.     dG : np.ndarray         Derivative of the n-dimensional Mexican hat function along each dimension.     \"\"\"     X = np.atleast_2d(X)     X0 = np.atleast_1d(X0)     sig = np.atleast_1d(sig)      # Compute the Mexican hat function values across the grid     exponent = np.sum((X - X0) ** 2 / (2 * sig**2), axis=-1)     M = A * (1 - exponent) * np.exp(-exponent) / sig**4      dM = -(         ((X - X0) / (np.pi * sig**6)).T * np.exp(-exponent) * (2 - exponent)     ).T  # shape: (n_points, ndim)      return M, dM In\u00a0[\u00a0]: Copied! <pre>Xp_1D = [0]  # Center of Gaussian\ncov_1D = 0.5  # Variance\nX_1D = np.linspace(-5, 5, proj_bins).reshape(-1, 1)  # 1D input points\nA_1D = 1.0  # Amplitude\n\nG_1D, dG_1D = mexican_hat_nd(X_1D, Xp_1D, cov_1D, A_1D)\n\nXp_2D = [0, 0]\nX_2D = 3 * np.random.randn(7500, 2)  # 2D input points\nA_2D = 1.0  # Amplitude\n\nG_2D, dG_2D = mexican_hat_nd(X_2D, Xp_2D, 1, A_2D)\n\n\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\naxes = axes.ravel()\n\naxes[0].plot(X_1D, G_1D, label=\"Mexican Hat\")\naxes[0].plot(X_1D, dG_1D.ravel(), label=\"Derivative\")\naxes[0].set_title(\"1D Mexican Hat\")\naxes[0].set_xlabel(\"Position\")\naxes[0].set_ylabel(\"Value\")\naxes[0].legend()\n\naxes[1].remove()\naxes[1] = fig.add_subplot(1, 4, 2, projection=\"3d\")\nX, Y = np.meshgrid(X_1D, X_1D)\nX_in = np.stack((X, Y), axis=-1)\nX_in = X_in.reshape(-1, 2)\n# test mexican hat once\naxes[1].plot_surface(\n    X,\n    Y,\n    mexican_hat_nd(X_in, [0, 0], 1, 1)[0].reshape(proj_bins, proj_bins),\n    cmap=\"plasma\",\n)\n\naxes[1].set_title(\"2D Mexican Hat\")\naxes[1].set_xlabel(\"X\")\naxes[1].set_ylabel(\"Y\")\n\naxes[2].remove()\naxes[2] = fig.add_subplot(1, 4, 3, projection=\"3d\")\naxes[2].plot_surface(\n    X,\n    Y,\n    mexican_hat_nd(X_in, [0, 0], 1, 1)[1][:, 0].reshape(proj_bins, proj_bins),\n    cmap=\"plasma\",\n)\naxes[2].set_title(\"2D Mexican Hat Derivative (X)\")\naxes[2].set_xlabel(\"X\")\naxes[2].set_ylabel(\"Y\")\n\naxes[3].remove()\naxes[3] = fig.add_subplot(1, 4, 4, projection=\"3d\")\naxes[3].plot_surface(\n    X,\n    Y,\n    mexican_hat_nd(X_in, [0, 0], 1, 1)[1][:, 1].reshape(proj_bins, proj_bins),\n    cmap=\"plasma\",\n)\naxes[3].set_title(\"2D Mexican Hat Derivative (Y)\")\naxes[3].set_xlabel(\"X\")\naxes[3].set_ylabel(\"Y\")\n\n\nplt.show()\n</pre> Xp_1D = [0]  # Center of Gaussian cov_1D = 0.5  # Variance X_1D = np.linspace(-5, 5, proj_bins).reshape(-1, 1)  # 1D input points A_1D = 1.0  # Amplitude  G_1D, dG_1D = mexican_hat_nd(X_1D, Xp_1D, cov_1D, A_1D)  Xp_2D = [0, 0] X_2D = 3 * np.random.randn(7500, 2)  # 2D input points A_2D = 1.0  # Amplitude  G_2D, dG_2D = mexican_hat_nd(X_2D, Xp_2D, 1, A_2D)   fig, axes = plt.subplots(1, 4, figsize=(20, 5)) axes = axes.ravel()  axes[0].plot(X_1D, G_1D, label=\"Mexican Hat\") axes[0].plot(X_1D, dG_1D.ravel(), label=\"Derivative\") axes[0].set_title(\"1D Mexican Hat\") axes[0].set_xlabel(\"Position\") axes[0].set_ylabel(\"Value\") axes[0].legend()  axes[1].remove() axes[1] = fig.add_subplot(1, 4, 2, projection=\"3d\") X, Y = np.meshgrid(X_1D, X_1D) X_in = np.stack((X, Y), axis=-1) X_in = X_in.reshape(-1, 2) # test mexican hat once axes[1].plot_surface(     X,     Y,     mexican_hat_nd(X_in, [0, 0], 1, 1)[0].reshape(proj_bins, proj_bins),     cmap=\"plasma\", )  axes[1].set_title(\"2D Mexican Hat\") axes[1].set_xlabel(\"X\") axes[1].set_ylabel(\"Y\")  axes[2].remove() axes[2] = fig.add_subplot(1, 4, 3, projection=\"3d\") axes[2].plot_surface(     X,     Y,     mexican_hat_nd(X_in, [0, 0], 1, 1)[1][:, 0].reshape(proj_bins, proj_bins),     cmap=\"plasma\", ) axes[2].set_title(\"2D Mexican Hat Derivative (X)\") axes[2].set_xlabel(\"X\") axes[2].set_ylabel(\"Y\")  axes[3].remove() axes[3] = fig.add_subplot(1, 4, 4, projection=\"3d\") axes[3].plot_surface(     X,     Y,     mexican_hat_nd(X_in, [0, 0], 1, 1)[1][:, 1].reshape(proj_bins, proj_bins),     cmap=\"plasma\", ) axes[3].set_title(\"2D Mexican Hat Derivative (Y)\") axes[3].set_xlabel(\"X\") axes[3].set_ylabel(\"Y\")   plt.show() In\u00a0[\u00a0]: Copied! <pre>Xp = np.asarray([[0, 0]])\nattractordepths = [0.25]\n\nntrials = 3000\niterations = 100\ndomain_bins = 20\nproj_bins = 25\ndt = 0.2  # time step\nnoise_fac = 0.1  # noise factor\n\n# Generate starting points\nposxstarts = np.linspace(-3.5, 3.5, 15)\nxstarts = np.asarray(np.meshgrid(posxstarts, posxstarts)).T.reshape(-1, 2)\nxstarts = np.repeat(xstarts, 5, axis=0)\nnp.random.shuffle(xstarts)\n\n# Simulate dynamics\nX_dyn = simulate_trials(\n    Xp,\n    attractordepths,\n    ntrials,\n    iterations,\n    dt,\n    noise_fac,\n    ngaussians=len(Xp),\n    xstarts=xstarts,\n    func=mexican_hat_nd,\n)\n</pre> Xp = np.asarray([[0, 0]]) attractordepths = [0.25]  ntrials = 3000 iterations = 100 domain_bins = 20 proj_bins = 25 dt = 0.2  # time step noise_fac = 0.1  # noise factor  # Generate starting points posxstarts = np.linspace(-3.5, 3.5, 15) xstarts = np.asarray(np.meshgrid(posxstarts, posxstarts)).T.reshape(-1, 2) xstarts = np.repeat(xstarts, 5, axis=0) np.random.shuffle(xstarts)  # Simulate dynamics X_dyn = simulate_trials(     Xp,     attractordepths,     ntrials,     iterations,     dt,     noise_fac,     ngaussians=len(Xp),     xstarts=xstarts,     func=mexican_hat_nd, ) In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\npotential_pos, potential_pos_t_nrns, grad_pos_t_svm, H, latentedges, domainedges = (\n    npy.ensemble.potential_landscape_nd(\n        X_dyn,\n        [np.linspace(-3.5, 3.5, proj_bins), np.linspace(-3.5, 3.5, proj_bins)],\n        domain_bins,\n    )\n)\n\nX = np.linspace(-3.5, 3.5, proj_bins)\ny = np.linspace(-3.5, 3.5, proj_bins)\nX, Y = np.meshgrid(X, y)\nX = np.stack((X, Y), axis=-1)\nX = X.reshape(-1, 2)\n\nE, dE = mix_functions(X, Xp, attractordepths, func=mexican_hat_nd)\n\ngaussians_nd = []\nfor i in range(len(Xp)):\n    gaussian = mexican_hat_nd(X, Xp[i], 1, attractordepths[i])[0].reshape(\n        proj_bins, proj_bins\n    )\n    gaussian = (gaussian - np.min(gaussian)) / (np.max(gaussian) - np.min(gaussian))\n    gaussians_nd.append(gaussian)\ngaussians_nd = np.stack(gaussians_nd, axis=-1)\n\n\ndef simulate(t=0):\n    fig = plt.figure(figsize=(17, 10))\n    axes = []\n\n    # make axes[0] 2D\n    axes.append(fig.add_subplot(2, 3, 1))\n    axes[-1].set_title(\"Potential Energy Landscape\")\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n\n    axes[-1].imshow(gaussians_nd[::-1], cmap=\"turbo\")\n\n    # make axes[1] 3D\n    axes.append(fig.add_subplot(2, 3, 4, projection=\"3d\"))\n    X, Y = np.meshgrid(\n        np.linspace(-3.5, 3.5, proj_bins), np.linspace(-3.5, 3.5, proj_bins)\n    )\n    axes[-1].plot_surface(X, Y, E.reshape(proj_bins, proj_bins), cmap=\"turbo\")\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n    axes[-1].set_zlabel(\"Energy\")\n\n    axes.append(fig.add_subplot(2, 3, 2))\n    for i in range(ntrials):\n        axes[-1].plot(*(X_dyn[i].T), color=\"black\", alpha=0.02)\n    axes[-1].plot(*(X_dyn[0].T), linewidth=1, label=\"Trial 1\", alpha=0.5)\n    # start with green and end with red\n    axes[-1].plot(*X_dyn[0, 0], \"go\", label=\"Start\")\n    axes[-1].plot(*X_dyn[0, -1], \"rx\", label=\"End\")\n    axes[-1].plot(*(X_dyn[-1].T), linewidth=1, label=f\"Trial {ntrials}\", alpha=0.5)\n    axes[-1].plot(*X_dyn[-1, 0], \"go\")\n    axes[-1].plot(*X_dyn[-1, -1], \"rx\")\n\n    axes[-1].set_title(\"Dynamics of trials\")\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n    axes[-1].legend()\n\n    axes.append(fig.add_subplot(2, 3, 3, projection=\"3d\"))\n    X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])\n\n    axes[-1].plot_surface(X, Y, np.nanmean(potential_pos, axis=0), cmap=\"turbo\")\n    axes[-1].set_title(\"Potential Estimation\")\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n    axes[-1].set_zlabel(\"Energy\")\n\n    axes.append(fig.add_subplot(2, 3, 5))\n    # plot vector field\n    X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])\n    U = grad_pos_t_svm[:, :, t, 0]\n    V = grad_pos_t_svm[:, :, t, 1]\n    axes[-1].quiver(X, Y, U, V)\n    axes[-1].set_title(\"Phase plane\")\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n\n    axes.append(fig.add_subplot(2, 3, 6, projection=\"3d\"))\n    X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])\n\n    axes[-1].plot_surface(\n        X, Y, np.nanmean(potential_pos_t_nrns[:, :, t], axis=-1), cmap=\"turbo\"\n    )\n    axes[-1].set_title(\"Time-resolved Potential Estimation\")\n    axes[-1].set_xlabel(\"Position X\")\n    axes[-1].set_ylabel(\"Position Y\")\n    axes[-1].set_zlabel(\"Energy\")\n\n    plt.show()\n\n\n_ = widgets.interact(simulate, t=(0, domain_bins - 1))\n</pre> %matplotlib inline potential_pos, potential_pos_t_nrns, grad_pos_t_svm, H, latentedges, domainedges = (     npy.ensemble.potential_landscape_nd(         X_dyn,         [np.linspace(-3.5, 3.5, proj_bins), np.linspace(-3.5, 3.5, proj_bins)],         domain_bins,     ) )  X = np.linspace(-3.5, 3.5, proj_bins) y = np.linspace(-3.5, 3.5, proj_bins) X, Y = np.meshgrid(X, y) X = np.stack((X, Y), axis=-1) X = X.reshape(-1, 2)  E, dE = mix_functions(X, Xp, attractordepths, func=mexican_hat_nd)  gaussians_nd = [] for i in range(len(Xp)):     gaussian = mexican_hat_nd(X, Xp[i], 1, attractordepths[i])[0].reshape(         proj_bins, proj_bins     )     gaussian = (gaussian - np.min(gaussian)) / (np.max(gaussian) - np.min(gaussian))     gaussians_nd.append(gaussian) gaussians_nd = np.stack(gaussians_nd, axis=-1)   def simulate(t=0):     fig = plt.figure(figsize=(17, 10))     axes = []      # make axes[0] 2D     axes.append(fig.add_subplot(2, 3, 1))     axes[-1].set_title(\"Potential Energy Landscape\")     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")      axes[-1].imshow(gaussians_nd[::-1], cmap=\"turbo\")      # make axes[1] 3D     axes.append(fig.add_subplot(2, 3, 4, projection=\"3d\"))     X, Y = np.meshgrid(         np.linspace(-3.5, 3.5, proj_bins), np.linspace(-3.5, 3.5, proj_bins)     )     axes[-1].plot_surface(X, Y, E.reshape(proj_bins, proj_bins), cmap=\"turbo\")     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")     axes[-1].set_zlabel(\"Energy\")      axes.append(fig.add_subplot(2, 3, 2))     for i in range(ntrials):         axes[-1].plot(*(X_dyn[i].T), color=\"black\", alpha=0.02)     axes[-1].plot(*(X_dyn[0].T), linewidth=1, label=\"Trial 1\", alpha=0.5)     # start with green and end with red     axes[-1].plot(*X_dyn[0, 0], \"go\", label=\"Start\")     axes[-1].plot(*X_dyn[0, -1], \"rx\", label=\"End\")     axes[-1].plot(*(X_dyn[-1].T), linewidth=1, label=f\"Trial {ntrials}\", alpha=0.5)     axes[-1].plot(*X_dyn[-1, 0], \"go\")     axes[-1].plot(*X_dyn[-1, -1], \"rx\")      axes[-1].set_title(\"Dynamics of trials\")     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")     axes[-1].legend()      axes.append(fig.add_subplot(2, 3, 3, projection=\"3d\"))     X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])      axes[-1].plot_surface(X, Y, np.nanmean(potential_pos, axis=0), cmap=\"turbo\")     axes[-1].set_title(\"Potential Estimation\")     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")     axes[-1].set_zlabel(\"Energy\")      axes.append(fig.add_subplot(2, 3, 5))     # plot vector field     X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])     U = grad_pos_t_svm[:, :, t, 0]     V = grad_pos_t_svm[:, :, t, 1]     axes[-1].quiver(X, Y, U, V)     axes[-1].set_title(\"Phase plane\")     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")      axes.append(fig.add_subplot(2, 3, 6, projection=\"3d\"))     X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])      axes[-1].plot_surface(         X, Y, np.nanmean(potential_pos_t_nrns[:, :, t], axis=-1), cmap=\"turbo\"     )     axes[-1].set_title(\"Time-resolved Potential Estimation\")     axes[-1].set_xlabel(\"Position X\")     axes[-1].set_ylabel(\"Position Y\")     axes[-1].set_zlabel(\"Energy\")      plt.show()   _ = widgets.interact(simulate, t=(0, domain_bins - 1)) <pre>interactive(children=(IntSlider(value=0, description='t', max=19), Output()), _dom_classes=('widget-interact',\u2026</pre> <p>Visualize the time-resolved potential estimation for the ring attractor.</p> In\u00a0[\u00a0]: Copied! <pre>from matplotlib.animation import FuncAnimation\n\n# Dummy data to simulate inputs\nt_steps = domain_bins  # Number of time steps\n\n\ndef animate(t):\n    \"\"\"\n    Update the plot for frame t and rotate the view.\n    \"\"\"\n    update_surface(t, ax, surf)\n    ax.view_init(elev=30, azim=t / 2 * 360 / t_steps)  # Rotate azimuth over time\n\n\n# Create figure and 3D plot\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111, projection=\"3d\")\nX, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0])\nZ = np.nanmean(potential_pos_t_nrns[:, :, 0], axis=-1)\n\nsurf = [ax.plot_surface(X, Y, Z, cmap=\"turbo\", edgecolor=\"none\")]\nax.set_xlabel(\"Position X\")\nax.set_ylabel(\"Position Y\")\nax.set_zlabel(\"Potential Energy\")\n\n# Animate\nanim = FuncAnimation(fig, animate, frames=t_steps, interval=100, blit=False)\nHTML(anim.to_jshtml())\n</pre> from matplotlib.animation import FuncAnimation  # Dummy data to simulate inputs t_steps = domain_bins  # Number of time steps   def animate(t):     \"\"\"     Update the plot for frame t and rotate the view.     \"\"\"     update_surface(t, ax, surf)     ax.view_init(elev=30, azim=t / 2 * 360 / t_steps)  # Rotate azimuth over time   # Create figure and 3D plot fig = plt.figure(figsize=(6, 6)) ax = fig.add_subplot(111, projection=\"3d\") X, Y = np.meshgrid(latentedges[:-1, 0], latentedges[:-1, 0]) Z = np.nanmean(potential_pos_t_nrns[:, :, 0], axis=-1)  surf = [ax.plot_surface(X, Y, Z, cmap=\"turbo\", edgecolor=\"none\")] ax.set_xlabel(\"Position X\") ax.set_ylabel(\"Position Y\") ax.set_zlabel(\"Potential Energy\")  # Animate anim = FuncAnimation(fig, animate, frames=t_steps, interval=100, blit=False) HTML(anim.to_jshtml()) Out[\u00a0]: Once Loop Reflect In\u00a0[\u00a0]: Copied! <pre>Xp = np.asarray(\n    [\n        [-1.5, -1.5, -1.5],\n        [0, 0, 0],\n        [1.5, 1.5, 1.5],\n    ]\n)\nattractordepths = [1, 0.95, 1]\n\nntrials = 5000\niterations = 500\ndomain_bins = 20\nproj_bins = 20\ndt = 0.1  # time step\nnoise_fac = 0.05  # noise factor\nattractorfunc = gaussian_nd\n\n# Generate starting points\nposxstarts = np.linspace(-3.5, 3.5, 30)\nxstarts = np.asarray(np.meshgrid(posxstarts, posxstarts, posxstarts)).T.reshape(-1, 3)\nxstarts = np.repeat(xstarts, 1, axis=0)\nnp.random.shuffle(xstarts)\n\n# Simulate dynamics\nX_dyn = simulate_trials(\n    Xp,\n    attractordepths,\n    ntrials,\n    iterations,\n    dt,\n    noise_fac,\n    ngaussians=len(Xp),\n    xstarts=xstarts,\n    func=attractorfunc,\n    std=1,\n)  # shape: (ntrials, iterations, 3)\n</pre> Xp = np.asarray(     [         [-1.5, -1.5, -1.5],         [0, 0, 0],         [1.5, 1.5, 1.5],     ] ) attractordepths = [1, 0.95, 1]  ntrials = 5000 iterations = 500 domain_bins = 20 proj_bins = 20 dt = 0.1  # time step noise_fac = 0.05  # noise factor attractorfunc = gaussian_nd  # Generate starting points posxstarts = np.linspace(-3.5, 3.5, 30) xstarts = np.asarray(np.meshgrid(posxstarts, posxstarts, posxstarts)).T.reshape(-1, 3) xstarts = np.repeat(xstarts, 1, axis=0) np.random.shuffle(xstarts)  # Simulate dynamics X_dyn = simulate_trials(     Xp,     attractordepths,     ntrials,     iterations,     dt,     noise_fac,     ngaussians=len(Xp),     xstarts=xstarts,     func=attractorfunc,     std=1, )  # shape: (ntrials, iterations, 3) In\u00a0[\u00a0]: Copied! <pre># convert X_dyn to 2D with columns as (trial number, time step, x, y, z)\nX_dyn_2D = np.concatenate(\n    [\n        np.repeat(np.arange(ntrials), iterations).reshape(-1, 1),\n        np.broadcast_to(np.arange(iterations), (ntrials, iterations))\n        .flatten()\n        .reshape(-1, 1),\n        X_dyn.reshape(-1, 3),\n    ],\n    axis=1,\n)\n</pre> # convert X_dyn to 2D with columns as (trial number, time step, x, y, z) X_dyn_2D = np.concatenate(     [         np.repeat(np.arange(ntrials), iterations).reshape(-1, 1),         np.broadcast_to(np.arange(iterations), (ntrials, iterations))         .flatten()         .reshape(-1, 1),         X_dyn.reshape(-1, 3),     ],     axis=1, ) In\u00a0[\u00a0]: Copied! <pre>viewer = napari.Viewer(ndisplay=3)\nviewer.add_tracks(X_dyn_2D, tail_width=1, name=\"Dynamics of trials\")\n\nanimation = Animation(viewer)\nviewer.update_console({\"animation\": animation})\n\nviewer.camera.angles = (0.0, 0.0, 90.0)\nanimation.capture_keyframe()\nmax_steps = int(viewer.dims.range[0][1])\n# rotate the camera 360 degrees while advancing the time\nfor i in range(0, max_steps, 3):\n    angle_inc = i * 360 / max_steps\n    viewer.camera.angles = (\n        0.0 + 0.075 * angle_inc,\n        0.0 + angle_inc,\n        90.0 + 0.1 * angle_inc,\n    )\n    viewer.dims.current_step = (i, *viewer.dims.current_step[1:])\n    animation.capture_keyframe(steps=1)\n\nanimation.animate(\"anim3DTrajs.mp4\", canvas_only=True)\n\nHTML('&lt;video controls src=\"anim3DTrajs.mp4\" /&gt;')\n</pre> viewer = napari.Viewer(ndisplay=3) viewer.add_tracks(X_dyn_2D, tail_width=1, name=\"Dynamics of trials\")  animation = Animation(viewer) viewer.update_console({\"animation\": animation})  viewer.camera.angles = (0.0, 0.0, 90.0) animation.capture_keyframe() max_steps = int(viewer.dims.range[0][1]) # rotate the camera 360 degrees while advancing the time for i in range(0, max_steps, 3):     angle_inc = i * 360 / max_steps     viewer.camera.angles = (         0.0 + 0.075 * angle_inc,         0.0 + angle_inc,         90.0 + 0.1 * angle_inc,     )     viewer.dims.current_step = (i, *viewer.dims.current_step[1:])     animation.capture_keyframe(steps=1)  animation.animate(\"anim3DTrajs.mp4\", canvas_only=True)  HTML('') <pre>Rendering frames...\n</pre> <pre>  0%|          | 0/168 [00:00&lt;?, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1322, 786) to (1328, 800) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n  1%|          | 1/168 [00:00&lt;00:59,  2.79it/s][swscaler @ 0x6294300] Warning: data is not aligned! This can lead to a speed loss\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 168/168 [00:38&lt;00:00,  4.34it/s]\n</pre> Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\npot_timeaveraged, potential_pos_t_nrns, grad_pos_t_svm, H, latentedges, domainedges = (\n    npy.ensemble.potential_landscape_nd(\n        X_dyn,\n        [\n            np.linspace(-3.5, 3.5, proj_bins),\n            np.linspace(-3.5, 3.5, proj_bins),\n            np.linspace(-3.5, 3.5, proj_bins),\n        ],\n        domain_bins,\n        nanborderempty=True,\n    )\n)\n\nX = np.linspace(-3.5, 3.5, proj_bins)\ny = np.linspace(-3.5, 3.5, proj_bins)\nX, Y, Z = np.meshgrid(X, y, y)\nX = np.stack((X, Y, Z), axis=-1)\nX = X.reshape(-1, 3)\n\nE, dE = mix_functions(X, Xp, attractordepths, func=mexican_hat_nd)\n\ngaussians_nd = []\nfor i in range(len(Xp)):\n    gaussian = attractorfunc(X, Xp[i], 1, attractordepths[i])[0].reshape(\n        proj_bins, proj_bins, proj_bins\n    )\n    gaussian = (gaussian - np.max(gaussian)) / (np.max(gaussian) - np.min(gaussian)) + 1\n    gaussians_nd.append(gaussian)\ngaussians_nd = np.stack(gaussians_nd, axis=-1)\n</pre> %matplotlib inline pot_timeaveraged, potential_pos_t_nrns, grad_pos_t_svm, H, latentedges, domainedges = (     npy.ensemble.potential_landscape_nd(         X_dyn,         [             np.linspace(-3.5, 3.5, proj_bins),             np.linspace(-3.5, 3.5, proj_bins),             np.linspace(-3.5, 3.5, proj_bins),         ],         domain_bins,         nanborderempty=True,     ) )  X = np.linspace(-3.5, 3.5, proj_bins) y = np.linspace(-3.5, 3.5, proj_bins) X, Y, Z = np.meshgrid(X, y, y) X = np.stack((X, Y, Z), axis=-1) X = X.reshape(-1, 3)  E, dE = mix_functions(X, Xp, attractordepths, func=mexican_hat_nd)  gaussians_nd = [] for i in range(len(Xp)):     gaussian = attractorfunc(X, Xp[i], 1, attractordepths[i])[0].reshape(         proj_bins, proj_bins, proj_bins     )     gaussian = (gaussian - np.max(gaussian)) / (np.max(gaussian) - np.min(gaussian)) + 1     gaussians_nd.append(gaussian) gaussians_nd = np.stack(gaussians_nd, axis=-1) <p>Potential energy landscape averaged over time provide a clearer view of the attractor structure.</p> In\u00a0[\u00a0]: Copied! <pre>viewer = napari.Viewer(ndisplay=3)\n\nviewer.add_image(\n    np.nanmean(pot_timeaveraged, axis=0),\n    colormap=\"twilight\",\n    interpolation2d=\"nearest\",\n    rendering=\"minip\",\n    name=\"Estimated Potential Energy Landscape\",\n)\n\nviewer.add_image(\n    gaussians_nd.mean(-1),\n    colormap=\"twilight\",\n    interpolation2d=\"nearest\",\n    rendering=\"minip\",\n    name=\"Original Potential Energy Landscape\",\n)\n\n# grid view\nviewer.grid.enabled = True\n\n# napari.utils.nbscreenshot(viewer, canvas_only=True)\n\nanimation = Animation(viewer)\nviewer.update_console({\"animation\": animation})\n\nviewer.camera.angles = (0.0, 0.0, 90.0)\nanimation.capture_keyframe()\nmax_steps = 360\n# rotate the camera 360 degrees while advancing the time\nfor i in range(0, max_steps, 3):\n    angle_inc = i * 360 / max_steps\n    viewer.camera.angles = (\n        0.0 + 0.01 * angle_inc,\n        0.0 + 0.02 * angle_inc,\n        90.0 + angle_inc,\n    )\n    animation.capture_keyframe(steps=1)\n\nanimation.animate(\"anim3DPot.mp4\", canvas_only=True)\n\nHTML('&lt;video controls src=\"anim3DPot.mp4\" /&gt;')\n</pre> viewer = napari.Viewer(ndisplay=3)  viewer.add_image(     np.nanmean(pot_timeaveraged, axis=0),     colormap=\"twilight\",     interpolation2d=\"nearest\",     rendering=\"minip\",     name=\"Estimated Potential Energy Landscape\", )  viewer.add_image(     gaussians_nd.mean(-1),     colormap=\"twilight\",     interpolation2d=\"nearest\",     rendering=\"minip\",     name=\"Original Potential Energy Landscape\", )  # grid view viewer.grid.enabled = True  # napari.utils.nbscreenshot(viewer, canvas_only=True)  animation = Animation(viewer) viewer.update_console({\"animation\": animation})  viewer.camera.angles = (0.0, 0.0, 90.0) animation.capture_keyframe() max_steps = 360 # rotate the camera 360 degrees while advancing the time for i in range(0, max_steps, 3):     angle_inc = i * 360 / max_steps     viewer.camera.angles = (         0.0 + 0.01 * angle_inc,         0.0 + 0.02 * angle_inc,         90.0 + angle_inc,     )     animation.capture_keyframe(steps=1)  animation.animate(\"anim3DPot.mp4\", canvas_only=True)  HTML('') <pre>Rendering frames...\n</pre> <pre>  0%|          | 0/121 [00:00&lt;?, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1307, 808) to (1312, 816) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n[swscaler @ 0x64e2300] Warning: data is not aligned! This can lead to a speed loss\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 121/121 [00:01&lt;00:00, 66.71it/s]\n</pre> Out[\u00a0]: <p>Time-resolved potential estimation visualizes the evolution of the landscape over time.</p> In\u00a0[\u00a0]: Copied! <pre>viewer = napari.Viewer(ndisplay=3)\npot_timeresolved = np.nanmean(\n    np.asarray(\n        [\n            potential_pos_t_nrns[:, :, :, :, nrn]\n            for nrn in range(potential_pos_t_nrns.shape[-1])\n        ]\n    ),\n    axis=0,\n)\n\nviewer.add_image(\n    pot_timeresolved.T,\n    colormap=\"twilight\",\n    interpolation=\"nearest\",\n    rendering=\"minip\",\n    name=\"Estimated Potential Energy Landscape\",\n)\n\n# grid view\nviewer.grid.enabled = True\n\n# napari.utils.nbscreenshot(viewer, canvas_only=True)\n\nanimation = Animation(viewer)\nviewer.update_console({\"animation\": animation})\n\nviewer.camera.angles = (0.0, 0.0, 90.0)\nanimation.capture_keyframe()\nmax_steps = int(viewer.dims.range[0][1])\n# rotate the camera 360 degrees while advancing the time\nfor i in np.linspace(0, max_steps - 1, max_steps * 4):\n    angle_inc = i * 360 / max_steps\n    viewer.camera.angles = (\n        0.0 + 0.075 * angle_inc,\n        0.0 + angle_inc,\n        90.0 + 0.1 * angle_inc,\n    )\n    viewer.dims.current_step = (i, *viewer.dims.current_step[1:])\n    animation.capture_keyframe(steps=1)\n\nanimation.animate(\"anim3DPotTimeResolved.mp4\", canvas_only=True)\n\nHTML('&lt;video controls src=\"anim3DPotTimeResolved.mp4\" /&gt;')\n</pre> viewer = napari.Viewer(ndisplay=3) pot_timeresolved = np.nanmean(     np.asarray(         [             potential_pos_t_nrns[:, :, :, :, nrn]             for nrn in range(potential_pos_t_nrns.shape[-1])         ]     ),     axis=0, )  viewer.add_image(     pot_timeresolved.T,     colormap=\"twilight\",     interpolation=\"nearest\",     rendering=\"minip\",     name=\"Estimated Potential Energy Landscape\", )  # grid view viewer.grid.enabled = True  # napari.utils.nbscreenshot(viewer, canvas_only=True)  animation = Animation(viewer) viewer.update_console({\"animation\": animation})  viewer.camera.angles = (0.0, 0.0, 90.0) animation.capture_keyframe() max_steps = int(viewer.dims.range[0][1]) # rotate the camera 360 degrees while advancing the time for i in np.linspace(0, max_steps - 1, max_steps * 4):     angle_inc = i * 360 / max_steps     viewer.camera.angles = (         0.0 + 0.075 * angle_inc,         0.0 + angle_inc,         90.0 + 0.1 * angle_inc,     )     viewer.dims.current_step = (i, *viewer.dims.current_step[1:])     animation.capture_keyframe(steps=1)  animation.animate(\"anim3DPotTimeResolved.mp4\", canvas_only=True)  HTML('') <pre>/tmp/ipykernel_208666/2859654884.py:7: DeprecationWarning: Argument 'interpolation' is deprecated, please use 'interpolation2d' instead. The argument 'interpolation' was deprecated in 0.4.17 and it will be removed in 0.6.0.\n  viewer.add_image(pot_timeresolved.T, colormap='twilight', interpolation='nearest', rendering='minip',\n</pre> <pre>Rendering frames...\n</pre> <pre>  0%|          | 0/81 [00:00&lt;?, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1307, 786) to (1312, 800) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n[swscaler @ 0x59a91c0] Warning: data is not aligned! This can lead to a speed loss\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81/81 [00:01&lt;00:00, 75.12it/s]\n</pre> Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre># Do PCA on the data\npca = sklearn.decomposition.PCA(n_components=2)\nX_dyn_pca = pca.fit_transform(X_dyn_2D[:, 2:])\n\nviewer = napari.Viewer(ndisplay=2)\nviewer.add_tracks(\n    np.hstack((X_dyn_2D[:, :2], X_dyn_pca)), tail_width=1, name=\"Dynamics of trials\"\n)\n</pre> # Do PCA on the data pca = sklearn.decomposition.PCA(n_components=2) X_dyn_pca = pca.fit_transform(X_dyn_2D[:, 2:])  viewer = napari.Viewer(ndisplay=2) viewer.add_tracks(     np.hstack((X_dyn_2D[:, :2], X_dyn_pca)), tail_width=1, name=\"Dynamics of trials\" ) Out[\u00a0]: <pre>&lt;Tracks layer 'Dynamics of trials' at 0x7294c10ae880&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>def project_ndimage(\n    pot, proj_bins, pca\n):  # -&gt; NDArray:# -&gt; NDArray:# -&gt; NDArray:# -&gt; list:\n    \"\"\"\n    Compute the projected potentials in a reduced PCA space.\n\n    Parameters\n    ----------\n    pot : list of np.ndarray\n        List of potential landscapes for each neuron, where each element is an n-dimensional array.\n    proj_bins : int\n        Number of bins for projecting the data into reduced dimensions.\n    pca : sklearn.decomposition.PCA\n        PCA object to transform high-dimensional data to lower dimensions.\n\n    Returns\n    -------\n    pot_pos_projs : list of np.ndarray\n        List of projected potentials in the PCA-reduced space for each neuron.\n    \"\"\"\n    nnrns = len(pot)  # Number of neurons\n    ndim = pot[0].ndim  # Dimensionality of the potential landscape\n\n    # Flatten potential landscapes into a table\n    indices = np.indices(pot[0].shape).reshape(ndim, -1).T\n    pots_table = np.empty((len(indices), nnrns), dtype=float)\n    for ix, ndix in enumerate(indices):\n        pots_table[ix] = [pot[nrn][tuple(ndix)] for nrn in range(nnrns)]\n\n    # Prepare grid for PCA transformation\n    X = np.linspace(-3, 3, proj_bins)\n    mid_points = (X[1:] + X[:-1]) / 2  # Midpoints of the bins\n    grid = np.meshgrid(*[mid_points for _ in range(ndim)])\n    X = np.stack(grid, axis=-1).reshape(-1, ndim)\n\n    # Transform high-dimensional grid to PCA space\n    ndspace_proj = pca.transform(X)\n\n    # Handle NaN values in the potentials table\n    pots_table[np.isnan(pots_table)] = 0\n\n    # Project potentials into PCA space\n    pot_pos_projs = []\n    for nrn in range(nnrns):\n        # Compute the binned statistics\n        pot_pos_proj = scipy.stats.binned_statistic_dd(\n            ndspace_proj, pots_table[:, nrn], statistic=\"sum\", bins=proj_bins\n        ).statistic\n\n        # Count occurrences in each bin\n        H = scipy.stats.binned_statistic_dd(\n            ndspace_proj, pots_table[:, nrn], statistic=\"count\", bins=proj_bins\n        ).statistic\n\n        # Normalize and handle division by zero\n        pot_pos_proj = np.divide(pot_pos_proj, H, where=H != 0)\n        pot_pos_proj[H == 0] = np.nan  # Assign NaN to empty bins\n\n        pot_pos_projs.append(pot_pos_proj)\n    pot_pos_projs = np.asarray(pot_pos_projs)\n\n    return pot_pos_projs\n</pre> def project_ndimage(     pot, proj_bins, pca ):  # -&gt; NDArray:# -&gt; NDArray:# -&gt; NDArray:# -&gt; list:     \"\"\"     Compute the projected potentials in a reduced PCA space.      Parameters     ----------     pot : list of np.ndarray         List of potential landscapes for each neuron, where each element is an n-dimensional array.     proj_bins : int         Number of bins for projecting the data into reduced dimensions.     pca : sklearn.decomposition.PCA         PCA object to transform high-dimensional data to lower dimensions.      Returns     -------     pot_pos_projs : list of np.ndarray         List of projected potentials in the PCA-reduced space for each neuron.     \"\"\"     nnrns = len(pot)  # Number of neurons     ndim = pot[0].ndim  # Dimensionality of the potential landscape      # Flatten potential landscapes into a table     indices = np.indices(pot[0].shape).reshape(ndim, -1).T     pots_table = np.empty((len(indices), nnrns), dtype=float)     for ix, ndix in enumerate(indices):         pots_table[ix] = [pot[nrn][tuple(ndix)] for nrn in range(nnrns)]      # Prepare grid for PCA transformation     X = np.linspace(-3, 3, proj_bins)     mid_points = (X[1:] + X[:-1]) / 2  # Midpoints of the bins     grid = np.meshgrid(*[mid_points for _ in range(ndim)])     X = np.stack(grid, axis=-1).reshape(-1, ndim)      # Transform high-dimensional grid to PCA space     ndspace_proj = pca.transform(X)      # Handle NaN values in the potentials table     pots_table[np.isnan(pots_table)] = 0      # Project potentials into PCA space     pot_pos_projs = []     for nrn in range(nnrns):         # Compute the binned statistics         pot_pos_proj = scipy.stats.binned_statistic_dd(             ndspace_proj, pots_table[:, nrn], statistic=\"sum\", bins=proj_bins         ).statistic          # Count occurrences in each bin         H = scipy.stats.binned_statistic_dd(             ndspace_proj, pots_table[:, nrn], statistic=\"count\", bins=proj_bins         ).statistic          # Normalize and handle division by zero         pot_pos_proj = np.divide(pot_pos_proj, H, where=H != 0)         pot_pos_proj[H == 0] = np.nan  # Assign NaN to empty bins          pot_pos_projs.append(pot_pos_proj)     pot_pos_projs = np.asarray(pot_pos_projs)      return pot_pos_projs In\u00a0[\u00a0]: Copied! <pre># project potential landscape to 2D\nest_pot = np.asarray(\n    [np.mean(potential_pos_t_nrns[:, :, :, nrn], axis=-1) for nrn in range(3)]\n)\n\nest_pot_pos_projs = project_ndimage(est_pot, proj_bins, pca)\n\norig_pot_pos_projs = project_ndimage(gaussians_nd.T, proj_bins + 1, pca)\n\nest_pot_pos_projs = project_ndimage(est_pot, proj_bins, pca)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\naxes[0].imshow(np.nanmean(orig_pot_pos_projs, axis=0))\naxes[0].set_title(\"Original Potential Landscape Projection\")\naxes[1].set_xlabel(\"PC 1\")\naxes[1].set_ylabel(\"PC 2\")\naxes[1].imshow(np.nanmean(est_pot_pos_projs, axis=0))\naxes[1].set_title(\"Estimated Potential Landscape Projection\")\naxes[1].set_xlabel(\"PC 1\")\naxes[1].set_ylabel(\"PC 2\")\n\nplt.show()\n</pre> # project potential landscape to 2D est_pot = np.asarray(     [np.mean(potential_pos_t_nrns[:, :, :, nrn], axis=-1) for nrn in range(3)] )  est_pot_pos_projs = project_ndimage(est_pot, proj_bins, pca)  orig_pot_pos_projs = project_ndimage(gaussians_nd.T, proj_bins + 1, pca)  est_pot_pos_projs = project_ndimage(est_pot, proj_bins, pca)  fig, axes = plt.subplots(1, 2, figsize=(15, 5)) axes[0].imshow(np.nanmean(orig_pot_pos_projs, axis=0)) axes[0].set_title(\"Original Potential Landscape Projection\") axes[1].set_xlabel(\"PC 1\") axes[1].set_ylabel(\"PC 2\") axes[1].imshow(np.nanmean(est_pot_pos_projs, axis=0)) axes[1].set_title(\"Estimated Potential Landscape Projection\") axes[1].set_xlabel(\"PC 1\") axes[1].set_ylabel(\"PC 2\")  plt.show() In\u00a0[\u00a0]: Copied! <pre>ndim = 4\nXp = np.asarray([np.ones(ndim) * 1.5, np.zeros(ndim), np.ones(ndim) * -1.5])\nattractordepths = [1, 0.95, 1]\n\nntrials = 1000\niterations = 500\ndomain_bins = 20\nproj_bins = 20\ndt = 0.1  # time step\nnoise_fac = 0.05  # noise factor\nattractorfunc = gaussian_nd\n\n# Generate starting points\nposxstarts = np.linspace(-3, 3, 8)\nxstarts = np.asarray(np.meshgrid(*[posxstarts] * ndim)).T.reshape(-1, ndim)\nxstarts = np.repeat(xstarts, 2, axis=0)\nnp.random.shuffle(xstarts)\n\n# Simulate dynamics\nX_dyn = simulate_trials(\n    Xp,\n    attractordepths,\n    ntrials,\n    iterations,\n    dt,\n    noise_fac,\n    ngaussians=len(Xp),\n    xstarts=xstarts,\n    func=attractorfunc,\n    std=1,\n)  # shape: (ntrials, iterations, 30)\n</pre> ndim = 4 Xp = np.asarray([np.ones(ndim) * 1.5, np.zeros(ndim), np.ones(ndim) * -1.5]) attractordepths = [1, 0.95, 1]  ntrials = 1000 iterations = 500 domain_bins = 20 proj_bins = 20 dt = 0.1  # time step noise_fac = 0.05  # noise factor attractorfunc = gaussian_nd  # Generate starting points posxstarts = np.linspace(-3, 3, 8) xstarts = np.asarray(np.meshgrid(*[posxstarts] * ndim)).T.reshape(-1, ndim) xstarts = np.repeat(xstarts, 2, axis=0) np.random.shuffle(xstarts)  # Simulate dynamics X_dyn = simulate_trials(     Xp,     attractordepths,     ntrials,     iterations,     dt,     noise_fac,     ngaussians=len(Xp),     xstarts=xstarts,     func=attractorfunc,     std=1, )  # shape: (ntrials, iterations, 30) In\u00a0[\u00a0]: Copied! <pre># convert X_dyn to 2D with columns as (trial number, time step, x, y, z)\n# Reduce high-dimensional data using PCA\npca = sklearn.decomposition.PCA(n_components=3)\n\nX_dyn_tabular = np.concatenate(\n    [\n        np.repeat(np.arange(ntrials), iterations).reshape(-1, 1),\n        np.broadcast_to(np.arange(iterations), (ntrials, iterations))\n        .flatten()\n        .reshape(-1, 1),\n        pca.fit_transform(X_dyn.reshape(-1, ndim)),\n    ],\n    axis=1,\n)\n</pre> # convert X_dyn to 2D with columns as (trial number, time step, x, y, z) # Reduce high-dimensional data using PCA pca = sklearn.decomposition.PCA(n_components=3)  X_dyn_tabular = np.concatenate(     [         np.repeat(np.arange(ntrials), iterations).reshape(-1, 1),         np.broadcast_to(np.arange(iterations), (ntrials, iterations))         .flatten()         .reshape(-1, 1),         pca.fit_transform(X_dyn.reshape(-1, ndim)),     ],     axis=1, ) In\u00a0[\u00a0]: Copied! <pre>viewer = napari.Viewer(ndisplay=3)\nviewer.add_tracks(X_dyn_tabular, tail_width=1, name=\"Dynamics of trials\")\n\nanimation = Animation(viewer)\nviewer.update_console({\"animation\": animation})\n\nviewer.camera.angles = (0.0, 0.0, 90.0)\nanimation.capture_keyframe()\nmax_steps = int(viewer.dims.range[0][1])\n# rotate the camera 360 degrees while advancing the time\nfor i in range(0, max_steps, 3):\n    angle_inc = i * 360 / max_steps\n    viewer.camera.angles = (\n        0.0 + 0.075 * angle_inc,\n        0.0 + angle_inc,\n        90.0 + 0.1 * angle_inc,\n    )\n    viewer.dims.current_step = (i, *viewer.dims.current_step[1:])\n    animation.capture_keyframe(steps=1)\n\nanimation.animate(f\"anim{ndim}DTrajs.mp4\", canvas_only=True)\n\nHTML(f'&lt;video controls src=\"anim{ndim}DTrajs.mp4\" /&gt;')\n</pre> viewer = napari.Viewer(ndisplay=3) viewer.add_tracks(X_dyn_tabular, tail_width=1, name=\"Dynamics of trials\")  animation = Animation(viewer) viewer.update_console({\"animation\": animation})  viewer.camera.angles = (0.0, 0.0, 90.0) animation.capture_keyframe() max_steps = int(viewer.dims.range[0][1]) # rotate the camera 360 degrees while advancing the time for i in range(0, max_steps, 3):     angle_inc = i * 360 / max_steps     viewer.camera.angles = (         0.0 + 0.075 * angle_inc,         0.0 + angle_inc,         90.0 + 0.1 * angle_inc,     )     viewer.dims.current_step = (i, *viewer.dims.current_step[1:])     animation.capture_keyframe(steps=1)  animation.animate(f\"anim{ndim}DTrajs.mp4\", canvas_only=True)  HTML(f'') <pre>Rendering frames...\n</pre> <pre>  0%|          | 0/168 [00:00&lt;?, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1322, 786) to (1328, 800) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n[swscaler @ 0x6c1c300] Warning: data is not aligned! This can lead to a speed loss\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 168/168 [00:08&lt;00:00, 18.73it/s]\n</pre> Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>(\n    potential_timeaveraged,\n    potential_pos_t_nrns,\n    grad_pos_t_svm,\n    H,\n    latentedges,\n    domainedges,\n) = npy.ensemble.potential_landscape_nd(\n    X_dyn,\n    [np.linspace(-3, 3, proj_bins) for i in range(ndim)],\n    domain_bins,\n    nanborderempty=True,\n)\n\nX = np.linspace(-3, 3, proj_bins)\nX = np.meshgrid(*[X for _ in range(ndim)])\nX = np.stack(X, axis=-1)\nX = X.reshape(-1, ndim)\n\nE, dE = mix_functions(X, Xp, attractordepths, func=attractorfunc)\n\ngaussians_nd = []\nfor i in range(len(Xp)):\n    gaussian = attractorfunc(X, Xp[i], 1, attractordepths[i])[0].reshape(\n        *[proj_bins for _ in range(ndim)]\n    )\n    gaussian = (gaussian - np.max(gaussian)) / (np.max(gaussian) - np.min(gaussian)) + 1\n    gaussians_nd.append(gaussian)\ngaussians_nd = np.stack(gaussians_nd, axis=-1)\n</pre> (     potential_timeaveraged,     potential_pos_t_nrns,     grad_pos_t_svm,     H,     latentedges,     domainedges, ) = npy.ensemble.potential_landscape_nd(     X_dyn,     [np.linspace(-3, 3, proj_bins) for i in range(ndim)],     domain_bins,     nanborderempty=True, )  X = np.linspace(-3, 3, proj_bins) X = np.meshgrid(*[X for _ in range(ndim)]) X = np.stack(X, axis=-1) X = X.reshape(-1, ndim)  E, dE = mix_functions(X, Xp, attractordepths, func=attractorfunc)  gaussians_nd = [] for i in range(len(Xp)):     gaussian = attractorfunc(X, Xp[i], 1, attractordepths[i])[0].reshape(         *[proj_bins for _ in range(ndim)]     )     gaussian = (gaussian - np.max(gaussian)) / (np.max(gaussian) - np.min(gaussian)) + 1     gaussians_nd.append(gaussian) gaussians_nd = np.stack(gaussians_nd, axis=-1) In\u00a0[\u00a0]: Copied! <pre>orig_pot_pos_projs = project_ndimage(gaussians_nd.T, proj_bins + 1, pca)\n\n# avoid RuntimeWarning: Mean of empty slice\nest_pot = np.asarray(\n    [\n        np.nanmean(potential_pos_t_nrns[:, :, :, :, :, nrn], axis=-1)\n        for nrn in range(ndim)\n    ]\n)\nest_pot_pos_projs = project_ndimage(est_pot, proj_bins, pca)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\naxes[0].imshow(np.nanmean(np.nanmean(orig_pot_pos_projs, axis=0), axis=-1))\naxes[0].set_title(\"Original Potential Landscape Projection\")\naxes[0].set_xlabel(\"PC 1\")\naxes[0].set_ylabel(\"PC 2\")\naxes[1].imshow(np.nanmean(np.nanmean(est_pot_pos_projs, axis=0), axis=-1))\naxes[1].set_title(\"Estimated Potential Landscape Projection\")\naxes[1].set_xlabel(\"PC 1\")\naxes[1].set_ylabel(\"PC 2\")\n\nplt.show()\n</pre> orig_pot_pos_projs = project_ndimage(gaussians_nd.T, proj_bins + 1, pca)  # avoid RuntimeWarning: Mean of empty slice est_pot = np.asarray(     [         np.nanmean(potential_pos_t_nrns[:, :, :, :, :, nrn], axis=-1)         for nrn in range(ndim)     ] ) est_pot_pos_projs = project_ndimage(est_pot, proj_bins, pca)  fig, axes = plt.subplots(1, 2, figsize=(15, 5)) axes[0].imshow(np.nanmean(np.nanmean(orig_pot_pos_projs, axis=0), axis=-1)) axes[0].set_title(\"Original Potential Landscape Projection\") axes[0].set_xlabel(\"PC 1\") axes[0].set_ylabel(\"PC 2\") axes[1].imshow(np.nanmean(np.nanmean(est_pot_pos_projs, axis=0), axis=-1)) axes[1].set_title(\"Estimated Potential Landscape Projection\") axes[1].set_xlabel(\"PC 1\") axes[1].set_ylabel(\"PC 2\")  plt.show() <p>Visualize the PCA-projected ground truth and estimated potential landscapes to understand the attractor structure in the n-dimensional systems.</p> In\u00a0[\u00a0]: Copied! <pre>viewer = napari.Viewer(ndisplay=3)\n\nviewer.add_image(\n    np.nanmean(est_pot_pos_projs, axis=0),\n    colormap=\"twilight\",\n    interpolation2d=\"nearest\",\n    rendering=\"minip\",\n    name=\"Estimated Potential Energy Landscape\",\n)\n\nviewer.add_image(\n    np.nanmean(orig_pot_pos_projs, axis=0),\n    colormap=\"twilight\",\n    interpolation2d=\"nearest\",\n    rendering=\"minip\",\n    name=\"Original Potential Energy Landscape\",\n)\n\nviewer.grid.enabled = True\n\n\nanimation = Animation(viewer)\nviewer.update_console({\"animation\": animation})\n\nviewer.camera.angles = (0.0, 0.0, 0.0)\nanimation.capture_keyframe()\nmax_steps = 360\n# rotate the camera 360 degrees while advancing the time\nfor i in range(0, max_steps, 3):\n    angle_inc = i * 360 / max_steps\n    viewer.camera.angles = (\n        0.0 + 0.01 * angle_inc,\n        0.0 + 0.02 * angle_inc,\n        0.0 + angle_inc,\n    )\n    animation.capture_keyframe(steps=1)\n\nanimation.animate(f\"anim{ndim}DPot.mp4\", canvas_only=True)\n# viewer.close()\n\nHTML(f'&lt;video controls src=\"anim{ndim}DPot.mp4\" /&gt;')\n</pre> viewer = napari.Viewer(ndisplay=3)  viewer.add_image(     np.nanmean(est_pot_pos_projs, axis=0),     colormap=\"twilight\",     interpolation2d=\"nearest\",     rendering=\"minip\",     name=\"Estimated Potential Energy Landscape\", )  viewer.add_image(     np.nanmean(orig_pot_pos_projs, axis=0),     colormap=\"twilight\",     interpolation2d=\"nearest\",     rendering=\"minip\",     name=\"Original Potential Energy Landscape\", )  viewer.grid.enabled = True   animation = Animation(viewer) viewer.update_console({\"animation\": animation})  viewer.camera.angles = (0.0, 0.0, 0.0) animation.capture_keyframe() max_steps = 360 # rotate the camera 360 degrees while advancing the time for i in range(0, max_steps, 3):     angle_inc = i * 360 / max_steps     viewer.camera.angles = (         0.0 + 0.01 * angle_inc,         0.0 + 0.02 * angle_inc,         0.0 + angle_inc,     )     animation.capture_keyframe(steps=1)  animation.animate(f\"anim{ndim}DPot.mp4\", canvas_only=True) # viewer.close()  HTML(f'') <pre>Rendering frames...\n</pre> <pre>  0%|          | 0/121 [00:00&lt;?, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1307, 808) to (1312, 816) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n[swscaler @ 0x6f58300] Warning: data is not aligned! This can lead to a speed loss\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 121/121 [00:01&lt;00:00, 77.39it/s]\n</pre> Out[\u00a0]: <p>Visualize the time-resolved potential estimation for the 4D system.</p> In\u00a0[\u00a0]: Copied! <pre>viewer = napari.Viewer(ndisplay=3)\n\npot_timeresolved = np.asarray(\n    [\n        np.nanmean(project_ndimage(potential_pos_nrns, proj_bins, pca), axis=0)\n        for potential_pos_nrns in np.transpose(potential_pos_t_nrns, (4, 5, 0, 1, 2, 3))\n    ]\n)\n\nviewer.add_image(\n    pot_timeresolved,\n    colormap=\"twilight\",\n    interpolation2d=\"nearest\",\n    rendering=\"minip\",\n    name=\"Estimated Potential Energy Landscape\",\n)\n\n# grid view\nviewer.grid.enabled = True\n\n# napari.utils.nbscreenshot(viewer, canvas_only=True)\n\nanimation = Animation(viewer)\nviewer.update_console({\"animation\": animation})\n\nviewer.camera.angles = (0.0, 0.0, 0.0)\nanimation.capture_keyframe()\nmax_steps = int(viewer.dims.range[0][1])\n# rotate the camera 360 degrees while advancing the time\nfor i in np.linspace(0, max_steps - 1, max_steps * 4):\n    angle_inc = i * 360 / max_steps\n    viewer.camera.angles = (\n        0.0 + 0.075 * angle_inc,\n        0.0 + angle_inc,\n        90.0 + 0.1 * angle_inc,\n    )\n    viewer.dims.current_step = (i, *viewer.dims.current_step[1:])\n    animation.capture_keyframe(steps=1)\n\nanimation.animate(\"anim{ndim}DPotTimeResolved.mp4\", canvas_only=True)\n\nHTML(f'&lt;video controls src=\"anim{ndim}DPotTimeResolved.mp4\" /&gt;')\n</pre> viewer = napari.Viewer(ndisplay=3)  pot_timeresolved = np.asarray(     [         np.nanmean(project_ndimage(potential_pos_nrns, proj_bins, pca), axis=0)         for potential_pos_nrns in np.transpose(potential_pos_t_nrns, (4, 5, 0, 1, 2, 3))     ] )  viewer.add_image(     pot_timeresolved,     colormap=\"twilight\",     interpolation2d=\"nearest\",     rendering=\"minip\",     name=\"Estimated Potential Energy Landscape\", )  # grid view viewer.grid.enabled = True  # napari.utils.nbscreenshot(viewer, canvas_only=True)  animation = Animation(viewer) viewer.update_console({\"animation\": animation})  viewer.camera.angles = (0.0, 0.0, 0.0) animation.capture_keyframe() max_steps = int(viewer.dims.range[0][1]) # rotate the camera 360 degrees while advancing the time for i in np.linspace(0, max_steps - 1, max_steps * 4):     angle_inc = i * 360 / max_steps     viewer.camera.angles = (         0.0 + 0.075 * angle_inc,         0.0 + angle_inc,         90.0 + 0.1 * angle_inc,     )     viewer.dims.current_step = (i, *viewer.dims.current_step[1:])     animation.capture_keyframe(steps=1)  animation.animate(\"anim{ndim}DPotTimeResolved.mp4\", canvas_only=True)  HTML(f'') <pre>Rendering frames...\n</pre> <pre>  0%|          | 0/81 [00:00&lt;?, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1307, 786) to (1312, 800) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n[swscaler @ 0x61401c0] Warning: data is not aligned! This can lead to a speed loss\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81/81 [00:01&lt;00:00, 77.96it/s]\n</pre> Out[\u00a0]:"},{"location":"tutorials/attractor_landscape/#attractor-estimation","title":"Attractor Estimation\u00b6","text":"<p>This tutorial demonstrates how to estimate potential energy landscapes for simulated dynamical systems with attractors in one, two, three, and higher dimensions. The goal is to understand the dynamics of systems governed by these landscapes and validate estimations against the ground truth.</p>"},{"location":"tutorials/attractor_landscape/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/attractor_landscape/#section-1-1d-energy-landscape","title":"Section 1: 1D Energy Landscape\u00b6","text":"<p>This section focuses on simulating and estimating a 1D energy landscape to provide a foundational understanding of attractor dynamics in a simple setting.</p>"},{"location":"tutorials/attractor_landscape/#section-11-simulate-trials-of-1d-activity-using-langevin-dynamics-with-noise","title":"Section 1.1: Simulate trials of 1D activity using Langevin dynamics with noise\u00b6","text":"<p>We simulate the dynamics of particles (or neural states) evolving under a 1D potential landscape influenced by noise. The dynamics are modeled using Langevin equations with parameters for attractor depths, positions, and noise.</p> <p>Goals:</p> <ul> <li>To visualize particle dynamics under a simple potential landscape.</li> <li>To understand how initial states and noise affects convergence to attractors and hence their estimation.</li> </ul> <p>Key Insights:</p> <ul> <li>Particle trajectories converge to the minima of the potential wells, confirming the attractor dynamics.</li> </ul>"},{"location":"tutorials/attractor_landscape/#section-12-analytically-estimate-the-1d-potential-energy-landscape","title":"Section 1.2: Analytically estimate the 1D potential energy landscape\u00b6","text":"<p>We analytically compute the 1D potential landscape and compare it to the ground-truth potential used to generate the synthetic data.</p> <p>Goal:</p> <ul> <li>To validate the simulation by comparing it to the analytical potential.</li> </ul> <p>Plot Descriptions:</p> <ol> <li>Potential Landscape: Displays the theoretical potential formed by the Gaussian mixture components.</li> <li>Dynamics of Trials: Shows the temporal evolution of particle positions for selected trials.</li> <li>Potential Landscape Estimation: Compares the analytical potential with the estimated potential.</li> </ol> <p>Key Insights:</p> <ul> <li>The estimated potential closely matches the analytical potential, confirming the method's validity.</li> </ul>"},{"location":"tutorials/attractor_landscape/#section-2-2d-energy-landscape","title":"Section 2: 2D Energy Landscape\u00b6","text":"<p>In this section, we extend the dynamics and estimation to two dimensions, introducing more complexity.</p>"},{"location":"tutorials/attractor_landscape/#section-21-simulate-trials-of-2d-activity-using-langevin-dynamics-with-noise","title":"Section 2.1: Simulate trials of 2D activity using Langevin dynamics with noise\u00b6","text":"<p>Simulate particle dynamics under a 2D potential landscape composed of Gaussian attractors.</p> <p>Goal:</p> <ul> <li>To explore how particle dynamics evolve in a 2D system with multiple attractors</li> </ul> <p>Key Insights:</p> <ul> <li>Particle trajectories converge to minima in 2D, clustering around attractor states.</li> </ul>"},{"location":"tutorials/attractor_landscape/#section-22-analytically-estimate-the-potential-energy-landscape","title":"Section 2.2: Analytically estimate the potential energy landscape\u00b6","text":"<p>Compute and validate the 2D potential landscape using analytical and simulated results.</p> <p>Plot Descriptions:</p> <ol> <li>Potential Energy Landscape: Visualizes the 2D attractor structure.</li> <li>Dynamics of Trials: Shows particle trajectories across the 2D plane.</li> <li>Phase Plane: Depicts vector fields indicating gradients toward attractor basins.</li> <li>Time-Resolved Potential Estimation: Highlights temporal changes in the estimated landscape.</li> </ol> <p>Key Insights:</p> <ul> <li>The agreement between the simulated dynamics and estimated landscapes demonstrates robustness in two dimensions.</li> </ul>"},{"location":"tutorials/attractor_landscape/#section-23-simulate-2d-trials-for-a-ring-attractor","title":"Section 2.3: Simulate 2D trials for a ring attractor\u00b6","text":""},{"location":"tutorials/attractor_landscape/#section-24-analytically-estimate-the-potential-energy-landscape","title":"Section 2.4: Analytically estimate the potential energy landscape\u00b6","text":"<p>Estimate the potential landscape for the ring attractor and compare it to the ground-truth potential.</p> <p>Plot Descriptions:</p> <ol> <li>Potential Energy Landscape: Displays the ring attractor structure.</li> <li>Dynamics of Trials: Shows particle trajectories in the 2D plane.</li> <li>Phase Plane: Depicts vector fields indicating gradients toward the ring attractor.</li> <li>Time-Resolved Potential Estimation: Highlights temporal changes in the estimated landscape.</li> </ol> <p>Key Insights:</p> <ul> <li>The estimated potential closely matches the analytical potential, confirming the method's validity.</li> </ul>"},{"location":"tutorials/attractor_landscape/#section-3-3d-energy-landscape","title":"Section 3: 3D Energy Landscape\u00b6","text":"<p>This section explores the dynamics and estimation of potential landscapes in three dimensions.</p>"},{"location":"tutorials/attractor_landscape/#section-31-simulate-3d-trials-using-langevin-dynamics-with-noise","title":"Section 3.1: Simulate 3D trials using Langevin dynamics with noise\u00b6","text":"<p>Simulate 3D dynamics under Gaussian attractors to examine the additional complexity introduced by higher dimensions.</p> <p>Purpose:</p> <ul> <li>To study how particle trajectories behave in a 3D space with noise.</li> </ul> <p>Plot Descriptions:</p> <ul> <li>Dynamics of Trials: Shows particle trajectories in the 3D space.</li> </ul> <p>Key Insights:</p> <ul> <li>Trajectories exhibit complex convergence patterns due to added dimensionality.</li> </ul>"},{"location":"tutorials/attractor_landscape/#section-32-analytically-estimate-the-potential-energy-landscape","title":"Section 3.2: Analytically estimate the potential energy landscape\u00b6","text":"<p>Visualize and validate the estimated 3D landscape against the analytical potential.</p> <p>Plot Descriptions:</p> <ul> <li>Potential Energy Landscape: Displays the 3D attractor structure.</li> <li>Time-Resolved Potential Estimation: Highlights temporal changes in the estimated landscape.</li> </ul> <p>Key Insights:</p> <ul> <li>The estimated potential closely matches the analytical potential, confirming the method's validity.</li> </ul>"},{"location":"tutorials/attractor_landscape/#section-33-potential-energy-landscape-dimensionality-reduction-via-pca","title":"Section 3.3: Potential energy landscape dimensionality reduction via PCA\u00b6","text":"<p>PCA is used to reduce the dimensionality of energy landscapes for better visualization and analysis.</p> <p>Purpose:</p> <ul> <li>To simplify the representation of high-dimensional landscapes.</li> </ul> <p>Key Insights:</p> <ul> <li>PCA-projected landscapes retain essential attractor characteristics.</li> </ul>"},{"location":"tutorials/attractor_landscape/#section-4-n-dimensional-energy-landscape","title":"Section 4: n-dimensional Energy Landscape\u00b6","text":""},{"location":"tutorials/attractor_landscape/#section-41-simulate-n-dimensional-trials-using-langevin-dynamics-with-noise","title":"Section 4.1: Simulate n-dimensional trials using Langevin dynamics with noise\u00b6","text":"<p>Simulate particle dynamics in an n-dimensional landscape to study the scalability of the methods.</p> <p>Purpose:</p> <ul> <li>To test how well the simulation and estimation approaches scale with dimensionality.</li> </ul> <p>Key Insights:</p> <ul> <li>Higher-dimensional systems pose challenges in visualization and computational requirements.</li> </ul>"},{"location":"tutorials/attractor_landscape/#section-42-analytically-estimate-the-potential-energy-landscape","title":"Section 4.2: Analytically estimate the potential energy landscape\u00b6","text":"<p>Validate the estimated n-dimensional potential against theoretical predictions using dimensionality reduction techniques like PCA.</p> <p>Key Insights:</p> <ul> <li>Projected landscapes in PCA space confirm the validity of estimations in high-dimensional systems.</li> </ul>"},{"location":"tutorials/attractor_landscape/#conclusion","title":"Conclusion\u00b6","text":"<p>This tutorial demonstrates the simulation and estimation of energy landscapes across varying dimensions. The analytical and simulated results show strong agreement, validating the robustness of the approach even in higher dimensions. These methods have potential applications in understanding complex neural systems governed by attractor dynamics.</p>"},{"location":"tutorials/batch_analysis/","title":"Batch Analysis","text":"In\u00a0[1]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\n\nfrom neuro_py.process import batch_analysis\n\nimport pandas as pd\nimport numpy as np\n</pre> %reload_ext autoreload %autoreload 2  from neuro_py.process import batch_analysis  import pandas as pd import numpy as np In\u00a0[2]: Copied! <pre>def toy_analysis(basepath, parameter_1=1, parameter_2=2):\n    results = pd.DataFrame()\n    results[\"basepath\"] = [basepath]\n    results[\"parameter_1\"] = parameter_1\n    results[\"parameter_2\"] = parameter_2\n    results[\"random_number\"] = np.random.randint(0, 100)\n    return results\n</pre> def toy_analysis(basepath, parameter_1=1, parameter_2=2):     results = pd.DataFrame()     results[\"basepath\"] = [basepath]     results[\"parameter_1\"] = parameter_1     results[\"parameter_2\"] = parameter_2     results[\"random_number\"] = np.random.randint(0, 100)     return results <p>For your project, you will have a <code>.csv</code> file with the <code>basepaths</code> you want to analyze. Here, I'm creating a <code>DataFrame</code> with the <code>basepaths</code> for the purpose of this notebook.</p> In\u00a0[3]: Copied! <pre>sessions = pd.DataFrame(dict(basepath=[\n    r\"U:\\data\\hpc_ctx_project\\HP01\\day_1_20240227\",\n    r\"U:\\data\\hpc_ctx_project\\HP01\\day_2_20240228\",\n    r\"U:\\data\\hpc_ctx_project\\HP01\\day_3_20240229\",\n]))\n</pre> sessions = pd.DataFrame(dict(basepath=[     r\"U:\\data\\hpc_ctx_project\\HP01\\day_1_20240227\",     r\"U:\\data\\hpc_ctx_project\\HP01\\day_2_20240228\",     r\"U:\\data\\hpc_ctx_project\\HP01\\day_3_20240229\", ])) <p>You will need to define the path where you want to save the results of your analysis.</p> <p>It's useful to nest the analysis version in a subfolder (<code>toy_analysis\\toy_analysis_v1</code>) to keep track of the different versions of your analysis.</p> In\u00a0[4]: Copied! <pre>save_path = r\"Z:\\home\\ryanh\\projects\\hpc_ctx\\toy_analysis\\toy_analysis_v1\"\n</pre> save_path = r\"Z:\\home\\ryanh\\projects\\hpc_ctx\\toy_analysis\\toy_analysis_v1\" In\u00a0[5]: Copied! <pre>batch_analysis.run(\n    sessions,\n    save_path,\n    toy_analysis,\n    parallel=False,\n    verbose=True,\n)\n</pre> batch_analysis.run(     sessions,     save_path,     toy_analysis,     parallel=False,     verbose=True, ) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 3007.39it/s]</pre> <pre>U:\\data\\hpc_ctx_project\\HP01\\day_1_20240227\nU:\\data\\hpc_ctx_project\\HP01\\day_2_20240228\nU:\\data\\hpc_ctx_project\\HP01\\day_3_20240229\n</pre> <pre>\n</pre> In\u00a0[6]: Copied! <pre>results = batch_analysis.load_results(save_path)\nresults\n</pre> results = batch_analysis.load_results(save_path) results Out[6]: basepath paramater_1 paramater_2 random_number 0 U:\\data\\hpc_ctx_project\\HP01\\day_1_20240227 1 2 34 1 U:\\data\\hpc_ctx_project\\HP01\\day_2_20240228 1 2 30 2 U:\\data\\hpc_ctx_project\\HP01\\day_3_20240229 1 2 66 In\u00a0[7]: Copied! <pre>import glob\nimport os\nimport pickle\n\n\ndef toy_analysis_2(basepath, paramater_1=1, paramater_2=2):\n    results_df = pd.DataFrame()\n    results_df[\"basepath\"] = [basepath]\n    results_df[\"paramater_1\"] = paramater_1\n    results_df[\"paramater_2\"] = paramater_2\n    results_df[\"random_number\"] = np.random.randint(0, 100)\n\n    window_starttime, window_stoptime = [-1, 1]\n    window_bins = int(np.ceil(((window_stoptime - window_starttime) * 1000)))\n    time_lags = np.linspace(window_starttime, window_stoptime, window_bins)\n    psths = pd.DataFrame(\n        index=time_lags,\n        columns=np.arange(1),\n    )\n    psths[:] = np.random.rand(window_bins, 1)\n\n    results = {\n        \"results_df\": results_df,\n        \"psth\": psths,\n    }\n    return results\n\n# custom loader\ndef load_results(save_path, verbose=False):\n\n    # check if folder exists\n    if not os.path.exists(save_path):\n        raise ValueError(f\"folder {save_path} does not exist\")\n\n    # get all the sessions\n    sessions = glob.glob(save_path + os.sep + \"*.pkl\")\n\n    results_df = []\n    psths = []\n\n    # iterate over the sessions\n    for session in sessions:\n        if verbose:\n            print(session)\n\n        # load the session\n        with open(session, \"rb\") as f:\n            results_ = pickle.load(f)\n\n        if results_ is None:\n            continue\n        results_df.append(results_[\"results_df\"])\n        psths.append(results_[\"psth\"])\n\n    results_df = pd.concat(results_df, axis=0, ignore_index=True)\n    psths = pd.concat(psths, axis=1, ignore_index=True)\n\n    return results_df, psths\n</pre> import glob import os import pickle   def toy_analysis_2(basepath, paramater_1=1, paramater_2=2):     results_df = pd.DataFrame()     results_df[\"basepath\"] = [basepath]     results_df[\"paramater_1\"] = paramater_1     results_df[\"paramater_2\"] = paramater_2     results_df[\"random_number\"] = np.random.randint(0, 100)      window_starttime, window_stoptime = [-1, 1]     window_bins = int(np.ceil(((window_stoptime - window_starttime) * 1000)))     time_lags = np.linspace(window_starttime, window_stoptime, window_bins)     psths = pd.DataFrame(         index=time_lags,         columns=np.arange(1),     )     psths[:] = np.random.rand(window_bins, 1)      results = {         \"results_df\": results_df,         \"psth\": psths,     }     return results  # custom loader def load_results(save_path, verbose=False):      # check if folder exists     if not os.path.exists(save_path):         raise ValueError(f\"folder {save_path} does not exist\")      # get all the sessions     sessions = glob.glob(save_path + os.sep + \"*.pkl\")      results_df = []     psths = []      # iterate over the sessions     for session in sessions:         if verbose:             print(session)          # load the session         with open(session, \"rb\") as f:             results_ = pickle.load(f)          if results_ is None:             continue         results_df.append(results_[\"results_df\"])         psths.append(results_[\"psth\"])      results_df = pd.concat(results_df, axis=0, ignore_index=True)     psths = pd.concat(psths, axis=1, ignore_index=True)      return results_df, psths In\u00a0[8]: Copied! <pre>save_path = r\"Z:\\home\\ryanh\\projects\\hpc_ctx\\toy_analysis\\toy_analysis_v2\"\n\nbatch_analysis.run(\n    sessions,\n    save_path,\n    toy_analysis_2,\n    parallel=False,\n    verbose=True,\n)\n</pre> save_path = r\"Z:\\home\\ryanh\\projects\\hpc_ctx\\toy_analysis\\toy_analysis_v2\"  batch_analysis.run(     sessions,     save_path,     toy_analysis_2,     parallel=False,     verbose=True, ) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 3008.11it/s]</pre> <pre>U:\\data\\hpc_ctx_project\\HP01\\day_1_20240227\nU:\\data\\hpc_ctx_project\\HP01\\day_2_20240228\nU:\\data\\hpc_ctx_project\\HP01\\day_3_20240229\n</pre> <pre>\n</pre> In\u00a0[9]: Copied! <pre>results_df, psths = load_results(save_path)\n\ndisplay(results_df)\ndisplay(psths)\n</pre> results_df, psths = load_results(save_path)  display(results_df) display(psths) basepath paramater_1 paramater_2 random_number 0 U:\\data\\hpc_ctx_project\\HP01\\day_1_20240227 1 2 56 1 U:\\data\\hpc_ctx_project\\HP01\\day_2_20240228 1 2 32 2 U:\\data\\hpc_ctx_project\\HP01\\day_3_20240229 1 2 56 0 1 2 -1.000000 0.190685 0.490553 0.248958 -0.998999 0.078999 0.689063 0.40577 -0.997999 0.094847 0.788747 0.966084 -0.996998 0.287616 0.804512 0.846309 -0.995998 0.723807 0.996373 0.850087 ... ... ... ... 0.995998 0.023565 0.136486 0.120244 0.996998 0.298943 0.844828 0.227437 0.997999 0.514455 0.847778 0.782702 0.998999 0.975054 0.795339 0.898294 1.000000 0.122129 0.228904 0.168518 <p>2000 rows \u00d7 3 columns</p>"},{"location":"tutorials/batch_analysis/#batch-analysis","title":"Batch Analysis\u00b6","text":""},{"location":"tutorials/batch_analysis/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/batch_analysis/#section-1-define-the-analysis","title":"Section 1: Define the analysis\u00b6","text":"<p>Here, I'm defining the analysis in the notebook, but in a real project, you would define it in a separate <code>.py</code> file and import it here.</p>"},{"location":"tutorials/batch_analysis/#section-2-run-the-analysis","title":"Section 2: Run the analysis\u00b6","text":"<p>Finally, you can run your analysis in batch mode. This will loop through the <code>basepaths</code> and save the results in the specified folder.</p> <p>The <code>batch_analysis</code> function is a general function that you can use for any analysis. You just need to pass the function you want to run, the <code>basepaths</code> you want to analyze, and the save path.</p> <p>If your analysis fails, running again will start from where it left off.</p> <p>There is a <code>parallel</code> option that you can set to <code>True</code> if you want to run the analysis in parallel. This will speed up the analysis if you have multiple cores.</p>"},{"location":"tutorials/batch_analysis/#section-3-load-the-results","title":"Section 3: Load the results\u00b6","text":"<p>There is a built in loader that concatenates the results of the analysis into a single <code>DataFrame</code>.</p>"},{"location":"tutorials/batch_analysis/#bonus-more-complicated-results","title":"Bonus: More complicated results\u00b6","text":"<p>Your results won't always fit nicely into a single <code>DataFrame</code>. Sometimes you will have multiple data types you need to save.</p> <p>For example, you might have values for each cell in a <code>DataFrame</code> and also PSTHs for each cell. Your analysis will store both in a dictionary and you will construct a custom loader in your analysis.</p>"},{"location":"tutorials/batch_analysis/#define-the-analysis","title":"Define the analysis\u00b6","text":""},{"location":"tutorials/batch_analysis/#run-the-analysis","title":"Run the analysis\u00b6","text":""},{"location":"tutorials/batch_analysis/#load-the-results","title":"Load the results\u00b6","text":""},{"location":"tutorials/bias_correlation/","title":"Bias Correlation","text":"In\u00a0[1]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\nimport copy\nimport logging\n\nimport ipywidgets as widgets\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\n\nimport neuro_py as npy\nimport nelpy as nel\n\n\n# Disable logging\nlogger = logging.getLogger()\nlogger.disabled = True\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n</pre> %reload_ext autoreload %autoreload 2 import copy import logging  import ipywidgets as widgets import numpy as np import matplotlib.pyplot as plt import scipy  import neuro_py as npy import nelpy as nel   # Disable logging logger = logging.getLogger() logger.disabled = True  # Set random seed for reproducibility np.random.seed(0) In\u00a0[2]: Copied! <pre>def simulate_sequential_spikes(\n    nneurons=30,\n    minseqduration=.05,\n    maxseqduration=.15,\n    duration=1.0,\n    jitter=.01,\n    reverseseqprob=0.0,\n    random=False,\n):\n    spikes = []\n    neuron_ids = []\n    max_nsequences = np.ceil(duration / minseqduration)\n    sequence_durations = np.random.uniform(\n        minseqduration, maxseqduration, int(max_nsequences))\n    # get index of last sequence that fits into duration\n    last_sequence = np.where(np.cumsum(sequence_durations) &lt;= duration)[0][-1]\n    sequence_durations = sequence_durations[:last_sequence+1]\n    sequence_epochs = np.cumsum(sequence_durations)\n    sequence_epochs = np.asarray((\n        np.r_[0, sequence_epochs][:-1],\n        sequence_epochs\n    )).T  # shape (nsequences, 2)\n\n    for seq_start, seq_end in sequence_epochs:\n        spike_ts = np.linspace(seq_start, seq_end, nneurons)\n        neuron_seqids = (\n            np.arange(nneurons) if np.random.rand() &gt; reverseseqprob\n            else np.arange(nneurons)[::-1])\n        # add jitter\n        spike_ts += np.random.uniform(-jitter, jitter, nneurons)\n        spike_ts = np.sort(spike_ts)\n        # clip to sequence bounds\n        spike_ts = np.clip(spike_ts, seq_start, seq_end)\n        spikes.append(spike_ts)\n        neuron_ids.append(neuron_seqids)\n\n    spikes = np.concatenate(spikes)\n    neuron_ids = np.concatenate(neuron_ids)\n\n    if random:\n        neuron_ids = np.random.permutation(neuron_ids)\n\n    return spikes, neuron_ids, sequence_epochs\n</pre> def simulate_sequential_spikes(     nneurons=30,     minseqduration=.05,     maxseqduration=.15,     duration=1.0,     jitter=.01,     reverseseqprob=0.0,     random=False, ):     spikes = []     neuron_ids = []     max_nsequences = np.ceil(duration / minseqduration)     sequence_durations = np.random.uniform(         minseqduration, maxseqduration, int(max_nsequences))     # get index of last sequence that fits into duration     last_sequence = np.where(np.cumsum(sequence_durations) &lt;= duration)[0][-1]     sequence_durations = sequence_durations[:last_sequence+1]     sequence_epochs = np.cumsum(sequence_durations)     sequence_epochs = np.asarray((         np.r_[0, sequence_epochs][:-1],         sequence_epochs     )).T  # shape (nsequences, 2)      for seq_start, seq_end in sequence_epochs:         spike_ts = np.linspace(seq_start, seq_end, nneurons)         neuron_seqids = (             np.arange(nneurons) if np.random.rand() &gt; reverseseqprob             else np.arange(nneurons)[::-1])         # add jitter         spike_ts += np.random.uniform(-jitter, jitter, nneurons)         spike_ts = np.sort(spike_ts)         # clip to sequence bounds         spike_ts = np.clip(spike_ts, seq_start, seq_end)         spikes.append(spike_ts)         neuron_ids.append(neuron_seqids)      spikes = np.concatenate(spikes)     neuron_ids = np.concatenate(neuron_ids)      if random:         neuron_ids = np.random.permutation(neuron_ids)      return spikes, neuron_ids, sequence_epochs <p>Set parameters for simulation of spike data.</p> In\u00a0[3]: Copied! <pre>N_NEURONS = 15\nMIN_SEQ_DURATION = .05\nMAX_SEQ_DURATION = .15\nDURATION = .5\n</pre> N_NEURONS = 15 MIN_SEQ_DURATION = .05 MAX_SEQ_DURATION = .15 DURATION = .5 In\u00a0[4]: Copied! <pre>task_spikes, task_neurons, task_seq_epochs = simulate_sequential_spikes(\n    nneurons=N_NEURONS,\n    minseqduration=MIN_SEQ_DURATION,\n    maxseqduration=MAX_SEQ_DURATION,\n    duration=DURATION,\n    random=False\n)\n\n# Visualize task spike data\nplt.figure(figsize=(12, 6))\nplt.scatter(task_spikes, task_neurons, c='k', marker='|', s=18)\nplt.title(\"Task Spike Data\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Neuron ID\")\nplt.show()\n</pre> task_spikes, task_neurons, task_seq_epochs = simulate_sequential_spikes(     nneurons=N_NEURONS,     minseqduration=MIN_SEQ_DURATION,     maxseqduration=MAX_SEQ_DURATION,     duration=DURATION,     random=False )  # Visualize task spike data plt.figure(figsize=(12, 6)) plt.scatter(task_spikes, task_neurons, c='k', marker='|', s=18) plt.title(\"Task Spike Data\") plt.xlabel(\"Time\") plt.ylabel(\"Neuron ID\") plt.show() In\u00a0[5]: Copied! <pre>post_spikes_sig, post_neurons_sig, post_sig_seq_epochs = simulate_sequential_spikes(\n    nneurons=N_NEURONS,\n    minseqduration=MIN_SEQ_DURATION,\n    maxseqduration=MAX_SEQ_DURATION,\n    duration=DURATION,\n    random=False\n)\npost_spikes_nonsig, post_neurons_nonsig, post_nonsig_seq_epochs = simulate_sequential_spikes(\n    nneurons=N_NEURONS,\n    minseqduration=MIN_SEQ_DURATION,\n    maxseqduration=MAX_SEQ_DURATION,\n    duration=DURATION,\n    random=True\n)\n\n# Visualize significant post-task spike data\nplt.figure(figsize=(12, 6))\nplt.scatter(post_spikes_sig, post_neurons_sig, c='k', marker='|', s=18)\nplt.title(\"Significant Post-Task Spike Data\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Neuron ID\")\nplt.show()\n\n# Visualize non-significant post-task spike data\nplt.figure(figsize=(12, 6))\nplt.scatter(post_spikes_nonsig, post_neurons_nonsig, c='k', marker='|', s=18)\nplt.title(\"Non-Significant Post-Task Spike Data\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Neuron ID\")\nplt.show()\n</pre> post_spikes_sig, post_neurons_sig, post_sig_seq_epochs = simulate_sequential_spikes(     nneurons=N_NEURONS,     minseqduration=MIN_SEQ_DURATION,     maxseqduration=MAX_SEQ_DURATION,     duration=DURATION,     random=False ) post_spikes_nonsig, post_neurons_nonsig, post_nonsig_seq_epochs = simulate_sequential_spikes(     nneurons=N_NEURONS,     minseqduration=MIN_SEQ_DURATION,     maxseqduration=MAX_SEQ_DURATION,     duration=DURATION,     random=True )  # Visualize significant post-task spike data plt.figure(figsize=(12, 6)) plt.scatter(post_spikes_sig, post_neurons_sig, c='k', marker='|', s=18) plt.title(\"Significant Post-Task Spike Data\") plt.xlabel(\"Time\") plt.ylabel(\"Neuron ID\") plt.show()  # Visualize non-significant post-task spike data plt.figure(figsize=(12, 6)) plt.scatter(post_spikes_nonsig, post_neurons_nonsig, c='k', marker='|', s=18) plt.title(\"Non-Significant Post-Task Spike Data\") plt.xlabel(\"Time\") plt.ylabel(\"Neuron ID\") plt.show() In\u00a0[6]: Copied! <pre>pbias = npy.ensemble.PairwiseBias(num_shuffles=100)\n\n# Analyze significant replay\nz_score_sig, p_value_sig, cosine_val_sig = pbias.fit_transform(\n    task_spikes, task_neurons, task_seq_epochs,\n    post_spikes_sig, post_neurons_sig, post_sig_seq_epochs\n)\n\nprint(\"Significant Replay Results:\")\nprint(f\"Z-scores: {z_score_sig}\")\nprint(f\"P-values: {p_value_sig}\")\nprint(f\"Cosine values: {cosine_val_sig}\")\n\n# Analyze non-significant replay\nz_score_nonsig, p_value_nonsig, cosine_val_nonsig = pbias.transform(\n    post_spikes_nonsig, post_neurons_nonsig, post_nonsig_seq_epochs\n)\n\nprint(\"\\nNon-significant Replay Results:\")\nprint(f\"Z-scores: {z_score_nonsig}\")\nprint(f\"P-values: {p_value_nonsig}\")\nprint(f\"Cosine values: {cosine_val_nonsig}\")\n</pre> pbias = npy.ensemble.PairwiseBias(num_shuffles=100)  # Analyze significant replay z_score_sig, p_value_sig, cosine_val_sig = pbias.fit_transform(     task_spikes, task_neurons, task_seq_epochs,     post_spikes_sig, post_neurons_sig, post_sig_seq_epochs )  print(\"Significant Replay Results:\") print(f\"Z-scores: {z_score_sig}\") print(f\"P-values: {p_value_sig}\") print(f\"Cosine values: {cosine_val_sig}\")  # Analyze non-significant replay z_score_nonsig, p_value_nonsig, cosine_val_nonsig = pbias.transform(     post_spikes_nonsig, post_neurons_nonsig, post_nonsig_seq_epochs )  print(\"\\nNon-significant Replay Results:\") print(f\"Z-scores: {z_score_nonsig}\") print(f\"P-values: {p_value_nonsig}\") print(f\"Cosine values: {cosine_val_nonsig}\") <pre>/home/cornell/Desktop/Kushaan/neuro_py/neuro_py/ensemble/replay.py:659: RuntimeWarning: Mean of empty slice\n</pre> <pre>Significant Replay Results:\nZ-scores: [6.01136055 5.2730831  4.95141898 5.19878733 5.22794518 5.24813793]\nP-values: [0.00990099 0.00990099 0.00990099 0.00990099 0.00990099 0.00990099]\nCosine values: [0.97669051 0.98048687 0.93398034 0.97551905 0.93848145 0.93954246]\n\nNon-significant Replay Results:\nZ-scores: [-0.97897708  0.46571559  0.25575207 -1.42404218]\nP-values: [0.82178218 0.31683168 0.43564356 0.92079208]\nCosine values: [-0.14344132  0.10520735  0.03944295 -0.16070039]\n</pre> In\u00a0[7]: Copied! <pre>fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n\nax1.bar(range(len(z_score_sig)), z_score_sig, alpha=0.7, label='Significant')\nax1.bar(range(len(z_score_nonsig)), z_score_nonsig, alpha=0.7, label='Non-significant')\nax1.set_title('Z-scores')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Z-score')\nax1.legend()\n\nax2.bar(range(len(p_value_sig)), p_value_sig, alpha=0.7, label='Significant')\nax2.bar(range(len(p_value_nonsig)), p_value_nonsig, alpha=0.7, label='Non-significant')\nax2.set_title('P-values')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('P-value')\nax2.axhline(y=0.05, color='r', linestyle='--', label='p=0.05')\nax2.legend()\n\nax3.bar(range(len(cosine_val_sig)), cosine_val_sig, alpha=0.7, label='Significant')\nax3.bar(range(len(cosine_val_nonsig)), cosine_val_nonsig, alpha=0.7, label='Non-significant')\nax3.set_title('Cosine Values')\nax3.set_xlabel('Epochs')\nax3.set_ylabel('Cosine Similarity')\nax3.legend()\n\nplt.tight_layout()\nplt.show()\n</pre> fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))  ax1.bar(range(len(z_score_sig)), z_score_sig, alpha=0.7, label='Significant') ax1.bar(range(len(z_score_nonsig)), z_score_nonsig, alpha=0.7, label='Non-significant') ax1.set_title('Z-scores') ax1.set_xlabel('Epochs') ax1.set_ylabel('Z-score') ax1.legend()  ax2.bar(range(len(p_value_sig)), p_value_sig, alpha=0.7, label='Significant') ax2.bar(range(len(p_value_nonsig)), p_value_nonsig, alpha=0.7, label='Non-significant') ax2.set_title('P-values') ax2.set_xlabel('Epochs') ax2.set_ylabel('P-value') ax2.axhline(y=0.05, color='r', linestyle='--', label='p=0.05') ax2.legend()  ax3.bar(range(len(cosine_val_sig)), cosine_val_sig, alpha=0.7, label='Significant') ax3.bar(range(len(cosine_val_nonsig)), cosine_val_nonsig, alpha=0.7, label='Non-significant') ax3.set_title('Cosine Values') ax3.set_xlabel('Epochs') ax3.set_ylabel('Cosine Similarity') ax3.legend()  plt.tight_layout() plt.show() In\u00a0[8]: Copied! <pre>post_spikes_rev, post_neurons_rev, post_rev_seq_epochs = simulate_sequential_spikes(\n    nneurons=N_NEURONS,\n    minseqduration=MIN_SEQ_DURATION,\n    maxseqduration=MAX_SEQ_DURATION,\n    duration=DURATION,\n    reverseseqprob=.5,\n    random=False\n)\n\n# Visualize reversed post-task spike data\nplt.figure(figsize=(12, 6))\nplt.scatter(post_spikes_rev, post_neurons_rev, c='k', marker='|', s=18)\nplt.title(\"Reversed Post-Task Spike Data\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Neuron ID\")\nplt.show()\n</pre> post_spikes_rev, post_neurons_rev, post_rev_seq_epochs = simulate_sequential_spikes(     nneurons=N_NEURONS,     minseqduration=MIN_SEQ_DURATION,     maxseqduration=MAX_SEQ_DURATION,     duration=DURATION,     reverseseqprob=.5,     random=False )  # Visualize reversed post-task spike data plt.figure(figsize=(12, 6)) plt.scatter(post_spikes_rev, post_neurons_rev, c='k', marker='|', s=18) plt.title(\"Reversed Post-Task Spike Data\") plt.xlabel(\"Time\") plt.ylabel(\"Neuron ID\") plt.show() In\u00a0[9]: Copied! <pre>z_score_rev_nonsig, p_value_rev_nonsig, cosine_val_rev_nonsig = pbias.transform(\n    post_spikes_rev, post_neurons_rev, post_rev_seq_epochs\n)\n\nprint(\"Reversed Replay Results without permitting reverse sequences:\")\nprint(f\"Z-scores: {z_score_rev_nonsig}\")\nprint(f\"P-values: {p_value_rev_nonsig}\")\nprint(f\"Cosine values: {cosine_val_rev_nonsig}\")\n\nz_score_rev_sig, p_value_rev_sig, cosine_val_rev_sig = pbias.transform(\n    post_spikes_rev, post_neurons_rev, post_rev_seq_epochs,\n    allow_reverse_replay=True\n)\n\nprint(\"\\nReversed Replay Results while permitting reverse sequences:\")\nprint(f\"Z-scores: {z_score_rev_sig}\")\nprint(f\"P-values: {p_value_rev_sig}\")\nprint(f\"Cosine values: {cosine_val_rev_sig}\")\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n\nax1.bar(range(len(z_score_rev_sig)), z_score_rev_sig, alpha=0.7, label='Permit Reverse')\nax1.bar(range(len(z_score_rev_nonsig)), z_score_rev_nonsig, alpha=0.7, label='No Reverse')\nax1.set_title('Z-scores')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('Z-score')\nax1.legend()\n\nax2.bar(range(len(p_value_rev_sig)), p_value_rev_sig, alpha=0.7, label='Permit Reverse')\nax2.bar(range(len(p_value_rev_nonsig)), p_value_rev_nonsig, alpha=0.7, label='No Reverse')\nax2.set_title('P-values')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('P-value')\nax2.axhline(y=0.05, color='r', linestyle='--', label='p=0.05')\nax2.legend()\n\nax3.bar(range(len(cosine_val_rev_sig)), cosine_val_rev_sig, alpha=0.7, label='Permit Reverse')\nax3.bar(range(len(cosine_val_rev_nonsig)), cosine_val_rev_nonsig, alpha=0.7, label='No Reverse')\nax3.set_title('Cosine Values')\nax3.set_xlabel('Epochs')\nax3.set_ylabel('Cosine Similarity')\nax3.legend()\n\nplt.tight_layout()\nplt.show()\n</pre> z_score_rev_nonsig, p_value_rev_nonsig, cosine_val_rev_nonsig = pbias.transform(     post_spikes_rev, post_neurons_rev, post_rev_seq_epochs )  print(\"Reversed Replay Results without permitting reverse sequences:\") print(f\"Z-scores: {z_score_rev_nonsig}\") print(f\"P-values: {p_value_rev_nonsig}\") print(f\"Cosine values: {cosine_val_rev_nonsig}\")  z_score_rev_sig, p_value_rev_sig, cosine_val_rev_sig = pbias.transform(     post_spikes_rev, post_neurons_rev, post_rev_seq_epochs,     allow_reverse_replay=True )  print(\"\\nReversed Replay Results while permitting reverse sequences:\") print(f\"Z-scores: {z_score_rev_sig}\") print(f\"P-values: {p_value_rev_sig}\") print(f\"Cosine values: {cosine_val_rev_sig}\")  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))  ax1.bar(range(len(z_score_rev_sig)), z_score_rev_sig, alpha=0.7, label='Permit Reverse') ax1.bar(range(len(z_score_rev_nonsig)), z_score_rev_nonsig, alpha=0.7, label='No Reverse') ax1.set_title('Z-scores') ax1.set_xlabel('Epochs') ax1.set_ylabel('Z-score') ax1.legend()  ax2.bar(range(len(p_value_rev_sig)), p_value_rev_sig, alpha=0.7, label='Permit Reverse') ax2.bar(range(len(p_value_rev_nonsig)), p_value_rev_nonsig, alpha=0.7, label='No Reverse') ax2.set_title('P-values') ax2.set_xlabel('Epochs') ax2.set_ylabel('P-value') ax2.axhline(y=0.05, color='r', linestyle='--', label='p=0.05') ax2.legend()  ax3.bar(range(len(cosine_val_rev_sig)), cosine_val_rev_sig, alpha=0.7, label='Permit Reverse') ax3.bar(range(len(cosine_val_rev_nonsig)), cosine_val_rev_nonsig, alpha=0.7, label='No Reverse') ax3.set_title('Cosine Values') ax3.set_xlabel('Epochs') ax3.set_ylabel('Cosine Similarity') ax3.legend()  plt.tight_layout() plt.show() <pre>Reversed Replay Results without permitting reverse sequences:\nZ-scores: [-5.30841933 -5.45090389  5.03316704 -5.58233051  5.68234554]\nP-values: [1.         1.         0.00990099 1.         0.00990099]\nCosine values: [-0.94051679 -0.99374011  0.98048687 -0.98048687  0.98048687]\n\nReversed Replay Results while permitting reverse sequences:\nZ-scores: [-4.9703529  -5.2933867   5.88903521 -6.48384634  5.22551579]\nP-values: [0.00990099 0.00990099 0.00990099 0.00990099 0.00990099]\nCosine values: [-0.94051679 -0.99374011  0.98048687 -0.98048687  0.98048687]\n</pre> In\u00a0[10]: Copied! <pre>nrn_order = np.argsort(np.nansum(pbias.task_normalized, axis=1))\nsig_swr_indices = np.where(p_value_sig &lt; 0.001)[0]\n\ndef plot_replay(n) -&gt; None:\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n    start, end = post_rev_seq_epochs[n]\n    start_idx = np.searchsorted(post_spikes_rev, start, side='left')\n    end_idx = np.searchsorted(post_spikes_rev, end, side='right')\n\n    filtered_spikes = post_spikes_rev[start_idx:end_idx]\n    filtered_neurons = post_neurons_rev[start_idx:end_idx]\n\n    im = ax1.imshow(pbias.task_normalized[nrn_order, :][:, nrn_order], cmap='magma')\n    ax1.set_title('Template Bias Matrix')\n    ax1.set_xlabel('Neuron ID (reordered by bias direction)')\n    ax1.set_ylabel('Neuron ID (reordered by bias direction)')\n    plt.colorbar(im, ax=ax1)\n\n    bias = npy.ensemble.bias_matrix_fast(\n        filtered_spikes,\n        filtered_neurons,\n        total_neurons=N_NEURONS,\n        fillneutral=.5,\n    )\n    bias = npy.ensemble.normalize_bias_matrix(bias)\n\n    im = ax2.imshow(bias[nrn_order, :][:, nrn_order], cmap='magma')\n    ax2.set_title('Replay Bias Matrix')\n    ax2.set_xlabel('Neuron ID (reordered by bias direction)')\n    ax2.set_ylabel('Neuron ID (reordered by bias direction)')\n    plt.colorbar(im, ax=ax2)\n\n    # order spikes and neuron ids by nrn_order\n    argsort = np.asarray(\n        sorted(\n            range(len(filtered_neurons)),\n            key=lambda x: nrn_order[int(filtered_neurons[x])]\n        )\n    )\n    spike_times = filtered_spikes[argsort]\n    spike_ids = filtered_neurons[argsort]\n    # raster plot\n    ax3.scatter(spike_times, spike_ids, c='k', marker='|', s=18)\n    ax3.set_title('Replay Raster Plot')\n    ax3.set_xlabel('Time')\n    ax3.set_ylabel('Neuron ID (reordered by bias direction)')\n    plt.tight_layout()\n    plt.show()\n\nwidgets.interact(plot_replay, n=widgets.IntSlider(min=0, max=len(post_rev_seq_epochs)-1, step=1, value=0));\n</pre> nrn_order = np.argsort(np.nansum(pbias.task_normalized, axis=1)) sig_swr_indices = np.where(p_value_sig &lt; 0.001)[0]  def plot_replay(n) -&gt; None:     fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))     start, end = post_rev_seq_epochs[n]     start_idx = np.searchsorted(post_spikes_rev, start, side='left')     end_idx = np.searchsorted(post_spikes_rev, end, side='right')      filtered_spikes = post_spikes_rev[start_idx:end_idx]     filtered_neurons = post_neurons_rev[start_idx:end_idx]      im = ax1.imshow(pbias.task_normalized[nrn_order, :][:, nrn_order], cmap='magma')     ax1.set_title('Template Bias Matrix')     ax1.set_xlabel('Neuron ID (reordered by bias direction)')     ax1.set_ylabel('Neuron ID (reordered by bias direction)')     plt.colorbar(im, ax=ax1)      bias = npy.ensemble.bias_matrix_fast(         filtered_spikes,         filtered_neurons,         total_neurons=N_NEURONS,         fillneutral=.5,     )     bias = npy.ensemble.normalize_bias_matrix(bias)      im = ax2.imshow(bias[nrn_order, :][:, nrn_order], cmap='magma')     ax2.set_title('Replay Bias Matrix')     ax2.set_xlabel('Neuron ID (reordered by bias direction)')     ax2.set_ylabel('Neuron ID (reordered by bias direction)')     plt.colorbar(im, ax=ax2)      # order spikes and neuron ids by nrn_order     argsort = np.asarray(         sorted(             range(len(filtered_neurons)),             key=lambda x: nrn_order[int(filtered_neurons[x])]         )     )     spike_times = filtered_spikes[argsort]     spike_ids = filtered_neurons[argsort]     # raster plot     ax3.scatter(spike_times, spike_ids, c='k', marker='|', s=18)     ax3.set_title('Replay Raster Plot')     ax3.set_xlabel('Time')     ax3.set_ylabel('Neuron ID (reordered by bias direction)')     plt.tight_layout()     plt.show()  widgets.interact(plot_replay, n=widgets.IntSlider(min=0, max=len(post_rev_seq_epochs)-1, step=1, value=0)); <pre>interactive(children=(IntSlider(value=0, description='n', max=4), Output()), _dom_classes=('widget-interact',)\u2026</pre> In\u00a0[11]: Copied! <pre>basepath = r'/run/user/1000/gvfs/smb-share:server=132.236.112.212,share=ayadata1/Data/GrosmarkAD/Achilles/Achilles_10252013'\n\nepoch_df = npy.io.load_epoch(basepath)\n# get session bounds to provide support\nsession_bounds = nel.EpochArray(\n    [epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]]\n)\n# compress repeated sleep sessions\nepoch_df = npy.session.compress_repeated_epochs(epoch_df)\nbeh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values.astype(float))\n\nst, cell_metrics = npy.io.load_spikes(\n    basepath, putativeCellType=\"Pyr\", brainRegion=\"CA1\"\n)\nspike_spindices = npy.spikes.get_spindices(st.data)\n\nswr = npy.io.load_ripples_events(basepath, return_epoch_array=True)\n\ntheta = nel.EpochArray(npy.io.load_SleepState_states(basepath)[\"THETA\"])\n\ntask_idx = npy.process.in_intervals(\n    spike_spindices.spike_times, (beh_epochs[1] &amp; theta).data\n)\n</pre> basepath = r'/run/user/1000/gvfs/smb-share:server=132.236.112.212,share=ayadata1/Data/GrosmarkAD/Achilles/Achilles_10252013'  epoch_df = npy.io.load_epoch(basepath) # get session bounds to provide support session_bounds = nel.EpochArray(     [epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]] ) # compress repeated sleep sessions epoch_df = npy.session.compress_repeated_epochs(epoch_df) beh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values.astype(float))  st, cell_metrics = npy.io.load_spikes(     basepath, putativeCellType=\"Pyr\", brainRegion=\"CA1\" ) spike_spindices = npy.spikes.get_spindices(st.data)  swr = npy.io.load_ripples_events(basepath, return_epoch_array=True)  theta = nel.EpochArray(npy.io.load_SleepState_states(basepath)[\"THETA\"])  task_idx = npy.process.in_intervals(     spike_spindices.spike_times, (beh_epochs[1] &amp; theta).data ) In\u00a0[12]: Copied! <pre>position_df = npy.io.load_animal_behavior(basepath)\n\n# put position into a nelpy position array for ease of use\npos = nel.AnalogSignalArray(\n    data=position_df[\"x\"].values.T,\n    timestamps=position_df.timestamps.values,\n)\n\n# get outbound and inbound epochs\n(outbound_epochs, inbound_epochs) = npy.behavior.get_linear_track_lap_epochs(\n    pos.abscissa_vals, pos.data[0], newLapThreshold=20\n)\n\noutbound_epochs, inbound_epochs\n</pre> position_df = npy.io.load_animal_behavior(basepath)  # put position into a nelpy position array for ease of use pos = nel.AnalogSignalArray(     data=position_df[\"x\"].values.T,     timestamps=position_df.timestamps.values, )  # get outbound and inbound epochs (outbound_epochs, inbound_epochs) = npy.behavior.get_linear_track_lap_epochs(     pos.abscissa_vals, pos.data[0], newLapThreshold=20 )  outbound_epochs, inbound_epochs Out[12]: <pre>(&lt;EpochArray at 0x72b5fc830640: 42 epochs&gt; of length 17:07:964 minutes,\n &lt;EpochArray at 0x72b5fc830b50: 43 epochs&gt; of length 17:17:974 minutes)</pre> In\u00a0[13]: Copied! <pre>overlap = npy.process.find_intersecting_intervals(swr, beh_epochs[-1], return_indices=False)\n\n# intervals in set 1 that completely overlap with set 2\noverlap = swr.lengths == overlap\n</pre> overlap = npy.process.find_intersecting_intervals(swr, beh_epochs[-1], return_indices=False)  # intervals in set 1 that completely overlap with set 2 overlap = swr.lengths == overlap In\u00a0[\u00a0]: Copied! <pre>pbias = npy.ensemble.PairwiseBias(num_shuffles=100)\n\npost_swrs = [swr[overlap][i] for i, length in enumerate(swr[overlap].lengths) if length &gt; 0.08]\npost_swrs = nel.EpochArray([ep.data for ep in post_swrs if np.sum([len(spks) &gt; 0 for spks in st[ep].data]) &gt; 4])\n\n# Analyze significant replay\nz_score_sig, p_value_sig, cosine_val_sig = pbias.fit_transform(\n    spike_spindices[\"spike_times\"].values,\n    spike_spindices[\"spike_id\"].values,\n    inbound_epochs.data,\n    spike_spindices[\"spike_times\"].values,\n    spike_spindices[\"spike_id\"].values,\n    post_swrs.data,\n    allow_reverse_replay=False,\n)\n\nprint(\"Significant Replay Results:\")\nprint(f\"Z-scores: {z_score_sig}\")\nprint(f\"P-values: {p_value_sig}\")\nprint(f\"Cosine values: {cosine_val_sig}\")\n</pre> pbias = npy.ensemble.PairwiseBias(num_shuffles=100)  post_swrs = [swr[overlap][i] for i, length in enumerate(swr[overlap].lengths) if length &gt; 0.08] post_swrs = nel.EpochArray([ep.data for ep in post_swrs if np.sum([len(spks) &gt; 0 for spks in st[ep].data]) &gt; 4])  # Analyze significant replay z_score_sig, p_value_sig, cosine_val_sig = pbias.fit_transform(     spike_spindices[\"spike_times\"].values,     spike_spindices[\"spike_id\"].values,     inbound_epochs.data,     spike_spindices[\"spike_times\"].values,     spike_spindices[\"spike_id\"].values,     post_swrs.data,     allow_reverse_replay=False, )  print(\"Significant Replay Results:\") print(f\"Z-scores: {z_score_sig}\") print(f\"P-values: {p_value_sig}\") print(f\"Cosine values: {cosine_val_sig}\") <pre>/home/cornell/Desktop/Kushaan/neuro_py/neuro_py/ensemble/replay.py:659: RuntimeWarning: Mean of empty slice\n</pre> <pre>Significant Replay Results:\nZ-scores: [ 0.11456589  1.83435469 -1.58224202 ... -0.57006536 -0.16616777\n -0.82254613]\nP-values: [0.46534653 0.01980198 0.95049505 ... 0.7029703  0.58415842 0.82178218]\nCosine values: [-0.00167047  0.04488326 -0.02968688 ... -0.00790766 -0.0016217\n -0.01274348]\n</pre> In\u00a0[15]: Copied! <pre>fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n\nax1.hist(z_score_sig, alpha=0.7, label='Significant')\n# ax1.hist(z_score_nonsig, alpha=0.7, label='Non-significant')\nax1.set_title('Z-scores')\nax1.set_ylabel('Count')\nax1.set_xlabel('Z-score')\nax1.legend()\n\nax2.hist((p_value_sig &lt; .05).astype(int), alpha=0.7, label='Significant')\n# ax2.hist(p_value_nonsig, alpha=0.7, label='Non-significant')\nax2.set_title('P-values')\nax2.set_xlabel('P-value')\nax2.set_ylabel('Count')\nax2.axvline(x=0.05, color='r', linestyle='--', label='p=0.05')\nax2.legend()\n\nax3.hist(cosine_val_sig, alpha=0.7, label='Significant')\nax3.set_title('Cosine Values')\nax3.set_xlabel('Cosine Similarity')\nax3.set_ylabel('Count')\nax3.legend()\n\nplt.tight_layout()\nplt.show()\n</pre> fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))  ax1.hist(z_score_sig, alpha=0.7, label='Significant') # ax1.hist(z_score_nonsig, alpha=0.7, label='Non-significant') ax1.set_title('Z-scores') ax1.set_ylabel('Count') ax1.set_xlabel('Z-score') ax1.legend()  ax2.hist((p_value_sig &lt; .05).astype(int), alpha=0.7, label='Significant') # ax2.hist(p_value_nonsig, alpha=0.7, label='Non-significant') ax2.set_title('P-values') ax2.set_xlabel('P-value') ax2.set_ylabel('Count') ax2.axvline(x=0.05, color='r', linestyle='--', label='p=0.05') ax2.legend()  ax3.hist(cosine_val_sig, alpha=0.7, label='Significant') ax3.set_title('Cosine Values') ax3.set_xlabel('Cosine Similarity') ax3.set_ylabel('Count') ax3.legend()  plt.tight_layout() plt.show() In\u00a0[16]: Copied! <pre>SPATIAL_BIN_SIZE = 3\nBEHAVIOR_TIME_BIN_SIZE = 0.05\nREPLAY_TIME_BIN_SIZE = 0.02\nSPEED_THRESHOLD = 3\nTUNING_CURVE_SIGMA = 1\nPLACE_CELL_MIN_SPKS = 100\nPLACE_CELL_MIN_RATE = 1\nPLACE_CELL_PEAK_MIN_RATIO = 1.5\n\nN_SHUFFLES = 100\n</pre> SPATIAL_BIN_SIZE = 3 BEHAVIOR_TIME_BIN_SIZE = 0.05 REPLAY_TIME_BIN_SIZE = 0.02 SPEED_THRESHOLD = 3 TUNING_CURVE_SIGMA = 1 PLACE_CELL_MIN_SPKS = 100 PLACE_CELL_MIN_RATE = 1 PLACE_CELL_PEAK_MIN_RATIO = 1.5  N_SHUFFLES = 100 In\u00a0[17]: Copied! <pre>def get_tuning_curves(\n    pos, st_all, dir_epoch, speed_thres, time_binsize, s_binsize, tuning_curve_sigma\n):\n\n    spatial_maps = npy.tuning.SpatialMap(\n        pos,\n        st_all,\n        dim=1,\n        dir_epoch=dir_epoch,\n        s_binsize=s_binsize,\n        speed_thres=speed_thres,\n        tuning_curve_sigma=tuning_curve_sigma,\n        minbgrate=0.01,  # decoding does not like 0 firing rate\n    )\n\n    # restrict spike trains to those epochs during which the animal was running\n    st_run = st_all[dir_epoch][spatial_maps.run_epochs]\n\n    # bin running:\n    bst_run = st_run.bin(ds=time_binsize)\n\n    # return class 'auxiliary.TuningCurve1D'\n    #   instead of class 'maps.SpatialMap' for decoding.py compatibility\n    return spatial_maps.tc, st_run, bst_run, spatial_maps.run_epochs\n\n\ntc, st_run, bst_run, run_epochs = get_tuning_curves(\n    pos, st, inbound_epochs, SPEED_THRESHOLD, BEHAVIOR_TIME_BIN_SIZE,\n    SPATIAL_BIN_SIZE, TUNING_CURVE_SIGMA\n)\n</pre> def get_tuning_curves(     pos, st_all, dir_epoch, speed_thres, time_binsize, s_binsize, tuning_curve_sigma ):      spatial_maps = npy.tuning.SpatialMap(         pos,         st_all,         dim=1,         dir_epoch=dir_epoch,         s_binsize=s_binsize,         speed_thres=speed_thres,         tuning_curve_sigma=tuning_curve_sigma,         minbgrate=0.01,  # decoding does not like 0 firing rate     )      # restrict spike trains to those epochs during which the animal was running     st_run = st_all[dir_epoch][spatial_maps.run_epochs]      # bin running:     bst_run = st_run.bin(ds=time_binsize)      # return class 'auxiliary.TuningCurve1D'     #   instead of class 'maps.SpatialMap' for decoding.py compatibility     return spatial_maps.tc, st_run, bst_run, spatial_maps.run_epochs   tc, st_run, bst_run, run_epochs = get_tuning_curves(     pos, st, inbound_epochs, SPEED_THRESHOLD, BEHAVIOR_TIME_BIN_SIZE,     SPATIAL_BIN_SIZE, TUNING_CURVE_SIGMA ) In\u00a0[18]: Copied! <pre>def restrict_to_place_cells(\n    tc,\n    st_run,\n    bst_run,\n    st_all,\n    cell_metrics,\n    place_cell_min_spks,\n    place_cell_min_rate,\n    place_cell_peak_mean_ratio,\n):\n    # locate pyr cells with &gt;= 100 spikes, peak rate &gt;= 1 Hz, peak/mean ratio &gt;=1.5\n    peak_firing_rates = tc.max(axis=1)\n    mean_firing_rates = tc.mean(axis=1)\n    ratio = peak_firing_rates / mean_firing_rates\n\n    idx = (\n        (st_run.n_events &gt;= place_cell_min_spks)\n        &amp; (tc.ratemap.max(axis=1) &gt;= place_cell_min_rate)\n        &amp; (ratio &gt;= place_cell_peak_mean_ratio)\n    )\n    unit_ids_to_keep = (np.where(idx)[0] + 1).squeeze().tolist()\n\n    sta_placecells = st_all._unit_subset(unit_ids_to_keep)\n    tc = tc._unit_subset(unit_ids_to_keep)\n    total_units = sta_placecells.n_active\n    bst_run = bst_run.loc[:, unit_ids_to_keep]\n\n    # restrict cell_metrics to place cells\n    cell_metrics_ = cell_metrics[idx]\n\n    return sta_placecells, tc, bst_run, cell_metrics_, total_units\n\nsta_placecells, tc, bst_run, cell_metrics_, total_units = \\\n    restrict_to_place_cells(\n        tc, st_run, bst_run, st, cell_metrics, PLACE_CELL_MIN_SPKS,\n        PLACE_CELL_MIN_RATE, PLACE_CELL_PEAK_MIN_RATIO\n    )\n</pre> def restrict_to_place_cells(     tc,     st_run,     bst_run,     st_all,     cell_metrics,     place_cell_min_spks,     place_cell_min_rate,     place_cell_peak_mean_ratio, ):     # locate pyr cells with &gt;= 100 spikes, peak rate &gt;= 1 Hz, peak/mean ratio &gt;=1.5     peak_firing_rates = tc.max(axis=1)     mean_firing_rates = tc.mean(axis=1)     ratio = peak_firing_rates / mean_firing_rates      idx = (         (st_run.n_events &gt;= place_cell_min_spks)         &amp; (tc.ratemap.max(axis=1) &gt;= place_cell_min_rate)         &amp; (ratio &gt;= place_cell_peak_mean_ratio)     )     unit_ids_to_keep = (np.where(idx)[0] + 1).squeeze().tolist()      sta_placecells = st_all._unit_subset(unit_ids_to_keep)     tc = tc._unit_subset(unit_ids_to_keep)     total_units = sta_placecells.n_active     bst_run = bst_run.loc[:, unit_ids_to_keep]      # restrict cell_metrics to place cells     cell_metrics_ = cell_metrics[idx]      return sta_placecells, tc, bst_run, cell_metrics_, total_units  sta_placecells, tc, bst_run, cell_metrics_, total_units = \\     restrict_to_place_cells(         tc, st_run, bst_run, st, cell_metrics, PLACE_CELL_MIN_SPKS,         PLACE_CELL_MIN_RATE, PLACE_CELL_PEAK_MIN_RATIO     ) In\u00a0[19]: Copied! <pre>plt.imshow(tc.reorder_units().ratemap, aspect='auto', interpolation='none')\nplt.colorbar()\nplt.title('Place Cell Tuning Curves')\nplt.xlabel('Spatial Bin')\nplt.ylabel('Neurons (reordered by peak rate)')\nplt.show()\n</pre> plt.imshow(tc.reorder_units().ratemap, aspect='auto', interpolation='none') plt.colorbar() plt.title('Place Cell Tuning Curves') plt.xlabel('Spatial Bin') plt.ylabel('Neurons (reordered by peak rate)') plt.show() In\u00a0[20]: Copied! <pre>def decode_and_score(bst, tc, pos):# -&gt; tuple[float, float] | tuple[Any, Any, Any]:\n    # access decoding accuracy on behavioral time scale\n    posteriors, lengths, mode_pth, mean_pth = nel.decoding.decode1D(\n        bst, tc, xmin=np.nanmin(pos.data), xmax=np.nanmax(pos.data)\n    )\n\n    actual_pos = np.interp(bst.bin_centers, pos.abscissa_vals, pos.data[0])\n\n    bad_idx = np.isnan(actual_pos) | np.isnan(mode_pth)\n    actual_pos = actual_pos[~bad_idx]\n    mode_pth = mode_pth[~bad_idx]\n    if len(actual_pos) == 0:\n        return np.nan, np.nan\n    slope, intercept, rvalue, pvalue, stderr = scipy.stats.linregress(actual_pos, mode_pth)\n    median_error = np.nanmedian(np.abs(actual_pos - mode_pth))\n\n    return mode_pth, rvalue, median_error\n\ndef pooled_incoherent_shuffle_bst(bst):\n    out = copy.deepcopy(bst)\n    data = out._data\n\n    for uu in range(bst.n_units):\n        segment = np.atleast_1d(np.squeeze(data[uu, :]))\n        segment = np.roll(segment, np.random.randint(len(segment)))\n        data[uu, :] = segment\n    out._data = data\n    return out\n\ndef decode_and_shuff(bst, tc, pos, n_shuffles=500):\n    \"\"\" \"\"\"\n    pos_decoded, rvalue, median_error = decode_and_score(bst, tc, pos)\n\n    if n_shuffles &gt; 0:\n        rvalue_time_swap = np.zeros((n_shuffles, 1))\n        median_error_time_swap = np.zeros((n_shuffles, 1))\n\n    for shflidx in range(n_shuffles):\n        bst_shuff = pooled_incoherent_shuffle_bst(bst)\n        _, rvalue_time_swap[shflidx], median_error_time_swap[shflidx] = decode_and_score(\n            bst_shuff, tc, pos\n        )\n\n    return pos_decoded, rvalue, median_error, rvalue_time_swap, median_error_time_swap\n\nbst_run_beh = sta_placecells[inbound_epochs][run_epochs].bin(ds=BEHAVIOR_TIME_BIN_SIZE)\npos_decoded, decoding_r2, median_error, decoding_r2_shuff, _ = decode_and_shuff(\n    bst_run_beh, tc, pos[inbound_epochs][run_epochs], n_shuffles=N_SHUFFLES\n)\n</pre> def decode_and_score(bst, tc, pos):# -&gt; tuple[float, float] | tuple[Any, Any, Any]:     # access decoding accuracy on behavioral time scale     posteriors, lengths, mode_pth, mean_pth = nel.decoding.decode1D(         bst, tc, xmin=np.nanmin(pos.data), xmax=np.nanmax(pos.data)     )      actual_pos = np.interp(bst.bin_centers, pos.abscissa_vals, pos.data[0])      bad_idx = np.isnan(actual_pos) | np.isnan(mode_pth)     actual_pos = actual_pos[~bad_idx]     mode_pth = mode_pth[~bad_idx]     if len(actual_pos) == 0:         return np.nan, np.nan     slope, intercept, rvalue, pvalue, stderr = scipy.stats.linregress(actual_pos, mode_pth)     median_error = np.nanmedian(np.abs(actual_pos - mode_pth))      return mode_pth, rvalue, median_error  def pooled_incoherent_shuffle_bst(bst):     out = copy.deepcopy(bst)     data = out._data      for uu in range(bst.n_units):         segment = np.atleast_1d(np.squeeze(data[uu, :]))         segment = np.roll(segment, np.random.randint(len(segment)))         data[uu, :] = segment     out._data = data     return out  def decode_and_shuff(bst, tc, pos, n_shuffles=500):     \"\"\" \"\"\"     pos_decoded, rvalue, median_error = decode_and_score(bst, tc, pos)      if n_shuffles &gt; 0:         rvalue_time_swap = np.zeros((n_shuffles, 1))         median_error_time_swap = np.zeros((n_shuffles, 1))      for shflidx in range(n_shuffles):         bst_shuff = pooled_incoherent_shuffle_bst(bst)         _, rvalue_time_swap[shflidx], median_error_time_swap[shflidx] = decode_and_score(             bst_shuff, tc, pos         )      return pos_decoded, rvalue, median_error, rvalue_time_swap, median_error_time_swap  bst_run_beh = sta_placecells[inbound_epochs][run_epochs].bin(ds=BEHAVIOR_TIME_BIN_SIZE) pos_decoded, decoding_r2, median_error, decoding_r2_shuff, _ = decode_and_shuff(     bst_run_beh, tc, pos[inbound_epochs][run_epochs], n_shuffles=N_SHUFFLES ) In\u00a0[21]: Copied! <pre>replay_scores = []\nfor swr_ep in post_swrs:\n    bst = sta_placecells[swr_ep].bin(ds=REPLAY_TIME_BIN_SIZE)\n\n    slope, intercept, r2values = nel.analysis.replay.linregress_bst(bst, tc)\n    replay_scores.append(r2values.item())\n</pre> replay_scores = [] for swr_ep in post_swrs:     bst = sta_placecells[swr_ep].bin(ds=REPLAY_TIME_BIN_SIZE)      slope, intercept, r2values = nel.analysis.replay.linregress_bst(bst, tc)     replay_scores.append(r2values.item()) In\u00a0[22]: Copied! <pre># correlate replay scores with p-values\nscipy.stats.spearmanr(\n    np.abs(cosine_val_sig),  # Absolute to also consider reverse sequences\n    replay_scores\n)\n</pre> # correlate replay scores with p-values scipy.stats.spearmanr(     np.abs(cosine_val_sig),  # Absolute to also consider reverse sequences     replay_scores ) Out[22]: <pre>SignificanceResult(statistic=0.1982981181876735, pvalue=9.404532275254252e-11)</pre> <p>Visualize the significant replay events amongst the SWR events.</p> In\u00a0[23]: Copied! <pre>nrn_order = np.argsort(np.nansum(pbias.task_normalized, axis=1))\nsig_swr_indices = np.where(p_value_sig &lt; 0.05)[0]\n\ndef plot_replay(n):\n    fig, axes = plt.subplots(2, 3, figsize=(11, 8))\n    swrs_sig = post_swrs[sig_swr_indices]\n    sig_swr_index = np.atleast_2d(swrs_sig[n].data)\n    bias = npy.ensemble.bias_matrix_fast(\n        spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_times\"].values,\n        spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_id\"].values,\n        total_neurons=st.n_units, fillneutral=.5\n    )\n    bias = npy.ensemble.normalize_bias_matrix(bias)\n    cossim = npy.ensemble.cosine_similarity_matrices(\n        pbias.task_normalized, bias\n    )\n    print(f\"Epoch {n} Cosine Similarity: {cossim}\")\n    ax = axes[0, 0]\n    ax.imshow(pbias.task_normalized[nrn_order, :][:, nrn_order], cmap='magma')\n    ax.set_title('Template Bias Matrix')\n    ax.set_xlabel('Neurons (reordered by bias direction)')\n    ax.set_ylabel('Neurons (reordered by bias direction)')\n\n    ax = axes[0, 1]\n    ax.imshow(bias[nrn_order, :][:, nrn_order], cmap='magma')\n    ax.set_title('Replay Bias Matrix')\n    ax.set_xlabel('Neurons (reordered by bias direction)')\n    ax.set_ylabel('Neurons (reordered by bias direction)')\n\n    spike_times = spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_times\"].values\n    spike_ids = spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_id\"].values\n    # # order spikes and neuron ids by nrn_order\n    # argsort = np.asarray(sorted(range(len(spike_ids)), key=lambda x: nrn_order[int(spike_ids[x])]))\n    # spike_times = spike_times[argsort]\n    # spike_ids = spike_ids[argsort]\n    spike_id_ordermap = dict(zip(nrn_order, range(len(nrn_order))))\n    spike_ids = np.asarray([spike_id_ordermap[int(spike_id)] for spike_id in spike_ids], dtype=int)\n    \n    # raster plot\n    ax = axes[0, 2]\n    ax.scatter(spike_times, spike_ids, c='k', marker='|', s=18)\n    ax.set_title('Replay Raster Plot')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Neurons (reordered by bias direction)')\n\n    bst = sta_placecells[swrs_sig[n]].bin(ds=REPLAY_TIME_BIN_SIZE)\n\n    posteriors, lengths, mode_pth, mean_pth = nel.decoding.decode1D(\n        bst, tc, xmin=np.nanmin(pos.data), xmax=np.nanmax(pos.data)\n    )\n\n    spike_times = spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_times\"].values\n    spike_ids = spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_id\"].values\n    spike_id_ordermap = np.asarray(tc.get_peak_firing_order_ids()) - 1\n    spike_id_ordermap = dict(zip(spike_id_ordermap, range(len(spike_id_ordermap))))\n    # filter non-place cells\n    spike_times = np.asarray([ts for i, ts in enumerate(spike_times) if spike_ids[i] in spike_id_ordermap])\n    spike_ids = np.asarray([id for id in spike_ids if id in spike_id_ordermap], dtype=int)\n    # replace spike_ids with their peak firing order\n    spike_ids = np.asarray([spike_id_ordermap[int(spike_id)] for spike_id in spike_ids], dtype=int)\n\n    ax = axes[1, 0]\n    ax.scatter(spike_times, spike_ids, c='k', marker='|')\n    ax.set_title('Replay Raster Plot')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Neurons (reordered by peak firing order)')\n\n    stswr = st[swrs_sig[n]]\n    stswr = stswr._unit_subset(tc.get_peak_firing_order_ids())\n    stswr.reorder_units_by_ids(tc.get_peak_firing_order_ids(), inplace=True)\n\n    ax = axes[1, 1]\n    ax.plot(mode_pth, color='r', label='Mode Path')\n    ax.plot(mean_pth, color='b', label='Mean Path')\n    ax.set_title('Decoded Path')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Position')\n    ax.legend()\n\n    ax = axes[1, 2]\n    ax.imshow(posteriors, aspect='auto')\n    ax.set_title('Posterior Probability')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Position Bins')\n\n    plt.tight_layout()\n    plt.show()\n\nwidgets.interact(plot_replay, n=widgets.IntSlider(min=0, max=len(sig_swr_indices)-1, step=1, value=0));\n</pre> nrn_order = np.argsort(np.nansum(pbias.task_normalized, axis=1)) sig_swr_indices = np.where(p_value_sig &lt; 0.05)[0]  def plot_replay(n):     fig, axes = plt.subplots(2, 3, figsize=(11, 8))     swrs_sig = post_swrs[sig_swr_indices]     sig_swr_index = np.atleast_2d(swrs_sig[n].data)     bias = npy.ensemble.bias_matrix_fast(         spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_times\"].values,         spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_id\"].values,         total_neurons=st.n_units, fillneutral=.5     )     bias = npy.ensemble.normalize_bias_matrix(bias)     cossim = npy.ensemble.cosine_similarity_matrices(         pbias.task_normalized, bias     )     print(f\"Epoch {n} Cosine Similarity: {cossim}\")     ax = axes[0, 0]     ax.imshow(pbias.task_normalized[nrn_order, :][:, nrn_order], cmap='magma')     ax.set_title('Template Bias Matrix')     ax.set_xlabel('Neurons (reordered by bias direction)')     ax.set_ylabel('Neurons (reordered by bias direction)')      ax = axes[0, 1]     ax.imshow(bias[nrn_order, :][:, nrn_order], cmap='magma')     ax.set_title('Replay Bias Matrix')     ax.set_xlabel('Neurons (reordered by bias direction)')     ax.set_ylabel('Neurons (reordered by bias direction)')      spike_times = spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_times\"].values     spike_ids = spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_id\"].values     # # order spikes and neuron ids by nrn_order     # argsort = np.asarray(sorted(range(len(spike_ids)), key=lambda x: nrn_order[int(spike_ids[x])]))     # spike_times = spike_times[argsort]     # spike_ids = spike_ids[argsort]     spike_id_ordermap = dict(zip(nrn_order, range(len(nrn_order))))     spike_ids = np.asarray([spike_id_ordermap[int(spike_id)] for spike_id in spike_ids], dtype=int)          # raster plot     ax = axes[0, 2]     ax.scatter(spike_times, spike_ids, c='k', marker='|', s=18)     ax.set_title('Replay Raster Plot')     ax.set_xlabel('Time')     ax.set_ylabel('Neurons (reordered by bias direction)')      bst = sta_placecells[swrs_sig[n]].bin(ds=REPLAY_TIME_BIN_SIZE)      posteriors, lengths, mode_pth, mean_pth = nel.decoding.decode1D(         bst, tc, xmin=np.nanmin(pos.data), xmax=np.nanmax(pos.data)     )      spike_times = spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_times\"].values     spike_ids = spike_spindices[npy.process.in_intervals(spike_spindices.spike_times, sig_swr_index)][\"spike_id\"].values     spike_id_ordermap = np.asarray(tc.get_peak_firing_order_ids()) - 1     spike_id_ordermap = dict(zip(spike_id_ordermap, range(len(spike_id_ordermap))))     # filter non-place cells     spike_times = np.asarray([ts for i, ts in enumerate(spike_times) if spike_ids[i] in spike_id_ordermap])     spike_ids = np.asarray([id for id in spike_ids if id in spike_id_ordermap], dtype=int)     # replace spike_ids with their peak firing order     spike_ids = np.asarray([spike_id_ordermap[int(spike_id)] for spike_id in spike_ids], dtype=int)      ax = axes[1, 0]     ax.scatter(spike_times, spike_ids, c='k', marker='|')     ax.set_title('Replay Raster Plot')     ax.set_xlabel('Time')     ax.set_ylabel('Neurons (reordered by peak firing order)')      stswr = st[swrs_sig[n]]     stswr = stswr._unit_subset(tc.get_peak_firing_order_ids())     stswr.reorder_units_by_ids(tc.get_peak_firing_order_ids(), inplace=True)      ax = axes[1, 1]     ax.plot(mode_pth, color='r', label='Mode Path')     ax.plot(mean_pth, color='b', label='Mean Path')     ax.set_title('Decoded Path')     ax.set_xlabel('Time')     ax.set_ylabel('Position')     ax.legend()      ax = axes[1, 2]     ax.imshow(posteriors, aspect='auto')     ax.set_title('Posterior Probability')     ax.set_xlabel('Time')     ax.set_ylabel('Position Bins')      plt.tight_layout()     plt.show()  widgets.interact(plot_replay, n=widgets.IntSlider(min=0, max=len(sig_swr_indices)-1, step=1, value=0)); <pre>interactive(children=(IntSlider(value=0, description='n', max=96), Output()), _dom_classes=('widget-interact',\u2026</pre>"},{"location":"tutorials/bias_correlation/#pairwise-bias-correlation-analysis","title":"Pairwise Bias Correlation Analysis\u00b6","text":"<p>In this tutorial, we'll explore how to use the <code>PairwiseBias</code> class to analyze neural replay events. We'll simulate spike data for both task and post-task periods, and then use the PairwiseBias transformer to detect significant replay events.</p>"},{"location":"tutorials/bias_correlation/#setup","title":"Setup\u00b6","text":"<p>First, let's import the necessary libraries and set up our environment.</p>"},{"location":"tutorials/bias_correlation/#imports","title":"Imports\u00b6","text":""},{"location":"tutorials/bias_correlation/#section-1-simulate-data","title":"Section 1: Simulate Data\u00b6","text":""},{"location":"tutorials/bias_correlation/#section-11-simulating-task-spike-data-with-replay-events","title":"Section 1.1: Simulating Task Spike Data with Replay Events\u00b6","text":"<p>We'll start by simulating spike data for a task period. This will represent the original neural activity that we want to detect in replay events.</p>"},{"location":"tutorials/bias_correlation/#section-12-simulating-post-task-spike-data-with-forward-replay","title":"Section 1.2: Simulating Post-Task Spike Data with Forward Replay\u00b6","text":"<p>Now, let's simulate post-task spike data for both significant and non-significant replay scenarios, and visualize them.</p>"},{"location":"tutorials/bias_correlation/#section-2-analyzing-replay-with-pairwisebias","title":"Section 2: Analyzing Replay with PairwiseBias\u00b6","text":"<p>Now that we have our simulated data, let's use the PairwiseBias transformer to analyze replay events.</p>"},{"location":"tutorials/bias_correlation/#section-3-visualizing-results","title":"Section 3: Visualizing Results\u00b6","text":"<p>Let's create a simple visualization to compare the significant and non-significant replay results.</p> <p>Each bar represents the statistics for a single epoch of interest which may be the occurrence of a Sharp Wave Ripple (SWR) event in the hippocampal neural activity.</p>"},{"location":"tutorials/bias_correlation/#section-4-simulation-post-task-spike-data-with-both-forward-and-reverse-replays","title":"Section 4: Simulation Post-Task Spike Data with both Forward and Reverse Replays\u00b6","text":"<p>Now, let's simulate post-task spike data for both forward and reverse replay scenarios, and visualize them.</p>"},{"location":"tutorials/bias_correlation/#section-5-analyzing-sequences-in-real-data","title":"Section 5: Analyzing sequences in real data\u00b6","text":"<p>Now that we have our simulated data, let's use the PairwiseBias transformer to analyze replay in the SWR events.</p>"},{"location":"tutorials/bias_correlation/#section-51-load-the-data","title":"Section 5.1: Load the data\u00b6","text":""},{"location":"tutorials/bias_correlation/#section-52-analyzing-forward-and-reverse-replays-in-swr-events-using-pairwisebias","title":"Section 5.2: Analyzing Forward and Reverse Replays in SWR Events using PairwiseBias\u00b6","text":""},{"location":"tutorials/bias_correlation/#section-53-forward-and-reverse-replays-in-swr-events-using-bayesian-decoder","title":"Section 5.3: Forward and Reverse Replays in SWR Events using Bayesian Decoder\u00b6","text":""},{"location":"tutorials/bias_correlation/#section-54-comparing-replay-detection-consistency-between-pairwisebias-and-bayesian-decoder","title":"Section 5.4: Comparing Replay Detection Consistency between PairwiseBias and Bayesian Decoder\u00b6","text":""},{"location":"tutorials/bias_correlation/#section-6-conclusion","title":"Section 6: Conclusion\u00b6","text":"<p>In this tutorial, we've demonstrated how to use the <code>PairwiseBias</code> class to analyze neural replay events. We simulated task and post-task spike data, and then used the PairwiseBias object to detect significant replay events. The results show that our method can effectively distinguish between significant and non-significant replay, as evidenced by the Z-scores and p-values. The results are also compared with Bayesian decoding to show the consistency of replay detection.</p> <p>Key observations:</p> <ol> <li>The task spike data visualization shows a clear sequential pattern across neurons.</li> <li>The significant post-task spike data maintains a similar sequential pattern, while the non-significant data appears more random.</li> <li>For significant replay, we see high Z-scores and low p-values (p &lt; 0.05).</li> <li>For non-significant replay, we see mostly lower Z-scores and higher p-values (p &gt; 0.05).</li> <li>The PairwiseBias and Bayesian decoder results show a reasonable degree of mutual consistency in replay detection.</li> </ol>"},{"location":"tutorials/decoding/","title":"Neural Decoding","text":"In\u00a0[1]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\nimport logging\nimport random\n\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport sklearn\nimport pandas as pd\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport neuro_py as npy\nimport nelpy as nel\nimport ratinabox as rbx\nimport ipywidgets as widgets\n\nfrom ratinabox.Neurons import PlaceCells\n\n# Disable logging\nlogger = logging.getLogger()\nlogger.disabled = True\n</pre> %reload_ext autoreload %autoreload 2 import logging import random  import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import sklearn import pandas as pd import torch from torch import nn import torch.nn.functional as F import neuro_py as npy import nelpy as nel import ratinabox as rbx import ipywidgets as widgets  from ratinabox.Neurons import PlaceCells  # Disable logging logger = logging.getLogger() logger.disabled = True In\u00a0[2]: Copied! <pre>def set_seed(seed=None, seed_torch=True):\n    if seed is None:\n        seed = np.random.choice(2 ** 32 - 1)\n    random.seed(seed)\n    np.random.seed(seed)\n    if seed_torch:\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cudnn.deterministic = True\n\n    print(f'Random seed {seed} has been set.')\n    return seed\n\ndef set_device():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if device != \"cuda\":\n        print(\"GPU is not enabled.\")\n    else:\n        print(\"GPU is enabled.\")\n\n    return device\n\nSEED = set_seed(2025)\nDEVICE = set_device()\n</pre> def set_seed(seed=None, seed_torch=True):     if seed is None:         seed = np.random.choice(2 ** 32 - 1)     random.seed(seed)     np.random.seed(seed)     if seed_torch:         torch.manual_seed(seed)         torch.cuda.manual_seed_all(seed)         torch.cuda.manual_seed(seed)         torch.backends.cudnn.benchmark = True         torch.backends.cudnn.deterministic = True      print(f'Random seed {seed} has been set.')     return seed  def set_device():     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"     if device != \"cuda\":         print(\"GPU is not enabled.\")     else:         print(\"GPU is enabled.\")      return device  SEED = set_seed(2025) DEVICE = set_device() <pre>Random seed 2025 has been set.\nGPU is enabled.\n</pre> In\u00a0[3]: Copied! <pre>def create_cheeseboard_maze(radius=6.8, nrewards=3, nwellsonaxis=17, nseparationunits=4):\n    \"\"\"\n    Creates a cheeseboard-inspired maze using RatInABox with a circular environment \n    and a grid of navigable positions.\n    \"\"\"\n    # make a bulb shaped boundary\n    environment = rbx.Environment(\n        params = dict(\n            boundary=[[0.5*np.cos(t),0.5*np.sin(t)] for t in np.linspace(0,2*np.pi,100)],\n            boundary_conditions=\"solid\",\n            dimensionality='2D',\n        )\n    )\n\n    # Generate the grid of positions\n    x = np.linspace(-radius*1.03, radius*1.03, nwellsonaxis)\n    y = np.linspace(-radius*1.03, radius*1.03, nwellsonaxis)\n    X, Y = np.meshgrid(x, y)\n    grid_positions = np.vstack([X.flatten(), Y.flatten()]).T\n    separationunit = np.sqrt((x[1] - x[0])**2 + (y[1] - y[0])**2)\n\n    # Filter positions to stay within the circle\n    circle_mask = (X**2 + Y**2) &lt;= radius**2\n    grid_positions_within_circle = grid_positions[circle_mask.flatten()]\n\n    # Randomly pick 3 points with a minimum separation\n    def pick_random_points(points, num_points=3, min_dist=4):\n        chosen_points = []\n        chosen_points_indices = []\n        while len(chosen_points) &lt; num_points:\n            candidate_idx = np.random.choice(len(points))\n            candidate = points[candidate_idx]\n            if all(np.linalg.norm(candidate - p) &gt;= min_dist for p in chosen_points):\n                chosen_points.append(candidate)\n                chosen_points_indices.append(candidate_idx)\n        return np.array(chosen_points_indices)\n\n    reward_indices = pick_random_points(\n        grid_positions_within_circle,\n        num_points=nrewards,\n        min_dist=separationunit*nseparationunits\n    )\n\n    # add wells\n    for i, gp in enumerate(grid_positions_within_circle):\n        environment.add_object(gp/(radius*2))\n\n    return environment, reward_indices\n\nenvironment, reward_indices = create_cheeseboard_maze()\n</pre> def create_cheeseboard_maze(radius=6.8, nrewards=3, nwellsonaxis=17, nseparationunits=4):     \"\"\"     Creates a cheeseboard-inspired maze using RatInABox with a circular environment      and a grid of navigable positions.     \"\"\"     # make a bulb shaped boundary     environment = rbx.Environment(         params = dict(             boundary=[[0.5*np.cos(t),0.5*np.sin(t)] for t in np.linspace(0,2*np.pi,100)],             boundary_conditions=\"solid\",             dimensionality='2D',         )     )      # Generate the grid of positions     x = np.linspace(-radius*1.03, radius*1.03, nwellsonaxis)     y = np.linspace(-radius*1.03, radius*1.03, nwellsonaxis)     X, Y = np.meshgrid(x, y)     grid_positions = np.vstack([X.flatten(), Y.flatten()]).T     separationunit = np.sqrt((x[1] - x[0])**2 + (y[1] - y[0])**2)      # Filter positions to stay within the circle     circle_mask = (X**2 + Y**2) &lt;= radius**2     grid_positions_within_circle = grid_positions[circle_mask.flatten()]      # Randomly pick 3 points with a minimum separation     def pick_random_points(points, num_points=3, min_dist=4):         chosen_points = []         chosen_points_indices = []         while len(chosen_points) &lt; num_points:             candidate_idx = np.random.choice(len(points))             candidate = points[candidate_idx]             if all(np.linalg.norm(candidate - p) &gt;= min_dist for p in chosen_points):                 chosen_points.append(candidate)                 chosen_points_indices.append(candidate_idx)         return np.array(chosen_points_indices)      reward_indices = pick_random_points(         grid_positions_within_circle,         num_points=nrewards,         min_dist=separationunit*nseparationunits     )      # add wells     for i, gp in enumerate(grid_positions_within_circle):         environment.add_object(gp/(radius*2))      return environment, reward_indices  environment, reward_indices = create_cheeseboard_maze() In\u00a0[4]: Copied! <pre>agent = rbx.Agent(environment)\n\nN_NEURONS = 10\nTIME = 10\nbins = int(TIME * 60 / agent.dt)\nplacecells = PlaceCells(\n    agent,\n    params=dict(\n        description=\"gaussian_threshold\", widths=0.3, n=N_NEURONS, color=\"C1\",\n        wall_geometry='line_of_sight'\n    ),\n)\n\n# simulate the agent in the maze\nfor i in range(bins):\n    agent.update()\n    placecells.update()\n\nagent.plot_trajectory();\n</pre> agent = rbx.Agent(environment)  N_NEURONS = 10 TIME = 10 bins = int(TIME * 60 / agent.dt) placecells = PlaceCells(     agent,     params=dict(         description=\"gaussian_threshold\", widths=0.3, n=N_NEURONS, color=\"C1\",         wall_geometry='line_of_sight'     ), )  # simulate the agent in the maze for i in range(bins):     agent.update()     placecells.update()  agent.plot_trajectory(); <pre>WARNING: This figure has not been saved.\n    \u2022 To AUTOMATICALLY save all plots (recommended), set  `ratinabox.autosave_plots = True`\n    \u2022 To MANUALLY save plots, call                        `ratinabox.utils.save_figure(figure_object, save_title).\n      This warning will not be shown again\nHINT: You can stylize plots to make them look like repo/paper by calling `ratinabox.stylize_plots()`\n      This hint will not be shown again\n</pre> In\u00a0[5]: Copied! <pre>fig, ax = placecells.plot_rate_map(chosen_neurons=\"all\")\n</pre> fig, ax = placecells.plot_rate_map(chosen_neurons=\"all\") In\u00a0[6]: Copied! <pre>placecells.plot_rate_map(method=\"history\");\n</pre> placecells.plot_rate_map(method=\"history\"); In\u00a0[7]: Copied! <pre>t_start = 0.0  # seconds\nt_end = bins\nt = np.array(placecells.history[\"t\"])\ni_start = (0 if t_start is None else np.argmin(np.abs(t - t_start)))\ni_end = (-1 if t_end is None else np.argmin(np.abs(t - t_end)))           \nt = t[i_start:i_end]  # subsample data for training (most of it is redundant anyway)\nneural_data = np.array(placecells.history[\"firingrate\"])[i_start:i_end]\ntrajectory = np.array(placecells.Agent.history[\"pos\"])[i_start:i_end]\n</pre> t_start = 0.0  # seconds t_end = bins t = np.array(placecells.history[\"t\"]) i_start = (0 if t_start is None else np.argmin(np.abs(t - t_start))) i_end = (-1 if t_end is None else np.argmin(np.abs(t - t_end)))            t = t[i_start:i_end]  # subsample data for training (most of it is redundant anyway) neural_data = np.array(placecells.history[\"firingrate\"])[i_start:i_end] trajectory = np.array(placecells.Agent.history[\"pos\"])[i_start:i_end]  <p>Format the neural data and behavioral variables (here, position) for decoding:</p> In\u00a0[8]: Copied! <pre># Convert neural data and trajectory into training format\nneural_data_df = pd.DataFrame(neural_data)  # Neural data as DataFrame\npredict_bv = [\"x\", \"y\"]\ntrajectory_df = pd.DataFrame(trajectory, columns=predict_bv)  # Positions\n\n# Split into training and testing sets (e.g., 80% train, 20% test)\nsplit_idx = int(0.8 * len(neural_data_df))\ntrain_neural, test_neural = neural_data_df[:split_idx], neural_data_df[split_idx:]\ntrain_trajectory, test_trajectory = trajectory_df[:split_idx], trajectory_df[split_idx:]\n</pre> # Convert neural data and trajectory into training format neural_data_df = pd.DataFrame(neural_data)  # Neural data as DataFrame predict_bv = [\"x\", \"y\"] trajectory_df = pd.DataFrame(trajectory, columns=predict_bv)  # Positions  # Split into training and testing sets (e.g., 80% train, 20% test) split_idx = int(0.8 * len(neural_data_df)) train_neural, test_neural = neural_data_df[:split_idx], neural_data_df[split_idx:] train_trajectory, test_trajectory = trajectory_df[:split_idx], trajectory_df[split_idx:]  In\u00a0[9]: Copied! <pre>plt.imshow(neural_data_df.T, aspect=\"auto\", cmap=\"inferno\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Neuron\")\nplt.title(\"Firing Rates of Place Cells\")\nplt.colorbar(label=\"Firing Rate (Hz)\")\nplt.show()\n</pre> plt.imshow(neural_data_df.T, aspect=\"auto\", cmap=\"inferno\") plt.xlabel(\"Time (s)\") plt.ylabel(\"Neuron\") plt.title(\"Firing Rates of Place Cells\") plt.colorbar(label=\"Firing Rate (Hz)\") plt.show() In\u00a0[10]: Copied! <pre>ridge = sklearn.linear_model.Ridge(alpha=0.01)\nridge.fit(train_neural, train_trajectory)\nridge.score(test_neural, test_trajectory)\n</pre> ridge = sklearn.linear_model.Ridge(alpha=0.01) ridge.fit(train_neural, train_trajectory) ridge.score(test_neural, test_trajectory) Out[10]: <pre>0.7262108789743416</pre> In\u00a0[11]: Copied! <pre># Initialise and fit model\nmodel_GP = sklearn.gaussian_process.GaussianProcessRegressor(\n    alpha=0.01,\n    kernel=sklearn.gaussian_process.kernels.RBF(\n        1 * np.sqrt(N_NEURONS / 20),  # &lt;-- kernel size scales with typical input size ~sqrt(N)\n        length_scale_bounds=\"fixed\",\n    ),\n)\nmodel_GP.fit(train_neural, train_trajectory)\nmodel_GP.score(test_neural, test_trajectory)\n</pre> # Initialise and fit model model_GP = sklearn.gaussian_process.GaussianProcessRegressor(     alpha=0.01,     kernel=sklearn.gaussian_process.kernels.RBF(         1 * np.sqrt(N_NEURONS / 20),  # &lt;-- kernel size scales with typical input size ~sqrt(N)         length_scale_bounds=\"fixed\",     ), ) model_GP.fit(train_neural, train_trajectory) model_GP.score(test_neural, test_trajectory) Out[11]: <pre>0.8795175641295223</pre> In\u00a0[12]: Copied! <pre>help(npy.ensemble.decoding.train_model)\n</pre> help(npy.ensemble.decoding.train_model) <pre>Help on function train_model in module neuro_py.ensemble.decoding.pipeline:\n\ntrain_model(partitions, hyperparams, resultspath=None, stop_partition=None)\n    Generic function to train a DNN model on the given data partitions.\n    \n    Parameters\n    ----------\n    partitions : array-like\n        K-fold partitions of the data with the following format:\n        [(nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test), ...]\n        Each element of the list is a tuple of numpy arrays containing the with\n        pairs of neural state vectors and behavioral variables for the training,\n        validation, and test sets. Each array has the shape\n        (ntrials, nbins, nfeats) where nfeats is the number of neurons for the\n        neural state vectors and number of behavioral features to be predicted\n        for the behavioral variables.\n    hyperparams : dict\n        Dictionary containing the hyperparameters for the model training. The\n        dictionary should contain the following keys:\n        - `model`: str, the type of the model to be trained. Multi-layer\n            Perceptron (MLP), Long Short-Term Memory (LSTM), many-to-many LSTM\n            (M2MLSTM), Transformer (NDT).\n        - `model_args`: dict, the arguments to be passed to the model\n            constructor. The arguments should be in the format expected by the\n            model constructor.\n            - `in_dim`: The number of input features.\n            - `out_dim`: The number of output features.\n            - `hidden_dim`: The number of hidden units each hidden layer of the\n                model. Can also take float values to specify the dropout rate.\n                For LSTM and M2MLSTM, it should be a tuple of the hidden size,\n                the number of layers, and the dropout rate.\n            - `args`:\n                - `clf`: If True, the model is a classifier; otherwise, it is a\n                    regressor.\n                - `activations`: The activation functions for each layer.\n                - `criterion`: The loss function to optimize.\n                - `epochs`: The number of complete passes through the training\n                    dataset.\n                - `lr`: Controls how much to change the model in response to the\n                    estimated error each time the model weights are updated. A\n                    smaller value ensures stable convergence but may slow down\n                    training, while a larger value speeds up training but risks\n                    overshooting.\n                - `base_lr`: The initial learning rate for the learning rate\n                    scheduler.\n                - `max_grad_norm`: The maximum norm of the gradients.\n                - `iters_to_accumulate`: The number of iterations to accumulate\n                    gradients.\n                - `weight_decay`: The L2 regularization strength.\n                - `num_training_batches`: The number of training batches. If\n                    None, the number of batches is calculated based on the batch\n                    size and the length of the training data.\n                - `scheduler_step_size_multiplier`: The multiplier for the\n                    learning rate scheduler step size. Higher values lead to\n                    faster learning rate decay.\n        - `bins_before`: int, the number of bins before the current bin to\n            include in the input data.\n        - `bins_current`: int, the number of bins in the current time bin to\n            include in the input data.\n        - `bins_after`: int, the number of bins after the current bin to include\n            in the input data.\n        - `behaviors`: list, the indices of the columns of behavioral features\n            to be predicted. Selected behavioral variable must have homogenous\n            data types across all features (continuous for regression and\n            categorical for classification)\n        - `batch_size`: int, the number of training examples utilized in one\n            iteration. Larger batch sizes offer stable gradient estimates but\n            require more memory, while smaller batches introduce noise that can\n            help escape local minima.\n        - `num_workers`: int, The number of parallel processes to use for data\n            loading. Increasing the number of workers can speed up data loading\n            but may lead to memory issues. Too many workers can also slow down\n            the training process due to contention for resources.\n        - `device`: str, the device to use for training. Should be 'cuda' or\n            'cpu'.\n        - `seed`: int, the random seed for reproducibility.\n    resultspath : str or None, optional\n        Path to the directory where the trained models and logs will be saved.\n    stop_partition : int, optional\n        Index of the partition to stop training at. Only useful for debugging,\n        by default None\n    \n    Returns\n    -------\n    tuple\n        Tuple containing the predicted behavioral variables for each fold,\n        the trained models for each fold, the normalization parameters for each\n        fold, and the evaluation metrics for each fold.\n\n</pre> In\u00a0[13]: Copied! <pre>decoder_type = 'MLP'  # Select decoder type (e.g., MLP)\nhyperparams = dict(\n    batch_size=512*4,\n    num_workers=5,\n    model=decoder_type,\n    model_args=dict(\n        in_dim=train_neural.shape[1],\n        out_dim=train_trajectory.shape[1],\n        hidden_dims=[64, 32, 16, 8],\n        args=dict(\n            clf=False,\n            activations=nn.CELU,\n            criterion=F.mse_loss,\n            epochs=10,\n            lr=3e-2,\n            base_lr=1e-2,\n            max_grad_norm=1.,\n            iters_to_accumulate=1,\n            weight_decay=1e-2,\n            num_training_batches=None,\n            scheduler_step_size_multiplier=1,\n        )\n    ),\n    behaviors=[0, 1],\n    bins_before=0,\n    bins_current=1,\n    bins_after=0,\n    device=DEVICE,\n    seed=SEED\n)\n</pre> decoder_type = 'MLP'  # Select decoder type (e.g., MLP) hyperparams = dict(     batch_size=512*4,     num_workers=5,     model=decoder_type,     model_args=dict(         in_dim=train_neural.shape[1],         out_dim=train_trajectory.shape[1],         hidden_dims=[64, 32, 16, 8],         args=dict(             clf=False,             activations=nn.CELU,             criterion=F.mse_loss,             epochs=10,             lr=3e-2,             base_lr=1e-2,             max_grad_norm=1.,             iters_to_accumulate=1,             weight_decay=1e-2,             num_training_batches=None,             scheduler_step_size_multiplier=1,         )     ),     behaviors=[0, 1],     bins_before=0,     bins_current=1,     bins_after=0,     device=DEVICE,     seed=SEED ) <p>Train the decoder using the <code>train_model</code> function:</p> In\u00a0[14]: Copied! <pre>partitions = [\n    (train_neural, train_trajectory,\n    test_neural, test_trajectory,\n    test_neural, test_trajectory)\n]\n\n# Train the model using the pipeline function from your decoder module\nresults_path = None  # checkpoints will be saved to this path, optimizing reruns\nbv_preds_folds, bv_models_folds, norm_params_folds, metrics_folds = npy.ensemble.decoding.train_model(\n    partitions, hyperparams, resultspath=results_path)\n</pre> partitions = [     (train_neural, train_trajectory,     test_neural, test_trajectory,     test_neural, test_trajectory) ]  # Train the model using the pipeline function from your decoder module results_path = None  # checkpoints will be saved to this path, optimizing reruns bv_preds_folds, bv_models_folds, norm_params_folds, metrics_folds = npy.ensemble.decoding.train_model(     partitions, hyperparams, resultspath=results_path)  <pre>Seed set to 2025\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA GeForce RTX 4080 SUPER') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | main | Sequential | 3.5 K  | train\n--------------------------------------------\n3.5 K     Trainable params\n0         Non-trainable params\n3.5 K     Total params\n0.014     Total estimated model params size (MB)\n10        Modules in train mode\n0         Modules in eval mode\n`Trainer.fit` stopped: `max_epochs=10` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_loss         \u2502   0.011781885288655758    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Variance weighed avg. coefficient of determination: 0.8003649387327375\nRMSE: 0.09671944566294162\n</pre> In\u00a0[15]: Copied! <pre>predicted_x = bv_preds_folds[0][:, 0]\npredicted_y = bv_preds_folds[0][:, 1]\ntrue_x = test_trajectory['x'].values\ntrue_y = test_trajectory['y'].values\n\n# Plotting\nfigsize=(15, 5)\n\nfig = plt.figure(figsize=figsize, constrained_layout=True)\nnrows, ncols = 2, 3\ngs = mpl.gridspec.GridSpec(*(nrows, ncols), figure=fig)#, height_ratios=[.5] * nrows, width_ratios=[.1]*ncols)\n\nax = fig.add_subplot(gs[0, 0:2])\n# X-coordinate plot\nax.plot(true_x, label=\"Actual X-Position\")\nax.plot(predicted_x, label=\"Predicted X-Position\")\nax.set_ylabel(\"X-Coordinate\")\nax.set_title(\"Decoding Performance (X-Coordinate)\")\nax.legend()\n\nax = fig.add_subplot(gs[1, 0:2])\n# Y-coordinate plot\nax.plot(true_y, label=\"Actual Y-Position\")\nax.plot(predicted_y, label=\"Predicted Y-Position\")\nax.set_xlabel(\"Time Step\")\nax.set_ylabel(\"Y-Coordinate\")\nax.set_title(\"Decoding Performance (Y-Coordinate)\")\nax.legend()\n\n\nax = fig.add_subplot(gs[:, 2])\nax.plot(*partitions[0][-1].to_numpy().T, label=\"Actual\")\nax.plot(*bv_preds_folds[0].T, label=\"Predicted\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_title(\"Predicted vs Actual Trajectory\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n</pre> predicted_x = bv_preds_folds[0][:, 0] predicted_y = bv_preds_folds[0][:, 1] true_x = test_trajectory['x'].values true_y = test_trajectory['y'].values  # Plotting figsize=(15, 5)  fig = plt.figure(figsize=figsize, constrained_layout=True) nrows, ncols = 2, 3 gs = mpl.gridspec.GridSpec(*(nrows, ncols), figure=fig)#, height_ratios=[.5] * nrows, width_ratios=[.1]*ncols)  ax = fig.add_subplot(gs[0, 0:2]) # X-coordinate plot ax.plot(true_x, label=\"Actual X-Position\") ax.plot(predicted_x, label=\"Predicted X-Position\") ax.set_ylabel(\"X-Coordinate\") ax.set_title(\"Decoding Performance (X-Coordinate)\") ax.legend()  ax = fig.add_subplot(gs[1, 0:2]) # Y-coordinate plot ax.plot(true_y, label=\"Actual Y-Position\") ax.plot(predicted_y, label=\"Predicted Y-Position\") ax.set_xlabel(\"Time Step\") ax.set_ylabel(\"Y-Coordinate\") ax.set_title(\"Decoding Performance (Y-Coordinate)\") ax.legend()   ax = fig.add_subplot(gs[:, 2]) ax.plot(*partitions[0][-1].to_numpy().T, label=\"Actual\") ax.plot(*bv_preds_folds[0].T, label=\"Predicted\") ax.set_xlabel(\"x\") ax.set_ylabel(\"y\") ax.set_title(\"Predicted vs Actual Trajectory\") ax.legend()  plt.tight_layout() plt.show() <pre>/tmp/ipykernel_1782887/1205989260.py:39: UserWarning: The figure layout has changed to tight\n</pre> <p>The results of decoding using DNNs are not that greater than the linear and GP decoders. This is because the data is cleanly simulated and the DNN is overkill for this task. However, in real data, the DNN decoder will likely outperform the linear and GP decoders.</p> In\u00a0[16]: Copied! <pre>environment, reward_indices = create_cheeseboard_maze()\n\nagent = rbx.Agent(environment)\ninit_pos = np.asarray([0, -.48])\nprint(f\"Agent initialized in Cheeseboard Maze: {agent.pos}\")\n\nN_NEURONS = 50\nN_TRIALS = 100\nplacecells = PlaceCells(\n    agent,\n    params=dict(\n        description=\"gaussian_threshold\", widths=0.3, n=N_NEURONS, color=\"C1\",\n        wall_geometry='line_of_sight'\n    ),\n)\n\ncheckpts_coords = [environment.objects['objects'][reward_idx] for reward_idx in reward_indices]\ncheckpts_coords = [init_pos] + checkpts_coords + [init_pos]\n\ntrial_epochs = []\ntrial_epoch = []\nfor n in range(N_TRIALS):\n    for goal_coords in checkpts_coords:\n        # simulate the agent in the maze\n        while True:\n            dir_to_goal = goal_coords - agent.pos\n            drift_vel = (\n                3 * agent.speed_mean * dir_to_goal / np.linalg.norm(dir_to_goal)\n            )\n            agent.update(drift_velocity=drift_vel)\n            placecells.update()\n\n            # if animal is close to reward, break\n            if np.linalg.norm(dir_to_goal) &lt; 0.01:\n                if np.all(goal_coords == init_pos):\n                    trial_epoch.append(agent.t)\n                    if len(trial_epoch) == 2:\n                        trial_epochs.append(trial_epoch)\n                        trial_epoch = []\n                break\n\nagent.plot_trajectory(trial_epochs[0][0], trial_epochs[-1][1], linewidth=1)\n# plot rewards\nfor i in reward_indices:\n    plt.plot(*environment.objects['objects'][i], 'ko')\n</pre> environment, reward_indices = create_cheeseboard_maze()  agent = rbx.Agent(environment) init_pos = np.asarray([0, -.48]) print(f\"Agent initialized in Cheeseboard Maze: {agent.pos}\")  N_NEURONS = 50 N_TRIALS = 100 placecells = PlaceCells(     agent,     params=dict(         description=\"gaussian_threshold\", widths=0.3, n=N_NEURONS, color=\"C1\",         wall_geometry='line_of_sight'     ), )  checkpts_coords = [environment.objects['objects'][reward_idx] for reward_idx in reward_indices] checkpts_coords = [init_pos] + checkpts_coords + [init_pos]  trial_epochs = [] trial_epoch = [] for n in range(N_TRIALS):     for goal_coords in checkpts_coords:         # simulate the agent in the maze         while True:             dir_to_goal = goal_coords - agent.pos             drift_vel = (                 3 * agent.speed_mean * dir_to_goal / np.linalg.norm(dir_to_goal)             )             agent.update(drift_velocity=drift_vel)             placecells.update()              # if animal is close to reward, break             if np.linalg.norm(dir_to_goal) &lt; 0.01:                 if np.all(goal_coords == init_pos):                     trial_epoch.append(agent.t)                     if len(trial_epoch) == 2:                         trial_epochs.append(trial_epoch)                         trial_epoch = []                 break  agent.plot_trajectory(trial_epochs[0][0], trial_epochs[-1][1], linewidth=1) # plot rewards for i in reward_indices:     plt.plot(*environment.objects['objects'][i], 'ko') <pre>Agent initialized in Cheeseboard Maze: [-0.24221668  0.15734778]\n</pre> In\u00a0[17]: Copied! <pre>nsv_trials = []\nbv_trials = []\nfor t_start, t_end in trial_epochs:    \n    t = np.array(placecells.history[\"t\"])\n    i_start = (0 if t_start is None else np.argmin(np.abs(t - t_start)))\n    i_end = (-1 if t_end is None else np.argmin(np.abs(t - t_end)))           \n    t = t[i_start:i_end]  # subsample data for training (most of it is redundant anyway)\n    neural_data = np.array(placecells.history[\"firingrate\"])[i_start:i_end]\n    # Inject some noise to the neural data\n    noise = np.random.normal(0, 0.1, neural_data.shape)\n    neural_data += noise\n    nsv_trials.append(pd.DataFrame(neural_data))\n    trajectory = np.array(placecells.Agent.history[\"pos\"])[i_start:i_end]\n    bv_trials.append(pd.DataFrame(trajectory))\nnsv_trials = np.asarray(nsv_trials, dtype=object)\nbv_trials = np.asarray(bv_trials, dtype=object)\n</pre> nsv_trials = [] bv_trials = [] for t_start, t_end in trial_epochs:         t = np.array(placecells.history[\"t\"])     i_start = (0 if t_start is None else np.argmin(np.abs(t - t_start)))     i_end = (-1 if t_end is None else np.argmin(np.abs(t - t_end)))                t = t[i_start:i_end]  # subsample data for training (most of it is redundant anyway)     neural_data = np.array(placecells.history[\"firingrate\"])[i_start:i_end]     # Inject some noise to the neural data     noise = np.random.normal(0, 0.1, neural_data.shape)     neural_data += noise     nsv_trials.append(pd.DataFrame(neural_data))     trajectory = np.array(placecells.Agent.history[\"pos\"])[i_start:i_end]     bv_trials.append(pd.DataFrame(trajectory)) nsv_trials = np.asarray(nsv_trials, dtype=object) bv_trials = np.asarray(bv_trials, dtype=object) In\u00a0[18]: Copied! <pre>def visualize_trial(t=0):\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    ax[0].imshow(nsv_trials[t].to_numpy().T, aspect=\"auto\", cmap=\"inferno\")\n    ax[0].set_xlabel(\"Time (s)\")\n    ax[0].set_ylabel(\"Neuron\")\n    ax[0].set_title(\"Firing Rates of Cells\")\n\n    environment.plot_environment(fig, ax=ax[1])\n    ax[1].plot(*bv_trials[t].to_numpy().T, label=\"Position\")\n    # plot rewards\n    ax[1].plot(*environment.objects['objects'][reward_indices].T, 'ko', label=\"Reward\")\n    ax[1].set_xlabel(\"x\")\n    ax[1].set_xlabel(\"y\")\n    ax[1].set_title(\"Trajectory\")\n    # plot first 2 legend only\n    ax[1].legend()\n    plt.show()\n\nwidgets.interact(visualize_trial, t=(0, len(nsv_trials) - 1, 1));\n</pre> def visualize_trial(t=0):     fig, ax = plt.subplots(1, 2, figsize=(10, 5))     ax[0].imshow(nsv_trials[t].to_numpy().T, aspect=\"auto\", cmap=\"inferno\")     ax[0].set_xlabel(\"Time (s)\")     ax[0].set_ylabel(\"Neuron\")     ax[0].set_title(\"Firing Rates of Cells\")      environment.plot_environment(fig, ax=ax[1])     ax[1].plot(*bv_trials[t].to_numpy().T, label=\"Position\")     # plot rewards     ax[1].plot(*environment.objects['objects'][reward_indices].T, 'ko', label=\"Reward\")     ax[1].set_xlabel(\"x\")     ax[1].set_xlabel(\"y\")     ax[1].set_title(\"Trajectory\")     # plot first 2 legend only     ax[1].legend()     plt.show()  widgets.interact(visualize_trial, t=(0, len(nsv_trials) - 1, 1)); <pre>interactive(children=(IntSlider(value=0, description='t', max=99), Output()), _dom_classes=('widget-interact',\u2026</pre> In\u00a0[19]: Copied! <pre>fold_indices = npy.ensemble.decoding.split_data(\n    nsv_trials, np.random.randint(0, 2, size=len(nsv_trials)), 0.6)\npartition_indices = npy.ensemble.decoding.partition_indices(fold_indices)\npartitions = npy.ensemble.decoding.partition_sets(\n    partition_indices, nsv_trials, bv_trials)\n</pre> fold_indices = npy.ensemble.decoding.split_data(     nsv_trials, np.random.randint(0, 2, size=len(nsv_trials)), 0.6) partition_indices = npy.ensemble.decoding.partition_indices(fold_indices) partitions = npy.ensemble.decoding.partition_sets(     partition_indices, nsv_trials, bv_trials) In\u00a0[20]: Copied! <pre>bv_test_pred_ridge = []\nfor nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test in partitions:\n    ridge = sklearn.linear_model.Ridge(alpha=0.01)\n    ridge.fit(np.concatenate(nsv_train), np.concatenate(bv_train))\n    bv_test_pred = ridge.predict(np.concatenate(nsv_test))\n    bv_test_pred_ridge.append(bv_test_pred)\n\nsklearn.metrics.r2_score(\n    np.concatenate([np.concatenate(p[-1]) for p in partitions]),\n    np.concatenate(bv_test_pred_ridge)\n)\n</pre> bv_test_pred_ridge = [] for nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test in partitions:     ridge = sklearn.linear_model.Ridge(alpha=0.01)     ridge.fit(np.concatenate(nsv_train), np.concatenate(bv_train))     bv_test_pred = ridge.predict(np.concatenate(nsv_test))     bv_test_pred_ridge.append(bv_test_pred)  sklearn.metrics.r2_score(     np.concatenate([np.concatenate(p[-1]) for p in partitions]),     np.concatenate(bv_test_pred_ridge) ) Out[20]: <pre>0.9649046060749725</pre> In\u00a0[21]: Copied! <pre>decoder_type = 'MLP'  # Select decoder type (e.g., MLP)\nhyperparams = dict(\n    batch_size=512*4,\n    num_workers=5,\n    model=decoder_type,\n    model_args=dict(\n        in_dim=nsv_trials[0].shape[1],\n        out_dim=bv_trials[0].shape[1],\n        hidden_dims=[64, 32, 16, 8],\n        args=dict(\n            clf=False,\n            activations=nn.CELU,\n            criterion=F.mse_loss,\n            epochs=10,\n            lr=3e-2,\n            base_lr=1e-2,\n            max_grad_norm=1.,\n            iters_to_accumulate=1,\n            weight_decay=1e-2,\n            num_training_batches=None,\n            scheduler_step_size_multiplier=1,\n        )\n    ),\n    behaviors=[0, 1],\n    bins_before=0,\n    bins_current=1,\n    bins_after=0,\n    device=DEVICE,\n    seed=SEED\n)\n</pre> decoder_type = 'MLP'  # Select decoder type (e.g., MLP) hyperparams = dict(     batch_size=512*4,     num_workers=5,     model=decoder_type,     model_args=dict(         in_dim=nsv_trials[0].shape[1],         out_dim=bv_trials[0].shape[1],         hidden_dims=[64, 32, 16, 8],         args=dict(             clf=False,             activations=nn.CELU,             criterion=F.mse_loss,             epochs=10,             lr=3e-2,             base_lr=1e-2,             max_grad_norm=1.,             iters_to_accumulate=1,             weight_decay=1e-2,             num_training_batches=None,             scheduler_step_size_multiplier=1,         )     ),     behaviors=[0, 1],     bins_before=0,     bins_current=1,     bins_after=0,     device=DEVICE,     seed=SEED ) In\u00a0[22]: Copied! <pre># Train the model using the pipeline function from your decoder module\nresults_path = \"results\"\nbv_preds_folds, bv_models_folds, norm_params_folds, metrics_folds = npy.ensemble.decoding.train_model(\n    partitions, hyperparams, resultspath=results_path)\n</pre> # Train the model using the pipeline function from your decoder module results_path = \"results\" bv_preds_folds, bv_models_folds, norm_params_folds, metrics_folds = npy.ensemble.decoding.train_model(     partitions, hyperparams, resultspath=results_path)  <pre>Seed set to 2025\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | main | Sequential | 6.0 K  | train\n--------------------------------------------\n6.0 K     Trainable params\n0         Non-trainable params\n6.0 K     Total params\n0.024     Total estimated model params size (MB)\n10        Modules in train mode\n0         Modules in eval mode\n`Trainer.fit` stopped: `max_epochs=50` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_loss         \u2502   0.0005353732849471271   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Seed set to 2025\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/cornell/Desktop/neuro_py/tutorials/results/models/2642124120 exists and is not empty.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | main | Sequential | 6.0 K  | train\n--------------------------------------------\n6.0 K     Trainable params\n0         Non-trainable params\n6.0 K     Total params\n0.024     Total estimated model params size (MB)\n10        Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Variance weighed avg. coefficient of determination: 0.9880966030454236\nRMSE: 0.023135812839817575\n</pre> <pre>`Trainer.fit` stopped: `max_epochs=50` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_loss         \u2502   0.000487027718918398    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Seed set to 2025\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/cornell/Desktop/neuro_py/tutorials/results/models/2642124120 exists and is not empty.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | main | Sequential | 6.0 K  | train\n--------------------------------------------\n6.0 K     Trainable params\n0         Non-trainable params\n6.0 K     Total params\n0.024     Total estimated model params size (MB)\n10        Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Variance weighed avg. coefficient of determination: 0.9887515784011035\nRMSE: 0.022055599437938077\n</pre> <pre>`Trainer.fit` stopped: `max_epochs=50` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_loss         \u2502   0.0005021715769544244   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Seed set to 2025\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/cornell/Desktop/neuro_py/tutorials/results/models/2642124120 exists and is not empty.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | main | Sequential | 6.0 K  | train\n--------------------------------------------\n6.0 K     Trainable params\n0         Non-trainable params\n6.0 K     Total params\n0.024     Total estimated model params size (MB)\n10        Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Variance weighed avg. coefficient of determination: 0.9879894805486882\nRMSE: 0.02239556298226532\n</pre> <pre>`Trainer.fit` stopped: `max_epochs=50` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_loss         \u2502   0.0005372166051529348   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Seed set to 2025\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/cornell/Desktop/neuro_py/tutorials/results/models/2642124120 exists and is not empty.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | main | Sequential | 6.0 K  | train\n--------------------------------------------\n6.0 K     Trainable params\n0         Non-trainable params\n6.0 K     Total params\n0.024     Total estimated model params size (MB)\n10        Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Variance weighed avg. coefficient of determination: 0.987543710812684\nRMSE: 0.02316057034376892\n</pre> <pre>`Trainer.fit` stopped: `max_epochs=50` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_loss         \u2502   0.0006134471041150391   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Variance weighed avg. coefficient of determination: 0.9855258951120814\nRMSE: 0.024758977393035616\n</pre> In\u00a0[23]: Copied! <pre>bv_preds_dnn_trials = np.concatenate([\n    bv_preds_fold + norm_params_folds[i]['y_train_mean']\n    for i, bv_preds_fold in enumerate(bv_preds_folds)\n])\nbv_preds_ridge_trials = np.concatenate(bv_test_pred_ridge)\nbv_actual_trials = np.concatenate([np.concatenate(p[-1]) for p in partitions])\ntrial_lengths = [len(trial) for p in partitions for trial in p[-1]]\ntrial_indices = np.cumsum(trial_lengths)\n\nbv_preds_dnn_trials = np.split(bv_preds_dnn_trials, trial_indices[:-1])\nbv_preds_ridge_trials = np.split(bv_preds_ridge_trials, trial_indices[:-1])\nbv_actual_trials = np.split(bv_actual_trials, trial_indices[:-1])\n\ndef visualize_predicted_trial(trial=0):\n    # Plotting\n    figsize=(15, 5)\n\n    fig = plt.figure(figsize=figsize)\n    nrows, ncols = 2, 4\n    gs = mpl.gridspec.GridSpec(*(nrows, ncols), figure=fig)#, height_ratios=[.5] * nrows, width_ratios=[.1]*ncols)\n\n    ax = fig.add_subplot(gs[0, 0:2])\n    # X-coordinate plot\n    ax.plot(bv_actual_trials[trial][:, 0], label=\"Actual X-Position\")\n    ax.plot(bv_preds_ridge_trials[trial][:, 0], label=\"Ridge Predicted X-Position\")\n    ax.plot(bv_preds_dnn_trials[trial][:, 0], label=f\"{decoder_type} Predicted X-Position\")\n    ax.set_ylabel(\"X-Coordinate\")\n    ax.set_title(\"Decoding Performance (X-Coordinate)\")\n    ax.legend()\n\n    ax = fig.add_subplot(gs[1, 0:2])\n    # Y-coordinate plot\n    ax.plot(bv_actual_trials[trial][:, 1], label=\"Actual Y-Position\")\n    ax.plot(bv_preds_ridge_trials[trial][:, 1], label=\"Ridge Predicted Y-Position\")\n    ax.plot(bv_preds_dnn_trials[trial][:, 1], label=f\"{decoder_type} Predicted Y-Position\")\n    ax.set_xlabel(\"Time Step\")\n    ax.set_ylabel(\"Y-Coordinate\")\n    ax.set_title(\"Decoding Performance (Y-Coordinate)\")\n    ax.legend()\n\n    ax = fig.add_subplot(gs[:, 2:])\n    environment.plot_environment(fig, ax=ax)\n    ax.plot(*bv_actual_trials[trial].T, label=\"Actual\")\n    ax.plot(*bv_preds_ridge_trials[trial].T, label=\"Ridge Predicted\")\n    ax.plot(*bv_preds_dnn_trials[trial].T, label=f\"{decoder_type} Predicted\")\n    # plot rewards\n    ax.plot(*environment.objects['objects'][reward_indices].T, 'ko', label=\"Reward\")\n    ax.set_title(\"Trajectory\")\n    # plot first 2 legend only\n    ax.legend()\n    plt.show()\n\nwidgets.interact(visualize_predicted_trial, trial=(0, len(nsv_trials) - 1, 1));\n</pre> bv_preds_dnn_trials = np.concatenate([     bv_preds_fold + norm_params_folds[i]['y_train_mean']     for i, bv_preds_fold in enumerate(bv_preds_folds) ]) bv_preds_ridge_trials = np.concatenate(bv_test_pred_ridge) bv_actual_trials = np.concatenate([np.concatenate(p[-1]) for p in partitions]) trial_lengths = [len(trial) for p in partitions for trial in p[-1]] trial_indices = np.cumsum(trial_lengths)  bv_preds_dnn_trials = np.split(bv_preds_dnn_trials, trial_indices[:-1]) bv_preds_ridge_trials = np.split(bv_preds_ridge_trials, trial_indices[:-1]) bv_actual_trials = np.split(bv_actual_trials, trial_indices[:-1])  def visualize_predicted_trial(trial=0):     # Plotting     figsize=(15, 5)      fig = plt.figure(figsize=figsize)     nrows, ncols = 2, 4     gs = mpl.gridspec.GridSpec(*(nrows, ncols), figure=fig)#, height_ratios=[.5] * nrows, width_ratios=[.1]*ncols)      ax = fig.add_subplot(gs[0, 0:2])     # X-coordinate plot     ax.plot(bv_actual_trials[trial][:, 0], label=\"Actual X-Position\")     ax.plot(bv_preds_ridge_trials[trial][:, 0], label=\"Ridge Predicted X-Position\")     ax.plot(bv_preds_dnn_trials[trial][:, 0], label=f\"{decoder_type} Predicted X-Position\")     ax.set_ylabel(\"X-Coordinate\")     ax.set_title(\"Decoding Performance (X-Coordinate)\")     ax.legend()      ax = fig.add_subplot(gs[1, 0:2])     # Y-coordinate plot     ax.plot(bv_actual_trials[trial][:, 1], label=\"Actual Y-Position\")     ax.plot(bv_preds_ridge_trials[trial][:, 1], label=\"Ridge Predicted Y-Position\")     ax.plot(bv_preds_dnn_trials[trial][:, 1], label=f\"{decoder_type} Predicted Y-Position\")     ax.set_xlabel(\"Time Step\")     ax.set_ylabel(\"Y-Coordinate\")     ax.set_title(\"Decoding Performance (Y-Coordinate)\")     ax.legend()      ax = fig.add_subplot(gs[:, 2:])     environment.plot_environment(fig, ax=ax)     ax.plot(*bv_actual_trials[trial].T, label=\"Actual\")     ax.plot(*bv_preds_ridge_trials[trial].T, label=\"Ridge Predicted\")     ax.plot(*bv_preds_dnn_trials[trial].T, label=f\"{decoder_type} Predicted\")     # plot rewards     ax.plot(*environment.objects['objects'][reward_indices].T, 'ko', label=\"Reward\")     ax.set_title(\"Trajectory\")     # plot first 2 legend only     ax.legend()     plt.show()  widgets.interact(visualize_predicted_trial, trial=(0, len(nsv_trials) - 1, 1)); <pre>interactive(children=(IntSlider(value=0, description='trial', max=99), Output()), _dom_classes=('widget-intera\u2026</pre> In\u00a0[24]: Copied! <pre>BIN_SIZE = 0.05\n</pre> BIN_SIZE = 0.05 In\u00a0[25]: Copied! <pre>basepath = r'/run/user/1000/gvfs/smb-share:server=132.236.112.212,share=ayadata1/Data/GrosmarkAD/Achilles/Achilles_10252013'\n\nepoch_df = npy.io.load_epoch(basepath)\n# get session bounds to provide support\nsession_bounds = nel.EpochArray(\n    [epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]]\n)\n# compress repeated sleep sessions\nepoch_df = npy.session.compress_repeated_epochs(epoch_df)\nbeh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values.astype(float))\n\nst, cell_metrics = npy.io.load_spikes(\n    basepath, putativeCellType=\"Pyr\", brainRegion=\"CA1\"\n)\nspike_spindices = npy.spikes.get_spindices(st.data)\n\nswr = npy.io.load_ripples_events(basepath, return_epoch_array=True)\n\ntheta = nel.EpochArray(npy.io.load_SleepState_states(basepath)[\"THETA\"])\n\ntask_idx = npy.process.in_intervals(\n    spike_spindices.spike_times, (beh_epochs[1] &amp; theta).data\n)\n</pre> basepath = r'/run/user/1000/gvfs/smb-share:server=132.236.112.212,share=ayadata1/Data/GrosmarkAD/Achilles/Achilles_10252013'  epoch_df = npy.io.load_epoch(basepath) # get session bounds to provide support session_bounds = nel.EpochArray(     [epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]] ) # compress repeated sleep sessions epoch_df = npy.session.compress_repeated_epochs(epoch_df) beh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values.astype(float))  st, cell_metrics = npy.io.load_spikes(     basepath, putativeCellType=\"Pyr\", brainRegion=\"CA1\" ) spike_spindices = npy.spikes.get_spindices(st.data)  swr = npy.io.load_ripples_events(basepath, return_epoch_array=True)  theta = nel.EpochArray(npy.io.load_SleepState_states(basepath)[\"THETA\"])  task_idx = npy.process.in_intervals(     spike_spindices.spike_times, (beh_epochs[1] &amp; theta).data ) In\u00a0[26]: Copied! <pre>position_df = npy.io.load_animal_behavior(basepath)\n\n# put position into a nelpy position array for ease of use\nposx = nel.AnalogSignalArray(\n    data=position_df[\"x\"].values.T,\n    timestamps=position_df.timestamps.values,\n)\nposy = nel.AnalogSignalArray(\n    data=position_df[\"y\"].values.T,\n    timestamps=position_df.timestamps.values,\n)\n\n# get outbound and inbound epochs\n(outbound_epochs, inbound_epochs) = npy.behavior.get_linear_track_lap_epochs(\n    posx.abscissa_vals, posx.data[0], newLapThreshold=20\n)\n\noutbound_epochs, inbound_epochs\n</pre> position_df = npy.io.load_animal_behavior(basepath)  # put position into a nelpy position array for ease of use posx = nel.AnalogSignalArray(     data=position_df[\"x\"].values.T,     timestamps=position_df.timestamps.values, ) posy = nel.AnalogSignalArray(     data=position_df[\"y\"].values.T,     timestamps=position_df.timestamps.values, )  # get outbound and inbound epochs (outbound_epochs, inbound_epochs) = npy.behavior.get_linear_track_lap_epochs(     posx.abscissa_vals, posx.data[0], newLapThreshold=20 )  outbound_epochs, inbound_epochs Out[26]: <pre>(&lt;EpochArray at 0x789e1bba5fa0: 42 epochs&gt; of length 17:07:964 minutes,\n &lt;EpochArray at 0x789e1bba5160: 43 epochs&gt; of length 17:17:974 minutes)</pre> In\u00a0[27]: Copied! <pre>bst_trials_inbound = [st[i].bin(ds=BIN_SIZE).smooth(sigma=BIN_SIZE, inplace=True) for i in outbound_epochs]\nbst_trials_outbound = [st[i].bin(ds=BIN_SIZE).smooth(sigma=BIN_SIZE, inplace=True) for i in inbound_epochs]\nbst_trials = bst_trials_inbound + bst_trials_outbound\nnsv_trials = np.asarray([pd.DataFrame(i.data.T) for i in bst_trials], dtype=object)\n</pre> bst_trials_inbound = [st[i].bin(ds=BIN_SIZE).smooth(sigma=BIN_SIZE, inplace=True) for i in outbound_epochs] bst_trials_outbound = [st[i].bin(ds=BIN_SIZE).smooth(sigma=BIN_SIZE, inplace=True) for i in inbound_epochs] bst_trials = bst_trials_inbound + bst_trials_outbound nsv_trials = np.asarray([pd.DataFrame(i.data.T) for i in bst_trials], dtype=object) In\u00a0[28]: Copied! <pre>pos_trials_with_missing = [\n    np.vstack((\n        posx.asarray(at=bst.bin_centers).yvals[0],\n        posy.asarray(at=bst.bin_centers).yvals[0]\n    ))\n    for bst in bst_trials\n]\n\npos_trials = []\nfor p in pos_trials_with_missing:\n    p = pd.DataFrame(p.T)\n\n    # find max contiguous sequence of NaNs\n    max_len_nan = 0\n    max_nan = 0\n    for i in range(len(p)):\n        if pd.isna(p.iloc[i, 0]):\n            max_nan += 1\n        else:\n            max_len_nan = max(max_len_nan, max_nan)\n            max_nan = 0\n    max_len_nan += 1\n\n    # fill missing values mean of surrounding values\n    p = p.fillna(p.rolling(max_len_nan, min_periods=1).mean())\n\n    # fill left edge nan values with first non-nan value\n    p = p.ffill().bfill()\n    pos_trials.append(p.values)\npos_trials = np.asarray(pos_trials, dtype=object)\n</pre> pos_trials_with_missing = [     np.vstack((         posx.asarray(at=bst.bin_centers).yvals[0],         posy.asarray(at=bst.bin_centers).yvals[0]     ))     for bst in bst_trials ]  pos_trials = [] for p in pos_trials_with_missing:     p = pd.DataFrame(p.T)      # find max contiguous sequence of NaNs     max_len_nan = 0     max_nan = 0     for i in range(len(p)):         if pd.isna(p.iloc[i, 0]):             max_nan += 1         else:             max_len_nan = max(max_len_nan, max_nan)             max_nan = 0     max_len_nan += 1      # fill missing values mean of surrounding values     p = p.fillna(p.rolling(max_len_nan, min_periods=1).mean())      # fill left edge nan values with first non-nan value     p = p.ffill().bfill()     pos_trials.append(p.values) pos_trials = np.asarray(pos_trials, dtype=object)  In\u00a0[29]: Copied! <pre># Format partitions for the decoder pipeline\nfold_indices = npy.ensemble.decoding.split_data(\n    nsv_trials, np.random.randint(0, 2, size=len(nsv_trials)), 0.6)\npartition_indices = npy.ensemble.decoding.partition_indices(fold_indices)\npartitions = npy.ensemble.decoding.partition_sets(\n    partition_indices, nsv_trials, pos_trials)\n</pre> # Format partitions for the decoder pipeline fold_indices = npy.ensemble.decoding.split_data(     nsv_trials, np.random.randint(0, 2, size=len(nsv_trials)), 0.6) partition_indices = npy.ensemble.decoding.partition_indices(fold_indices) partitions = npy.ensemble.decoding.partition_sets(     partition_indices, nsv_trials, pos_trials) In\u00a0[30]: Copied! <pre>bv_test_pred_ridge = []\nfor nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test in partitions:\n    ridge = sklearn.linear_model.Ridge(alpha=0.01)\n    ridge.fit(np.concatenate(nsv_train), np.concatenate(bv_train))\n    bv_test_pred = ridge.predict(np.concatenate(nsv_test))\n    bv_test_pred_ridge.append(bv_test_pred)\n\nsklearn.metrics.r2_score(\n    np.concatenate([np.concatenate(p[-1]) for p in partitions]),\n    np.concatenate(bv_test_pred_ridge)\n)\n</pre> bv_test_pred_ridge = [] for nsv_train, bv_train, nsv_val, bv_val, nsv_test, bv_test in partitions:     ridge = sklearn.linear_model.Ridge(alpha=0.01)     ridge.fit(np.concatenate(nsv_train), np.concatenate(bv_train))     bv_test_pred = ridge.predict(np.concatenate(nsv_test))     bv_test_pred_ridge.append(bv_test_pred)  sklearn.metrics.r2_score(     np.concatenate([np.concatenate(p[-1]) for p in partitions]),     np.concatenate(bv_test_pred_ridge) ) Out[30]: <pre>0.4274710535820969</pre> In\u00a0[31]: Copied! <pre>decoder_type = 'MLP'  # Select decoder type (e.g., MLP)\nhyperparams = dict(\n    batch_size=512*8,\n    num_workers=5,\n    model=decoder_type,\n    model_args=dict(\n        in_dim=nsv_trials[0].shape[1],\n        out_dim=pos_trials[0].shape[1],\n        hidden_dims=[256, 256, .15, 256],\n        args=dict(\n            clf=False,\n            activations=nn.CELU,\n            criterion=F.mse_loss,\n            epochs=40,\n            lr=3e-2,\n            base_lr=1e-2,\n            max_grad_norm=1.,\n            iters_to_accumulate=1,\n            weight_decay=1e-2,\n            num_training_batches=None,\n            scheduler_step_size_multiplier=1,\n        )\n    ),\n    behaviors=[0, 1],\n    bins_before=1,\n    bins_current=1,\n    bins_after=0,\n    device=DEVICE,\n    seed=SEED\n)\n</pre> decoder_type = 'MLP'  # Select decoder type (e.g., MLP) hyperparams = dict(     batch_size=512*8,     num_workers=5,     model=decoder_type,     model_args=dict(         in_dim=nsv_trials[0].shape[1],         out_dim=pos_trials[0].shape[1],         hidden_dims=[256, 256, .15, 256],         args=dict(             clf=False,             activations=nn.CELU,             criterion=F.mse_loss,             epochs=40,             lr=3e-2,             base_lr=1e-2,             max_grad_norm=1.,             iters_to_accumulate=1,             weight_decay=1e-2,             num_training_batches=None,             scheduler_step_size_multiplier=1,         )     ),     behaviors=[0, 1],     bins_before=1,     bins_current=1,     bins_after=0,     device=DEVICE,     seed=SEED ) In\u00a0[32]: Copied! <pre># Train the model using the pipeline function from your decoder module\nresults_path = \"results\"\nbv_preds_folds, bv_models_folds, norm_params_folds, metrics_folds = npy.ensemble.decoding.train_model(\n    partitions, hyperparams, resultspath=results_path)\n</pre> # Train the model using the pipeline function from your decoder module results_path = \"results\" bv_preds_folds, bv_models_folds, norm_params_folds, metrics_folds = npy.ensemble.decoding.train_model(     partitions, hyperparams, resultspath=results_path)  <pre>Seed set to 2025\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | main | Sequential | 255 K  | train\n--------------------------------------------\n255 K     Trainable params\n0         Non-trainable params\n255 K     Total params\n1.023     Total estimated model params size (MB)\n9         Modules in train mode\n0         Modules in eval mode\n`Trainer.fit` stopped: `max_epochs=40` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_loss         \u2502     861.2627563476562     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Seed set to 2025\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/cornell/Desktop/neuro_py/tutorials/results/models/1990776941 exists and is not empty.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | main | Sequential | 255 K  | train\n--------------------------------------------\n255 K     Trainable params\n0         Non-trainable params\n255 K     Total params\n1.023     Total estimated model params size (MB)\n9         Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Variance weighed avg. coefficient of determination: 0.6773325030958686\nRMSE: 24.213068433497135\n</pre> <pre>`Trainer.fit` stopped: `max_epochs=40` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_loss         \u2502      1386.150390625       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Seed set to 2025\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/cornell/Desktop/neuro_py/tutorials/results/models/1990776941 exists and is not empty.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | main | Sequential | 255 K  | train\n--------------------------------------------\n255 K     Trainable params\n0         Non-trainable params\n255 K     Total params\n1.023     Total estimated model params size (MB)\n9         Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Variance weighed avg. coefficient of determination: 0.46727911396418176\nRMSE: 32.120959673056355\n</pre> <pre>`Trainer.fit` stopped: `max_epochs=40` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_loss         \u2502      1221.5458984375      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Seed set to 2025\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/cornell/Desktop/neuro_py/tutorials/results/models/1990776941 exists and is not empty.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | main | Sequential | 255 K  | train\n--------------------------------------------\n255 K     Trainable params\n0         Non-trainable params\n255 K     Total params\n1.023     Total estimated model params size (MB)\n9         Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Variance weighed avg. coefficient of determination: 0.6286478191387428\nRMSE: 28.07934763958197\n</pre> <pre>`Trainer.fit` stopped: `max_epochs=40` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_loss         \u2502     650.6585693359375     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Seed set to 2025\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/cornell/Desktop/neuro_py/tutorials/results/models/1990776941 exists and is not empty.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/home/cornell/.conda/envs/pfc/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n\n  | Name | Type       | Params | Mode \n--------------------------------------------\n0 | main | Sequential | 255 K  | train\n--------------------------------------------\n255 K     Trainable params\n0         Non-trainable params\n255 K     Total params\n1.023     Total estimated model params size (MB)\n9         Modules in train mode\n0         Modules in eval mode\n</pre> <pre>Variance weighed avg. coefficient of determination: 0.7983677020380157\nRMSE: 21.3463354244628\n</pre> <pre>`Trainer.fit` stopped: `max_epochs=40` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503        Test metric        \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502         test_loss         \u2502     786.2012939453125     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre>Variance weighed avg. coefficient of determination: 0.6337113438298102\nRMSE: 23.376468307662268\n</pre> In\u00a0[33]: Copied! <pre>bv_preds_dnn_trials = np.concatenate([\n    bv_preds_fold + norm_params_folds[i]['y_train_mean']\n    for i, bv_preds_fold in enumerate(bv_preds_folds)\n])\nbv_preds_ridge_trials = np.concatenate(bv_test_pred_ridge)\nbv_actual_trials = np.concatenate([np.concatenate(p[-1]) for p in partitions])\ntrial_lengths = [len(trial) for p in partitions for trial in p[-1]]\ntrial_indices = np.cumsum(trial_lengths)\n\nbv_preds_dnn_trials = np.split(bv_preds_dnn_trials, trial_indices[:-1])\nbv_preds_ridge_trials = np.split(bv_preds_ridge_trials, trial_indices[:-1])\nbv_actual_trials = np.split(bv_actual_trials, trial_indices[:-1])\n\ndef visualize_predicted_trial(trial=0):\n    # Plotting\n    figsize=(15, 5)\n\n    fig = plt.figure(figsize=figsize)\n    nrows, ncols = 2, 3\n    gs = mpl.gridspec.GridSpec(*(nrows, ncols), figure=fig)#, height_ratios=[.5] * nrows, width_ratios=[.1]*ncols)\n\n    ax = fig.add_subplot(gs[0, 0:2])\n    # X-coordinate plot\n    ax.plot(bv_actual_trials[trial][:, 0], label=\"Actual X-Position\")\n    ax.plot(bv_preds_ridge_trials[trial][:, 0], label=\"Ridge Predicted X-Position\", alpha=0.75)\n    ax.plot(bv_preds_dnn_trials[trial][:, 0], label=f\"{decoder_type} Predicted X-Position\", alpha=0.75)\n    ax.set_ylabel(\"X-Coordinate\")\n    ax.set_title(\"Decoding Performance (X-Coordinate)\")\n    ax.legend()\n\n    ax = fig.add_subplot(gs[1, 0:2])\n    # Y-coordinate plot\n    ax.plot(bv_actual_trials[trial][:, 1], label=\"Actual Y-Position\")\n    ax.plot(bv_preds_ridge_trials[trial][:, 1], label=\"Ridge Predicted Y-Position\", alpha=0.75)\n    ax.plot(bv_preds_dnn_trials[trial][:, 1], label=f\"{decoder_type} Predicted Y-Position\", alpha=0.75)\n    ax.set_xlabel(\"Time Step\")\n    ax.set_ylabel(\"Y-Coordinate\")\n    ax.set_title(\"Decoding Performance (Y-Coordinate)\")\n    ax.legend()\n\n    ax = fig.add_subplot(gs[:, 2])\n    ax.plot(*bv_actual_trials[trial].T, label=\"Actual\")\n    ax.plot(*bv_preds_ridge_trials[trial].T, label=\"Ridge Predicted\", alpha=0.75)\n    ax.plot(*bv_preds_dnn_trials[trial].T, label=f\"{decoder_type} Predicted\", alpha=0.75)\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_title(\"Trajectory\")\n    ax.legend()\n\n    plt.tight_layout()\n    plt.show()\n\nwidgets.interact(visualize_predicted_trial, trial=(0, len(nsv_trials) - 1, 1));\n</pre> bv_preds_dnn_trials = np.concatenate([     bv_preds_fold + norm_params_folds[i]['y_train_mean']     for i, bv_preds_fold in enumerate(bv_preds_folds) ]) bv_preds_ridge_trials = np.concatenate(bv_test_pred_ridge) bv_actual_trials = np.concatenate([np.concatenate(p[-1]) for p in partitions]) trial_lengths = [len(trial) for p in partitions for trial in p[-1]] trial_indices = np.cumsum(trial_lengths)  bv_preds_dnn_trials = np.split(bv_preds_dnn_trials, trial_indices[:-1]) bv_preds_ridge_trials = np.split(bv_preds_ridge_trials, trial_indices[:-1]) bv_actual_trials = np.split(bv_actual_trials, trial_indices[:-1])  def visualize_predicted_trial(trial=0):     # Plotting     figsize=(15, 5)      fig = plt.figure(figsize=figsize)     nrows, ncols = 2, 3     gs = mpl.gridspec.GridSpec(*(nrows, ncols), figure=fig)#, height_ratios=[.5] * nrows, width_ratios=[.1]*ncols)      ax = fig.add_subplot(gs[0, 0:2])     # X-coordinate plot     ax.plot(bv_actual_trials[trial][:, 0], label=\"Actual X-Position\")     ax.plot(bv_preds_ridge_trials[trial][:, 0], label=\"Ridge Predicted X-Position\", alpha=0.75)     ax.plot(bv_preds_dnn_trials[trial][:, 0], label=f\"{decoder_type} Predicted X-Position\", alpha=0.75)     ax.set_ylabel(\"X-Coordinate\")     ax.set_title(\"Decoding Performance (X-Coordinate)\")     ax.legend()      ax = fig.add_subplot(gs[1, 0:2])     # Y-coordinate plot     ax.plot(bv_actual_trials[trial][:, 1], label=\"Actual Y-Position\")     ax.plot(bv_preds_ridge_trials[trial][:, 1], label=\"Ridge Predicted Y-Position\", alpha=0.75)     ax.plot(bv_preds_dnn_trials[trial][:, 1], label=f\"{decoder_type} Predicted Y-Position\", alpha=0.75)     ax.set_xlabel(\"Time Step\")     ax.set_ylabel(\"Y-Coordinate\")     ax.set_title(\"Decoding Performance (Y-Coordinate)\")     ax.legend()      ax = fig.add_subplot(gs[:, 2])     ax.plot(*bv_actual_trials[trial].T, label=\"Actual\")     ax.plot(*bv_preds_ridge_trials[trial].T, label=\"Ridge Predicted\", alpha=0.75)     ax.plot(*bv_preds_dnn_trials[trial].T, label=f\"{decoder_type} Predicted\", alpha=0.75)     ax.set_xlabel(\"x\")     ax.set_ylabel(\"y\")     ax.set_title(\"Trajectory\")     ax.legend()      plt.tight_layout()     plt.show()  widgets.interact(visualize_predicted_trial, trial=(0, len(nsv_trials) - 1, 1)); <pre>interactive(children=(IntSlider(value=0, description='trial', max=84), Output()), _dom_classes=('widget-intera\u2026</pre>"},{"location":"tutorials/decoding/#neural-decoding","title":"Neural Decoding\u00b6","text":"<p>In this tutorial, we'll explore how to use the Deep Neural Network decoders in <code>decoding</code> submodule to analyze the neural correlates of behavior.</p> <p>We will begin with synthetic data generation, followed by training and evaluating models on real neural data. The goal is to understand how neural activity can be decoded to predict behavioral outcomes.</p>"},{"location":"tutorials/decoding/#setup","title":"Setup\u00b6","text":"<p>First, let's import the necessary libraries and set up our environment.</p>"},{"location":"tutorials/decoding/#imports","title":"Imports\u00b6","text":""},{"location":"tutorials/decoding/#set-device-and-random-seed-for-reproducibility","title":"Set device and random seed for reproducibility\u00b6","text":""},{"location":"tutorials/decoding/#section-1-decode-synthetic-neural-data","title":"Section 1: Decode Synthetic Neural Data\u00b6","text":""},{"location":"tutorials/decoding/#section-11-generate-synthetic-data","title":"Section 1.1: Generate Synthetic Data\u00b6","text":""},{"location":"tutorials/decoding/#section-111-create-environment","title":"Section 1.1.1: Create Environment\u00b6","text":"<p>The following function generates a cheeseboard-inspired maze environment for the agent:</p>"},{"location":"tutorials/decoding/#section-112-create-agent-and-place-cells-encoding-agents-position","title":"Section 1.1.2: Create Agent and Place Cells encoding agent's position\u00b6","text":""},{"location":"tutorials/decoding/#section-12-prepare-data-for-decoding","title":"Section 1.2: Prepare Data for Decoding\u00b6","text":"<p>Extract firing rates from the place cells and the agent's positional trajectory</p>"},{"location":"tutorials/decoding/#section-13-train-linear-and-gaussian-process-decoders","title":"Section 1.3: Train Linear and Gaussian Process Decoders\u00b6","text":"<p>Train simple decoders for baseline comparison with DNN decoders:</p> <p>For best practices, it is important to always compare the performance of the DNN decoders with simpler models to ensure that the additional complexity is justified.</p>"},{"location":"tutorials/decoding/#section-131-linear-decoder-ridge-regression","title":"Section 1.3.1: Linear Decoder (Ridge Regression)\u00b6","text":""},{"location":"tutorials/decoding/#section-132-gaussian-process-gp-decoder","title":"Section 1.3.2: Gaussian Process (GP) Decoder\u00b6","text":""},{"location":"tutorials/decoding/#section-14-train-and-evaluate-the-dnn-decoder","title":"Section 1.4: Train and Evaluate the DNN Decoder\u00b6","text":"<p>Set up the decoder model and hyperparameters. For example, use an MLP decoder.</p> <p>We refer description of key hyperparameters that impact model performance:</p>"},{"location":"tutorials/decoding/#section-15-visualize-results","title":"Section 1.5: Visualize Results\u00b6","text":"<p>Plot predictions versus ground truth to evaluate decoding performance:</p>"},{"location":"tutorials/decoding/#section-2-trial-segmented-decoding","title":"Section 2: Trial Segmented Decoding\u00b6","text":"<p>Before we move on to real neural data, let's first understand how to decode neural data in a trial-segmented manner. This is useful when the neural data is segmented into trials, and we want to decode each trial separately.</p>"},{"location":"tutorials/decoding/#section-21-generate-synthetic-trial-segmented-data","title":"Section 2.1: Generate Synthetic Trial Segmented Data\u00b6","text":""},{"location":"tutorials/decoding/#section-22-prepare-data-for-decoding","title":"Section 2.2: Prepare Data for Decoding\u00b6","text":""},{"location":"tutorials/decoding/#section-23-train-linear-decoder","title":"Section 2.3: Train Linear Decoder\u00b6","text":"<p>Although Gaussian Process decoders can also be used for trial-segmented decoding, it is computationally expensive. Here, we will use only a linear decoder for simplicity.</p>"},{"location":"tutorials/decoding/#section-24-train-and-evaluate-the-dnn-decoder","title":"Section 2.4: Train and Evaluate the DNN Decoder\u00b6","text":""},{"location":"tutorials/decoding/#section-25-visualize-results","title":"Section 2.5: Visualize Results\u00b6","text":""},{"location":"tutorials/decoding/#section-3-decoding-real-data","title":"Section 3: Decoding Real Data\u00b6","text":"<p>Now that we've seen how to decode synthetic data, let's try decoding real data.</p>"},{"location":"tutorials/decoding/#section-31-load-the-data","title":"Section 3.1: Load the data\u00b6","text":""},{"location":"tutorials/decoding/#section-32-prepare-data-for-decoding","title":"Section 3.2: Prepare Data for Decoding\u00b6","text":""},{"location":"tutorials/decoding/#section-33-train-linear-decoder","title":"Section 3.3: Train Linear Decoder\u00b6","text":""},{"location":"tutorials/decoding/#section-34-train-and-evaluate-the-dnn-decoder","title":"Section 3.4: Train and Evaluate the DNN Decoder\u00b6","text":""},{"location":"tutorials/decoding/#section-35-visualize-results","title":"Section 3.5: Visualize Results\u00b6","text":""},{"location":"tutorials/explained_variance/","title":"Explained Variance","text":"In\u00a0[1]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\n\nimport matplotlib.pyplot as plt\nfrom neuro_py.plotting.events import plot_peth_fast\nfrom neuro_py.plotting.figure_helpers import set_plotting_defaults\nimport nelpy as nel\nfrom neuro_py.io import loading\nfrom neuro_py.ensemble.explained_variance import ExplainedVariance\n\nset_plotting_defaults()\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nfig_save_path = r'Z:\\home\\ryanh\\projects\\hpc_ctx\\figures\\panels'\n</pre> %reload_ext autoreload %autoreload 2  import matplotlib.pyplot as plt from neuro_py.plotting.events import plot_peth_fast from neuro_py.plotting.figure_helpers import set_plotting_defaults import nelpy as nel from neuro_py.io import loading from neuro_py.ensemble.explained_variance import ExplainedVariance  set_plotting_defaults() %matplotlib inline %config InlineBackend.figure_format = 'retina' fig_save_path = r'Z:\\home\\ryanh\\projects\\hpc_ctx\\figures\\panels' In\u00a0[2]: Copied! <pre>basepath = r\"U:\\data\\HMC\\HMC1\\day8\"\n\nst, cm = loading.load_spikes(basepath, brainRegion=\"CA1\", putativeCellType=\"Pyr\")\n\nepoch_df = loading.load_epoch(basepath)\nbeh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values)\n\nstate_dict = loading.load_SleepState_states(basepath)\nnrem_epochs = nel.EpochArray(\n    state_dict[\"NREMstate\"],\n)\n\nepoch_df\n</pre> basepath = r\"U:\\data\\HMC\\HMC1\\day8\"  st, cm = loading.load_spikes(basepath, brainRegion=\"CA1\", putativeCellType=\"Pyr\")  epoch_df = loading.load_epoch(basepath) beh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values)  state_dict = loading.load_SleepState_states(basepath) nrem_epochs = nel.EpochArray(     state_dict[\"NREMstate\"], )  epoch_df Out[2]: name startTime stopTime environment behavioralParadigm notes manipulation stimuli basepath 0 preSleep_210411_064951 0.0000 9544.56315 sleep NaN NaN NaN NaN U:\\data\\HMC\\HMC1\\day8 1 maze_210411_095201 9544.5632 11752.80635 linear 1 novel NaN NaN U:\\data\\HMC\\HMC1\\day8 2 postSleep_210411_103522 11752.8064 23817.68955 sleep NaN NaN NaN NaN U:\\data\\HMC\\HMC1\\day8 In\u00a0[3]: Copied! <pre>ev = ExplainedVariance(\n    st=st,\n    template=beh_epochs[1], # task\n    matching=beh_epochs[2], # post sleep\n    control=beh_epochs[0], # pre sleep\n    window=None, # window size to calculate correlations, None is the full epoch\n)\n\nprint(f\"explained variance: {ev.ev[0]}, reverse explained variance: {ev.rev[0]}\")\n</pre> ev = ExplainedVariance(     st=st,     template=beh_epochs[1], # task     matching=beh_epochs[2], # post sleep     control=beh_epochs[0], # pre sleep     window=None, # window size to calculate correlations, None is the full epoch )  print(f\"explained variance: {ev.ev[0]}, reverse explained variance: {ev.rev[0]}\") <pre>WARNING:root:ignoring events outside of eventarray support\n</pre> <pre>explained variance: 0.10214717079493098, reverse explained variance: 0.002093670634183538\n</pre> <p>You can calculate a p-value for the explained variance by comparing the explained variance to the explained variance of shuffled data (shuffled template correlations).</p> <p>NOTE: This only works when returning single explained variance value, not a set of explained variance values over time.</p> In\u00a0[4]: Copied! <pre>pvalue = ev.pvalue()\nprint(f\"pvalue: {pvalue}\")\n</pre> pvalue = ev.pvalue() print(f\"pvalue: {pvalue}\") <pre>pvalue: 0.000999000999000999\n</pre> In\u00a0[5]: Copied! <pre>ev = ExplainedVariance(\n    st=st,\n    template=beh_epochs[1], # task\n    matching=beh_epochs, # entire epoch (uses start,stop)\n    control=beh_epochs[0], # pre sleep\n    window=200, \n)\n\n# conviently plot the explained variance with built in method\nev.plot()\n</pre> ev = ExplainedVariance(     st=st,     template=beh_epochs[1], # task     matching=beh_epochs, # entire epoch (uses start,stop)     control=beh_epochs[0], # pre sleep     window=200,  )  # conviently plot the explained variance with built in method ev.plot()  <pre>WARNING:root:ignoring events outside of eventarray support\n</pre> In\u00a0[6]: Copied! <pre>plt.figure(figsize=(8, 3))\nax =plt.gca()\nplot_peth_fast(ev.partial_corr.T,ts=ev.matching_time,ax=ax,label=\"EV\")\nplot_peth_fast(ev.rev_partial_corr.T,ts=ev.matching_time,ax=ax,color=\"grey\",label=\"REV\")\nplt.axvspan(nrem_epochs.data[0,0],nrem_epochs.data[0,1],color='purple',alpha=0.5,label='NREM')\nfor nrem in nrem_epochs.data:\n    plt.axvspan(nrem[0],nrem[1],color='purple',alpha=0.5)\nplt.axvspan(beh_epochs[1].data[0,0],beh_epochs[1].data[0,1],color='b',alpha=0.25,zorder=-100,label='Task')\nplt.legend()\nplt.show()\n</pre> plt.figure(figsize=(8, 3)) ax =plt.gca() plot_peth_fast(ev.partial_corr.T,ts=ev.matching_time,ax=ax,label=\"EV\") plot_peth_fast(ev.rev_partial_corr.T,ts=ev.matching_time,ax=ax,color=\"grey\",label=\"REV\") plt.axvspan(nrem_epochs.data[0,0],nrem_epochs.data[0,1],color='purple',alpha=0.5,label='NREM') for nrem in nrem_epochs.data:     plt.axvspan(nrem[0],nrem[1],color='purple',alpha=0.5) plt.axvspan(beh_epochs[1].data[0,0],beh_epochs[1].data[0,1],color='b',alpha=0.25,zorder=-100,label='Task') plt.legend() plt.show() In\u00a0[7]: Copied! <pre>nrem_ev_avg = ev.ev_signal[beh_epochs[2] &amp; nrem_epochs].mean()\nnrem_rev_avg = ev.rev_signal[beh_epochs[2] &amp; nrem_epochs].mean()\n\nprint(f\"explained variance: {nrem_ev_avg}, reverse explained variance: {nrem_rev_avg}\")\n</pre> nrem_ev_avg = ev.ev_signal[beh_epochs[2] &amp; nrem_epochs].mean() nrem_rev_avg = ev.rev_signal[beh_epochs[2] &amp; nrem_epochs].mean()  print(f\"explained variance: {nrem_ev_avg}, reverse explained variance: {nrem_rev_avg}\")  <pre>WARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\n</pre> <pre>explained variance: 0.10133556865860369, reverse explained variance: 0.001632476383922529\n</pre>"},{"location":"tutorials/explained_variance/#explained-variance","title":"Explained Variance\u00b6","text":""},{"location":"tutorials/explained_variance/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/explained_variance/#section-1-load-spike-data-session-epochs-and-non-rem-epochs","title":"Section 1: Load spike data, session epochs, and Non-REM epochs\u00b6","text":""},{"location":"tutorials/explained_variance/#section-2-single-output-metrics-explained-and-reverse-explained-variance","title":"Section 2: Single output metrics: Explained and Reverse Explained Variance\u00b6","text":"<p>Here we are calculating explained variance over the entire post-task interval</p>"},{"location":"tutorials/explained_variance/#section-3-time-resolved-explained-variance","title":"Section 3: Time-resolved explained variance\u00b6","text":""},{"location":"tutorials/explained_variance/#section-31-calculate-explained-variance-over-time","title":"Section 3.1: Calculate explained variance over time\u00b6","text":"<p>We can also see the time course of explained variance</p>"},{"location":"tutorials/explained_variance/#section-32-inspect-when-the-explained-variance-is-high-and-low","title":"Section 3.2: Inspect when the explained variance is high and low\u00b6","text":"<p>You can see it is high during the task (by definition it should be) and during NREM sleep</p>"},{"location":"tutorials/explained_variance/#section-33-use-epocharray-to-get-average-explained-variance-in-post-task-nrem-sleep","title":"Section 3.3: Use EpochArray to get average explained variance in post-task NREM sleep\u00b6","text":"<p>The outcome is similar to above when we used the entire post-task epoch</p>"},{"location":"tutorials/neural_geodynamics/","title":"Neural Geodynamics","text":"In\u00a0[1]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\nimport ipywidgets as widgets\nimport numpy as np\nimport matplotlib\nimport matplotlib.gridspec\nimport matplotlib.pyplot as plt\nimport sklearn\n\nimport neuro_py as npy\nimport nelpy as nel\n\nfrom IPython.display import HTML\nfrom matplotlib.animation import FuncAnimation\n</pre> %reload_ext autoreload %autoreload 2 import ipywidgets as widgets import numpy as np import matplotlib import matplotlib.gridspec import matplotlib.pyplot as plt import sklearn  import neuro_py as npy import nelpy as nel  from IPython.display import HTML from matplotlib.animation import FuncAnimation In\u00a0[2]: Copied! <pre>def plot_3d_trajs(ax, x, y, z1, z2, color1='tab:blue', color2='tab:green'):\n    ax.scatter(x, y, z1, color=color1, s=100)\n    ax.plot(x, y, z1, color=color1, alpha=0.25)\n    ax.scatter(x, y, z2, color=color2, s=100)\n    ax.plot(x, y, z2, color=color2, alpha=0.25)\n\n    return ax\n\ndef vis_nss_metrics(x, y, z1, z2, metric='proximity', color1='tab:blue', color2='tab:red', ax=None):\n    if ax is None:\n        fig = plt.figure(figsize=(6, 10))\n        ax = fig.add_subplot(111, projection='3d')\n    plot_3d_trajs(ax, x, y, z1, z2, color1=color1, color2=color2)\n    n_points = len(x)\n    i = n_points // 2\n    if metric == 'proximity':\n        for j in range(n_points):\n            ax.plot([x[i], x[j]], [y[i], y[j]], [z1[i], z2[j]], color='gray', linestyle='dotted')\n        ax.quiver(x[i], y[i], z1[i], x[i]-x[i], y[i]-y[i], z2[i]-z1[i], color='black', arrow_length_ratio=0.25, pivot='tail', linewidth=2)\n        ax.quiver(x[i], y[i], z2[i], x[i]-x[i], y[i]-y[i], z1[i]-z2[i], color='black', arrow_length_ratio=0.25, pivot='tail', linewidth=2)\n        # insert a small plot on top right of current plot\n        ax_inset = ax.inset_axes([0.825, 0.825, 0.165, 0.165])\n        ax_inset.plot(x[i], z1[i], 'o', color=color1)\n        ax_inset.plot(x[i], z2[i], 'o', color=color2)\n        # join 2 points with quiver\n        ax_inset.quiver(x[i], z1[i], np.zeros_like(x[i]), z2[i] - z1[i], scale_units='xy', scale=1, color='black', pivot='tail', width=.05)\n        # opposite quiver\n        ax_inset.quiver(x[i], z2[i], np.zeros_like(x[i]), z1[i] - z2[i], scale_units='xy', scale=1, color='black', pivot='tail', width=.05)\n        ax_inset.tick_params(axis='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n        # thick frame\n        ax_inset.spines['top'].set_linewidth(2)\n        ax_inset.spines['right'].set_linewidth(2)\n        ax_inset.spines['left'].set_linewidth(2)\n        ax_inset.spines['bottom'].set_linewidth(2)\n\n        ratio = 0.15\n        #add 20% padding from current limits\n        x0, x1 = ax_inset.get_xlim()\n        y0, y1 = ax_inset.get_ylim()\n        ax_inset.set_xlim(x0 - ratio * abs(x1-x0), x1 + ratio * abs(x1-x0))\n        ax_inset.set_ylim(y0 - ratio * abs(y1-y0), y1 + ratio * abs(y1-y0))\n    elif metric == 'cosine':\n        ax.quiver(x[i], y[i], z1[i], x[i+1]-x[i], y[i+1]-y[i], z1[i+1]-z1[i], color='black', arrow_length_ratio=0.2,\n            pivot='tail',\n            linewidth=2,\n            edgecolor='black',\n            facecolor='black',\n            alpha=1)\n        # reverse the arrow\n        ax.quiver(x[i], y[i], z2[i], x[i+1]-x[i], y[i+1]-y[i], z2[i+1]-z2[i], color='black', arrow_length_ratio=0.2,\n                    pivot='tail',\n                    linewidth=2,\n                    edgecolor='black',\n                    facecolor='black',\n                    alpha=1)\n        ax_inset = ax.inset_axes([0.825, 0.825, 0.165, 0.165])\n        ax_inset.plot(x[i+1], z1[i+1], 'o', color=color1)\n        ax_inset.plot(x[i+1], z2[i+1], 'o', color=color2)\n        # visualize the angle between the 2 vectors with quiver using vertex as midpoint between the 2 vectors\n        angle = np.arccos(np.dot([x[i+1]-x[i], z1[i+1]-z1[i]], [x[i+1]-x[i], z2[i+1]-z2[i]]) / (np.linalg.norm([x[i+1]-x[i], z1[i+1]-z1[i]]) * np.linalg.norm([x[i+1]-x[i], z2[i+1]-z2[i]])))\n        # visualize the angle between the 2 vectors with quiver using vertex as origin\n        midpt = (x[i], (z2[i]+z1[i])/2)\n        ax_inset.quiver(*midpt, x[i+1]-midpt[0], z2[i+1]-midpt[1], angles='xy', scale_units='xy', scale=1, color='black', width=.05)\n        ax_inset.quiver(*midpt, x[i+1]-midpt[0], z1[i+1]-midpt[1], angles='xy', scale_units='xy', scale=1, color='black', width=.05)\n\n        leftvec = (x[i+1], z2[i+1]) if angle &gt; 0 else (x[i+1], z1[i+1])\n        rightvec = (x[i+1], z1[i+1]) if angle &gt; 0 else (x[i+1], z2[i+1])\n        npy.plotting.AngleAnnotation(\n            midpt, leftvec, rightvec, ax=ax_inset, size=32.5, linewidth=2,\n            text=\"$\\\\theta$\", linestyle=\"-\", color=\"darkslategray\",\n            textposition=\"outside\",\n            text_kw=dict(fontsize=11, color=\"darkslategray\")\n        )\n        \n        ax_inset.tick_params(axis='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n        # thick frame\n        ax_inset.spines['top'].set_linewidth(2)\n        ax_inset.spines['right'].set_linewidth(2)\n        ax_inset.spines['left'].set_linewidth(2)\n        ax_inset.spines['bottom'].set_linewidth(2)\n\n        ratio = 0.15\n        #add 20% padding from current limits\n        x0, x1 = ax_inset.get_xlim()\n        y0, y1 = ax_inset.get_ylim()\n        ax_inset.set_xlim(x0 - ratio * abs(x1-x0), x1 + ratio * abs(x1-x0))\n        ax_inset.set_ylim(y0 - ratio * abs(y1-y0), y1 + ratio * abs(y1-y0))\n    ax.set_xlabel('Unit 1')\n    ax.set_ylabel('Unit 2')\n    ax.set_zlabel('Unit 3')\n    ax.tick_params(axis='both', pad=0, width=0)\n    npy.plotting.clean_plot3d(ax)\n\n    return ax\n</pre> def plot_3d_trajs(ax, x, y, z1, z2, color1='tab:blue', color2='tab:green'):     ax.scatter(x, y, z1, color=color1, s=100)     ax.plot(x, y, z1, color=color1, alpha=0.25)     ax.scatter(x, y, z2, color=color2, s=100)     ax.plot(x, y, z2, color=color2, alpha=0.25)      return ax  def vis_nss_metrics(x, y, z1, z2, metric='proximity', color1='tab:blue', color2='tab:red', ax=None):     if ax is None:         fig = plt.figure(figsize=(6, 10))         ax = fig.add_subplot(111, projection='3d')     plot_3d_trajs(ax, x, y, z1, z2, color1=color1, color2=color2)     n_points = len(x)     i = n_points // 2     if metric == 'proximity':         for j in range(n_points):             ax.plot([x[i], x[j]], [y[i], y[j]], [z1[i], z2[j]], color='gray', linestyle='dotted')         ax.quiver(x[i], y[i], z1[i], x[i]-x[i], y[i]-y[i], z2[i]-z1[i], color='black', arrow_length_ratio=0.25, pivot='tail', linewidth=2)         ax.quiver(x[i], y[i], z2[i], x[i]-x[i], y[i]-y[i], z1[i]-z2[i], color='black', arrow_length_ratio=0.25, pivot='tail', linewidth=2)         # insert a small plot on top right of current plot         ax_inset = ax.inset_axes([0.825, 0.825, 0.165, 0.165])         ax_inset.plot(x[i], z1[i], 'o', color=color1)         ax_inset.plot(x[i], z2[i], 'o', color=color2)         # join 2 points with quiver         ax_inset.quiver(x[i], z1[i], np.zeros_like(x[i]), z2[i] - z1[i], scale_units='xy', scale=1, color='black', pivot='tail', width=.05)         # opposite quiver         ax_inset.quiver(x[i], z2[i], np.zeros_like(x[i]), z1[i] - z2[i], scale_units='xy', scale=1, color='black', pivot='tail', width=.05)         ax_inset.tick_params(axis='both', bottom=False, left=False, labelbottom=False, labelleft=False)         # thick frame         ax_inset.spines['top'].set_linewidth(2)         ax_inset.spines['right'].set_linewidth(2)         ax_inset.spines['left'].set_linewidth(2)         ax_inset.spines['bottom'].set_linewidth(2)          ratio = 0.15         #add 20% padding from current limits         x0, x1 = ax_inset.get_xlim()         y0, y1 = ax_inset.get_ylim()         ax_inset.set_xlim(x0 - ratio * abs(x1-x0), x1 + ratio * abs(x1-x0))         ax_inset.set_ylim(y0 - ratio * abs(y1-y0), y1 + ratio * abs(y1-y0))     elif metric == 'cosine':         ax.quiver(x[i], y[i], z1[i], x[i+1]-x[i], y[i+1]-y[i], z1[i+1]-z1[i], color='black', arrow_length_ratio=0.2,             pivot='tail',             linewidth=2,             edgecolor='black',             facecolor='black',             alpha=1)         # reverse the arrow         ax.quiver(x[i], y[i], z2[i], x[i+1]-x[i], y[i+1]-y[i], z2[i+1]-z2[i], color='black', arrow_length_ratio=0.2,                     pivot='tail',                     linewidth=2,                     edgecolor='black',                     facecolor='black',                     alpha=1)         ax_inset = ax.inset_axes([0.825, 0.825, 0.165, 0.165])         ax_inset.plot(x[i+1], z1[i+1], 'o', color=color1)         ax_inset.plot(x[i+1], z2[i+1], 'o', color=color2)         # visualize the angle between the 2 vectors with quiver using vertex as midpoint between the 2 vectors         angle = np.arccos(np.dot([x[i+1]-x[i], z1[i+1]-z1[i]], [x[i+1]-x[i], z2[i+1]-z2[i]]) / (np.linalg.norm([x[i+1]-x[i], z1[i+1]-z1[i]]) * np.linalg.norm([x[i+1]-x[i], z2[i+1]-z2[i]])))         # visualize the angle between the 2 vectors with quiver using vertex as origin         midpt = (x[i], (z2[i]+z1[i])/2)         ax_inset.quiver(*midpt, x[i+1]-midpt[0], z2[i+1]-midpt[1], angles='xy', scale_units='xy', scale=1, color='black', width=.05)         ax_inset.quiver(*midpt, x[i+1]-midpt[0], z1[i+1]-midpt[1], angles='xy', scale_units='xy', scale=1, color='black', width=.05)          leftvec = (x[i+1], z2[i+1]) if angle &gt; 0 else (x[i+1], z1[i+1])         rightvec = (x[i+1], z1[i+1]) if angle &gt; 0 else (x[i+1], z2[i+1])         npy.plotting.AngleAnnotation(             midpt, leftvec, rightvec, ax=ax_inset, size=32.5, linewidth=2,             text=\"$\\\\theta$\", linestyle=\"-\", color=\"darkslategray\",             textposition=\"outside\",             text_kw=dict(fontsize=11, color=\"darkslategray\")         )                  ax_inset.tick_params(axis='both', bottom=False, left=False, labelbottom=False, labelleft=False)         # thick frame         ax_inset.spines['top'].set_linewidth(2)         ax_inset.spines['right'].set_linewidth(2)         ax_inset.spines['left'].set_linewidth(2)         ax_inset.spines['bottom'].set_linewidth(2)          ratio = 0.15         #add 20% padding from current limits         x0, x1 = ax_inset.get_xlim()         y0, y1 = ax_inset.get_ylim()         ax_inset.set_xlim(x0 - ratio * abs(x1-x0), x1 + ratio * abs(x1-x0))         ax_inset.set_ylim(y0 - ratio * abs(y1-y0), y1 + ratio * abs(y1-y0))     ax.set_xlabel('Unit 1')     ax.set_ylabel('Unit 2')     ax.set_zlabel('Unit 3')     ax.tick_params(axis='both', pad=0, width=0)     npy.plotting.clean_plot3d(ax)      return ax <p>Simulate two population vector trajectories and analyze the representational geometry and dynamics between them.</p> In\u00a0[3]: Copied! <pre>fig, axes = plt.subplots(2, 2, figsize=(10, 10), subplot_kw={'projection': '3d'})\naxes = axes.ravel()\n\n# Create data\nN_POINTS = 20\nx = np.sin(np.linspace(0, 2 * np.pi, N_POINTS))\ny = np.cos(np.linspace(0, 2 * np.pi, N_POINTS))\nz1 = np.linspace(0, 1, N_POINTS)\nz2 = np.linspace(0, 1, N_POINTS) + np.sin(np.linspace(0.25, 0.75, N_POINTS))\nprox_traj1 = np.array([x, y, z1]).T  # shape (N_POINTS, N_NEURONS)\nprox_traj2 = np.array([x, y, z2]).T  # shape (N_POINTS, N_NEURONS)\n\nvis_nss_metrics(x, y, z1, z2, metric='proximity', ax=axes[0])\naxes[0].set_title('Proximity')\n\n# Create data\nN_POINTS = 20\nx = -np.sin(np.linspace(0, 2 * np.pi, N_POINTS))\ny = np.cos(np.linspace(0, 2 * np.pi, N_POINTS))\nz1 = -np.linspace(0, 1, N_POINTS)\nz2 = -np.linspace(0, 1, N_POINTS) + np.sin(np.linspace(np.pi/2, 1.5*np.pi, N_POINTS))\ncos_traj1 = np.array([x, y, z1]).T  # shape (N_POINTS, N_NEURONS)\ncos_traj2 = np.array([x, y, z2]).T  # shape (N_POINTS, N_NEURONS)\n\nvis_nss_metrics(x, y, z1, z2, metric='cosine', ax=axes[1])\naxes[1].set_title('Cosine similarity of dynamics')\n\n# convert axes to 2D\naxes[2].axis('off')\naxes[3].axis('off')\n\n# insert axis 2d replacing the 3d plot\nax2d = fig.add_subplot(axes[2].get_position(), frame_on=True)\nax2d.plot(npy.ensemble.proximity(prox_traj1, prox_traj2), '.-')\n\nax2d.set_xlabel('Time')\nax2d.set_ylabel('Proximity')\nax2d.set_title('Proximity between trajectories')\n\n# insert axis 2d replacing the 3d plot\nax2d = fig.add_subplot(axes[3].get_position(), frame_on=True)\nax2d.plot(\n    npy.ensemble.cosine_similarity(\n        np.diff(cos_traj1.T).T,\n        np.diff(cos_traj2.T).T\n    ),\n    '.-'\n)\n\nax2d.set_xlabel('Time')\nax2d.set_ylabel('Cosine similarity')\nax2d.set_title('Cosine similarity of temporal differences')\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(2, 2, figsize=(10, 10), subplot_kw={'projection': '3d'}) axes = axes.ravel()  # Create data N_POINTS = 20 x = np.sin(np.linspace(0, 2 * np.pi, N_POINTS)) y = np.cos(np.linspace(0, 2 * np.pi, N_POINTS)) z1 = np.linspace(0, 1, N_POINTS) z2 = np.linspace(0, 1, N_POINTS) + np.sin(np.linspace(0.25, 0.75, N_POINTS)) prox_traj1 = np.array([x, y, z1]).T  # shape (N_POINTS, N_NEURONS) prox_traj2 = np.array([x, y, z2]).T  # shape (N_POINTS, N_NEURONS)  vis_nss_metrics(x, y, z1, z2, metric='proximity', ax=axes[0]) axes[0].set_title('Proximity')  # Create data N_POINTS = 20 x = -np.sin(np.linspace(0, 2 * np.pi, N_POINTS)) y = np.cos(np.linspace(0, 2 * np.pi, N_POINTS)) z1 = -np.linspace(0, 1, N_POINTS) z2 = -np.linspace(0, 1, N_POINTS) + np.sin(np.linspace(np.pi/2, 1.5*np.pi, N_POINTS)) cos_traj1 = np.array([x, y, z1]).T  # shape (N_POINTS, N_NEURONS) cos_traj2 = np.array([x, y, z2]).T  # shape (N_POINTS, N_NEURONS)  vis_nss_metrics(x, y, z1, z2, metric='cosine', ax=axes[1]) axes[1].set_title('Cosine similarity of dynamics')  # convert axes to 2D axes[2].axis('off') axes[3].axis('off')  # insert axis 2d replacing the 3d plot ax2d = fig.add_subplot(axes[2].get_position(), frame_on=True) ax2d.plot(npy.ensemble.proximity(prox_traj1, prox_traj2), '.-')  ax2d.set_xlabel('Time') ax2d.set_ylabel('Proximity') ax2d.set_title('Proximity between trajectories')  # insert axis 2d replacing the 3d plot ax2d = fig.add_subplot(axes[3].get_position(), frame_on=True) ax2d.plot(     npy.ensemble.cosine_similarity(         np.diff(cos_traj1.T).T,         np.diff(cos_traj2.T).T     ),     '.-' )  ax2d.set_xlabel('Time') ax2d.set_ylabel('Cosine similarity') ax2d.set_title('Cosine similarity of temporal differences')  plt.tight_layout() plt.show() <pre>/tmp/ipykernel_316323/3793467759.py:54: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n</pre> In\u00a0[4]: Copied! <pre>basepath = r'/run/user/1000/gvfs/smb-share:server=132.236.112.212,share=ayadata1/Data/GrosmarkAD/Achilles/Achilles_10252013'\n\nepoch_df = npy.io.load_epoch(basepath)\n# get session bounds to provide support\nsession_bounds = nel.EpochArray(\n    [epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]]\n)\n# compress repeated sleep sessions\nepoch_df = npy.session.compress_repeated_epochs(epoch_df)\nbeh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values.astype(float))\n\nst, cell_metrics = npy.io.load_spikes(\n    basepath, putativeCellType=\"Pyr\", brainRegion=\"CA\"\n)\n\nposition_df = npy.io.load_animal_behavior(basepath)\n\nposition_df_no_nan = position_df.query(\"x.isnull() == False\")\n# put position into a nelpy position array for ease of use\npos = nel.AnalogSignalArray(\n    data=position_df_no_nan[\"x\"].values.T,\n    timestamps=position_df_no_nan.timestamps.values,\n)\n\n# get outbound and inbound epochs\noutbound_epochs, inbound_epochs = \\\n    npy.behavior.get_linear_track_lap_epochs(\n        pos.abscissa_vals, pos.data[0], newLapThreshold=20\n    )\ninbound_epochs = npy.behavior.find_good_lap_epochs(pos, inbound_epochs, thres=0.5, binsize=3, min_laps=10)\noutbound_epochs = npy.behavior.find_good_lap_epochs(pos, outbound_epochs, thres=0.5, binsize=3, min_laps=10)\n\noutbound_epochs, inbound_epochs\n</pre> basepath = r'/run/user/1000/gvfs/smb-share:server=132.236.112.212,share=ayadata1/Data/GrosmarkAD/Achilles/Achilles_10252013'  epoch_df = npy.io.load_epoch(basepath) # get session bounds to provide support session_bounds = nel.EpochArray(     [epoch_df.startTime.iloc[0], epoch_df.stopTime.iloc[-1]] ) # compress repeated sleep sessions epoch_df = npy.session.compress_repeated_epochs(epoch_df) beh_epochs = nel.EpochArray(epoch_df[[\"startTime\", \"stopTime\"]].values.astype(float))  st, cell_metrics = npy.io.load_spikes(     basepath, putativeCellType=\"Pyr\", brainRegion=\"CA\" )  position_df = npy.io.load_animal_behavior(basepath)  position_df_no_nan = position_df.query(\"x.isnull() == False\") # put position into a nelpy position array for ease of use pos = nel.AnalogSignalArray(     data=position_df_no_nan[\"x\"].values.T,     timestamps=position_df_no_nan.timestamps.values, )  # get outbound and inbound epochs outbound_epochs, inbound_epochs = \\     npy.behavior.get_linear_track_lap_epochs(         pos.abscissa_vals, pos.data[0], newLapThreshold=20     ) inbound_epochs = npy.behavior.find_good_lap_epochs(pos, inbound_epochs, thres=0.5, binsize=3, min_laps=10) outbound_epochs = npy.behavior.find_good_lap_epochs(pos, outbound_epochs, thres=0.5, binsize=3, min_laps=10)  outbound_epochs, inbound_epochs <pre>WARNING:root:fs was not specified, so we try to estimate it from the data...\nWARNING:root:fs was estimated to be 39.06263603480421 Hz\nWARNING:root:creating support from abscissa_vals and sampling rate, fs!\nWARNING:root:'fs' has been deprecated; use 'step' instead\n</pre> Out[4]: <pre>(&lt;EpochArray at 0x7a610db358b0: 42 epochs&gt; of length 17:07:964 minutes,\n &lt;EpochArray at 0x7a610db351c0: 42 epochs&gt; of length 15:28:585 minutes)</pre> In\u00a0[5]: Copied! <pre>SPATIAL_BIN_SIZE = 3\nBEHAVIOR_TIME_BIN_SIZE = 0.05\nREPLAY_TIME_BIN_SIZE = 0.02\nSPEED_THRESHOLD = 3\nTUNING_CURVE_SIGMA = 2\nPLACE_CELL_MIN_SPKS = 100\nPLACE_CELL_MIN_RATE = 1\nPLACE_CELL_PEAK_MIN_RATIO = 1.5\n\nN_SHUFFLES = 100\ndef get_tuning_curves(\n    pos, st, x_min, x_max, speed_thres, s_binsize,\n    tuning_curve_sigma\n):\n\n    spatial_maps = npy.tuning.SpatialMap(\n        pos,\n        st,\n        dim=1,\n        x_minmax=(x_min, x_max),\n        s_binsize=s_binsize,\n        speed_thres=speed_thres,\n        tuning_curve_sigma=tuning_curve_sigma,\n        minbgrate=0,  # decoding does not like 0 firing rate\n        # min_duration=0,\n    )\n\n    return spatial_maps.tc\n\n\nx_max = np.ceil(np.nanmax(pos.data))\nx_min = np.floor(np.nanmin(pos.data))\n\ntc_in = get_tuning_curves(\n    pos[inbound_epochs], st[inbound_epochs], x_min, x_max,\n    SPEED_THRESHOLD, SPATIAL_BIN_SIZE, TUNING_CURVE_SIGMA\n)\ntc_out = get_tuning_curves(\n    pos[outbound_epochs], st[outbound_epochs], x_min, x_max,\n    SPEED_THRESHOLD, SPATIAL_BIN_SIZE, TUNING_CURVE_SIGMA\n)\n</pre> SPATIAL_BIN_SIZE = 3 BEHAVIOR_TIME_BIN_SIZE = 0.05 REPLAY_TIME_BIN_SIZE = 0.02 SPEED_THRESHOLD = 3 TUNING_CURVE_SIGMA = 2 PLACE_CELL_MIN_SPKS = 100 PLACE_CELL_MIN_RATE = 1 PLACE_CELL_PEAK_MIN_RATIO = 1.5  N_SHUFFLES = 100 def get_tuning_curves(     pos, st, x_min, x_max, speed_thres, s_binsize,     tuning_curve_sigma ):      spatial_maps = npy.tuning.SpatialMap(         pos,         st,         dim=1,         x_minmax=(x_min, x_max),         s_binsize=s_binsize,         speed_thres=speed_thres,         tuning_curve_sigma=tuning_curve_sigma,         minbgrate=0,  # decoding does not like 0 firing rate         # min_duration=0,     )      return spatial_maps.tc   x_max = np.ceil(np.nanmax(pos.data)) x_min = np.floor(np.nanmin(pos.data))  tc_in = get_tuning_curves(     pos[inbound_epochs], st[inbound_epochs], x_min, x_max,     SPEED_THRESHOLD, SPATIAL_BIN_SIZE, TUNING_CURVE_SIGMA ) tc_out = get_tuning_curves(     pos[outbound_epochs], st[outbound_epochs], x_min, x_max,     SPEED_THRESHOLD, SPATIAL_BIN_SIZE, TUNING_CURVE_SIGMA ) <pre>WARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\n</pre> In\u00a0[6]: Copied! <pre>pv_trials = []\nfor ep in inbound_epochs + outbound_epochs:\n    spatial_maps = npy.tuning.SpatialMap(\n        pos[ep],\n        st[ep],\n        dim=1,\n        x_minmax=(x_min, x_max),\n        s_binsize=SPATIAL_BIN_SIZE,\n        speed_thres=SPEED_THRESHOLD,\n        tuning_curve_sigma=TUNING_CURVE_SIGMA,\n        minbgrate=0,  # decoding does not like 0 firing rate\n        min_duration=0\n    )\n    pv_trials.append(spatial_maps.tc.ratemap.T)\npv_trials = np.asarray(pv_trials)\npv_trials.shape  # (n_trials, n_neurons, n_bins)\n</pre> pv_trials = [] for ep in inbound_epochs + outbound_epochs:     spatial_maps = npy.tuning.SpatialMap(         pos[ep],         st[ep],         dim=1,         x_minmax=(x_min, x_max),         s_binsize=SPATIAL_BIN_SIZE,         speed_thres=SPEED_THRESHOLD,         tuning_curve_sigma=TUNING_CURVE_SIGMA,         minbgrate=0,  # decoding does not like 0 firing rate         min_duration=0     )     pv_trials.append(spatial_maps.tc.ratemap.T) pv_trials = np.asarray(pv_trials) pv_trials.shape  # (n_trials, n_neurons, n_bins) <pre>WARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring signal outside of support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring events outside of eventarray support\nWARNING:root:ignoring signal outside of support\n</pre> Out[6]: <pre>(84, 65, 241)</pre> In\u00a0[7]: Copied! <pre>def vis_spatial_tc(ntrial):\n    inbound_nrnorder = np.asarray(tc_in.get_peak_firing_order_ids())-1\n    outbound_nrnorder = np.asarray(tc_out.get_peak_firing_order_ids())-1\n    CMAP = 'terrain'\n\n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    axes = axes.ravel()\n    scaler = sklearn.preprocessing.MinMaxScaler()\n    zscored_tc_in = scaler.fit_transform(tc_in.ratemap.T).T\n    zscored_tc_out = scaler.fit_transform(tc_out.ratemap.T).T\n    zscored_trial = scaler.fit_transform(pv_trials[ntrial])\n\n    ax = axes[0]\n    im = ax.imshow(zscored_tc_in[inbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)\n    plt.colorbar(im, ax=ax)\n    ax.set_title('Tuning Curves Inbound')\n    ax.set_xlabel('Spatial Bin')\n    ax.set_ylabel('Neurons (sorted by peak firing order in inbound TC)')\n\n    ax = axes[1]\n    im = ax.imshow(zscored_tc_out[inbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)\n    plt.colorbar(im, ax=ax)\n    ax.set_title('Tuning Curves Outbound')\n    ax.set_xlabel('Spatial Bin')\n    ax.set_ylabel('Neurons (sorted by peak firing order in inbound TC)')\n\n    ax = axes[2]\n    im = ax.imshow(zscored_trial.T[inbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)\n    plt.colorbar(im, ax=ax)\n    ax.set_title('Spatial Binned Trial')\n    ax.set_xlabel('Spatial Bin')\n    ax.set_ylabel('Neurons (sorted by peak firing order in inbound TC)')\n\n    ax = axes[3]\n    im = ax.imshow(zscored_tc_in[outbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)\n    plt.colorbar(im, ax=ax)\n    ax.set_title('Tuning Curves Inbound')\n    ax.set_xlabel('Spatial Bin')\n    ax.set_ylabel('Neurons (sorted by peak firing order in outbound TC)')\n\n    ax = axes[4]\n    im = ax.imshow(zscored_tc_out[outbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)\n    plt.colorbar(im, ax=ax)\n    ax.set_title('Tuning Curves Outbound')\n    ax.set_xlabel('Spatial Bin')\n    ax.set_ylabel('Neurons (sorted by peak firing order in outbound TC)')\n\n    ax = axes[5]\n    im = ax.imshow(zscored_trial.T[outbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)\n    plt.colorbar(im, ax=ax)\n    ax.set_title('Spatial Binned Trial')\n    ax.set_xlabel('Spatial Bin')\n    ax.set_ylabel('Neurons (sorted by peak firing order in outbound TC)')\n\n    plt.show()\n\nwidgets.interact(vis_spatial_tc, ntrial=(0, pv_trials.shape[0]-1, 1));\n</pre> def vis_spatial_tc(ntrial):     inbound_nrnorder = np.asarray(tc_in.get_peak_firing_order_ids())-1     outbound_nrnorder = np.asarray(tc_out.get_peak_firing_order_ids())-1     CMAP = 'terrain'      fig, axes = plt.subplots(2, 3, figsize=(15, 10))     axes = axes.ravel()     scaler = sklearn.preprocessing.MinMaxScaler()     zscored_tc_in = scaler.fit_transform(tc_in.ratemap.T).T     zscored_tc_out = scaler.fit_transform(tc_out.ratemap.T).T     zscored_trial = scaler.fit_transform(pv_trials[ntrial])      ax = axes[0]     im = ax.imshow(zscored_tc_in[inbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)     plt.colorbar(im, ax=ax)     ax.set_title('Tuning Curves Inbound')     ax.set_xlabel('Spatial Bin')     ax.set_ylabel('Neurons (sorted by peak firing order in inbound TC)')      ax = axes[1]     im = ax.imshow(zscored_tc_out[inbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)     plt.colorbar(im, ax=ax)     ax.set_title('Tuning Curves Outbound')     ax.set_xlabel('Spatial Bin')     ax.set_ylabel('Neurons (sorted by peak firing order in inbound TC)')      ax = axes[2]     im = ax.imshow(zscored_trial.T[inbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)     plt.colorbar(im, ax=ax)     ax.set_title('Spatial Binned Trial')     ax.set_xlabel('Spatial Bin')     ax.set_ylabel('Neurons (sorted by peak firing order in inbound TC)')      ax = axes[3]     im = ax.imshow(zscored_tc_in[outbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)     plt.colorbar(im, ax=ax)     ax.set_title('Tuning Curves Inbound')     ax.set_xlabel('Spatial Bin')     ax.set_ylabel('Neurons (sorted by peak firing order in outbound TC)')      ax = axes[4]     im = ax.imshow(zscored_tc_out[outbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)     plt.colorbar(im, ax=ax)     ax.set_title('Tuning Curves Outbound')     ax.set_xlabel('Spatial Bin')     ax.set_ylabel('Neurons (sorted by peak firing order in outbound TC)')      ax = axes[5]     im = ax.imshow(zscored_trial.T[outbound_nrnorder], aspect='auto', interpolation='none', cmap=CMAP)     plt.colorbar(im, ax=ax)     ax.set_title('Spatial Binned Trial')     ax.set_xlabel('Spatial Bin')     ax.set_ylabel('Neurons (sorted by peak firing order in outbound TC)')      plt.show()  widgets.interact(vis_spatial_tc, ntrial=(0, pv_trials.shape[0]-1, 1)); <pre>interactive(children=(IntSlider(value=41, description='ntrial', max=83), Output()), _dom_classes=('widget-inte\u2026</pre> <p>We first visualize the population vector trajectories using Principal Component Analysis (PCA) and then compute the proximity and cosine similarity.</p> <p>While for simple cases, the proximity and cosine similarity can be interpreted from the PCA plot, the <code>proximity</code> and <code>cosine_similarity</code> functions provide a quantitative measure of the representational geometry and dynamics between the two population vector trajectories in the high-dimensional neural state space.</p> In\u00a0[8]: Copied! <pre># Preprocessing and PCA\nscaler = sklearn.preprocessing.StandardScaler()\npca = sklearn.decomposition.PCA(n_components=3)\n\nbst = pv_trials.reshape(-1, pv_trials.shape[-1])  # (n_trials * n_neurons, n_bins)\nbst = scaler.fit_transform(bst)\nclip = 4\nbst = np.clip(bst, -clip, clip)\nbst = pca.fit_transform(bst)\n\nscaled_tc_in = scaler.transform(tc_in.ratemap.T)\nscaled_tc_in = np.clip(scaled_tc_in, -clip, clip)\nscaled_tc_out = scaler.transform(tc_out.ratemap.T)\nscaled_tc_out = np.clip(scaled_tc_out, -clip, clip)\npca.fit(scaled_tc_in)\npca_tc_in = pca.transform(scaled_tc_in)\npca_tc_out = pca.transform(scaled_tc_out)\n\nproximity = npy.ensemble.proximity(tc_in.ratemap.T, tc_out.ratemap.T)\n\ncossim = npy.ensemble.cosine_similarity(\n    np.gradient(tc_in.ratemap.T, axis=0),\n    np.gradient(tc_out.ratemap.T, axis=0)\n)\n\n# Create figure and axes\nfig = plt.figure(figsize=(6, 10))\nax3d = fig.add_subplot(111, projection='3d')\n# ax = fig.add_subplot(111)\n\n# Function to update the plot for each frame\ndef update(num, x, y, z1, z2):\n    ax3d.cla()  # Clear the current axes\n    # set limits of axes to be consistent on basis of all data\n    ax3d.set_xlim(\n        min(pca_tc_in[:, 0].min(), pca_tc_out[:, 0].min()),\n        max(pca_tc_in[:, 0].max(), pca_tc_out[:, 0].max())\n    )\n    ax3d.set_ylim(\n        min(pca_tc_in[:, 1].min(), pca_tc_out[:, 1].min()),\n        max(pca_tc_in[:, 1].max(), pca_tc_out[:, 1].max())\n    )\n    ax3d.set_zlim(\n        min(pca_tc_in[:, 2].min(), pca_tc_out[:, 2].min()),\n        max(pca_tc_in[:, 2].max(), pca_tc_out[:, 2].max())\n    )\n    ax3d.scatter(x[:num], y[:num], z1[:num], color='tab:blue', s=100, label='Inbound')\n    ax3d.plot(x[:num], y[:num], z1[:num], color='tab:blue', alpha=0.25)\n    ax3d.scatter(x[:num], y[:num], z2[:num], color='tab:green', s=100, label='Outbound')\n    ax3d.plot(x[:num], y[:num], z2[:num], color='tab:green', alpha=0.25)\n\n    # Set labels and title\n    ax3d.set_xlabel('PC1')\n    ax3d.set_ylabel('PC2')\n    ax3d.set_zlabel('PC3')\n    ax3d.set_title('Trajectories of tuning curves in PCA space')\n    ax3d.legend()\n\n# Create animation\nani = FuncAnimation(\n    fig,\n    update,\n    frames=len(pca_tc_in),  # Number of frames in the animation\n    fargs=(pca_tc_in[:, 0], pca_tc_in[:, 1], pca_tc_in[:, 2], pca_tc_out[:, 2]),\n    interval=100  # Interval between frames in milliseconds\n)\n\nHTML(ani.to_jshtml())\n</pre> # Preprocessing and PCA scaler = sklearn.preprocessing.StandardScaler() pca = sklearn.decomposition.PCA(n_components=3)  bst = pv_trials.reshape(-1, pv_trials.shape[-1])  # (n_trials * n_neurons, n_bins) bst = scaler.fit_transform(bst) clip = 4 bst = np.clip(bst, -clip, clip) bst = pca.fit_transform(bst)  scaled_tc_in = scaler.transform(tc_in.ratemap.T) scaled_tc_in = np.clip(scaled_tc_in, -clip, clip) scaled_tc_out = scaler.transform(tc_out.ratemap.T) scaled_tc_out = np.clip(scaled_tc_out, -clip, clip) pca.fit(scaled_tc_in) pca_tc_in = pca.transform(scaled_tc_in) pca_tc_out = pca.transform(scaled_tc_out)  proximity = npy.ensemble.proximity(tc_in.ratemap.T, tc_out.ratemap.T)  cossim = npy.ensemble.cosine_similarity(     np.gradient(tc_in.ratemap.T, axis=0),     np.gradient(tc_out.ratemap.T, axis=0) )  # Create figure and axes fig = plt.figure(figsize=(6, 10)) ax3d = fig.add_subplot(111, projection='3d') # ax = fig.add_subplot(111)  # Function to update the plot for each frame def update(num, x, y, z1, z2):     ax3d.cla()  # Clear the current axes     # set limits of axes to be consistent on basis of all data     ax3d.set_xlim(         min(pca_tc_in[:, 0].min(), pca_tc_out[:, 0].min()),         max(pca_tc_in[:, 0].max(), pca_tc_out[:, 0].max())     )     ax3d.set_ylim(         min(pca_tc_in[:, 1].min(), pca_tc_out[:, 1].min()),         max(pca_tc_in[:, 1].max(), pca_tc_out[:, 1].max())     )     ax3d.set_zlim(         min(pca_tc_in[:, 2].min(), pca_tc_out[:, 2].min()),         max(pca_tc_in[:, 2].max(), pca_tc_out[:, 2].max())     )     ax3d.scatter(x[:num], y[:num], z1[:num], color='tab:blue', s=100, label='Inbound')     ax3d.plot(x[:num], y[:num], z1[:num], color='tab:blue', alpha=0.25)     ax3d.scatter(x[:num], y[:num], z2[:num], color='tab:green', s=100, label='Outbound')     ax3d.plot(x[:num], y[:num], z2[:num], color='tab:green', alpha=0.25)      # Set labels and title     ax3d.set_xlabel('PC1')     ax3d.set_ylabel('PC2')     ax3d.set_zlabel('PC3')     ax3d.set_title('Trajectories of tuning curves in PCA space')     ax3d.legend()  # Create animation ani = FuncAnimation(     fig,     update,     frames=len(pca_tc_in),  # Number of frames in the animation     fargs=(pca_tc_in[:, 0], pca_tc_in[:, 1], pca_tc_in[:, 2], pca_tc_out[:, 2]),     interval=100  # Interval between frames in milliseconds )  HTML(ani.to_jshtml()) Out[8]: Once Loop Reflect In\u00a0[9]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].plot(npy.ensemble.proximity(tc_in.ratemap.T, tc_out.ratemap.T))\naxes[0].set_title('Proximity')\naxes[0].set_xlabel('Spatial bin')\naxes[0].set_ylabel('Proximity')\n\naxes[1].plot(\n    npy.ensemble.cosine_similarity(\n        np.gradient(tc_in.ratemap.T, axis=0),\n        np.gradient(tc_out.ratemap.T, axis=0)\n    )\n)\naxes[1].set_title('Cosine similarity of dynamics')\naxes[1].set_xlabel('Spatial bin')\naxes[1].set_ylabel('Cosine similarity')\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].plot(npy.ensemble.proximity(tc_in.ratemap.T, tc_out.ratemap.T)) axes[0].set_title('Proximity') axes[0].set_xlabel('Spatial bin') axes[0].set_ylabel('Proximity')  axes[1].plot(     npy.ensemble.cosine_similarity(         np.gradient(tc_in.ratemap.T, axis=0),         np.gradient(tc_out.ratemap.T, axis=0)     ) ) axes[1].set_title('Cosine similarity of dynamics') axes[1].set_xlabel('Spatial bin') axes[1].set_ylabel('Cosine similarity')  plt.tight_layout() plt.show() <p>Finally, we visualize the evolution of the proximity and cosine similarity between the two population vector trajectories over spatial bins.</p> In\u00a0[10]: Copied! <pre># Create figure and axes\nfig = plt.figure(figsize=(10, 6), constrained_layout=True)\nnrows, ncols = 4, 6 \ngs = matplotlib.gridspec.GridSpec(*(nrows, ncols), figure=fig)\nax3d = fig.add_subplot(gs[0:4, 0:4], projection='3d')\nax_proximity = fig.add_subplot(gs[0:2, 4:6])\nax_cossim = fig.add_subplot(gs[2:4, 4:6])\n\n# Initialize plots\nax_proximity.set_title(\"Proximity\")\nax_proximity.set_xlabel(\"Spatial bin\")\nax_proximity.set_ylabel(\"Proximity\")\nax_cossim.set_title(\"Cosine Similarity\")\nax_cossim.set_xlabel(\"Spatial bin\")\nax_cossim.set_ylabel(\"Cosine Similarity\")\n\n# Function to update the plots for each frame\ndef update(num, x, y, z1, z2):\n    ax3d.cla()  # Clear the current axes\n    # set limits of axes to be consistent on basis of all data\n    ax3d.set_xlim(\n        min(pca_tc_in[:, 0].min(), pca_tc_out[:, 0].min()),\n        max(pca_tc_in[:, 0].max(), pca_tc_out[:, 0].max())\n    )\n    ax3d.set_ylim(\n        min(pca_tc_in[:, 1].min(), pca_tc_out[:, 1].min()),\n        max(pca_tc_in[:, 1].max(), pca_tc_out[:, 1].max())\n    )\n    ax3d.set_zlim(\n        min(pca_tc_in[:, 2].min(), pca_tc_out[:, 2].min()),\n        max(pca_tc_in[:, 2].max(), pca_tc_out[:, 2].max())\n    )\n    ax3d.scatter(x[:num], y[:num], z1[:num], color='tab:blue', s=100, label='Inbound')\n    ax3d.plot(x[:num], y[:num], z1[:num], color='tab:blue', alpha=0.25)\n    ax3d.scatter(x[:num], y[:num], z2[:num], color='tab:green', s=100, label='Outbound')\n    ax3d.plot(x[:num], y[:num], z2[:num], color='tab:green', alpha=0.25)\n\n\n    # Update proximity plot\n    ax_proximity.cla()\n    ax_proximity.set_xlim(-2, len(proximity))\n    ax_proximity.set_ylim(proximity.min(), proximity.max())\n    ax_proximity.plot(range(num), proximity[:num], color='tab:blue')\n    ax_proximity.set_title(\"Proximity\")\n    ax_proximity.set_xlabel(\"Spatial bin\")\n    ax_proximity.set_ylabel(\"Proximity\")\n\n    # Update cosine similarity plot\n    ax_cossim.cla()\n    ax_cossim.set_xlim(-2, len(cossim))\n    ax_cossim.set_ylim(cossim.min(), cossim.max())\n    ax_cossim.plot(range(num), cossim[:num], color='tab:green')\n    ax_cossim.set_title(\"Cosine Similarity\")\n    ax_cossim.set_xlabel(\"Spatial bin\")\n    ax_cossim.set_ylabel(\"Cosine Similarity\")\n\n# Create animation\nani = FuncAnimation(\n    fig,\n    update,\n    frames=len(pca_tc_in),   # Number of frames in the animation\n    fargs=(pca_tc_in[:, 0], pca_tc_in[:, 1], pca_tc_in[:, 2], pca_tc_out[:, 2]),\n    interval=150             # Interval between frames in milliseconds\n)\n\nHTML(ani.to_jshtml())\n</pre> # Create figure and axes fig = plt.figure(figsize=(10, 6), constrained_layout=True) nrows, ncols = 4, 6  gs = matplotlib.gridspec.GridSpec(*(nrows, ncols), figure=fig) ax3d = fig.add_subplot(gs[0:4, 0:4], projection='3d') ax_proximity = fig.add_subplot(gs[0:2, 4:6]) ax_cossim = fig.add_subplot(gs[2:4, 4:6])  # Initialize plots ax_proximity.set_title(\"Proximity\") ax_proximity.set_xlabel(\"Spatial bin\") ax_proximity.set_ylabel(\"Proximity\") ax_cossim.set_title(\"Cosine Similarity\") ax_cossim.set_xlabel(\"Spatial bin\") ax_cossim.set_ylabel(\"Cosine Similarity\")  # Function to update the plots for each frame def update(num, x, y, z1, z2):     ax3d.cla()  # Clear the current axes     # set limits of axes to be consistent on basis of all data     ax3d.set_xlim(         min(pca_tc_in[:, 0].min(), pca_tc_out[:, 0].min()),         max(pca_tc_in[:, 0].max(), pca_tc_out[:, 0].max())     )     ax3d.set_ylim(         min(pca_tc_in[:, 1].min(), pca_tc_out[:, 1].min()),         max(pca_tc_in[:, 1].max(), pca_tc_out[:, 1].max())     )     ax3d.set_zlim(         min(pca_tc_in[:, 2].min(), pca_tc_out[:, 2].min()),         max(pca_tc_in[:, 2].max(), pca_tc_out[:, 2].max())     )     ax3d.scatter(x[:num], y[:num], z1[:num], color='tab:blue', s=100, label='Inbound')     ax3d.plot(x[:num], y[:num], z1[:num], color='tab:blue', alpha=0.25)     ax3d.scatter(x[:num], y[:num], z2[:num], color='tab:green', s=100, label='Outbound')     ax3d.plot(x[:num], y[:num], z2[:num], color='tab:green', alpha=0.25)       # Update proximity plot     ax_proximity.cla()     ax_proximity.set_xlim(-2, len(proximity))     ax_proximity.set_ylim(proximity.min(), proximity.max())     ax_proximity.plot(range(num), proximity[:num], color='tab:blue')     ax_proximity.set_title(\"Proximity\")     ax_proximity.set_xlabel(\"Spatial bin\")     ax_proximity.set_ylabel(\"Proximity\")      # Update cosine similarity plot     ax_cossim.cla()     ax_cossim.set_xlim(-2, len(cossim))     ax_cossim.set_ylim(cossim.min(), cossim.max())     ax_cossim.plot(range(num), cossim[:num], color='tab:green')     ax_cossim.set_title(\"Cosine Similarity\")     ax_cossim.set_xlabel(\"Spatial bin\")     ax_cossim.set_ylabel(\"Cosine Similarity\")  # Create animation ani = FuncAnimation(     fig,     update,     frames=len(pca_tc_in),   # Number of frames in the animation     fargs=(pca_tc_in[:, 0], pca_tc_in[:, 1], pca_tc_in[:, 2], pca_tc_out[:, 2]),     interval=150             # Interval between frames in milliseconds )  HTML(ani.to_jshtml()) Out[10]: Once Loop Reflect"},{"location":"tutorials/neural_geodynamics/#neural-trajectories-comparative-geometry-and-dynamics","title":"Neural Trajectories Comparative Geometry and Dynamics\u00b6","text":"<p>Here, we will show how to use the <code>proximity</code> and <code>cosine_similarity</code> functions to analyze the representational geometry and dynamics between two population vector trajectories.</p>"},{"location":"tutorials/neural_geodynamics/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/neural_geodynamics/#section-1-proximity-and-cosine-similarity-of-dynamics-of-simulated-data","title":"Section 1: Proximity and cosine similarity of dynamics of simulated data\u00b6","text":""},{"location":"tutorials/neural_geodynamics/#section-2-proximity-and-cosine-similarity-of-dynamics-of-real-data","title":"Section 2: Proximity and cosine similarity of dynamics of real data\u00b6","text":""},{"location":"tutorials/neural_geodynamics/#section-21-load-data","title":"Section 2.1: Load data\u00b6","text":""},{"location":"tutorials/neural_geodynamics/#section-22-compute-population-vectors-trajectories-in-space","title":"Section 2.2: Compute population vectors trajectories in space\u00b6","text":""},{"location":"tutorials/neural_geodynamics/#section-23-visualize-single-neuron-tuning-properties-in-relation-to-the-behavioral-trajectory","title":"Section 2.3: Visualize single neuron tuning properties in relation to the behavioral trajectory\u00b6","text":"<p>Visualize the spatial tuning curves of the neurons such that the neurons are sorted by their peak firing order in the inbound and outbound tuning curves</p>"},{"location":"tutorials/neural_geodynamics/#section-24-compute-the-proximity-and-cosine-similarity-of-the-dynamics-of-the-population-vector-trajectories","title":"Section 2.4: Compute the proximity and cosine similarity of the dynamics of the population vector trajectories\u00b6","text":""},{"location":"tutorials/reactivation/","title":"Reactivation","text":"In\u00a0[1]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\n\n# from neuro_py\nfrom neuro_py.ensemble.assembly_reactivation import AssemblyReact\nfrom neuro_py.process.peri_event import event_triggered_average_fast\nfrom neuro_py.io import loading\n\n# core tools\nimport nelpy as nel\nimport numpy as np\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> %reload_ext autoreload %autoreload 2  # from neuro_py from neuro_py.ensemble.assembly_reactivation import AssemblyReact from neuro_py.process.peri_event import event_triggered_average_fast from neuro_py.io import loading  # core tools import nelpy as nel import numpy as np  # plotting import matplotlib.pyplot as plt import seaborn as sns In\u00a0[2]: Copied! <pre>basepath = r\"Z:\\Data\\HMC1\\day8\"\n\nassembly_react = AssemblyReact(\n    basepath=basepath,\n    brainRegion=\"CA1\",\n    putativeCellType=\"Pyr\",\n    z_mat_dt=0.01,\n    )\n</pre> basepath = r\"Z:\\Data\\HMC1\\day8\"  assembly_react = AssemblyReact(     basepath=basepath,     brainRegion=\"CA1\",     putativeCellType=\"Pyr\",     z_mat_dt=0.01,     ) <p>Also, load brain states for later use.</p> In\u00a0[3]: Copied! <pre># load theta epochs\nstate_dict = loading.load_SleepState_states(basepath)\ntheta_epochs = nel.EpochArray(\n    state_dict[\"THETA\"],\n)\nnrem_epochs = nel.EpochArray(\n    state_dict[\"NREMstate\"],\n)\ntheta_epochs, nrem_epochs\n</pre> # load theta epochs state_dict = loading.load_SleepState_states(basepath) theta_epochs = nel.EpochArray(     state_dict[\"THETA\"], ) nrem_epochs = nel.EpochArray(     state_dict[\"NREMstate\"], ) theta_epochs, nrem_epochs Out[3]: <pre>(&lt;EpochArray at 0x22d5cbc65e0: 125 epochs&gt; of length 35:04 minutes,\n &lt;EpochArray at 0x22d7f5ac730: 88 epochs&gt; of length 2:16:25 hours)</pre> In\u00a0[4]: Copied! <pre># load need data (spikes, ripples, epochs)\nassembly_react.load_data()\nassembly_react\n</pre> # load need data (spikes, ripples, epochs) assembly_react.load_data() assembly_react Out[4]: <pre>&lt;AssemblyReact: 75 units&gt; of length 6:36:57:689 hours</pre> <p>Locate the session from which you want to detect assemblies.</p> <p>Here we can see a novel <code>linear</code> track is the second epoch.</p> In\u00a0[5]: Copied! <pre>assembly_react.epoch_df\n</pre> assembly_react.epoch_df Out[5]: name startTime stopTime environment behavioralParadigm notes basepath 0 preSleep_210411_064951 0.0 9544.56315 sleep NaN NaN Z:\\Data\\HMC1\\day8 1 maze_210411_095201 9544.5632 11752.80635 linear 1 novel Z:\\Data\\HMC1\\day8 2 postSleep_210411_103522 11752.8064 23817.68955 sleep NaN NaN Z:\\Data\\HMC1\\day8 In\u00a0[6]: Copied! <pre>assembly_react.get_weights(epoch=assembly_react.epochs[1] &amp; theta_epochs)\nassembly_react\n</pre> assembly_react.get_weights(epoch=assembly_react.epochs[1] &amp; theta_epochs) assembly_react Out[6]: <pre>&lt;AssemblyReact: 75 units, 15 assemblies&gt; of length 6:36:57:689 hours</pre> In\u00a0[7]: Copied! <pre>assembly_react.plot()\nplt.show()\n</pre> assembly_react.plot() plt.show() In\u00a0[8]: Copied! <pre>assembly_act = assembly_react.get_assembly_act()\nassembly_act\n</pre> assembly_act = assembly_react.get_assembly_act() assembly_act Out[8]: <pre>&lt;AnalogSignalArray at 0x22d56d63670: 15 signals&gt; for a total of 6:36:57:680 hours</pre> In\u00a0[9]: Copied! <pre>nrem_ripples = assembly_react.ripples &amp; nrem_epochs\n\npsth_swr_pre = event_triggered_average_fast(\n    assembly_act.data,\n    nrem_ripples[assembly_react.epochs[0]].starts,\n    sampling_rate=assembly_act.fs,\n    window=[-0.5, 0.5],\n    return_average=True,\n    return_pandas=True,\n)\npsth_swr_task = event_triggered_average_fast(\n    assembly_act.data,\n    assembly_react.ripples[assembly_react.epochs[1]].starts,\n    sampling_rate=assembly_act.fs,\n    window=[-0.5, 0.5],\n    return_average=True,\n    return_pandas=True,\n)\npsth_swr_post = event_triggered_average_fast(\n    assembly_act.data,\n    nrem_ripples[assembly_react.epochs[2]].starts,\n    sampling_rate=assembly_act.fs,\n    window=[-0.5, 0.5],\n    return_average=True,\n    return_pandas=True,\n)\n\n# round time index to 3 decimals for plotting\npsth_swr_pre.index = np.round(psth_swr_pre.index,3)\npsth_swr_task.index = np.round(psth_swr_task.index,3)\npsth_swr_post.index = np.round(psth_swr_post.index,3)\n</pre> nrem_ripples = assembly_react.ripples &amp; nrem_epochs  psth_swr_pre = event_triggered_average_fast(     assembly_act.data,     nrem_ripples[assembly_react.epochs[0]].starts,     sampling_rate=assembly_act.fs,     window=[-0.5, 0.5],     return_average=True,     return_pandas=True, ) psth_swr_task = event_triggered_average_fast(     assembly_act.data,     assembly_react.ripples[assembly_react.epochs[1]].starts,     sampling_rate=assembly_act.fs,     window=[-0.5, 0.5],     return_average=True,     return_pandas=True, ) psth_swr_post = event_triggered_average_fast(     assembly_act.data,     nrem_ripples[assembly_react.epochs[2]].starts,     sampling_rate=assembly_act.fs,     window=[-0.5, 0.5],     return_average=True,     return_pandas=True, )  # round time index to 3 decimals for plotting psth_swr_pre.index = np.round(psth_swr_pre.index,3) psth_swr_task.index = np.round(psth_swr_task.index,3) psth_swr_post.index = np.round(psth_swr_post.index,3) In\u00a0[10]: Copied! <pre>fig,ax = plt.subplots(2,3,figsize=(15,8),sharey=False, sharex=False)\nax = ax.flatten()\n\n# share y axis of first row\nax[0] = plt.subplot(231, sharey = ax[1])\nax[2] = plt.subplot(233, sharey = ax[0])\n\n# plot assembly ripple psth\npsth_swr_pre.plot(ax=ax[0],legend=False)\npsth_swr_post.plot(ax=ax[1],legend=False)\n(psth_swr_post - psth_swr_pre).plot(ax=ax[2])\n\n# plot mean assembly ripple psth\npsth_swr_pre.mean(axis=1).plot(ax=ax[0],color='k',legend=False)\npsth_swr_post.mean(axis=1).plot(ax=ax[1],color='k',legend=False)\n(psth_swr_post - psth_swr_pre).mean(axis=1).plot(ax=ax[2],color='k')\n\n# plot assembly ripple psth heatmap\nsns.heatmap(psth_swr_pre.T,ax=ax[3],cbar=False,vmin=0,vmax=5)\nsns.heatmap(psth_swr_post.T,ax=ax[4],cbar=False,vmin=0,vmax=5)\nsns.heatmap((psth_swr_post - psth_swr_pre).T,ax=ax[5],cbar=False,vmin=-5,vmax=5, cmap='coolwarm')\n \nfor ax_ in ax[:3]:\n    # dashed line at zero\n    ax_.axvline(0,linestyle='--',color='k',linewidth=1)\n    # set x axis limits\n    ax_.set_xlim(-0.5,0.5)\n    # add grid lines \n    ax_.grid()\n\nax[0].set_title('Pre')\nax[1].set_title('Post')\nax[2].set_title('Post - Pre')\n\n# move legend\nax[2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,frameon=False,title='assembly')\n\n# add labels\nax[0].set_ylabel('assembly activity')\nax[3].set_ylabel('assembly')\nax[3].set_xlabel('time from SWR start (s)')\n\n# clean axis using seaborn\nsns.despine()\n\nplt.show()\n</pre> fig,ax = plt.subplots(2,3,figsize=(15,8),sharey=False, sharex=False) ax = ax.flatten()  # share y axis of first row ax[0] = plt.subplot(231, sharey = ax[1]) ax[2] = plt.subplot(233, sharey = ax[0])  # plot assembly ripple psth psth_swr_pre.plot(ax=ax[0],legend=False) psth_swr_post.plot(ax=ax[1],legend=False) (psth_swr_post - psth_swr_pre).plot(ax=ax[2])  # plot mean assembly ripple psth psth_swr_pre.mean(axis=1).plot(ax=ax[0],color='k',legend=False) psth_swr_post.mean(axis=1).plot(ax=ax[1],color='k',legend=False) (psth_swr_post - psth_swr_pre).mean(axis=1).plot(ax=ax[2],color='k')  # plot assembly ripple psth heatmap sns.heatmap(psth_swr_pre.T,ax=ax[3],cbar=False,vmin=0,vmax=5) sns.heatmap(psth_swr_post.T,ax=ax[4],cbar=False,vmin=0,vmax=5) sns.heatmap((psth_swr_post - psth_swr_pre).T,ax=ax[5],cbar=False,vmin=-5,vmax=5, cmap='coolwarm')   for ax_ in ax[:3]:     # dashed line at zero     ax_.axvline(0,linestyle='--',color='k',linewidth=1)     # set x axis limits     ax_.set_xlim(-0.5,0.5)     # add grid lines      ax_.grid()  ax[0].set_title('Pre') ax[1].set_title('Post') ax[2].set_title('Post - Pre')  # move legend ax[2].legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,frameon=False,title='assembly')  # add labels ax[0].set_ylabel('assembly activity') ax[3].set_ylabel('assembly') ax[3].set_xlabel('time from SWR start (s)')  # clean axis using seaborn sns.despine()  plt.show() <pre>C:\\Users\\Cornell\\AppData\\Local\\Temp\\ipykernel_27712\\1971619650.py:5: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\nC:\\Users\\Cornell\\AppData\\Local\\Temp\\ipykernel_27712\\1971619650.py:6: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n</pre>"},{"location":"tutorials/reactivation/#reactivation","title":"Reactivation\u00b6","text":"<p>Here, we will show how to use the <code>AssemblyReact</code> class to identify assemblies and assess reactivation during post-task sleep</p>"},{"location":"tutorials/reactivation/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/reactivation/#section-1-pick-basepath-and-initialize-assemblyreact-class","title":"Section 1: Pick basepath and initialize AssemblyReact class\u00b6","text":"<p>Here we will use CA1 pyramidal cells.</p>"},{"location":"tutorials/reactivation/#section-2-load-spike-data-session-epochs-and-ripple-events","title":"Section 2: Load spike data, session epochs, and ripple events\u00b6","text":"<p>You can see there there are nice printouts that display important information about the class</p>"},{"location":"tutorials/reactivation/#section-3-detect-assembles-in-linear-track-during-theta","title":"Section 3: Detect assembles in linear track during theta\u00b6","text":"<p>You can see we have detected 15 assemblies</p>"},{"location":"tutorials/reactivation/#section-4-analyze-the-obtained-assemblies","title":"Section 4: Analyze the obtained assemblies\u00b6","text":""},{"location":"tutorials/reactivation/#section-41-visualize-assembly-weights","title":"Section 4.1: Visualize assembly weights\u00b6","text":"<p>Each column is a assembly and each row is a cell</p> <p>The color indicates if the cell was a significant contributor (members) to that assembly</p> <ul> <li>you can find these members with assembly_members = assembly_react.find_members()</li> </ul>"},{"location":"tutorials/reactivation/#section-42-compute-time-resolved-activations-for-each-assembly","title":"Section 4.2: Compute time-resolved activations for each assembly\u00b6","text":"<p>Will take around a minute to run.</p>"},{"location":"tutorials/reactivation/#section-43-get-assembly-strengths-around-ripples-in-pre-sleep-the-task-and-in-post-sleep-epochs","title":"Section 4.3: Get assembly strengths around ripples in pre-sleep, the task, and in post-sleep epochs\u00b6","text":""},{"location":"tutorials/reactivation/#section-44-visualize-reactivation-dynamics-during-post-task-ripples","title":"Section 4.4: Visualize reactivation dynamics during post-task ripples\u00b6","text":"<p>Here, we have plotted Pre, Post, and Post subtracted by Pre to estimate the difference.</p> <p>You can see that many of the assembles have a higher reactivation during the post-task ripples compared to the pre-task ripples.</p>"},{"location":"tutorials/spatial_map/","title":"Spatial Map","text":"In\u00a0[1]: Copied! <pre>%reload_ext autoreload\n%autoreload 2\n\nimport matplotlib.pyplot as plt\nimport nelpy as nel\nimport nelpy.plotting as npl\nimport numpy as np\nimport seaborn as sns\nfrom neuro_py.behavior.linear_positions import get_linear_track_lap_epochs\nfrom neuro_py.ensemble.assembly_reactivation import AssemblyReact\nfrom neuro_py.io import loading\nfrom neuro_py.plotting.figure_helpers import set_size\nfrom neuro_py.tuning import maps\nfrom skimage import measure\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport logging\n\nlogging.getLogger().setLevel(logging.ERROR)\n</pre> %reload_ext autoreload %autoreload 2  import matplotlib.pyplot as plt import nelpy as nel import nelpy.plotting as npl import numpy as np import seaborn as sns from neuro_py.behavior.linear_positions import get_linear_track_lap_epochs from neuro_py.ensemble.assembly_reactivation import AssemblyReact from neuro_py.io import loading from neuro_py.plotting.figure_helpers import set_size from neuro_py.tuning import maps from skimage import measure  %matplotlib inline %config InlineBackend.figure_format = 'retina'  import logging  logging.getLogger().setLevel(logging.ERROR)  In\u00a0[2]: Copied! <pre>basepath = r\"Z:\\Data\\Kenji\\ec013.961_974\"\n</pre> basepath = r\"Z:\\Data\\Kenji\\ec013.961_974\" In\u00a0[3]: Copied! <pre># load position\nposition_df = loading.load_animal_behavior(basepath)\n\n# put position into a nelpy position array for ease of use\npos = nel.AnalogSignalArray(\n    data=position_df[[\"x\", \"y\"]].values.T,\n    timestamps=position_df.timestamps.values,\n)\n# calculate speed\nspeed = nel.utils.ddt_asa(pos, smooth=True, sigma=0.250, norm=True)\n\n# load in spike data from hpc pyramidal cells\nst, cm = loading.load_spikes(\n    basepath, putativeCellType=\"Pyr\", brainRegion=\"CA1|CA2|CA3\"\n)\n</pre> # load position position_df = loading.load_animal_behavior(basepath)  # put position into a nelpy position array for ease of use pos = nel.AnalogSignalArray(     data=position_df[[\"x\", \"y\"]].values.T,     timestamps=position_df.timestamps.values, ) # calculate speed speed = nel.utils.ddt_asa(pos, smooth=True, sigma=0.250, norm=True)  # load in spike data from hpc pyramidal cells st, cm = loading.load_spikes(     basepath, putativeCellType=\"Pyr\", brainRegion=\"CA1|CA2|CA3\" ) In\u00a0[4]: Copied! <pre>epoch_df = loading.load_epoch(basepath)\nbeh_epochs = nel.EpochArray(np.array([epoch_df.startTime, epoch_df.stopTime]).T)\n\n# you can change this based on which session you want to look at\nbehavior_idx = 11\n\n# print out data frame\nepoch_df\n</pre> epoch_df = loading.load_epoch(basepath) beh_epochs = nel.EpochArray(np.array([epoch_df.startTime, epoch_df.stopTime]).T)  # you can change this based on which session you want to look at behavior_idx = 11  # print out data frame epoch_df Out[4]: name startTime stopTime environment behavioralParadigm manipulation stimuli notes basepath 0 ec013.961_sleep 0.0000 321.9456 sleep 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 1 ec013.962_sleep 321.9456 2475.2456 sleep 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 2 ec013.963_sleep 2475.2456 5748.3596 sleep 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 3 ec013.964_sleep 5748.3596 6035.4892 sleep 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 4 ec013.965_linear 6035.4892 7481.7872 linear 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 5 ec013.966_linear 7481.7872 9050.3872 linear 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 6 ec013.967_sleep 9050.3872 10959.9422 sleep 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 7 ec013.968_sleep 10959.9422 13312.2752 sleep 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 8 ec013.969_linear 13312.2752 15073.9652 linear 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 9 ec013.970_bigSquare 15073.9652 17064.4652 bigSquare 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 10 ec013.971_bigSquare 17064.4652 18704.5652 bigSquare 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 11 ec013.972_bigSquare 18704.5652 21714.0652 bigSquare 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 12 ec013.973_wheel 21714.0652 23200.5032 wheel 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 13 ec013.974_wheel 23200.5032 24717.6612 wheel 10 NaN NaN NaN Z:\\Data\\Kenji\\ec013.961_974 In\u00a0[5]: Copied! <pre>spatial_maps = maps.SpatialMap(\n    pos[beh_epochs[behavior_idx]],\n    st[beh_epochs[behavior_idx]],\n    s_binsize=3,\n    speed=speed,\n    tuning_curve_sigma=3,\n    place_field_min_size=15,\n    place_field_max_size=1000,\n    place_field_sigma=3,\n)\nspatial_maps.find_fields()\n\nspatial_maps\n</pre> spatial_maps = maps.SpatialMap(     pos[beh_epochs[behavior_idx]],     st[beh_epochs[behavior_idx]],     s_binsize=3,     speed=speed,     tuning_curve_sigma=3,     place_field_min_size=15,     place_field_max_size=1000,     place_field_sigma=3, ) spatial_maps.find_fields()  spatial_maps Out[5]: <pre>&lt;TuningCurve2D at 0x1831723fa00&gt; with shape (16, 64, 64)</pre> <p>You can also shuffle your data to find which cell has more spatial information than chance. Depending on how many cells you have this can take a bit of time, but it shouldn't take more than a minute with the default 500 shuffles.</p> In\u00a0[26]: Copied! <pre>spatial_info_pvalues = spatial_maps.shuffle_spatial_information()\n</pre> spatial_info_pvalues = spatial_maps.shuffle_spatial_information() <p>There are many more class methods available within <code>spatial_maps.tc</code>:</p> In\u00a0[12]: Copied! <pre>[k for k in dir(spatial_maps.tc) if not k.startswith('_')]\n</pre> [k for k in dir(spatial_maps.tc) if not k.startswith('_')] Out[12]: <pre>['bin_centers',\n 'bins',\n 'field_mask',\n 'field_peak_rate',\n 'field_width',\n 'information_rate',\n 'is2d',\n 'isempty',\n 'label',\n 'mask',\n 'max',\n 'mean',\n 'min',\n 'n_bins',\n 'n_fields',\n 'n_units',\n 'n_xbins',\n 'n_ybins',\n 'normalize',\n 'occupancy',\n 'ratemap',\n 'reorder_units_by_ids',\n 'shape',\n 'smooth',\n 'spatial_information',\n 'spatial_selectivity',\n 'spatial_sparsity',\n 'std',\n 'unit_ids',\n 'unit_labels',\n 'unit_tags',\n 'xbin_centers',\n 'xbins',\n 'ybin_centers',\n 'ybins']</pre> In\u00a0[27]: Copied! <pre># pull out velocity restricted position\ncurrent_pos = pos[beh_epochs[behavior_idx]][spatial_maps.run_epochs]\n\n# iterate over the tuning curves\nfor ratemap_i, ratemap__ in enumerate(spatial_maps.tc.ratemap):\n    ratemap_ = ratemap__.copy()\n    ratemap_[spatial_maps.tc.occupancy &lt; 0.01] = np.nan\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    sns.heatmap(ratemap_.T, ax=ax[0])\n\n    field_ids = np.unique(spatial_maps.tc.field_mask[ratemap_i])\n\n    # plot field boundaries over the heat map\n    if len(field_ids) &gt; 1:\n        for field_i in range(len(field_ids) - 1):\n            bc = measure.find_contours(\n                (spatial_maps.tc.field_mask[ratemap_i] == field_i + 1).T,\n                0,\n                fully_connected=\"low\",\n                positive_orientation=\"low\",\n            )\n            for c in bc:\n                ax[0].plot(c[:, 1], c[:, 0], linewidth=4)\n    ax[0].invert_yaxis()\n\n    # plot xy coords for animal position\n    npl.plot2d(current_pos, lw=2, c=\"0.8\", ax=ax[1])\n\n    # plot xy coords for each spike\n    x_time, pos_at_spikes = pos.asarray(\n        at=st[beh_epochs[behavior_idx]][spatial_maps.run_epochs].data[ratemap_i]\n    )\n    ax[1].plot(pos_at_spikes[0, :], pos_at_spikes[1, :], \".\", color=\"k\")\n    ax[1].set_aspect(\"equal\")\n    ax[0].set_aspect(\"equal\")\n    ax[1].set_title(f\"{cm.iloc[ratemap_i].brainRegion} {cm.iloc[ratemap_i].UID}\")\n    ax[0].set_title(\n        f\"field width, {spatial_maps.tc.field_width[ratemap_i]:.2f}. peak rate {spatial_maps.tc.field_peak_rate[ratemap_i]:.2f}.\"\n    )\n    plt.show()\n</pre> # pull out velocity restricted position current_pos = pos[beh_epochs[behavior_idx]][spatial_maps.run_epochs]  # iterate over the tuning curves for ratemap_i, ratemap__ in enumerate(spatial_maps.tc.ratemap):     ratemap_ = ratemap__.copy()     ratemap_[spatial_maps.tc.occupancy &lt; 0.01] = np.nan     fig, ax = plt.subplots(1, 2, figsize=(10, 5))     sns.heatmap(ratemap_.T, ax=ax[0])      field_ids = np.unique(spatial_maps.tc.field_mask[ratemap_i])      # plot field boundaries over the heat map     if len(field_ids) &gt; 1:         for field_i in range(len(field_ids) - 1):             bc = measure.find_contours(                 (spatial_maps.tc.field_mask[ratemap_i] == field_i + 1).T,                 0,                 fully_connected=\"low\",                 positive_orientation=\"low\",             )             for c in bc:                 ax[0].plot(c[:, 1], c[:, 0], linewidth=4)     ax[0].invert_yaxis()      # plot xy coords for animal position     npl.plot2d(current_pos, lw=2, c=\"0.8\", ax=ax[1])      # plot xy coords for each spike     x_time, pos_at_spikes = pos.asarray(         at=st[beh_epochs[behavior_idx]][spatial_maps.run_epochs].data[ratemap_i]     )     ax[1].plot(pos_at_spikes[0, :], pos_at_spikes[1, :], \".\", color=\"k\")     ax[1].set_aspect(\"equal\")     ax[0].set_aspect(\"equal\")     ax[1].set_title(f\"{cm.iloc[ratemap_i].brainRegion} {cm.iloc[ratemap_i].UID}\")     ax[0].set_title(         f\"field width, {spatial_maps.tc.field_width[ratemap_i]:.2f}. peak rate {spatial_maps.tc.field_peak_rate[ratemap_i]:.2f}.\"     )     plt.show() In\u00a0[28]: Copied! <pre># change to the first linear track session\nbehavior_idx = 4\n\n# print out data frame\nepoch_df\n</pre> # change to the first linear track session behavior_idx = 4  # print out data frame epoch_df Out[28]: name startTime stopTime environment behavioralParadigm basepath 0 ec013.961_sleep 0.0000 321.9456 sleep 10 Z:\\Data\\Kenji\\ec013.961_974 1 ec013.962_sleep 321.9456 2475.2456 sleep 10 Z:\\Data\\Kenji\\ec013.961_974 2 ec013.963_sleep 2475.2456 5748.3596 sleep 10 Z:\\Data\\Kenji\\ec013.961_974 3 ec013.964_sleep 5748.3596 6035.4892 sleep 10 Z:\\Data\\Kenji\\ec013.961_974 4 ec013.965_linear 6035.4892 7481.7872 linear 10 Z:\\Data\\Kenji\\ec013.961_974 5 ec013.966_linear 7481.7872 9050.3872 linear 10 Z:\\Data\\Kenji\\ec013.961_974 6 ec013.967_sleep 9050.3872 10959.9422 sleep 10 Z:\\Data\\Kenji\\ec013.961_974 7 ec013.968_sleep 10959.9422 13312.2752 sleep 10 Z:\\Data\\Kenji\\ec013.961_974 8 ec013.969_linear 13312.2752 15073.9652 linear 10 Z:\\Data\\Kenji\\ec013.961_974 9 ec013.970_bigSquare 15073.9652 17064.4652 bigSquare 10 Z:\\Data\\Kenji\\ec013.961_974 10 ec013.971_bigSquare 17064.4652 18704.5652 bigSquare 10 Z:\\Data\\Kenji\\ec013.961_974 11 ec013.972_bigSquare 18704.5652 21714.0652 bigSquare 10 Z:\\Data\\Kenji\\ec013.961_974 12 ec013.973_wheel 21714.0652 23200.5032 wheel 10 Z:\\Data\\Kenji\\ec013.961_974 13 ec013.974_wheel 23200.5032 24717.6612 wheel 10 Z:\\Data\\Kenji\\ec013.961_974 In\u00a0[29]: Copied! <pre>pos = nel.AnalogSignalArray(\n    data=position_df[\"linearized\"].values.T,\n    timestamps=position_df.timestamps.values,\n)\npos = pos[beh_epochs[behavior_idx]]\nspeed = nel.utils.ddt_asa(pos, smooth=True, sigma=0.250, norm=True)\npos\n</pre> pos = nel.AnalogSignalArray(     data=position_df[\"linearized\"].values.T,     timestamps=position_df.timestamps.values, ) pos = pos[beh_epochs[behavior_idx]] speed = nel.utils.ddt_asa(pos, smooth=True, sigma=0.250, norm=True) pos Out[29]: <pre>&lt;AnalogSignalArray at 0x2530053c5e0: 1 signals&gt; for a total of 24:06:298 minutes</pre> In\u00a0[30]: Copied! <pre>plt.figure(figsize=(20, 4))\nplt.plot(pos.data.T)\n</pre> plt.figure(figsize=(20, 4)) plt.plot(pos.data.T) Out[30]: <pre>[&lt;matplotlib.lines.Line2D at 0x25306ee1100&gt;]</pre> In\u00a0[31]: Copied! <pre># get outbound and inbound epochs\n(outbound_epochs, inbound_epochs) = get_linear_track_lap_epochs(\n    pos.abscissa_vals, pos.data[0], newLapThreshold=20\n)\n\n\noutbound_epochs, inbound_epochs\n</pre> # get outbound and inbound epochs (outbound_epochs, inbound_epochs) = get_linear_track_lap_epochs(     pos.abscissa_vals, pos.data[0], newLapThreshold=20 )   outbound_epochs, inbound_epochs Out[31]: <pre>(&lt;EpochArray at 0x25300357760: 19 epochs&gt; of length 9:45:574 minutes,\n &lt;EpochArray at 0x25300d924f0: 20 epochs&gt; of length 13:36:844 minutes)</pre> In\u00a0[32]: Copied! <pre>plt.figure(figsize=(20, 4))\nplt.scatter(\n    pos[outbound_epochs].abscissa_vals,\n    pos[outbound_epochs].data,\n    color=\"r\",\n    s=1,\n    label=\"outbound_epochs\",\n)\nplt.scatter(\n    pos[inbound_epochs].abscissa_vals,\n    pos[inbound_epochs].data,\n    color=\"b\",\n    s=1,\n    label=\"inbound_epochs\",\n)\nplt.legend()\n</pre> plt.figure(figsize=(20, 4)) plt.scatter(     pos[outbound_epochs].abscissa_vals,     pos[outbound_epochs].data,     color=\"r\",     s=1,     label=\"outbound_epochs\", ) plt.scatter(     pos[inbound_epochs].abscissa_vals,     pos[inbound_epochs].data,     color=\"b\",     s=1,     label=\"inbound_epochs\", ) plt.legend() Out[32]: <pre>&lt;matplotlib.legend.Legend at 0x253073d2b20&gt;</pre> In\u00a0[33]: Copied! <pre>spatial_maps = maps.SpatialMap(\n    pos[outbound_epochs],\n    st[outbound_epochs],\n    place_field_min_size=5,\n    tuning_curve_sigma=4,\n    place_field_sigma=0.1,\n)\nspatial_maps.find_fields()\n</pre> spatial_maps = maps.SpatialMap(     pos[outbound_epochs],     st[outbound_epochs],     place_field_min_size=5,     tuning_curve_sigma=4,     place_field_sigma=0.1, ) spatial_maps.find_fields() In\u00a0[34]: Copied! <pre>tc = spatial_maps.tc\n\nw, h = set_size(\"thesis\", fraction=0.3, subplots=(3, 1))\n\nwith npl.FigureManager(show=True, figsize=(w, h)) as (fig_, ax):\n    npl.utils.skip_if_no_output(fig_)\n    ax1 = npl.plot_tuning_curves1D(\n        tc.reorder_units(), normalize=True, pad=1, fill=True, alpha=0.5\n    )\n\n    leg_lines = ax1.get_lines()\n    plt.setp(leg_lines, linewidth=0.5)\n    ax.set_xlabel(\"position (cm)\")\n    ax.set_ylabel(\"n pyr cells\")\n    ax.set_xticks([0, tc.bins.max()])\n</pre> tc = spatial_maps.tc  w, h = set_size(\"thesis\", fraction=0.3, subplots=(3, 1))  with npl.FigureManager(show=True, figsize=(w, h)) as (fig_, ax):     npl.utils.skip_if_no_output(fig_)     ax1 = npl.plot_tuning_curves1D(         tc.reorder_units(), normalize=True, pad=1, fill=True, alpha=0.5     )      leg_lines = ax1.get_lines()     plt.setp(leg_lines, linewidth=0.5)     ax.set_xlabel(\"position (cm)\")     ax.set_ylabel(\"n pyr cells\")     ax.set_xticks([0, tc.bins.max()]) In\u00a0[35]: Copied! <pre>for ratemap_i, ratemap__ in enumerate(spatial_maps.tc.ratemap):\n    ratemap_ = ratemap__.copy()\n    x = np.arange(len(ratemap_)) * spatial_maps.s_binsize\n    plt.figure(figsize=(5, 2))\n    plt.plot(x, ratemap_, color=\"k\")\n    # field_ids = np.unique(spatial_maps.tc.field_mask[ratemap_i])\n    plt.plot(x, spatial_maps.tc.field_mask[ratemap_i], color=\"r\")\n    plt.title(\n        f\"field width, {spatial_maps.tc.field_width[ratemap_i]:.2f}. peak rate {spatial_maps.tc.field_peak_rate[ratemap_i]:.2f}.\"\n    )\n    plt.show()\n</pre> for ratemap_i, ratemap__ in enumerate(spatial_maps.tc.ratemap):     ratemap_ = ratemap__.copy()     x = np.arange(len(ratemap_)) * spatial_maps.s_binsize     plt.figure(figsize=(5, 2))     plt.plot(x, ratemap_, color=\"k\")     # field_ids = np.unique(spatial_maps.tc.field_mask[ratemap_i])     plt.plot(x, spatial_maps.tc.field_mask[ratemap_i], color=\"r\")     plt.title(         f\"field width, {spatial_maps.tc.field_width[ratemap_i]:.2f}. peak rate {spatial_maps.tc.field_peak_rate[ratemap_i]:.2f}.\"     )     plt.show() In\u00a0[36]: Copied! <pre># pull out velocity restricted position\ncurrent_pos = pos[beh_epochs[behavior_idx]][spatial_maps.run_epochs]\n\n# iterate over the tuning curves\nfor ratemap_i, ratemap__ in enumerate(spatial_maps.tc.ratemap):\n    ratemap_ = ratemap__.copy()\n    ratemap_[spatial_maps.tc.occupancy &lt; 0.01] = np.nan\n    fig, ax = plt.subplots(\n        2, 1, figsize=(5, 3), sharex=True, gridspec_kw={\"height_ratios\": [1, 3]}\n    )\n\n    # plot tuning curve\n    x = (spatial_maps.x_edges + np.abs(np.diff(spatial_maps.x_edges)[0] / 2))[:-1]\n    ax[0].plot(x, ratemap_, color=\"k\")\n\n    # plot xy coords for animal position\n    ax[1].plot(current_pos.data.T, current_pos.abscissa_vals, \"grey\")\n\n    # plot xy coords for each spike\n    x_time, pos_at_spikes = pos.asarray(\n        at=st[beh_epochs[behavior_idx]][spatial_maps.run_epochs].data[ratemap_i]\n    )\n    ax[1].plot(pos_at_spikes.flatten(), x_time, \".\", color=\"k\")\n\n    ax[1].set_xlim(x.min(), x.max())\n\n    # add figure labels\n    ax[0].set_ylabel(\"rate (Hz)\")\n    ax[1].set_ylabel(\"time (sec)\")\n    ax[1].set_xlabel(\"position (cm)\")\n\n    ax[0].set_title(\n        f\"field width, {spatial_maps.tc.field_width[ratemap_i]:.2f}. peak rate {spatial_maps.tc.field_peak_rate[ratemap_i]:.2f} {cm.iloc[ratemap_i].brainRegion} {cm.iloc[ratemap_i].UID}\"\n    )\n    sns.despine()\n    plt.show()\n</pre> # pull out velocity restricted position current_pos = pos[beh_epochs[behavior_idx]][spatial_maps.run_epochs]  # iterate over the tuning curves for ratemap_i, ratemap__ in enumerate(spatial_maps.tc.ratemap):     ratemap_ = ratemap__.copy()     ratemap_[spatial_maps.tc.occupancy &lt; 0.01] = np.nan     fig, ax = plt.subplots(         2, 1, figsize=(5, 3), sharex=True, gridspec_kw={\"height_ratios\": [1, 3]}     )      # plot tuning curve     x = (spatial_maps.x_edges + np.abs(np.diff(spatial_maps.x_edges)[0] / 2))[:-1]     ax[0].plot(x, ratemap_, color=\"k\")      # plot xy coords for animal position     ax[1].plot(current_pos.data.T, current_pos.abscissa_vals, \"grey\")      # plot xy coords for each spike     x_time, pos_at_spikes = pos.asarray(         at=st[beh_epochs[behavior_idx]][spatial_maps.run_epochs].data[ratemap_i]     )     ax[1].plot(pos_at_spikes.flatten(), x_time, \".\", color=\"k\")      ax[1].set_xlim(x.min(), x.max())      # add figure labels     ax[0].set_ylabel(\"rate (Hz)\")     ax[1].set_ylabel(\"time (sec)\")     ax[1].set_xlabel(\"position (cm)\")      ax[0].set_title(         f\"field width, {spatial_maps.tc.field_width[ratemap_i]:.2f}. peak rate {spatial_maps.tc.field_peak_rate[ratemap_i]:.2f} {cm.iloc[ratemap_i].brainRegion} {cm.iloc[ratemap_i].UID}\"     )     sns.despine()     plt.show() In\u00a0[38]: Copied! <pre>basepath = r\"Z:\\Data\\HMC1\\day8\"\n\n# load position\nposition_df = loading.load_animal_behavior(basepath)\n\n# put position into a nelpy position array for ease of use\npos = nel.AnalogSignalArray(\n    data=position_df[\"linearized\"].values.T,\n    timestamps=position_df.timestamps.values,\n)\n\nbehavior_idx = 1\n\n# load epoch data\nepoch_df = loading.load_epoch(basepath)\nbeh_epochs = nel.EpochArray(np.array([epoch_df.startTime, epoch_df.stopTime]).T)\n\npos = pos[beh_epochs[behavior_idx]]\n\n# get outbound and inbound epochs\noutbound_epochs, inbound_epochs = get_linear_track_lap_epochs(\n    pos.abscissa_vals, pos.data[0], newLapThreshold=20\n)\n\n# calculate speed\nspeed = nel.utils.ddt_asa(pos, smooth=True, sigma=0.250, norm=True)\nrun_epochs = nel.utils.get_run_epochs(speed, v1=4, v2=4).merge()\n\n\nassembly_react = AssemblyReact(\n    basepath=basepath,\n    brainRegion=\"CA1\",\n    putativeCellType=\"Pyr\",\n    z_mat_dt=0.01,\n)\n\n# load need data (spikes, ripples, epochs)\nassembly_react.load_data()\n\n# load theta epochs\nstate_dict = loading.load_SleepState_states(basepath)\ntheta_epochs = nel.EpochArray(\n    state_dict[\"THETA\"],\n)\n\nassembly_react.get_weights(epoch=beh_epochs[behavior_idx] &amp; theta_epochs)\nassembly_react\n</pre> basepath = r\"Z:\\Data\\HMC1\\day8\"  # load position position_df = loading.load_animal_behavior(basepath)  # put position into a nelpy position array for ease of use pos = nel.AnalogSignalArray(     data=position_df[\"linearized\"].values.T,     timestamps=position_df.timestamps.values, )  behavior_idx = 1  # load epoch data epoch_df = loading.load_epoch(basepath) beh_epochs = nel.EpochArray(np.array([epoch_df.startTime, epoch_df.stopTime]).T)  pos = pos[beh_epochs[behavior_idx]]  # get outbound and inbound epochs outbound_epochs, inbound_epochs = get_linear_track_lap_epochs(     pos.abscissa_vals, pos.data[0], newLapThreshold=20 )  # calculate speed speed = nel.utils.ddt_asa(pos, smooth=True, sigma=0.250, norm=True) run_epochs = nel.utils.get_run_epochs(speed, v1=4, v2=4).merge()   assembly_react = AssemblyReact(     basepath=basepath,     brainRegion=\"CA1\",     putativeCellType=\"Pyr\",     z_mat_dt=0.01, )  # load need data (spikes, ripples, epochs) assembly_react.load_data()  # load theta epochs state_dict = loading.load_SleepState_states(basepath) theta_epochs = nel.EpochArray(     state_dict[\"THETA\"], )  assembly_react.get_weights(epoch=beh_epochs[behavior_idx] &amp; theta_epochs) assembly_react Out[38]: <pre>&lt;AssemblyReact: 75 units, 15 assemblies&gt; of length 6:36:57:689 hours</pre> In\u00a0[39]: Copied! <pre>assembly_react.plot()\nplt.show()\n</pre> assembly_react.plot() plt.show() In\u00a0[40]: Copied! <pre>assembly_act = assembly_react.get_assembly_act(epoch=beh_epochs[behavior_idx])\nassembly_act\n</pre> assembly_act = assembly_react.get_assembly_act(epoch=beh_epochs[behavior_idx]) assembly_act Out[40]: <pre>&lt;AnalogSignalArray at 0x25300cf4af0: 15 signals&gt; for a total of 36:48:240 minutes</pre> In\u00a0[45]: Copied! <pre>spatial_map_outbound_assembly = maps.SpatialMap(\n    pos[outbound_epochs],\n    assembly_act[outbound_epochs],\n    speed=speed,\n    tuning_curve_sigma=6,\n    min_duration=0,\n)\n\nspatial_map_inbound_assembly = maps.SpatialMap(\n    pos[inbound_epochs],\n    assembly_act[inbound_epochs],\n    speed=speed,\n    tuning_curve_sigma=6,\n    min_duration=0,\n)\n</pre> spatial_map_outbound_assembly = maps.SpatialMap(     pos[outbound_epochs],     assembly_act[outbound_epochs],     speed=speed,     tuning_curve_sigma=6,     min_duration=0, )  spatial_map_inbound_assembly = maps.SpatialMap(     pos[inbound_epochs],     assembly_act[inbound_epochs],     speed=speed,     tuning_curve_sigma=6,     min_duration=0, ) In\u00a0[46]: Copied! <pre>for tc in [spatial_map_outbound_assembly.tc, spatial_map_inbound_assembly.tc]:\n    w, h = set_size(\"thesis\", fraction=0.3, subplots=(3, 1))\n\n    with npl.FigureManager(show=True, figsize=(w, h)) as (fig_, ax):\n        npl.utils.skip_if_no_output(fig_)\n        ax1 = npl.plot_tuning_curves1D(\n            tc.reorder_units(), normalize=True, pad=1, fill=True, alpha=0.5\n        )\n\n        leg_lines = ax1.get_lines()\n        plt.setp(leg_lines, linewidth=0.5)\n        ax.set_xlabel(\"position (cm)\")\n        ax.set_ylabel(\"n assemblies\")\n        ax.set_xticks([0, tc.bins.max()])\n</pre> for tc in [spatial_map_outbound_assembly.tc, spatial_map_inbound_assembly.tc]:     w, h = set_size(\"thesis\", fraction=0.3, subplots=(3, 1))      with npl.FigureManager(show=True, figsize=(w, h)) as (fig_, ax):         npl.utils.skip_if_no_output(fig_)         ax1 = npl.plot_tuning_curves1D(             tc.reorder_units(), normalize=True, pad=1, fill=True, alpha=0.5         )          leg_lines = ax1.get_lines()         plt.setp(leg_lines, linewidth=0.5)         ax.set_xlabel(\"position (cm)\")         ax.set_ylabel(\"n assemblies\")         ax.set_xticks([0, tc.bins.max()])"},{"location":"tutorials/spatial_map/#spatial-map","title":"Spatial Map\u00b6","text":"<p>This short notebook will guide you over loading data and using <code>SpatialMap</code> class.</p> <p>We first go over 2D maps and then have 1D maps, and finally mapping continuous variables.</p>"},{"location":"tutorials/spatial_map/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/spatial_map/#section-1-specify-basepath-to-your-session-of-interest","title":"Section 1: Specify basepath to your session of interest\u00b6","text":""},{"location":"tutorials/spatial_map/#section-2-load-in-behavior-and-spike-data","title":"Section 2: load in behavior and spike data\u00b6","text":""},{"location":"tutorials/spatial_map/#section-3-load-in-epochs-and-make-a-note-of-which-session-you-want-to-analyze","title":"Section 3: Load in epochs and make a note of which session you want to analyze\u00b6","text":"<p>Here, I'm first interested in the last bigSquare session</p>"},{"location":"tutorials/spatial_map/#section-4-make-tuning-curves-and-detect-2d-place-fields-using-the-spatialmap-class","title":"Section 4: Make tuning curves and detect 2D place fields using the SpatialMap class\u00b6","text":"<p>You can see I am specifying positions and spikes within our epoch of choice</p> <p>There are many other parameters you can change, so check out the documentation of the class</p>"},{"location":"tutorials/spatial_map/#section-5-inspect-the-results-by-looking-at-the-tuning-curves-and-spikes-on-path-plots","title":"Section 5: Inspect the results by looking at the tuning curves and spikes on path plots\u00b6","text":""},{"location":"tutorials/spatial_map/#section-6-now-lets-make-1d-maps-using-the-same-class","title":"Section 6: Now lets make 1D maps using the same class\u00b6","text":""},{"location":"tutorials/spatial_map/#section-61-reconstruct-the-position-array-with-linearized-coords","title":"Section 6.1: Reconstruct the position array with linearized coords\u00b6","text":"<p>Note, you can see if you print out <code>nelpy</code> arrays, there is very helpful information.</p>"},{"location":"tutorials/spatial_map/#section-62-visualize-the-1d-position","title":"Section 6.2: Visualize the 1D position\u00b6","text":""},{"location":"tutorials/spatial_map/#section-62-get-outbound-and-inbound-epochs","title":"Section 6.2: Get outbound and inbound epochs\u00b6","text":""},{"location":"tutorials/spatial_map/#section-63-make-tuning-curves-for-the-outbound-trajectories","title":"Section 6.3: Make tuning curves for the outbound trajectories\u00b6","text":""},{"location":"tutorials/spatial_map/#section-64-visualize-the-tuning-curves","title":"Section 6.4: Visualize the tuning curves\u00b6","text":""},{"location":"tutorials/spatial_map/#section-65-inspect-the-field-detection","title":"Section 6.5: Inspect the field detection\u00b6","text":""},{"location":"tutorials/spatial_map/#section-7-map-continuous-behavioral-or-neurophysiological-variables","title":"Section 7: Map continuous behavioral or neurophysiological variables\u00b6","text":"<ul> <li>You can map any continuous variable (speed, calcium signals, theta phase, etc.), but here we will use assembly activity</li> </ul>"},{"location":"tutorials/spatial_map/#section-71-visualize-assembly-weights","title":"Section 7.1: Visualize assembly weights\u00b6","text":""},{"location":"tutorials/spatial_map/#section-72-compute-time-resolved-activations-for-each-assembly","title":"Section 7.2: Compute time-resolved activations for each assembly\u00b6","text":"<p>These are continous signals that we can now map using the SpatialMap class</p>"},{"location":"tutorials/spatial_map/#section-73-calculate-spatial-maps-for-each-assembly-for-both-directions","title":"Section 7.3: Calculate spatial maps for each assembly for both directions\u00b6","text":""},{"location":"tutorials/spatial_map/#section-74-visualize-spatial-maps-for-each-assembly","title":"Section 7.4: Visualize spatial maps for each assembly\u00b6","text":""}]}